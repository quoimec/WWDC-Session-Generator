JONO WELLS: Good morning. Welcome to using Safari to deliver and debug, a responsive web design. My name is Jono Wells. I am an engineer on the Safari and WebKit team with Apple, and I'm really excited to talk to you today. I have a great privilege of being able to develop for the Development Tools that are a part of the Safari web browser. I feel I'm privileged because I love the web, as I'm sure you do. I think Safari is a great browser, a really great platform. It deserves world-class Development Tools, wouldn't you agree? Especially because we are not just making content in the web browser, we're making content in our apps as well, using web views and so we want that content to run great and look great. So this morning, of course we're going to be talking about Web Inspector, a tool most of you have probably used before, hopefully. We're also going to be talking about a brand-new tool called Responsive Design Mode. With these two tools together we're going to very rapidly and very smartly improve the quality of our site and get a good solid responsive design as a result. Let's begin by talking about Web Inspector. This is Web Inspector, of course. This is the one you've been working with for the last year. We've come up with a lot of great new features. We've also gotten some great feedback from you over the last year about the interface. We took a lot of that to heart and we changed things around a little bit so this is the Web Inspector that we're releasing with this release of Safari. Here it is. JONO WELLS: I suspect some of you may be applauding because there is a dedicated elements tab as you see up there. We have a tab based interface, all the tools that you would expect in Web Inspector are here; resources, timeline, the debugger, the storage tab and a console tab. But these are tabs so of course you can get rid of ones that you want to get out of your way if there are tools you don't work with as often you can just move them out of the way, you can reorganize the ones that are already there, if you want to bring one back, you just hit the plus button on the right-hand side and open up whichever one you want to. We give you the ability to customize Web Inspector to look more like what you want, to work the way you do. We've made improvements to the interface there. Now let's talk about what new things we can do with Web Inspector and how it can help us. The first one is, it can help us get great performance out of our content. It helps you get great performance. Now it almost dragoons you to get great performance and we'll talk about how. What do I mean when I say great performance? The gold standard as I'm sure most of you know, is 60 frames per second, because our screens are refreshing at 60 Hertz, 60 times a second. This means that if we are trying to render a smooth animation, right, for any given rendering frame, right, in a turn of the event loop we need to make sure all of our scripts, our timers, and our layout and rendering can happen within 16.67 milliseconds. If we blow this budget, then our animation will stutter and that's not something that we want. How can we get better information to help us debug the performance of our content? We have this idea. What if you could just look at the content and every time a paint occurs, we just flash a region of the screen? We added something called Paint Flashing. When you're in the elements tab, in the upper right-hand corner, you will see this paintbrush. Click that, turn that on. When that is lit up blue, every single time a paint occurs on your page it will light up red with a translucent red box. You're going to want to try this on your content. It going to light up like a Christmas tree. It is amazing! It's going to look like a game of Simon if you ever played that with a kid -- as a kid. That's going to give you a sense that perhaps you're painting too often, there's too much happening on your page. But what about what's happening behind the scenes? To help you get more information there and get you towards 60 frames per second performance, we've added a great new tool called the Rendering Frames Time Line. If you go to the time lines tab you can get to it by clicking right here on Rendering Frames and this is the interface you will see. When -- so you can record, use your site and we will start drawing these bars. Each one of these bars represents a rendering frame. They're segmented into different colors based on what is happening in each frame. Purple for scripts, event timers and such, red for layout and rendering and gray for behind the scenes engine work that the engine is doing. The goal is to keep each of these bars underneath the middle line right here, the 60 frames per second line. If your bars tend to go above that, it means you're possibly not going to be getting 60 frames per second performance and we want to work on that. You can select a region of the bars just like you can on the other timeline, and we're going to show you a lot of good information about that. First of all, we're going to show you this graph right here, this is an aggregate of the different activity that is happening in that particular region. We're going to give you a breakdown of each frame. You can open up every single rendering frame and see exactly what's happening. For things like event timers and rendering, style and validation inside of your code we have the ability, of course, to click to the relevant line of source codes so you can see what's going on. We also show you this table of timing information. One of my favorite things to do, I have been using this for a bit, what's been really helpful for me is to sort by total time for reach Rendering Frame and then I open up the one on the top because it is most likely that the most offensive frames are at the top and then I can see what is holding back my performance. This tool is going to tell you a great story about your content. Even if you think you're performing well, you're going to want to hookup to this and see if you can get additional performance out of your site. It tells you a great story, it allows you to do without thinking, that's really what we want our development tools to do. Just to put the information out in front of you and say here, here it is, you don't have to work too hard to get it. One of the things that we've been noticing is that you have to jump around between different tools within Web Inspector a lot. We thought it'd be great if you could focus on a particular one. More often if you can just stay in one tool that might save you time. We've done this in a few ways. One of the ways is for errors and warnings we don't just show those in the console now, we show them in a few other places so that you can spot them more quickly. In the debugger side-bar for instance on the left, any resource that has an error warning we'll just show them underneath, they'll appear so you can just jump right to them. In all of the JavaScript code in your site, we'll show the errors and warnings in line so you'll see those messages. So if you're just browsing, everything will just show up. It's a lot nicer. We have also improved the way they look in the console so errors and warnings, they're highlighted, the colors are easily distinguishable. They just look better too, we've made the fonts better, it's great. We have this other idea, and again we want to Do Without Thinking, we want to put important information in front of you. JavaScript of course, is a dynamically typed language. That means that the engine itself assigns the types for you, you don't have to. This is what's happening behind the scenes in this case. Right. We have a string, we have an integer, we have an object. If you're working with some code, here's a function that's designed to add two numbers together, and this is a mistake that might happen. We're accidentally passing in a string, and so that is going to lead to some unexpected results we're going to call a dysfunction. This is something that you can discover by trying to debug, and play around with your code, might be a, in a more complicated case, might be hard to find this error. But, what if you could see something like this where we just show you as your code is running, we show you the types that have been assigned to all the different variables, passed in as parameters, or return from functions. We created just such a thing. We call this the Type Profiling Tool. If you look in the upper right-hand corner when you're looking at any JavaScript source you'll see this T button right here. If you click that, from that point forward, when your code runs, your code is going to go from looking like this to looking like this. JONO WELLS: I love this tool! It's so cool! I've been using it a lot working on Web Inspector. We're going to show you as best as we can type information as your variables are assigned values. You'll also notice that some of the code is grayed out. Since we're tracking what code has been covered, any code that hasn't been reached yet will be grayed out for you. So this also serves as a great code coverage tool. Right. It going to allow you to see if a function isn't being run or if an else condition isn't being reached, things like that. You'll also notice that we show you the return value from a function. In this case I have a function that's returning something called a DeveloperPerson, it's a class I wrote. You'll also notice that we're showing you the name of that object. If you have constructer functions, right, like this one, if you're using this classical model using prototypes and you construct an object that way, we will show you the name of the constructer function. We'll do the same thing of course, if you're using ES6 classes which just landed in this release, we'll show you the name of the class. Obviously parameters can get past multiple values over the lifetime of your application. What happens if you have multiple types? For objects here is how we handle it. Let's say I have a function called register. Right. So we're going to register somebody for a conference or something like that. Say I call it once, and I pass in a new DeveloperPerson. Well, we're just going to show you that it's a DeveloperPerson. If we pass in another object, say a ManagerPerson, we'll show you that. Let's say we pass in both a DeveloperPerson and a ManagerPerson, then since they both have a common ancestor, person, that's what we're going to show you. If we pass in something else that isn't in this inheritance chain at all like an extraterrestrial for instance, we'll just show you object, because of course everything derives from object, right? Now here are all the different types that we might show you. If there are multiple types, a string, and a number that get assigned to a value, this is what you're going to see in your code, it's just going to show you many. There is an exception to this, say you have a simple type like a double and it's a double sometimes and it's null or undefined sometimes, we'll just add a question mark to the top, that means it's double sometimes or sometimes it's a null value. This is such a nice feature, I just want to show you a few more shots. This is code from the Web Inspector. It's just really nice. It gives you a bird's eye view of what's happening with your application. You can quickly spot behavior that isn't interesting to you. Right. Here is an object assignment. Again, you can see the coverage, you can see areas that are grayed out. This has been incredibly useful in my development with Web Inspector, I think you're going to find it just as useful as well. This is the type profiling tool. JONO WELLS: All right. We have also made improvements to the console. In a similar way we want to be able to put really useful information in front of you without you having to work too hard. Here are some objects that we have launched in the console. And you're going to notice a few things. The first one is that we're creating -- we give you object preview, so you have really good looking object previews so you might spot information inside your object much more quickly. You're also going to notice something else, every time something is output to the console, we're going to store it in a variable for you. This starts at dollar sign 1 and then dollar sign 2, then dollar sign 3, on and on and on. You can reference them again just by putting them in the console. This is great. The feedback that we got from you, because these used to represent recently selected elements in the elements panel and the feedback was that really everyone was only using one of those, which is dollar sign 0, so don't worry, that's still the currently selected element, still there, it'll still work as you expect it to, but starting with dollar sign one, those'll be recently logged variables. That can be really useful, right, if you're paused in the debugger and there's a value in the scope that you want to remember, you no longer have to go into your source code and create a variable just for debugging purposes and then save it, and then run the app, then delete it later, or forget. You don't have to do any of that anymore, we'll take care of that for you. What about when you open up one of the objects. You're going to see this. It's a lot nicer. First of all, we've just improved the look and used colors to distinguish the different types of information that can be inside of an object. Everyone of the objects inside has its own preview. You'll notice that this is an array, siblings, this one has a special type of preview, we've created special previews for certain things, Maps, sets, weak sets, weak maps, promises. Right. We show you the types of the objects inside the array and the size of the array on the right. When you open it, we show you the index of each object inside. Of course, each of those have a preview. I want you to notice here that we also have made it a lot easier to inspect the prototype chain so I can then open up the prototype for developer person and for person, and I might see something like this. I love this very much. This gives me the ability to essentially get an API view of my object in the classes that constructed this object. Look at this. I can quickly spot and see okay, I have a function called change name that takes in a first name and last name. I have a custom setter called firstName. I have a custom getter called fullName. I can tell that because there's an eyeball next to it. I can click on that eyeball and it'll actually run the custom getter. This is almost better than the documentation in some cases. It is even great for native API exploration, and some of you are probably thinking, I can't remember how local storage works, no problem, just put it in the console. I can see all of the functions, it looks a lot better, it's really nice. Really great for object exploration, if you're exploring third-party APIs, it's great. It's not just in the console, either. Anywhere we show object views, for instance, if you're paused in the debugger and you have the view of the variables in scope on the right, or when you're mousing over, a variable in the source view, let me show you that - same beautiful object views in all of those places. Nicely organized for the type of information that is in there, you're going to spot information more quickly and that is really the goal of all of this. I now would like to invite my friend and colleague Andy Estes to come up to the stage and he's been working on a site that he's excited to show you and maybe some of these tools will help him out. Andy. ANDY ESTES: Thank you, Jono and thank you to all of you for being here this morning. I'm really excited to be here and show you these new tools in Web Inspector. First, a bit of a confession, I was up late last night working on this site, I wanted it to look good. It's supposed to display a bunch of photography I've taken and I think I've introduced a bug and if you don't mind, I just want to see if I can figure out this bug I introduced and fix it real quick so I'm just going to go into Web Inspector. By the way, if you never opened the inspector before, you do it from the develop menu and you don't have a develop menu in your Safari, it's really easy to enable, you simply go to Safari's preferences, and in the advanced tab you'll see the option, show development menu in mini bar, I already have that checked so I'll just go right in there. I'm going to say Develop, show Web Inspector. It will open this up along the bottom and right away I see that I have an error here in the console. I can click it, it looks like undefined is not an object, I can jump to the debugger, I even have the error here in the side-bar, I can click it, it'll take me right to the line of code. Let's take a look at this. Somewhere in this line of code I'm seeing an undefined value where I don't expect it. Now typically what I would do is I would set a break point and maybe spend some time debugging but I don't have time for that, instead I think I'm just going to enable the type profiling to see if maybe that can tell me where my error is. I'm going to click this T to enable type profiling and I'm going to reload the site. Great, so there's my error. And now my source code is annotated with these helpful pills that tell me the types of all my variables. Let's just see if we can quickly work this out. I have galleries, which is an array. I have current gallery which is an imager. The result of that shouldn't be undefined but I initialized galleries up here, and one of the things I'm noticing is that the body of my four loop is grayed out, that means that code hasn't been covered, and it looks like that's because gallery data itself is undefined. That's somewhat unexpected, I should be passing a valid gallery data in here. Gallery data itself comes from this variable, this.options, let's take a look at where I set that. I do that up in my constructer for navigation galleries controller. And it does look like I'm passing a valid object to the constructer, I see the object pill right there. I'm not passing a null or some other kind of type. Let's find where I construct navigation galleries controller and see if that tells me what my mistake is. I'm going to just copy this. I'm going to use Web Inspectors' global search and I can get to that by command-shift-F and it will search all of the resources in the site. And I have exactly one place where I have a navigation gallery controller, it's right here and I immediately spot my mistake. I was expecting to pass an object that had gallery data as a property, instead I'm just passing gallery data directly. Let's fix that real quick. I can use the convenient ES6 shorthand syntax for declaring a new object that's going to have both the property name as gallery data and also the value as gallery data's value. I made that change. When I'm editing local files in Web Inspector I can save those changes directly and Web Inspector will prompt me just to make sure I really want to over ride a local file and of course, I do in this case. Once you do that once for a file, it'll never ask you again, it'll just save directly. Let me reload, and see if that worked. There we go, so now I have my beautiful photography that I totally took myself. This is looking really great. You can see that I have these galleries, I've laid them out so they take up the full width of the display, I can hover over them to see the name of the galleries and I also have this logo in the middle I can hover over that to see a menu that comes out to the side and you'll notice that occasionally these galleries rotate between photos and I have a nice dissolve effect to go from one to the other. I want to make sure that this transition is as smooth and as efficient as possible so hopefully some of these tools that Jono told us about will help me do that. The first thing I want to do is I want to turn on paint flashing. I'm going to go to the elements tab and the paintbrush tab is here, I'll select that. Now you can see the red rectangles as the images transition from one image to the next. The first thing I'm noticing is that for the duration of that transition, I see a solid red rectangle, that indicates to me that I'm probably painting every single frame of the transition. Now, I wonder if that's too expensive? What I'd like to use is use the I want to use the frames timeline to tell me whether I'm doing too much work and maybe blowing my budget for a 60 frames per second animation. I can switch to the timelines tab. And I can see that when I reloaded it automatically recorded a timeline for me so I don't need to do that again. For the most part, you know, this is looking really good. It's looking like I'm staying within 60 frames per second. But there were some frames here that exceeded my budget like this one right here. I can click and drag to select a subset of this and then you can see that it filters the records tab to just include that subset. I can expand some of these and see the work that was done, style and validation, recalculation, time refiring, things like that. Here's one that's interesting. It's telling me that an animation frame is fired based on request animation frame, it's even telling me that I've invalidated a style and it's going to let me jump right to that line that did that style invalidation. That's right here. In fact, the way this animation is working is I'm using request animation frame and each frame I'm running with a duration of one second, I can even pause here and inspect these values. Running for a duration of one second and I'm just animating the opacity value from zero to one over that duration of one second. It looks like, at least in some cases, this animation is too expensive to stay within my 60 frames per second budget. I've actually been planning to transition this to a CSS transition instead which I think might be more efficient. Let's do that and see if things get a little better. I'll select this. I still want to set this current class on the image and I'll talk about that later. I want to comment out the rest of this. I can save that local resource. I'll confirm that. Let's go to the elements tab and I can use the picker to select one of these galleries. I'll choose this one. Down here we can see that there are three images that are stacked on top of each other. The one with the current class right here, is the one that's currently being displayed. In the side-bar I can actually see what style properties that current class has. Right now it is Z index 3. Let's add some more properties here. For one I want to animate its opacity -- or I want to set its opacity to one then I want to use a transition to animate opacity over one second using a linear animation curve. I'm going to make those changes. I'm going to save again and I'm going to reload the page. Great. Right away you can see that there is almost no flashing going on here. There is an occasional flash in the beginning and the end, but for the rest of the time there is no flashing whatsoever. That indicates I've gone from doing software compositing in my request animation frame code to letting the engine do -- to composite the opacity and hardware which is going to be a much faster, smoother experience. This is looking pretty good but I'm actually seeing a problem. I did see one frame animate but then none of the others did. It looks like I might have introduced some sort of bug. Let's see if we can figure out what that is. I happen to know that in my code I am -- that there's show next image right here. Show next image returns a promise -- let me reload so we can actually hit my break point. Show next image is going to return a promise, when the promise is resolved I'm start a timeout for the next animation. We can even log the promise here, show next image and it is going to tell me that the state is pending. This may not be surprising that it is pending because I'm pausing the debugger, I might not have let the code that will eventually resolve that promise run, so let me resume here. These start animating every 2 seconds, and so by now this promise should be resolved if everything is working correctly. Even though I'm no longer pausing that function, I still have that promise bound to dollar sign 1 so I can just inspect it again. I can see that it's still pending. I actually think I know the mistake I made. I'm going to go back to my animation frame and I can see here that I was resolving the promise at the end of the request animation frame. The last animation frame but I'm not doing that with the CSS transition. Let me just fix that real quick. I'll use an on transition end listener and resolve that. Let me save that change and reload. And of course I hit my brake point again. So let me reload again. Great. Now we see these smooth hardware accelerated transitions, everything seems to be looking great. We can see how we can use these powerful features like type profiling, paint flashing, Rendering Frames timeline, improved console, improved debugging to quickly identify our issues and fix them. Thank you, and back to Jono. JONO WELLS: Thank you, Andy. So I think you're going to find really great use out of this tool. You're going to want to take the new features in Web Inspector, make your site faster, use the paint flashing tool, use the rendering frames timeline and try this in iOS, this works of course, if you're connected to a web view, or Safari on iOS, you should really try this out. Performance is particularly critical on those devices. Make your code better. Use the type profiler to get a lot of great information, play with the console, play around with native APIs. Right. Use these to work smarter and to work faster. That's what we have introduced with Web Inspector. Now let's go ahead and talk about a new tool. It is called Responsive Design Mode. It is new in this release of Safari 9 and we'll show you what it is in just a moment. First I want to talk about responsive design for a moment and why it is particularly important now. I'm sure most of you know what responsive design is, it is making sure that your content will automatically fit and give a great experience for any screen size. It is particularly important that you do this correctly. Now, up until this point some of you may have said, well, I can just, you know, sniff the user agent string, I can figure out what kind of device I'm on. I know that I'm running a web view inside of an app, I know exactly what devices I'm targeting. So I don't really need to do responsive design the way it was meant to be. We've made that a bit trickier for you now because of multitasking. Now web views might be resizing, you don't exactly know how wide they're going to be and they're going to be resizing without reloading your content. Right. We want to instead of targeting a device, we want to make sure we're using features like @supports, media queries, source set, serving different pixel resolution images to your users, we want to make sure that our content will stretch to any screen. To help you do that without having to go, and open up the iOS simulator and play around, we have brought to you a new tool that going to help you test your layouts and just make sure that they're properly responsive and give you some ways to test them in the sizes for our devices as well. That's Responsive Design Mode. You can find it in the develop menu and when you turn it on, your content will shrink to this view. This is Responsive Design Mode. It has a few interesting features and the first one, we have handles on the side so you can stretch your content as small or as big as you want to. It will actually automatically scale so if you want to simulate your content on a giant 70-inch display let's say, you can do that. We also built in navigation with presets for all of our devices so that you can automatically set it to different screens. We actually have built in all the multitasking configurations in here as well, as you will see in a moment. For the iPads that are up there, you can click, it will show you, it'll automatically resize your content to those configurations We also allow you to test to make sure you're covering 1X, 2X, and 3X images. Additionally, we do give you a tool to set the user agent string because you might need to support a very old browser, legacy browser and make sure you get away from using user agent strings to target the different screens that your concept will run on. Right. To be forward thinking. This tool will help you make sure that you don't break anything when the user agent string changes and make sure that you have properly gotten away from that as well. Now to show you this tool in action and how it can be useful, I would like to bring Andy back up to the stage to take his site a little bit further. Andy. ANDY ESTES: Thank you, again, Jono. Let's just jump right in to Responsive Design Mode. Here we are. I currently have a preset set for the iPad mini 3. Let's also explore some of these desktop sizes. One of the great things about this is I can actually size it to a resolution larger than my monitor. Right now you can see that I have a width of 2536 pixels which is definitely larger than this monitor. So that's useful, but I already knew that my site looked good on desktop Viewport sizes. I want to make sure that everything works well on iOS devices as well, iPhones and iPads, I can click these and quickly cycle through, here is an iPad error in landscape mode. Here are some of the multitasking modes. If I keep going it'll rotate to portrait and I can also look at the multitasking modes there. I'm starting to notice that in some of these configurations my site -- the Viewport is really too narrow to be useful. For example, I really can't see much of these photos if I rollover this menu, the text is clipped. This isn't a really good experience. Let's see if I can make some changes to make this -- make my site more responsive at these types of Viewports. I'm going to bring the Web Inspector back up again. One of the great things about this tool is that you can use the Inspector in conjunction with Responsive Design Mode. Let's dock it on the side so that I have more screen real estate here. Let's turn off the paint flashing which is annoying at this point. Let's do a few quick changes. First off, let's get rid of this logo in the middle since the menu doesn't have room to fit, I don't think it's worth displaying and I also show the names of the galleries on hover so let's set that to display non-grade. In terms of these images, I have so little screen real estate I'd really like them to take up the entire Viewport. Right now I'm specifying that they are using 50% of the Viewport width and height, let's make that 100. Great. So now I have just one gallery per screen which is really nice. Now you'll notice I have these hover effects which works great on a desktop but on iOS devices you won't have hover and I also don't have my menu it would be great if this text is just always there. Let's explore doing that. I'm going to enable the hover pseudo class here. You see that it displayed. There are a few things that happen on hover. The first is that the UI is capacity one, but let's just make that always happen. Second, you will notice there is an overlay that goes to opacity 0. Let's just make it always opacity 0. I have my labels all the time which is what I want. One more change I want to make, I went to the What's New in WebKit session yesterday and I learned about this new feature called Scroll Snap Points, and I would love it if as I scroll through the galleries if it snapped exactly to each gallery. I'll try to add that real quick, I'm going to the body tag and I'll do a WebKit scroll snap type, make it mandatory and then I'll do WebKit scroll snap points and in the wide direction I'll repeat every Viewport. Let's look at that. Perfect. Beautiful, it just snaps right into place as I scroll. I really like that. There is a number of other changes that I have decided to make, and I have actually baked these into a media query that I've added right here. I'll just uncomment that and save it then reload my site. You can see I added a little text scroll to see more at the bottom, which is nice. I moved my icon to the upper right, it doesn't bring up a menu. And now with these settings -- this is looking good at this Viewport, let's make sure things work well at other Viewports. If I go to the wider size, I still get my original design, and then as I go down to smaller sizes, I see the narrow Viewport responsive design. This is great. It is really easy to use this tool in conjunction with the Web Inspector to see how your contents look at different Viewport sizes and make quick changes to effect the design. Thank you, and back to Jono. JONO WELLS: All right. Andy, thank you for showing us that. That's Responsive Design Mode. If you're not doing responsive design for the content of your applications or for your web content, now is the time to start. It is a good idea because, again, something like multitasking just come out of nowhere just like that and mess your content up. You don't want that to happen. We have added Responsive Design Mode. The focus is on layout, image resolution and being able to stay focused in Safari in the Development Tools so you don't have to leave and go off to simulator or a plug-in of an actual device to test your layouts. We have all the presets for the devices we make, you can set your own presets as you saw, it works right alongside Web Inspector so you can debug as you normally would. This is new in Safari 9. It is a really cool tool. Try it out. Along with Web Inspector you now have a very powerful set of tools to take your designs, make them smooth, feel great, make sure that they're error free and make sure that they work across different screen sizes. When you leave here please -- first of all, get OS X, El Capitan, get the developer release, if you don't have it already, turn on the develop menu in Safari. Customize Web Inspector to your liking, play with the interface, because I think you're really going to like it. It is a lot cleaner, everything is a lot easier to find. Play around with paint flashing and the rendering frames timeline. Even if you think that you have the best performance that web content has ever had in the history of all web content, this will tell you something about your site and how it is working on WebKit, how it's working in Safari, right, and you might discover something you didn't know. Right. The second I use paint flashing for the first time for instance I had a site, I discovered I had some painting happening in the background I didn't want that was stuttering the animation for everything else, I was able to eliminate it. It just happened immediately. It was great. Use the type information profiler. Right. Take -- we're going to -- this tool, it takes all of the code, makes it look like a light bright board if you ever played with one of those. It's amazing, it's going to give you great information. Play with that. Play in the console with the object views, try Responsive Design Mode. With all of these tools, use them to make your content shine. Now, it is really important that we get feedback from you. We very much want your feedback on these new tools and so for that, you should send questions or ideas to Jon Davis, he's our web evangelist and you can reach him at web-evangelist@apple.com. We also have these resources here for information on developing with Safari, resources for Safari developers and the Safari developer library. In the second link right now there is a document called What's New in Safari 9. To learn more about these features, plus all of the new features on the platform itself you'll definitely want to check out that document. Also, we have two labs coming up for Safari and WebKit, we have win right after this session and Andy and I will be there along with many developers and engineers on the Safari and WebKit team to answer your questions. There is another one on Friday at noon. Please come, check it out. We'll answer questions, get help for the content. I hope you enjoy the rest of your conference. Thank you very much for being with us this morning.  CHAD WOOLF: Thank you. Thank you. I'm Chad Woolf. KRIS MARKEL: I'm Kris Markel. CHAD WOOLF: We are performance tools engineers for Apple. This is session 412. We'll talking about profiling in depth. This will talk about the time profiler in Instruments and how to use it to optimize your applications. Time profiler is place to go when you're trying to figure out where your application is spending the bulk of its time. An example. It is useful when trying to discover what the application is doing at runtime. You want to see who is calling what. Our session breaks down a bit like this. We'll talk a bit about the motivation on why we wanted to create a session devoted to the time profiler only. But the session will revolve around demonstrations and showing you details on how it works and how your applications work below the source level. Then we'll give you final tips on how to use the time profiler on your own. Quickly about our motivation, it comes from the creation of Instruments 7 itself. With Instrument 7 we wanted to go with a new look and feel which meant new artwork but it also meant that for the new feel, we wanted it to be more responsive and we wanted to feel smoother. We'll do performance optimizations for our UI in general. We also wanted to try new graphing styles. These graphing styles were things that we wanted to do in the past but didn't have the performance to in our existing graphic code. We knew that we would have to focus on the rendering which is a pretty difficult piece of application. Instruments have to deal with hundreds of thousands, sometimes millions of data points and has to try to crunch that down into a presentation that's both simple and easy to understand. There is definitely algorithm complexities in there. What that meant for us, we had to rewrite a significant portion of our application which is the track view which lives up at the top of the app. Chris and I over winter took our design for the new track view and we started building it up out of a series of prototypes. We didn't do the prototyping in Instruments itself, we broke it out in a separate application to keep it simple. This is what one of the last prototypes looked like. Now while we were prototyping, a thing we did, we set a performance budget. As we layered on feature after feature we were constantly evaluating our performance of our code against the budget. When we found we were exceeding the budget we would use the time profiler to figure out where in our application things were going wrong. Sometimes it was an easy fix but sometimes it wasn't. Since we were prototyping, even some of the bigger structural changes we had to make to get the performance back on track was still fairly easy. When we integrated it back into Instruments we use the time profiler again to find the hot spots in our integration points, and after a few iterations, we had a version of Instruments 7 which was meeting our performance goals. We were thrilled with how the time profiler got us through this winter, that when WWDC 2015 came, we wanted to create a session that talks about time profiler and the problems that it's really good at solving. We wanted to share our experience when writing the track view. What we did this year, we created a demonstration application on iOS that looks similar to the first track view prototypes and we set performance targets for ourselves, 100,000 data points is what we wanted to graph, we wanted a perfect 60 frame per second for panning and zooming. We wanted to make it work on a iPad mini 1st generation. We picked the iPad mini 1st gen -- you know what I'm talking about -- we knew it would work well on all of the other platforms especially the later platforms. Chris will show you the application, he'll time profile it, and he's going to show you the things that we found when putting this together for you. KRIS MARKEL: Thank you, Chad. Here I have the prototype application of an Xcode, and I want to call out a few things. Our initial implementation we discovered can't handle even close to 100,000 data points. Initially we're working with 10,000 data points to get going. Another important point, you should do your time profiling on release builds, you want to take advantage of the compiler optimizations while you're profiling. I'll go ahead, start profiling the application and to do that I'll, from the product menu, choose profile. This will build the application, install it on the iPad and bring up the Instrument template user. Here it is, time profiler is selected for me. I'll click choose. Here you will see Instruments in our new track view and we'll get back to that in a minute. Right now I'll go ahead and start recording, click the record button, we'll show you the app. I want to emphasize what you see here, this is not the simulator, this is QuickTime mirroring that which is already on the app. Here's our graph, I'll go ahead and scroll, scrolling isn't bad, it is not bad, I'll zoom out by pinching, it is okay at fist and then starts to stutter, stutter, stutter, that's pretty bad. Now, finally, I'm going to just scroll back and forth with my finger, I'm actually moving my finger but the display is not updating, it is very, very laggy. That's very poor performance. Let's look at what's going on. Let's go back to Xcode -- Instruments. We'll stop the profiler and quickly show you the new track view. Here we have the CPU usage. What this is, this is the average CPU usage for specific unit of time. That time depends on your current zoom level. You see the different parts as I use my app where it was spending time. This is scrolling, the zooming out, this is the scrolling back and forth while zoomed out. A nice thing about the new track view is that I can go ahead and use this pinch gesture to zoom in on a piece of data that I'm interested in. If you're not using a Trackpad, you can hold down the option key and scroll up and down to zoom in and out. I want to focus on this particular piece of data, the activity right here. I was scrolling here. To do that, I'll apply a filter, that's just a click and drag and it selecting just those specific samples so I can focus on that particular data. I'll create more space down here. Down here this is our detailed view. What this is showing us is how many -- the percentage of time profiler samples we have inside of a particular function or method and then we have the symbol name. Here is our percentage and here is our symbols. Now the first thing you usually do when you time profile is you start to expand these out and looking for kind of a -- comparing the numbers over here with specific methods over here, function. Looking for things that kind of stand out as, you know, jump out at you as something that's worthy of investigating. Another option, if we go over to the inspector pane and click on the extended detail, we see the heaviest stack trace. This is for the main thread. This is where focusing here is where I'll get the most bang for my buck when doing a time profiling trying to make performance improvements. Let's see what's going on. The main is calling application main, run loop, there is core animation work happening, some -- there is nothing standing out as something that seems out of the ordinary. In fact, this is a really common occurrence when profiling. You look at what the application is doing the most, well it doesn't look like it is doing anything special. There is nothing -- no call to compute the 40th Fibonacci number here in here or something. However, you know, looking at this call stack, this stack trace, there is something I know what my application does. It is a simple prototype app, what it does, it builds a path and it draws a path. I can actually see in here that there is a call, a CG context path. It is not called by my code according to the stack trace. It is here, it is taking up a fair amount of time. I'll go ahead, click on that, take a look at that. If we look back at our call tree I do see something interesting here. What we have, we have the draw path according to this call tree is called by this draw layer method on UI view. That's also calling our drawRect for the graph view. That's taking a fair amount of time. That's one of the things that the app does. If I look at the time over here, context draw path is taking up, you know, 55% of the samples but the drawRect, it in very few. This is interesting, if I double-click on the drawRect method it will take me to the source code. I see, if you look to the bottom, I actually do call draw path from the drawRect method but it is not showing up in any sample. Everything in here is an add path. This is unusual especially since my expectation is my own drawRect method would take a while to run. It is basically half of what my app does. Looking at this, I actually notice that while drawRect returns a void and context draw path is the last method called and this is probably a case of what's called Tail Call Elimination. To tell us more about Tail Call Elimination and how to verify that, back to Chad. CHAD WOOLF: Okay. So to explain the situation that Kris is seeing we have to understand how the time profiler sees what's calling what inside of your application. This is going to get technical, I'll walk through it step by step. On the left, we have the code for drawRect and on the right, we have what you would imagine the stack to look like for that thread, right before the UIKit calls into the drawRect. When that call is made to the drawRect it will do what most functions and methods do, to establish their own call frame. That starts off by first pushing the return address from the link register and the previous value of the frame Pointer on the stack. Now drawRect knows how to return to its caller and restore the frame Pointer. Now the next thing that happens, we take the frame Pointer set up to the new base. Then drawRect will make room for its local variables and the compiler scratch space and that's -- now we have a frame for drawRect. Now the code starts to run and we come down to draw path and draw path does the same thing. It pushes the own frame on to the stack. The way time profiler works, it uses a service in the kernel that will sample what the CPUs are doing at about 1000x per second. In this case, if we take a sample, we see that we're running inside of context draw path. Then the kernal looks at the frame Pointer register to see where the base of that function's frame is and find the return address of who called it. Now we can see that drawRect called into draw path. If we want to see who called into drawRect we can use the frame Pointers that were pushed on the stack to find the base of drawRect and then continuously go back through the stack until we hit the bottom. This is a backtrace. If we take enough of these and put them in the call tree view you can get a pretty good picture of what's going on inside of your application. I want to point out, the frame Pointers on the stack are absolutely required. If you're compiling code with fomit-frame-pointer turn that off to try to do the time type of profiling that we're doing here. Let's look at the optimize cases. This is where drawRect was compiled with compiler optimizations enabled. Same situation, we have a drawRect frame. We're about to call into draw path. You notice when draw path returns drawRect is finished. Doesn't need to do anything. It is going to return. The way it returns, it is it pops the stack frame, restores the previous value of the frame Pointer and jumps back to the caller. The compiler looks at this and says well, why does draw path need anything from drawRect's stack frame. It doesn't. Furthermore, why come in to drawRect -- back to drawRect at all when it is going to return to its caller? What it does, it rearranges the code like this. It is going to pop the stack frame, restore the frame Pointer and make a direct call back into draw path meaning that we don't need to jump back to the caller. That's harder to explain than it is to see. Let's imagine what it would look like when running this code. We'll pop the stack frame off to get rid of the local variables. We'll restore the frame Pointer to the original value and the value of the link register. Then we'll jump to the beginning of the code for draw path. Draw path is going to push its own frame back on to the stack using the value that it found in the link register and the frame Pointer. From draw path's perspective it was called directly from draw layer in context from UIKit. If we take a time sample at this point, we'll see the exact same story. Even though that's not the actual call sequence that happened, this is what the time profiler saw. That's what we ended up seeing in our call trees. This is called Tail Call Elimination, it is common in highly optimized code and has some benefits. It saves stack memory. In the process of saving stack memory, it keeps the caches hot, that reuses the memory, the caches and data. It has a profound effect on recursive code, especially tail call recursive code, where a function or method calls itself as the last thing and then returns. Without pushing those frames a Tail Call Elimination inside of a recursive function can make the performance as good as its iterative version, so you don't get the stack growth and you get the high performance as well. This optimization leave on with highly recursive code. If you want to turn it off for the sake of profiling to show a cleaner stack trace you can turn it off by going in the build settings of the project and setting the compiler flag, the CFLAGS to FNO-optimize-sibling-calls, turning off the optimization, and unfortunately the performance gains with it, but you'll get a better result in the time profiler. If you choose to live with it, and you want to identify if a tail call is happening, then what you can do, you can look at the disassembly and call sight of the last call. If it is a normal call, it is going to use a branch and link family of instructions, that's the first example there. What that does, it jumps to the new function and saves the return value in the link register. If it is a tail call case and we're going to jump directly into the new function, it will be a direct branch instruction. Without the BL. That could be a call instruction for a branch and link and the branch would be a jump instruction, if you see that, it looks similar. Now it is up to Kris if he wants to disable the optimization and recompile, or he can carry on, it is your choice. KRIS MARKEL: I'll look at the disassembly. In Instruments, upper right-hand corner of the detailed view, there's a button, view disassembly, if I click that, I see the disassembly for the method and we can confirm the call to context add path is a branch and link, the call to context draw path, it is just a simple branch. I'm confident that this is a case of Tail Call Elimination that 55% that I saw on the call tree that was not attributed to my drawRect should be attributed to the drawRect. That is good news. I know now my drawRect is on my heaviest deck frame, my heaviest stack trace and it is consuming 55 to 60% of my time. This is great. I know where to optimize. I optimize drawRect, I'm good to go. Let's look at this drawRect. Looking that the drawRect, if I had a table I would flip it. There is not much to optimize here. It is hard to think of a simpler drawRect that actually is functional. We have 4 function calls, context, you know, CG calls, this drawRect really does not do much. It turns out that this is actually a really common occurrence when doing profiling. You will take a look at your hot spots and code, there is not much you can do in your code directly to improve your performance. You know, this junction, what do you do? You know, other than flipping tables, crying into your pillow at night, what we did was we went through, started to look at the core graphics documentation and other drawing, you know, Cocoa drawing documentation. We came across this particular property here. This drawsAsynchrously. Lo and behold, there was a make my code faster button that was created by an Apple engineer. This is excellent. Above that, you see I copied out of the documentation, pasted it in there. It says a couple of things that are interesting. It says, first of all, it may improve performance, it may not. You should always measure. You know, okay, dad. Let's measure. Let's see if this improves in performance. This time to start the Instruments, I'm just going to do command-I for Instruments. It will do the same thing. It will build the application and install on the device, bring up the template chooser. It takes a moment. Two moments. Three moments. Here we go. Another shortcut I like to use, if you take a look at the choose button down here. If I hold down the option button it changes to profile. What it means, when I click this, the application is going to start recording. It saves me a step or two. I'll do that now. Now the time profiler will come up. This is measuring the app. I'll do some quick scrolling back and forth, capturing some data. I think that's enough. Let's go ahead, stop the recording. I'm going to filter to specifically the scrolling data. If we go ahead down here looking at the detailed view. This is promising. I'm actually -- you can see, there is multiple threads here. The threads are doing work. That's really good. If we go ahead, if I hold down option, click the disclosure triangle I can see what the thread is calling, there is some dispatch calls, some CG calls, that's good. That's the drawing code. We'll go ahead, check the other one to see. Hold down the option key. Dispatch, CG calls. This is good. This is looking promising. I'm multithreaded, in theory my app is faster. However, multithreading does not necessarily mean faster. We should confirm this is actually doing anything for us. One way to do that, I happen to know this device is two CPUs, if the CPUs operate in parallel at max capacity I should see a 200% CPU usage in my graph up here. I'm not seeing anything over 100%, that's a bit of a warning sign, it doesn't necessarily mean that they're not both doing work at the same time. It means that I need to check further. How do we check further? Instruments has what we call strategies, it is different ways of partitioning the data to look at them. There is three of them. The first one is the Instrument strategies, the default, we're looking at it here. The second one is the CPU strategy, it shows the data per CPU or CPU relevant data and the final one is the thread strategy. It shows you details on what each thread is doing. Let's look at the CPU strategy. We can see we have each of the CPUs, we can see how much work it is doing. At the bottom, we see the combined usage. A nice thing to do here, when I zoom in far enough, the details, what the graph shows me, it will actually change. Rather than show making the average usage, it will show if the CPU is active or not, it will go from average usage, to either on or off. Now each CPU shows an on state or off state, whether or not it is working. What you notice here, the CPUs are never working at the same time, there is no parallelism here. You know, this is not good. If we want to feel worse, we look at the thread strategy. What this is showing us, each icon, it represents a sample the time profiler took, you can click on it. See the call stack. Here this is on a background thread and you see the core graphics calls, here is the main thread, you see the main -- the work we're doing on the main thread. As you see, if I zoom in to the right level, it is probably here, I scroll around, you see there is not really anywhere where two threads are working at the same time. It is jumping from one thread to another. So the drawsAsynchronously, this has not really done anything for us, in theory, it may have slowed us down. Now not only we doing the same drawing work but also managing, you know, core graphics system managed the threads it is working on, that sort of thing. That didn't really help. I'll turn it off. I'll flip another table I guess. It is not clear, the magic button didn't help. What do we do now? This is a really common occurrence again in time profiling. You try lots of things, most don't work. We step back. What does the app do? It builds a path and draws a path. We have seen the draw path code. Let's think about the build path code. That's right in here. What we wanted to do, we wanted to investigate the actual path we're building. What this code does, it loops the data elements, creating a path and adds a line to that path for each data element. We want to know how many lines we're adding to the path. This is something that the time profiler can't tell us. It can't tell you how long, or how many times a particular method or function has been called. It doesn't know the difference between a slow function that's called a few times or a fast function that's called a lot. In this case we resorted to NSLog and we just have a thing, every time we add a path we increment our counter and then we log it when we're done with the loop. Something important to point out, NSLog, it is not a very fast function. You don't want it in high performance code. You probably don't want to use it for anything other than gathering diagnostic information or debugging. When you're done, delete it from the code. In this case we just comment it out so you can see it. What we have found, we're adding 10,000 lines to the point in cases where we did not need to do that. In fact, there is no way to display 10,000 lines on this device. Especially when you're zoomed out far enough that all the data fits within 100 screen points. There is no reason to draw 10,000 lines in there. We need to draw 100 lines. It is a lot less work. We went ahead, we created an implementation that did that. If multiple data elements, data points are within a single screen point it just finds the max and draws a single line. If we're using 100 screen points we're creating 100 screen lines. We'll go ahead and switch to that implementation. . . We're feeling good about that. We'll change the element count up to the goal of 100,000 rather than 10,000. We'll see if that helped us at all. I'll use command-I to start up the Instruments. Since Instruments is already open, it is just going to bring it to the foreground and start recording immediately. Here we go. A new recording. We'll go ahead and scroll, the scrolling seems fine. I'll zoom out. Zooming performance is much, much better. It takes longer because I have more data to zoom out. It is actually looking pretty good. I'm going to do the swiping back and forth. It is tracking my finger really well now. It is actually keeping up with it, doing a fantastic job. Hooray! All done! Except not quite. If we look at the actual amount of CPU we're using while scrolling back and forth, we can see, you know, sometimes it is down to 60%, it is usually in the 70s or 80s. Technically we're meeting our performance goals. What are we doing -- what's the next thing to do with this app or prototype? We'll add additional features. We know that we need more headroom than what we have here. How do we make it faster, how do we make the app better and meet performance goals? We'll focus on this. We'll filter to that data. I'll give myself a little room. In this case, I'm going to hold down the option key and click main and expand this out. I can actually go down here and I can see this method here. You know, now that the drawing of the paths is fast enough, it is the building of the path that becomes the bottleneck. I want to focus on this method. I will click the focus button. That just moves aside everything outside of the method and normalizes our percentages to within this method. This method is spending 55% of time in the get next element and 10, 11% of time in objc msgSend. Something that I know, objc msgSend, it is a super fast method. It is super optimized. But, if I can get that 10% back, I want it. If we look inside of our code here we can see it is actually very clear. Most of our time is spent on getting the next element. This percentage here, it is a bit higher than in the tree view because it includes the objc msgSend time. If I get rid of that and make this iterator faster, I hopefully can get the performance boost that I want. To give us ideas on how much to do that, it is back to Chad. CHAD WOOLF: Let's talk about objc msgSend a bit. It implicitly gets inserted by the compiler whenever you use the square bracket notation or whenever you use the dot notation to access a property on an object. Its purpose is to look up the method implementation for the selector and invoke that method. That's a long way of saying that's how we do dynamic dispatch in Objective-C. Objc msgSend is extremely fast and does not push a stack frame. When you look at your time profiles, you typically won't see its effect. The times that you do see it would be a perfect example like we have where we see it in our iterator. What we're doing, we're iterating over 100,000 points and calling it the get next method with a small method body. Just increments a couple values and returns a structure. What's happening, all of that overhead time from the Objective-C message send is accumulating into something that's measurable. Is there a way to get around that overhead? Not exactly. Objective-C by design is a dynamic language, you have to make the objc msgSend call when accessing methods for objects and classes. It does this every time because you can switch out method implementations at runtime. There is no compile time in Objective-C saying I want to call in this particular method body. The only exception here, if you do what's called method caching, where you look up the method implementation yourself and you call it through its function Pointer. In general I don't recommend that you do that. It is fragile as you can imagine. In general, in my experience it hasn't given me the kind of performance wins I'm expecting because you got to think about why we're here in the first place. The reason we're here, the get next element method has that small method body. Even if you invoke it through the function Pointer you still have to have to marshal the arguments and push the frames on to the stack and pop them when you're done. That's exactly what you saw in the previous set of slides, that can be a lot of overhead with an increment and then return. I want to make sure I point it out that method caching is not as fast as inlining, what we really want in this case is that small method body to be inlined. How do you that in Objective-C? Well, you have alternatives. The first one, you could have used C and you could have used structures instead of an iterator, you could pass a C ray into the method for example. If you want that OO flavor you can use C++. The way you use C++ in Objective-C, is you rename the file from a .m to a .mm and then you can use C++ syntax. Because Arc is usually on by default, then you take Objective-C objects and put them in STL containers, and put them in instance variables on your classes and structures. This is handy and you get the performance benefits of C++ and I have used it extensively in Instruments to get as much speed as I could out of the track view and other critical areas of Instruments. I know from firsthand experience also there is a major downside to this. That is that you have to know ahead of time which parts of your code are going to benefit from C++ and which codes will benefit from Objective-C? Sometimes, until you're doing profiling, you can make a mistake there as we did in the demo case. We wrote our iterator in Objective-C not realizing it would show up in our time profiles. Is there a better alternative than what I just mentioned here? There is. You knew it was coming. Swift is ideal, because unlike Objective-C it is only dynamic when it notes to be. If you make sure that the performance critical classes are internal and you use whole module optimization, the compiler or the whole tool chain can determine when there's only one method implementation and inlines it right into the call site giving you some significant wins especially on the iterator case. Because we're prototyping, rewriting the iterator in the View Controller in Swift is easy. Kris has done that. KRIS MARKEL: I have a Swift implementation ready to go, this is an easy port of the Objective-C implementation with a couple of the suggestions they made in this morning's session about improving Swift code, specifically turning on whole module optimization. Let's profile this. Command-I. It will build. Install to the device. It should just start profiling. Okay. I'm going to bring the application forward so you can see. Here is the scrolling. Looking good. Zooming out. There you go. Zooming out. Staying nice and fast. A lot of data to zoom out of. Now, if I go ahead, move back and forth, it moves really fast. KRIS MARKEL: It is very nice. Thanks. Actually we can go ahead in here and look at what the CPU usage is. You know, we've got, actually further improvement than what we expected, we're expecting 5, 6% improvement from removing the objc msgSend, this is lower and I can actually if I turn down this disclosure triangle you see the two runs next to each other and you see the previous run in the lower -- the current run, it is clearly much lower. Actually if I go ahead, I go in here, I look for my build path method I have to search for it now, oh, that's not how you do a search. If I hit command-F, it will bring up this dialogue here. I can type in build path and it shows me my method here. If we look at this, you see here is my Swift code. My get next call which is right here, it isn't showing up in any samples. You know, no samples landed on this. Why? It is because Swift was able to inline it, whip means that there's no function overhead, no method call overhead whatsoever. Because the code for the iterator is inline with the rest of the code it makes further improvements explaining the better performance than what we hoped for by skipping the dynamic dispatch. Chad, anything else to tell these nice people? CHAD WOOLF: We have 5 minutes! Of course I do. Some tips for you to explore Instruments on your own. The first one to point out, under the recording settings, it's called record waiting threads. I mentioned that the service and kernel we use samples the active CPUs but if you have threads sitting around, blocked on a lock or waiting for I/O, you check this checkbox, and the service will sample the idle threads as well. If you have code that's contending over a lock you see the hot pods show up when you enable record waiting threads. Another thing that I find interesting, in the display settings, in the call tree section, it is invert call tree. Figuratively what it does, it flips the call tree upside down. Instead of seeing the leafs at the bottom nodes of the tree, that's functions that don't call into anything, they appear at the top. If a utility function is being called from 5, 6 places, you invert that call tree to see who is actually calling into that particular function. It gives you a different perspective on the data in the call tree. When you right click a node in the call tree, you get a context menu and there is interesting stuff under there as well. One thing that I use from time to time is the charge to caller, so what you can do, you can charge a function on method to the caller. You can charge an entire Library of Framework to the caller. There's an option in there as well to prune a subtree. You don't want to work on a specific problem at the time, you can prune that out of the data and focus on what you want to focus on. What did we learn? Through all of this, the first things I want to remind you about, is to incorporate performance targets early. If you're doing a big performance rewrite like we were, start with a budget and keep monitoring it because once you start layering a lot of code on top of that, it is harder to change. Secondly, always measure. Through all of our demonstrations we were taking time profiles with the time profiler, we used that data to find the hot spots and retuning it, by the end we had a well-performing application. If you go in blind, depends, you may be lucky, hit it. I would start with a measurement and go after that first. In the third one, this is most important to me, I want to encourage you to keep digging. Some of these performance problems you look at at first may have seemed impassable, you say that's happening in someone else's code or a side effect of the runtime. The reason we gave you the details on the time profiler, the runtimer, what the disassembly view looks like, is we wanted to show you there is a whole world out there of more details. By using that, you can solve the performance problems that we were solving today. I encourage you to keep digging, and with creativity, going out there into sessions like this, I know you guys will be able to fix and hit the performance targets you want. That just about does it for today. Stefan Lesser is our dev tools evangelist, contact him if you have any questions. We related sessions, Energy debugging issues, turns out that if you have efficient code on the CPU it uses less energy. There was that session on Wednesday. Also tomorrow, there is performance on iOS and Watch OS, good news, the time profiler can target apps on the watch. That's a big boon. Tomorrow we'll be in labs from 9:00 to 2:00. Enjoy the rest of your conference.  PRABHAKAR LAKHERA: Thank you and good morning. Welcome to Your App and Next Generation Network session. I am Prabhakar Lakhera and with me I have my colleague Stuart Cheshire. And this session is in two parts. For the first topic I will talk about IPv6, and for the second topic Stuart will talk about how to make your applications run faster. We will start off with IPv6 first. Now, what's new in IPv6? IPv6 RFC was published almost 17 years back. So you must be wondering, why are we talking about IPv6 now? We are seeing more and more of IPv6 deployment in enterprise and cellular networks. And you want to make sure that your applications work in those networks. That is the reason we will also be mandating your applications to be IPv6 compliant. Now, we will talk more about that and what it means to you as developers, but before we do that, let's begin with a bit of history. Now, a long, long time ago, client devices had real and unique IPv4 addresses; these were the good olden days and you had end-to-end network connectivity. However, we soon realized that we were running out of IPv4 addresses way too fast. So we added a NAT in the middle. Now, this works, but the larger scale NAT device is both expensive and fragile. So carriers are now deploying IPv6 in their network. Now, with this, they again have end-to-end network connectivity and there's no translation needed in the data path. I will now show you how IPv6 deployment looks for three major cellular carriers in the USA. Now, two things are obvious here. One, all the lines are going up. And second, more than half of the subscribers are now connecting to cellular data networks over IPv6. So that's great, right? Turns out it's actually worse for cellular carriers than it was before. And the reason is now they are having to support both IPv4 as well as IPv6 in their network. So what they really want to do is to drop IPv4 from their access network. Now, when you do that, you lose connectivity to the IPv4-only part of the Internet, which is still in majority. So now they have deployed DNS64 and NAT64 in their network, and the way it works is when the application on the client device makes a hostname query to get the IPv6 address for an IPv4-only server, DNS64 and the network synthesizes an IPv6 address and gives it back to the client device. Now that the client device has this IPv6 address to work with, it can start writing traffic to the network. The network itself is configured in a way that search packets get shorted to the NAT64 engine, which then translates IPv6 traffic to IPv4 and vice versa on the way back. Now, the important thing to note here is that to the applications running on the client device, your IPv4-only server looks like an IPv6-only server. And this is important because till now, some of you in the room may have been thinking that my server is only configured for IPv4, so I do not need to test for a client accessing it over an IPv6 network. Your assumption just got broken. Now the transition to this type of network will happen very soon, and when it happens, we want it to be an absolutely seamless experience for our consumers. And that is the reason your app has to be IPv6 compliant, and this will be an app submission requirement. So great, we have a new app submission requirement, right? And you must be wondering how do I test my application for this network? Where will I find this NAT64 type of network? I have a great news for all of you. Starting today, with just your Mac devices on top of IPv4 connectivity, you can create your very own NAT64 networks and start testing your applications. Now, this feature is meant to be used by the developers, so it's somewhat hidden, and to make it visible, all you have to do is to Option-click on Sharing, then Option-click on Internet Sharing, and now you will see everything looks just the same. But there's a new checkbox here that says "Create NAT64 Network." So you check that, you choose your interfaces for Internet sharing, and now you can host this NAT64 network and start testing your applications. Now, for the example here, I have my IPv4 connectivity on the Internet and I'm sharing this as an IPv6-only access network with NAT64/DNS64 on my Wi-Fi interface. So when I start it, I see that the Wi-Fi icon has grayed out and it has an arrow pointing up. That means now the Wi-Fi interface is in access point mode. What that really means is now it's hosting a Wi-Fi hotspot, and you can connect your other client devices and start testing your applications. Now, a typical test bed will look like this. Now, here I have my IPv4 Internet connectivity on the WAN side, my iMac has the DNS64/NAT64 engine running on it, and it is hosting an IPv6 network on the Wi-Fi interface. Now the application that you want to test is either installed in one of the client machines, or you could very well be testing it on a simulator that's running on another Mac device that's a client of this Internet sharing environment. So now that we have made testing easy for you, for this type of network, what we really want you to do is to make sure it is part of your development process. That is, any time you are writing a new application, or you are writing an update for your application, make sure you are testing for NAT64 network environment before you submit your applications. Now, the good news is for almost 70% of you, you will see that your applications are working just fine. And that's great, right? Just keep testing your applications, release after release and make sure there's no regression. But for almost a third of you, you will see that either your application is severely limited in a NAT64 network environment or it does not work at all. Now, fortunately, most of the issues are simple to fix, and here's a sample list. Now, if you are using only IPv4-only data structures or IPv4-only APIs, or you are using an API that supports both IPv4 and IPv6, but maybe you're passing an argument that says that only get me results for IPv4, all of these things will make your application IPv4 only. And what that means is it will not work in an IPv6-only access network environment. Now, there's another interesting thing that we have seen with some of these applications that do not work, that sometimes there's a preflight check that checks for IPv4 connectivity even before attempting a connection. And that is the reason sometimes you get errors like this. So in this case, my iPhone was actually connected to the NAT64 network I had just created on my Mac device, and I could browse Internet with Safari, I could stream videos, music... I had my Internet connectivity, but for some reason this application thinks I have no Internet connectivity whatsoever. And if you read the error message it says that my device is in airplane mode, but if you look at the top bar, that's not the case, right? So what's going on here? It's precisely the application making a preflight check to check for IPv4 connectivity, and if you remember, in IPv6-only access network like NAT64 network, you do not have IPv4 connectivity; the entire world looks like an IPv6-only world to you. Even the IPv4-only server looks like an IPv6-only server. So if you do this check, it will fail. Now, in this case, application is saying retry. So I retried and it gave me the same error message, and then I retried again, and the same error message. And it never went off, and I was just stuck with this application. So, what are the recommendations? Well, just attempt a connection. Right? If it connects, that's great. If it does not, then handle that case gracefully. The second recommendation would be to use higher networking frameworks like NSURLSession or CFNetwork API, and the reason is networking with multihome devices like iPhone and Mac devices can be somewhat complex. Like for iPhone, you have Wi-Fi interface and you have cellular interface. And with Mac devices, you may have multiple Ethernet interfaces, and you have a Wi-Fi interface also, and at a given time, you may have different kinds of connectivity to all these interfaces. Now, which interface you use and how to connect which type of connectivity do you use for a given destination, writing that code yourself can be a lot tedious. So please use higher networking frameworks. It will make your application code a lot cleaner and simpler. Now if for some reason you cannot use a higher networking framework and if you are having to work with sockets, we will definitely recommend you to read RFC 4038, and it talks about in great length how to write your applications in a way that's address-family agnostic. Now, our final recommendation would be to use hostnames when possible and not to use IP address literals, whether IPv4 or IPv6. When you are writing your very own protocol, a proprietary protocol, or when you are writing your application, make sure you are not using IP address literals but preferring hostnames. And the reason is, remember, in a NAT64/DNS64 network environment, the client device first has to make a DNS query, right? To get the IPv6 address for the IPv4 server. So you have to work with the hostname. If you are working with an IPv4 address literal, the client device will not make that DNS query, and the DNS64 network will not synthesize the IPv6 address for you. So, given that, we do understand that sometimes there's no way to avoid working with an IPv4 address, and an example of that would be Safari. So you go to a web page, it loads up just fine. But within the web page, there may be some other links, and some of those links may have IPv4 addresses embedded. Now, before today, on Safari if you clicked on one such link, it would not load up. Starting iOS 9 and OS X 10.11, not only Safari but any user of NSURLSession or CFNetwork API will be able to work even with IPv4 address literals in a NAT64/DNS64 network. The way it works is when you are using hostnames, the DNS64 in the network is synthesizing IPv6 addresses for you. But when you are working with an IPv4 address literal and using one of the higher networking API, the OS is discovering what the network would have done and how it would have synthesized an IPv6 address for you, and it will do that locally. So this is another reason why you should be working with higher network frameworks. So please do that. Now, with these data points and with the new tool, we really hope that you will be able to find and fix issues with your applications. Now, what we want you to do is after this session is over, install the seed build on your Mac devices and start creating your very own NAT64 networks, and then use them to test your applications. Now, remember this will be an app submission requirement. So please take this message to other developers who are not in this session and also take this message back to your company and make sure you are doing NAT64 testing for your applications. Now, with that, I now invite Stuart Cheshire to talk about other networking features we have for you that make your applications faster and be more responsive to the users. Stuart? STUART CHESHIRE: Thank you, Prabhakar. What I want to talk about now is making your applications run faster. In the last couple of decades, we have seen a phenomenal increase in network throughput. I remember a time when the 56-kilobit modem was the latest technology; now we live in a world where 50 megabits per second is quite commonplace. But things don't feel a thousand times faster. We still spend a lot of time sitting waiting for a web page to load. And why is that? It's because our industry has put a huge focus on increasing throughput and it has sorely neglected the other sources of delay. The speed of light hasn't gotten any faster and we can't do anything about that, but there are other areas of delay that we can fix and it's time for us to start doing that. So that's what I'm going to talk about today. I'm going to talk about four sources of delay when users are using your applications. The first one is the delay that happens when you have weak Wi-Fi connectivity, and connection attempts are not succeeding. The second area is a technique called Explicit Congestion Notification, which along with smart queuing reduces delays in the network. The TCP NOTSENT Low-Water Mark option reduces delays in the sending machine, and then we are going to finish up with a sneak peek of an exciting new technology called TCP Fast Open. So let's start off with reliable network fallback. I'm sure everybody in this room has had the experience at the end of the day: They leave work. They go out to their car. They pull out their phone. You want to check maps, weather forecast, email, whatever, and you are staring at the phone and it's not loading and it's not loading and you are walking to your car, and it's still not loading and you get frustrated, you go into Settings, you turn Wi-Fi off. Now you're on LTE and bam! The page loads. And then you forget to turn Wi-Fi back on, and a week later you've got a huge cellular data bill that you didn't want. Well, that's not a good user experience. What we are doing now is we have some intelligent logic about doing parallel connections. So if your iPhone thinks it's on Wi-Fi, but the TCP connection setup attempt is not succeeding, then very rapidly, it will initiate a second parallel connection over cellular data. Now, it won't kill the Wi-Fi connection. It won't give up on it. It will let that one continue to run in parallel and if that one completes first, that's great. You have a connection over Wi-Fi. But if it doesn't, and the cellular connection completes first, then that's the connection your application will get with a delay so short, the user won't notice anything odd. Of course, we only use this for apps that are allowed to use cellular data. If the user has gone into the settings and turned off mobile data for that app, we won't do the fallback. And if we do fall back, then we hide the Wi-Fi indicator, so now the user knows they are not on Wi-Fi anymore. This is something that you get for free, as long as you are using the higher layer APIs, and you should see no difference in your applications apart from a better user experience. There's one additional thing you can do, and that is when you are running over cellular, whether you started on cellular or whether you fell back to cellular, doesn't matter. The user can walk back in range of Wi-Fi and at that point, if you pay attention to the Better Route notification, you can then decide what to do about that. You may want to tear down the connections you have and reconnect over Wi-Fi, or if you are 99% of the way through sending an email, you may want to just let that complete, but by paying attention to the Better Route notification, you can make an intelligent decision that minimizes the user's cellular data bill. My next topic is delays in the network. And this is something that came out of work I did on Apple TV. We are working on trying to make Apple TV more responsive and understanding where the delays came from. And I expect everybody in this room has heard about bufferbloat. I did some experiments, and I want to share the results of those experiments with you so you can understand too how critically important it is for all of our applications and products that we fix bufferbloat in the network. I tested a simulated network environment with a 10-megabit downstream connection, which is plenty for watching streaming video, and first I'm going to show you the results using a representative network setup, a simpleminded first-in, first-out queue where packets are buffered until the queue is full and can't hold anymore, and then the new arrivals get dropped. This is very typical of consumer home gateways today. And then I will show you a comparison using smarter queuing and ECN. I'm going to show you some plots generated using tcptrace. I expect many people in this room have used tcptrace, but if you haven't, I strongly urge you to go to TCPtrace.org and download it. If you are working on networking code, and you are not using tcptrace to look at your packets, then you have no way to really know what is going on or understand the performance characteristics of your app and your protocol. When we write apps, we pay attention to the memory usage. We profile the code to find out what's taking the CPU and then we optimize the code that needs it to improve CPU efficiency and improve battery life. To do those things, to care that much about CPU and memory, but neglect the networking part doesn't make any sense. And tcptrace is the tool that lets you do similar kind of profiling and analysis of your network traffic. Here's a TCP trace of the first 10 seconds of some streaming video. For those of you who haven't looked at tcptrace before, I will give a quick overview. The little white lines represent data packets. The horizontal position of that white line tells you the moment in time where the packet was captured. The height of the white line tells you how many bytes of payload are in the packets, and the vertical position of where that line appears tells you where those bytes fall within the overall logical TCP sequence number space. So here we can see a stream of packets being sent out in order, spaced a few microseconds apart. One round trip later, we get the acknowledgment back from the receiver saying it was received. And that green line is the cumulative acknowledgment line. Everything up to and including the green line has been acknowledged by the receiver. So we should never see any white packets below the green line. That would indicate a bug, and we don't see any white packets below the line, so that's good. The yellow line indicates the receive window. When you open up a TCP connection, the receiver indicates how much RAM it has put aside for your data, and you should not exceed the amount of RAM that you have been allocated. If we see any white packets above the yellow line, then that would be a bug, and we don't. So that's good. This looks look a relatively nice, straight line data transfer. The slope of that curve is exactly 10 megabits per second, which is what we expect, but every few seconds, we see something like this going on. So let's zoom in and take a closer look at what's going on down there. There is so much information on these TCP trace plots, I could spend an hour just talking about this one slide. But we don't have time for that, so I will just cover some highlights that jump out just with a glance at this plot. One is the white packet line is kind of pulling away from the green ack line. What that means is the rate that we are injecting data into the network is faster than the rate data is coming out of the other side and being acknowledged. Well, if we are putting it in faster than we are pulling it out, something has got to be going somewhere. It's sitting in buffers and we can see that the amount of stale data sitting in buffers in the network is growing. And because the amount of buffering is growing, that means the round trip delay between when a packet is sent and when it's acknowledged is getting longer. When we have so much buffered that the gateway can't buffer anymore, we start losing packets. And then this mess happens, and it really is quite a big mess, because packets are coming into the tail of the queue faster than the queue is draining, and we get a packet, we lose it, we get another one, we lose it. The queue drains a bit, we get a packet, we accept it. So, at the tail end of the queue, it's carnage. It's get a packet, lose one, lose one, lose one, get one, lose one. But over at the front of the queue, we've got 200 packets queued up, in order, they are neatly going out over that 10-megabit bottleneck link, in order, no gaps, no problem. It's only after that entire queue has drained that we actually witness the results of that packet loss carnage at the receiver; that gets reflected back to the sender in the selective acks and the recovery starts. So this is a big mess. Because of the way networking APIs work, data has to be delivered in order. If you lose one packet, then all the packets that arrive after it get delayed in the kernel until the gap has been filled in. Now, there are good reasons for this. People have talked many times about out-of-order delivery, but it turns out with almost all applications it's hard to use data out of order. If you are trying to decode H.264 video, having frames that depend on an I-frame you don't have is not helpful. So it turns out that in-order delivery really is the model that applications want. Because of that in-order delivery, we get these long plateaus where no data is delivered. And for the Apple TV, video playback process, that equals a period of starvation where it's getting no data. And because we don't want the video to freeze, that's why all streaming video applications need a playback buffer. And having a big playback buffer is why when you watch a streaming video, you see that spinning wheel saying buffering, buffering, buffering, because it has to fill up the playback buffer so that it can weather the storms when no data arrives for a long period of time. When the missing packet arrives, we then fill in the gap and deliver it all at once. That puts an excessive burden on the network receiving thread which takes away CPU time from the video playback threads, and that results in the stuttering of the smooth video playback. So that's bad. So this uneven delivery in the network has bad consequences for a device like Apple TV; when we are trying to make a very affordable, cost-effective device, these long plateaus of starvation equate to needing more RAM in the device for more buffering and slower video start-up and a poor user experience. And these spikes in delivery result in needing a faster CPU in the device than we otherwise might have needed, which pushes the price up. So this uneven delivery is very damaging for streaming video. One interesting thing to note, though: If you visually track the slope of the yellow ack line -- the yellow window line and the green ack line, you'll see that at the end of the trace, they pretty much end up back where they should have been, if no loss had happened because TCP does do an awesome job of filling in exactly what needs to be retransmitted, exactly once, and not retransmitting anything that didn't need to be retransmitted. So it gets back to where it should have been. If you measure this network using Iperf and look at the number that comes out, it will tell you 10 megabits per second, and you will say thumbs up, my network is working perfectly. But distilling all of this information down to a single figure loses all the subtlety of what's really going on on the network. So now that I understood what was going on and causing the sluggish performance, I decided to experiment with a smarter network. For this experiment, I used a smart cueing algorithm called CoDel, which is short for Controlled Delay. The way it works is instead of filling the queue until it overflows and loses data, it monitors the state of the queue and as soon as a standing queue starts to build up, then it considers that to be a sign of congestion. And when I say congestion, a lot of people think I'm talking about something that happens rarely at peak times, and it's not. It's important to understand that in networking, congestion is what happens all the time. It's the steady state of the network. It's the job of any transport protocol like TCP to maximize its use of the network, to find out how much the network can carry and make the best use of that. And the way a transport protocol does that is it sends data faster and faster and faster. It keeps probing and it keeps trying a bit more, until it loses a packet, and then it knows that was too much and it backs off. So it's constantly doing this hunting to track to find the right rate and what that means is it's always pushing the network into congestion, and then backing off, and then congestion and backing off. What CoDel does is not wait until things have gotten really, really bad before it signals congestion. As soon as the first sign of it starts to happen, it tells the sender. The other thing that I did for this experiment was instead of indicating congestion by losing packets, which requires a retransmission, we used a new technology called Explicit Congestion Notification, and that way the smart queuing algorithm, instead of dropping the packet, it sets a bit in the IP header saying congestion experienced. That is echoed back to the sender and it responds by slowing down without the destructive effects of a packet loss. So this is our graph of the same data transfer, using CoDel and ECN, and if I zoom in to the same part we were looking at before, you can see an absolutely phenomenal difference. When I was doing these experiments, I had planned a week to do the work and gather the data, and I was finished after two hours. I did one plot with the standard configuration and one plot with CoDel. This is my first experimental run that I'm showing you. I was expecting to have to tweak parameters and retry and rerun the experiment. No. This is -- the difference is that obvious, that one trial was all it took. I'm hearing applause. Thank you. We have no plateaus of starvation. We have no spikes of peak delivery. Every time the slightest hint of a queue builds up, we get these little polite nudges from the CoDel algorithm saying slow down. The CWR on that plot is Congestion Window Reduced. That's TCP's acknowledgment saying message received and understood; I have slowed down. Absolutely wonderful! So, simple summary, CoDel and other smart cueing algorithms are great. ECN are great. Put them together, it's totally awesome. So if it's so great, where is it? Well, historically, packet loss and retransmission has not caused big problems for the traditional networking applications like file transfer and sending email. When you transfer a file, the transport layer in principle could send the first packet last and the last packet first and all the ones in the middle in a random order; as long as they all get there and they are reassembled in the correct order you have your file, and that's all you care about. But when you are watching streaming video, you don't want to see the end first, and the start last. You want it in order. So in order delivery has become a much more pressing problem, now that we are using the Internet for streaming video. One way I characterize this is that we used to have applications like clicking send on an email, where you have a predetermined amount of data and how long it takes to send it is variable. Basically, the time you'd like the network to take to send your email is as little as possible. There's really no such thing as sending an email too fast. So the time is variable. You would like it to be fast. Now we have applications where you are watching a two-hour movie, streaming over the Internet. It doesn't help to watch it in half an hour or in eight hours. It has to take two hours. So now we have adaptive applications where the time is fixed but the amount of data that could be sent in that time has to adjust to accommodate the network conditions. Where are we now? Well, amazingly, because it is in Linux and turned on by default, more than half of the top million web servers in the world already support ECN, which is phenomenal adoption for a technology that no one is using. [Laughter] Clients. Well, the clients are not asking for ECN connections. They are not requesting ECN because pretty much none of the Internet supports ECN marking. So if you turn on that option, there might be some risk of exposing bugs and there's no immediate benefit. What are the routers doing? Well, none of the routers are doing marking because none of the clients are asking for it, so why put engineering into something that might have risk that no one is asking for? Well, I'm happy to announce today that Apple is taking the initiative to break this log jam. In the seeds that you all have, ECN is now turned on by default for all TCP connections for all applications. We're not expecting to see any problems; in our testing, everything has gone smoothly. I have been running it on my own laptop for a long time; of course, we want to hear your experience. Please take the seed builds, run them on your networks at home, at work, in your hotel, at the airport, and as usual, if you find any bugs, please report them to Apple. If we are successful, in a few months' time we could have a billion devices running ECN, and that should be enough incentive for the ISPs to start offering that service. Now we are going to move on from network delays to end system delays. Like many advances in technology, this was borne out of a personal pain point for me. I use screen sharing to connect to my Mac at home, and it's absolutely wonderful. Being able to control it remotely, being able to access data on it, being able to start a long video transcode going that I want to have finished when I get back home: these are all wonderful things. And at the time I was doing these experiments, I had a fairly slow DSL line. And, of course, DSL is asymmetric. It's typically ten times faster in the downstream direction than the upstream direction, and when you are doing screen sharing, the data is coming in the wrong direction in that sense. So it's kind of to be expected it will be a bit slow. It's like the famous joke about the dancing bear: when you see a dancing bear, you are not supposed to be impressed that it dances well. You are supposed to be impressed that it dances at all. So for many years, like many of us, I kind of suffered with this painful, barely usable experience, and I would find that when I clicked on a menu, it took three or four seconds for the menu to appear, and after a few minutes of using the computer this way, it's really, really frustrating. There were times it felt like it would be quicker to drive home and do it. And I had been working on bufferbloat and excessive queuing in the network. So naturally, that was the first thing I blamed, and I started digging in. I started investigating. I was all ready to be thoroughly indignant about this stupid DSL modem that had all of this excessive bufferbloat in it. And I pinged the machine, and the ping time was 35 milliseconds, but when I click the mouse, it takes 3 seconds for a menu to appear. So now I'm realizing this is not as obvious as I thought. Where is the delay coming from? Well, I did some investigation. The default socket send buffer at that time was 120 kilobytes, my throughput was about 50 kilobytes a second, so that's about 2.5 seconds, which was about the delay I was seeing. Now, the socket send buffer serves a very important purpose. When we use a transport protocol like TCP, if it just sent one packet and waited for the acknowledgment and one packet and waited for the acknowledgment, we would get terrible performance. We need multiple packets in flight; we need enough packets in flight to fill the bandwidth delay product of that path to the destination and back, and those packets have to be buffered so that if they are lost, they can be retransmitted. That's all good and useful and necessary to maximize the throughput of that connection. But any excess buffering above that requirement just adds delay for no benefit. It doesn't help the throughput. It just adds delay. And we end up with something that looks like this. We have a little bit of data in flight, which is buffered, in case it needs to be retransmitted, and we have a whole lot, just sitting in the kernel waiting for its turn to go out. Well, this was an eye-opening revelation for me. There aren't just delays in the network. There are big delays in the hosts themselves. Screen sharing would grab a frame, put it in the buffer, grab a frame, put it in the buffer, grab a frame, put it in the buffer, and then the kernel would let those frames mature like a fine wine before it was time to put them out on the network. [Laughter] Because of that, we came up with the TCP NOTSENT low-water mark socket option. When you send that option, the socket send buffer remains unchanged. The difference is that kevent or your run loop will not report the socket as being writable until the unsent data has drained to some fairly low threshold, typically 8 or 16 kilobytes works well. When the socket becomes writable, you then write a single useful atomic chunk into the buffer. You don't loop, cramming as much data into the kernel as it can take, because RAM is cheap these days, it can take a lot. You just write a sensible unit. And in the case of screen sharing, that's one frame. And now the picture looks like this. We have data in flight, that's buffered. We have a little bit waiting for its turn to go out. As that drains and reaches the threshold, the socket becomes writable. We write a single chunk and let that drain before we write some more. So with that, I would like to show you a demo of this in action. Here I'm using screen sharing from this machine, connecting to this. I'm using a gateway running sarawert [assumed spelling] to simulate a DSL performance connection. And let's bring up a Terminal window. Okay, there we go. I'm going to move this window. You can see the mouse pointer move because it's generated locally. The actual graphical updates are generated by the remote machine. So let's move this window over here. No, not over here. So let's move it -- actually, no I like it back where it is. Hands off the keyboard. [Laughter] Let's pull down some menus: Shell. There we go. Let's look at Edit. No, maybe View. If this feels like a demo that's going painfully badly, this is what my life was like trying to use my computer remotely. It takes the patience of a saint to put up with this. Well, now I have a new option. Let's turn on the not sent low-water mark, and once we reconnect, now I will try dragging this window around. Thank you. And that was years of suffering unusable screen-sharing connections just for the sake of a silly oversight in the BSD networking stack. The great news is that we have fixed this. It is available in -- it's now being used by screen sharing in the last software update in 10.10.3, so if you noticed that screen sharing seemed to get a lot more snappy, then this is the reason why. It's used by AirPlay, and it's available in Linux too because this option applies at the source of the data on the sending side. So for those of you who are running Linux servers, this option is available for your servers too. The benefit of this delay reduction is really obvious for real-time applications. And we started making slides for this presentation where we had two columns. We had the apps that should use the low-water mark option and the apps that shouldn't. And we couldn't think of any to go in the shouldn't column. Every time we thought of a traditional app like file transfer, well, that doesn't need it. We realized you've had that experience where you change your mind about a file transfer and you press Control-C and it seems to take about 30 seconds to cancel. It's because it had over-committed all of this data into the kernel and it had to wait for it to drain because there's no way to change your mind. So, yeah, actually, file transfer does not benefit from over-committing data, and we couldn't think of any application that does benefit from over-stuffing the kernel. So once we had that realization, we decided starting in the next seed, this option will be turned on automatically for all connections using the higher layer NSURLSession and CFNetwork APIs. All you have to do to make best use of this is when your socket becomes writable, don't loop writing as much data as you can until you get an EWOULDBLOCK. Just write a sensible-sized unit, and then wait to be told it's time for more. And that way if the user has changed their mind or something else has changed about the environment, next time you find your socket is writable, you can make an intelligent decision. You can do just-in-time data generation where you generate the data so it is as fresh as possible and based on the current information, not based on the information from five or ten seconds ago. And that brings me to our final part of the presentation, and that is a sneak peek of a brand-new technology called TCP Fast Open. The way TCP traditionally works is this: we do one round trip to set up the TCP connection. And then we do a second round trip to send the request and get the response. TCP Fast Open combines the connection setup and the data exchange into one packet exchange. This is not turned on by default for all applications and there's a reason for that. There is a caveat you need to be aware of, and that is that this is only safe for idempotent data. I'll explain what that means. When you use a TFO operation, the handshake and data are combined, the server gets the message, the server acts on that, sends you the response, and then you close the connection. But the service model of the Internet doesn't guarantee that packets can't be duplicated. The success of the Internet has been due to a very simple service model, which is deliver the packets fast and cheap. And don't worry about them being in order. Don't worry if some get corrupted, some get lost, some get duplicated, just be fast and cheap. And the end systems are smart enough to deal with that network model. Packets can be duplicated for many reasons. It could just be a bug in a router, it could be Wi-Fi link layer retransmission that accidentally sends the packet twice. It could be end-system retransmission: If you send a packet and don't get an acknowledgment because the acknowledgment was lost, perhaps, you will then retransmit the packet and then immediately you've got two copies of the same packet in the network. Well, if one of those copies is delayed and shows up much later, then to the server that looks like a perfectly valid TFO request, and whatever the operation was, it will do it again. If that operation was sending you a JPEG image, doing it twice may be no big deal. If that operation was sending you a pair of shoes from Zappos, then doing it twice might not be what you want. So this is something where you have to intelligently decide when it's appropriate and safe and when it's not for your application. You use this through the connectx system call. This is a sneak peek technology preview for the early adopters. Later, we will look at how to expose this through the higher level APIs but now it's only available through connectx. The server you are talking to has to support TFO as well, and the application has to opt in. For those of you running Linux servers, if you have the very, very latest Linux kernel as of a couple of weeks ago, that now supports the standard ITF, TFO, TCP option code, the same as OS X. So to wrap up what we would like you to remember from today's session, wherever you can, use the highest layer network API as possible and that way you get the full benefit of all the work that those APIs can do for you. You absolutely must be testing on a NAT64 network for your applications, and thankfully we have made it really, really easy for you to do that with just an Option click. Reliable network fallback will give your customers a better user experience with your applications. What you can do is pay attention to the Better Route notification so that you migrate back to Wi-Fi when it's available. Explicit Congestion Notification is a new feature for the seed. It will enable the Internet to move to this much more responsive mode of operation with lower queuing and lower packet loss. So we would like you to test that and report any problems. The TCP NOTSENT Low-Water mark is something that you can set for yourself as a socket option or get for free starting in the next seed, and that will reduce the amount of stagnant data buffered in the sending machine. And then finally, for the people excited about TCP Fast Open, we've made that available too. There's a bunch of sources of good documentation that you can look at. I'm not expecting anybody to write down these URLs; you can click those in the PDF version. There are some good forums where you can ask questions and have discussions with questions you have about networking. I encourage you to watch the videos of the NSURLSession presentation and the Network Extensions presentation and, of course, come and join us in person after lunch in the networking lab and we can answer all of your questions there. Thank you.  ELIZA BLOCK: Hi, everybody, I'm Eliza and with me is Paul Salzman. We will be telling you about complications, what they are and how you can make them. So, complications are small pieces of information that appear right on your clock face alongside the time. On these clock faces you see here, if we remove the time from the equation, everything left is a complication. So the ones you see here are all built into the OS, but starting in watchOS 2 you can create your own complications for the clock face and we will tell you how to do that, and I'm going to use the example of building a complication to display the upcoming matches in a soccer tournament. So let's take a look imagining you already built such a complication at what it would look like to go select it. So focusing in on the middle clock face here, if you were to Force Touch the screen, you can customize the face, swiping to the right allows you to start tapping on your different complications, and turning the Digital Crown allows you to pick one. So if we scroll all the way to the end of the list, we will see our not-yet-built soccer tournament complication and we could select it and we start seeing the live data displayed on the clock face. How are you going to provide the data to populate this complication? As you can see from looking at the face there is a consistent look and feel to all of our clock faces and that's what part of what makes them pleasant to interact with so we wanted to do this while preserving that consistency. So your extension which is running on the watch will provide the data for these complications in the form of text and images. And then the clock face will actually draw it in a way that fits with the rest of the face. So if I were to install this complication on my watch, I will see it right away every time that I raise my wrist. And that's a great thing, and it's one of the great things about building a complication, but it also presents a kind of a challenge because as time passes and the information that's displayed in this complication needs to update, since it's visible as soon as you raise your wrist, that update needs to already have happened by the time the screen turns on. And if you think about it, there could be five different complications showing on this single watch face. There is no way we would have time to launch all of them, pull for new data, potentially involving a network request and have all of them come back with data in time for the screen to come on so to solve this problem, what we're going do is collect the data for your complications in the form of a timeline in advance. So you will give us a timeline of data, how much of the timeline depends on the density of the data for your application. And that way as I glance at my wrist throughout the day, every time I look at it, the complication has already updated to display the information that makes sense at that moment. Now, timelines are a really powerful concept and Paul will talk about them later in the session, but I wanted to mention one other thing that this buys us, which is the Time Travel feature. So Time Travel is a new feature we introduced in watchOS 2 which allows you to turn the Digital Crown and right on the clock face you can see what your complications will be showing at different times of the day. So you could peek forward to see when does the next match start or what meeting should I have been at an hour ago and so on. So to see how that works with this complication, there is no extra work that you had to do in order to make this happen. By providing your data in the form of a timeline, we can just make Time Travel happen for free. Getting started, so when you make a new Xcode project or convert an old project over for watchOS 2, you can choose to add a complication. You can check that box, and that will cause Xcode to automatically generate the files that you need and set things up so it's super easy. There is one other thing you need to do. If you navigate to your WatchKit app extension's general info pane, you provide the dataSource class which we will talk about later, and you also provide checkboxes for the families of complication that you want to support. So now what are these families of complication? Complications on watchOS look different ways on different clock faces and we have divided this up into five different families and you can choose individually whether to support each one of these families so I want to show you what these families look like. This is the Modular face, and it has two complication families on it. The ModularSmall family gives you these square-shaped small complications, and the ModularLarge family is the one, one complication in the middle of the face that can show a fair amount more data so that's the one we have been focusing on with this example. A number of analog faces also support complications. And these complications are in the Utilitarian families. There is a UtilitarianSmall which kind of fit in the corners of various analog faces and then there is UtilitarianLarge which gets the entire region along the bottom. And finally, on the Color face, there is a single family which we have called Circular Small. Here is what this looks like in code. It's an enumeration named as you would expect. All right. So when the user activates your complication on a face, they are going to choose it for a particular position that the complication can appear and that position will be associated with a particular family. So you will be told that you are providing data for this family, and at that point, you need to decide how you want to lay that data out. And there is a number of ways that you can lay out your data for each one of these families and I want to show you the collection of templates that our designers have created. So for the ModularSmall complication family, we have these templates. There is a whole bunch of different ones. You can have small text, two different rows of text, an image and text, you can do a big piece of text, a big image. These ones along near the bottom here with a ring filling up can show you your progress towards something in the form of a floating point number that can change between 0 and 1 and you can put text or an image inside of that and finally there is a column template which allows you to do something like show a sports score. ModularLarge also has a number of different ways you can layout your data. There is a simple standard three-line template with an optional image. You can do a column-style template, two different column-style templates, actually. And finally you could do a template with a large piece of text which is suitable for something like a kitchen timer or a date, many other possible things you could do with that. UtilitarianSmall, most of these guys are flat and they have an optional image in the corner, but you can also have a larger image and you can also do this ring style with UtilitarianSmall. UtilitarianLarge, there is just one template for this. There is an image that's optional and some text next to it. And finally, CircularSmall has templates that are similar to the ModularSmall ones although with slightly different sizes. So that's the templates that you have access to in watchOS 2. I'm going to take a look at what that means in code. So let's zoom in on our soccer club digital ModularLarge template. This has four pieces. There is a header image that you can provide. Header text. And then there is two lines of body text. So you may have noticed that there is a lot of these complication templates. All of these correspond to a subclass of CLKComplicationTemplate which is the superclass of all of them, and you can choose which one you want to use and fill out its various properties so it's pretty simple. So this one is the CLKComplicationTemplate ModularLargeStandardBody which is a bit of a mouthful, but it conveys one really important piece of information right in the name, which is that this template is for the ModularLarge complication family. And it's really crucial when you are asked to provide complication data for a particular family that you provide a template that matches that family, and that's why we have built the name of the family right into the name of the class, so there can be no confusion about it. It obviously wouldn't work to produce a template that looks like this to appear on the circular face. Now, you may have noticed something else that's a little strange here. We have an image and three text properties but instead of UI images and NSStrings in the code, we have this imageProvider and textProvider business. So what's going on here? That brings us to our next section, how do you provide images and text, and I want to explain what these classes are for and what they can do for you. Let's start with images. Here is our complication in the color editing screen. So the user can customize what complications they see and they can pick your complication in the course of that. They can also customize the look and feel of the face. And that includes changing the color. So when you provide images for your complications, these images need to take part in that same color scheme that the user has selected for their face. So they need to be able to change color as you see. So there is our soccer ball obediently turning whatever color the user is choosing. So an image provider is a sort of a bundle of properties that manages to achieve this effect. So you can provide a background image and you provide it as a template image, an image that only contains alpha information and no color of its own. It can be -- the pixels can be whatever color you want, but we are only going to pay attention to the alpha channel. It will be colored depending on the user's selected color, but you can do a little bit more with this. You can also provide a background image and a foreground image and these will be laid on top of each other. The background image will be colored according to the color of the face and the foreground image will be superimposed on it so you can get a little bit more detail in your images. You can also choose to make the foreground image black. So let's take a look at the code for this. You give us a background image. You give us an optional foreground image. You can choose a background color for your background image. For the most part the color is going to be determined by what the user has picked so this background color represents what the color you would like your background image to be if you can choose and there are some contexts in which that would be honored, but as long as this is appearing on a face where the user chooses the color is will override the color you supply here so this is an optional property. Okay. That's image providers. So now what about text providers? These are really cool! So I'm really excited to get to tell you about them! When we started out building complications in watchOS 1 we had challenges which were mostly due to the fact that complications were tiny compared to all of the other UI you are used to creating for all of our platforms. Some of them are as small as 44 pixels square, and we are trying to fit information that's of use in this very small space, but it can be challenging to do that without having all of your text truncate. So a good example, so the idea here for text providers is that because the space is very constrained, you want to leverage some of the work that we have already done to figure out how to format and fit the text in the small space. So we are introducing text providers as a way for you to declare your intentions to us rather than always passing us an already formatted string and then we will handle the details of formatting and fitting that string for you. So an example is formatting dates. There is a CLKDate text provider that does this for you and I want to show you what that's useful for. Imagine you wanted to display the date, Wednesday, September 23rd in one of your complications. Now, space is really constrained so pretty much in most contexts you are more likely to see something that looks like this, which is obviously not very informative. We have lost kind of the bulk of the information. So instead of truncating, it would be better if we could fall back on increasingly narrow renditions of this string that were still informative. So, for example, we could start abbreviating some of the elements. You could abbreviate more of them, and if that still didn't fit, we could even start dropping some of the elements instead of truncating, preferable to see Sep23 versus Wed dot dot dot. And finally, if we had way less space to deal with we could drop down to displaying the day of the month. This is what CLKDate text provider will do for you. You have a date you want to display, you have units you would ideally like to display, in this case the weekday, the month and the day. You create a text provider from these pieces and then that very text provider can be attached to one of these templates and it will look different depending on the context. So here it is, in the ModularLarge complication displaying one of the longer renditions of this date. Here are two of these very same text providers produced with that very same code displayed with widths of various degrees of further constraint and here is the same text provider providing a date in one of these CircularSmall complications and there is no truncating anywhere and there is no work you have to do beyond the code that you see here. That's date text providers. There is other kinds of text providers as well. The one you are most likely to use most of the time is the simple text provider this allows you to provide arbitrary text. And but it's better than an NSString because in addition to providing the text you would like to see, you can also provide a shorter version and we will fall back on the shorter version before truncating the ideal version. There is a time text provider which formats times for you nicely. You get the nice small caps that match the rest of the system and will drop the a.m. /p.m. if there isn't room to fit it. As you see in the sunrise/sunset complication which is using this text provider at the bottom. There is also a time interval text provider. This text provider is good for formatting ranges of times. We use it in the calendar complication and it has some nice formatting features as well. It will look like this if the first date is in the morning and the second date is in the afternoon. If they are both in the afternoon, it's smart, and it drops the redundant a.m. /p.m. symbol to make this look nicer. It will also fall back on narrower versions if this rendition won't fit. These are useful and we encourage you to use them. There is one more I wanted to talk about now, and before I get to it, let me show you the problem it's solving. So here you see our moon phase digital ModularLarge complication. And at the bottom of that, the third row in that moon phase complication it's telling us the amount of time until the moonset in terms of hours and minutes. So the moonset is at 2:19 today, which is in 3 hours and 1 minutes from 11:18. So as time ticks by, this string changes at 11:19, it shows 3 hours, at 11:20 it shows 2 hours 59 minutes, and so on. Now, imagine if you were creating a timeline to populate this complication. You would need to provide a new template for every minute of the entire day. That's a lot of templates. And it could be even worse because that's the moon phase complication ticking down by the minute. What about the timer which ticks down by the second? Imagine how many templates that is. It's more than we could possibly reasonably cache and it's incredibly wasteful because these strings are derivable from two pieces of information. The date you are counting down to, and the date that it is now. And we know the date that it is now. That's what we are doing. We're a clock. So we wanted to give you a way to produce these strings without populating your timeline full of just a massive quantity of redundant information, that's what the relative date text provider does. Here is what it looks like in code if we were trying to produce the third line of text that you see here. We would get the date for the moonset. We would choose the units that we wanted to display and actually I will show on the next screen there is a lot of different units you can use here. We would choose a style, and, again, I will show you the possible styles. This one you see here is the natural style. You would make a relative date text provider out of these elements. And then you would just set that text provider as your body 2 text provider for your template and the clock face does the rest. It will always display the relative date to the date that you gave us at every given moment without you having to do any further work. So these are some styles that are available, and as you can see if you look at the natural and style and the offset style, you can get either fine grained relative dates or very course grained relative dates so depending on the date that you wanted -- depending on how far it is to the date you want to display, we can show weeks and days, months and years, so on, all the way down to seconds. All right. So that's text providers and image providers. So I want to sum up how it is that you are giving us your content. You choose a template from one of the number of possible templates, choosing one that matches the complication family you are being asked to provide data for. And then you populate that with image providers and text providers. And then you are going to hand us a whole bunch of those in the form of a timeline and to talk about more, to talk more about how to form one of these timelines I want to invite up Paul. Here he is. PAUL SALZMAN: Awesome! Hello everyone. So as Eliza mentioned we are going to be gathering our data for your complication in the form of a timeline. This helps to support two things: the brand-new Time Travel feature and we are going to able to present the user with your content immediately on wrist raise without any delay. Let's take a look at about how to think about timelines and complications. We are going to start by creating the weather complication up here on the bottom left corner. We are showing 57 degrees because right now when the watch is showing 10:00 a.m., our forecast says that 57 degrees is the temperature outside for this location. And in fact, we actually have a forecast by hour until 7:00 p.m. today for this location that we can take advantage of. For timelines we don't describe a range we associate the data we want to show with a point in time. So let's slide these over here. As the clock progresses throughout the day. We will show the most recent data you have provided on the timeline on the watch face. So as we get past 11:00 a.m. to 55 degrees. As we inch closer to noon at 11:59 and 59 seconds we are still at 55 degrees. Once we hit noon we will update the template. This works similarly for the Time Travel feature. As the user navigates throughout the day we will show the most recent data available at that point in your timeline. So as we get past 1:00 p.m., we're going to update your content to 54 degrees. The other complication is kind of an easy example because your data matches perfectly to unblocked time on the timeline. Let's look at something more complex by trying to build the calendar complication here. If you are lucky your calendar has plenty of gaps throughout the day. Today I'm going to go have brunch and later I will get a haircut and when I look better I will meet with friends and watch a movie. So we can be naive about this and associate the templates we want to display for these events for they begin and clear them out when the event ends. Let's see how this works in practice. So at noon today we will show that we are at a brunch, but once brunch is over and it's 1:00 p.m. we no longer have content displaying on the wrist watch. That's a bad user experience. What's worse though is we get closer and closer to 4:00 p.m. I have no idea I need to get into my car and get a haircut. So you never want to tick off the person that's doing your hair. So now it's too late and I'm going to get a perm and it's not going to work well. Let's fix this. The first mistake is assuming we should have blocks for unused time frames in the timeline. So let's get rid of those. And showing an event isn't useful for a calendar complication. We should put the templates at the end of the previous event so you have adequate time to get to your next event. So the first event over here actually, there is no previous event so it might be useful to actually tag it at midnight to you get adequate warning in the morning when you wake up and on the right we should let users know there is nothing they should be worried about for the rest of the day with an indication there is no more events. How does this look? At noon just like before we will know we are at brunch but once brunch is over we have adequate heads-up to know we need to get a haircut. We will meet with our friends because we didn't miss any events and when we are done watching a movie we can go home with knowing we didn't miss out on anything else. So how do you get the data points into your code? You will use the complication timeline entry. So Eliza described earlier generating templates using text and image providers and all we need to associate with that is an NSState. We will stuff the objects into the CLKComplication timeline entry. When you hand that to the clock face we will inspect the date and make a note on the timeline so we know to display the template when you reach that time. You can see in code what the object looks like with the Date property and complicationTemplate property. So how do you actually communicate this data to us? You will have an object in your project that implements the CLKComplication DataSource protocol. This object is annotated in your Xcode WatchKit extensions target settings as Eliza showed you during setting up your project. There are a bunch of callbacks you will get on the object. They are generally per complication. And we will pass you a CLKComplication object that has a Family property you will want to switch on. At this point you will decide is this ModularLarge? Which template should I use? Is this ModularSmall? Which text providers go with the template I'm choosing for that? And in addition to passing the complication that you are interested in providing content for, we are also going to give you a handler. And you use this handler to give us the data we have requested and let us know you are done running. This is very important because you are going to have opportunities to refresh your content in the background. We want to know when you are done running. So let's start building up our timeline. You will see we have our clock face on the left, your extension on the right and the timeline we want to build across the bottom. Inside of your extension you will have your complication controller object. Now, this is the default object that Xcode will create for you that implements the ComplicationDataSource protocol. This is the object we are going to be communicating with. So to get started, we are going to ask you which timeline entry should we be displaying right now. So you will package up a timeline entry and send it over the wire via the handler, and then we're going to add it onto the timeline right then. What's important to note with all of these questions we are asking you is that we are basing the next questions on the information you have given in the previous one. So we are going to be incrementally building this timeline out. We want the data to be super accurate. You don't want to blindly tag the current NSDate for this entry. If it's 10:30 a.m. and we want 10:00 a.m. forecast data you should tag this complication timeline entry with 10:00 a.m. So the function we will ask is GetCurrentTimelineEntry ForComplication. Again, we are going to pass you a CLKComplication. This has a Family property you should switch on to decide which template, which text and image providers you want to supply, and a handler that takes the timeline entry that you should call when you are done with this callback. So now we have your current entry and we need to start fleshing out your timeline to the future and the past. Let's start looking to the future. We are going to ask you incrementally what timelines you have after a specific date. This date will generally be based off of previous data you have handed us and what's still in our cache. So we will hand you this date and you will package up an array of CLKComplication timeline entries that start after this date non-inclusively. A good rule of thumb is charting a day's worth of data. When you send this data over the wire, we will add it to your timeline and if we feel we need to cache more data we will ask you again. In which case if you have more data to give us you happily provide the array. Let's say we keep asking you and there is no more content to show. You can pass an empty NSArray or nil and that will give us the hint to leave you alone. As time progresses all of your entries will be further and further into the past so it's possible to increase our cache, we might have to query you again in the future. So the future we are calling here is getTimelineEntries ForComplication. We are always going to pass you the complication we are curious about, so look at the Family property. And then, of course, we are going to give you the after date. This is the non-inclusive date. You will package up the adjacent forward-looking timeline entries from this date. And to make sure we don't get overloaded with data, we will pass you a limit parameter here. So don't put any more content into the arrays than this limit provided here. If you do, we will remove them in a way we are not going to guarantee won't change and you are not going to find out what that means. And then, of course, a handler that takes this array. And corresponding to go into the future we have the before date feature of this function that helps flesh out the past. So depending on the type of complication you are providing your needs for Time Travel may vary. Whether a complication that only provides future forecast doesn't want to Time Travel into the past and a stock complication can only Time Travel to the past because we haven't perfected looking into the future for that. So we will ask you on setup which directions you support. So for the case of our weather complication, we will say we only support the future and that way our timeline will only look forward. If the user Time Travels into the past we'll actually dim out your content so they know there's nothing there to look at. Similarly for the stock complication, you may say you only provide the past. And we will dim in the other direction. It's possible that you don't want to support Time Travel, but you still want to get contents onto your watch face. In that case, you want to supply none as an option. We will still show your content but as soon as we enter Time Travel we'll actually dim it out. It's important to note that you might know some of the content you want to show in the future. We will ask you, even though you're not supporting Time Travel forward, for data we might possibly be able to cache. We will never ask you about the past because right now time doesn't travel backwards. And if you want to a truly bi-directional Time Travel experience you can support forward and backward. So the function we will call to get this information during setup is getSupportedTimeTravel DirectionsForComplication. We will pass you the complication we are asking about as well as a handler that accepts these directions. Now, giving an indication to what directions you support may not be the full story for your Time Travel complication. For instance, our weather complication only had a forecast up until 7:00 p.m. but Time Travel goes beyond that. So let's take a look at what happens. As we Time Travel forward to 4:00 p.m. we still have data. All of our complications are updating. But once we get to 7:09, just past 7:00 p.m., we don't want our users to think we have valid data for the temperature in the area so we will dim out your complication here. So the way that we figure out when to actually dim out your complication is by querying you for the end or beginning of your timeline depending on the directions you support so we will ask you during setup how far out into the future if you are a forward complication, and you will give us an NSDate back. If you support Time Travel to the past, similarly, you can provide us another NSDate to this question and we will adjust accordingly. We will adjust accordingly. So the function we are going to call to find out how far into the future we should Time Travel is getTimelineAndDate ForComplication. We will pass you the complication and, of course, a handler that accepts the NSDate for the end of your timeline. Correspondingly, we have getTimelineStartDate to see when your timeline should begin. So when a user is customizing the complication, the watch face and they want to select your complication, they will select the slot in this case ModularLarge and use the Digital Crown to end up on the San Francisco soccer club complication. You will see here a couple of things about the state of customization. There is a caption but your complications entry that says your application's name and this is provided by the system. Now, that we are in the modular large slot we are able to provide a ModularLarge template that describes what our users expect to see and give them the right context for why they should select our complication. After they select the complication, go back to the switcher and back to the live face. We will start querying you for data and populate the timeline. We call those templates we present in customization placeholder templates. We will query you once on installation and cache that so we don't have to launch multiple extensions during customization. So during installation, for all of the complications you support, we will query you for your placeholder template to which you provide us a CLKComplicationTemplate. There is no timeline on the bottom, this isn't happening live while we are using it. And we are not using a timeline entry in this callback right here, it's just a complication template because there is no date to associate. So the function we are calling is getPlaceholderTemplate ForComplication with a complication and as you are used to with this pattern, a handler. So now that you are very comfortable constructing your timeline, I would like to bring Eliza Block up to give you a demo of how to construct this in code [applause]. ELIZA BLOCK: Okay. So I have got a project here that I haven't done very much to yet. We are going to actually build the complication that we have been showing you pictures of throughout the session. So let me just show you a little bit about what's going on with Xcode project. I created a new project, I dragged a couple resources in, including a model that I had previously written to provide this schedule. And I configured it to work with complications, so if I navigate over here to my -- oops! General settings for my Watch extension. There it is. If I zoom in, you can see at the top that I have got my dataSource class. It looks a little ugly in Xcode at the moment but if you click it you can see it is pointing to our complication class. And then I have, for now, unchecked all of the supported families except for ModularLarge just to make things simpler in the demo we will just focus on building the ModularLarge complication. We recommend that you do try to support as many of these different families as you can because your users will be interested in choosing different faces and so the more of these families you support, the more likely they are to be able to use your content in their watch face. So when you created this Xcode project, Xcode and say that you want to support complications, Xcode actually makes a complicationController object with stubs for all of the methods that you need to implement. And this is pretty handy. We can go through it and just flesh it out and have it give us the information that we need. So I'm going to start at the bottom here. I'm going to add some extra space so we can see. At this getPlaceholder TemplateForComplication. I want to do this first so that we can actually go ahead and pick the complication and have it look the way that Paul just showed in the slides. So I'm going to remove the boilerplate that Xcode provided. And we are going to make an actual complication template of the ModularLargeStandardBody class. I'm going to give it an image which is my soccer ball image. Now, so I have got my image as a UI image and I'm creating an image provider using that image and I will not bother with the background color for now. Let me show you what that image looks like. I have it here in Xcode and I can actually open it up in the Finder. And here is Preview. Let me zoom way in. So as you can see, this is a template image. It's monochrome. It has alpha, an alpha channel so this is the format you want your images to be in so they can get properly colorized. Okay. So there is our image and we also need to provide some text. So my header text provider is going to say "Match schedule" and my body 1 text provider is going to say "2015 Women's Tournament." I will not provide a body 2 text provider, that's optional for this template. And my goal here is for the text in the first line to wrap to the second line which will happen if you omit the second text provider. We can build and run this with only that much code added. And then we should be able to pick our complication in editing. So I will switch over to the simulators. And let me just double check that the installation happened. Yes, there is our app. That's good. So I'm going to force press on the simulator, customize, scroll over -- oops! To the complication pane. Actually, you know what I'm going to do first -- as Paul mentioned we only call this placeholder template method once, when your app is first installed, so if you do something like that, you are going to want to uninstall it so that then -- oh, this is -- make sure that it's gone from the -- oops! There we go. So make sure it gets uninstalled. It's gone. And then we will have it show up again. So that way our code will ask you again for the placeholder template. So let's try this again. So let's have it not be there and then -- all right. We will add it back in. And let's try this one more time. Yay! Okay [applause]. All right. So we have our template and now we can actually select the complication. So I'm going to go ahead and home out of there, and as you can see the complication, of course, is not showing, switching to showing live data because we haven't written the part that provides the live data. So the -- so it's just going to stick with the placeholder template which is all of the information it's got so far. Let's go back to the code and we will add the part that actually implements the rest of the protocol methods. Okay. So let me show you about, a little bit about the model that I'm using. I will just switch to the header. I have got a model and written my model in Objective-C. You can mix and match Objective-C and Swift in projects so you can take code written in an existing app and pull it over into your new watchOS 2 app and as long as you include the header for that in your Swift bridging header, it will just work, which is cool. So here is my soccer match model object. And these guys have a date which is the date that the match begins. They have a team description which tells you who is playing, and what group in the tournament this is. And also the model can tell me what the first match in the schedule is, what the last match is and then each match can tell me the previous and the next. So with that handy, we can start writing the code that's actually going to populate this complication. So I'm going to write two helper methods to solve the two problems that we need to figure out, basically the two design questions that we need to figure out for this complication. The first one is what should our template actually look like? And for that, I'm going to write a method that takes a soccer match and provides a CLKComplicationTemplate object. So this is pretty straightforward. We want to build a -- all is well -- build a ModularLarge standard body template like we did for the place holder and get that same soccer ball image provided as an image provider and then we are going to have three lines of text for this one. So the header is going to be the time of the match. And I'm using a CLKTime text provider for that purpose. And then the first line of text underneath the header will tell us which teams are playing and I'm using a simple text provider with my matches team description, and then finally, the third line is the group description. So now we have got a template. We also have to decide how we are going to arrange these templates on our timeline. Now, the naive solution which Paul mentioned in the context of a calendar complication would be to use the match start date to be the date of our timeline entry, but that would have the drawback that it has for the calendar as well which is you wouldn't be able to look at your complication to see what game is about to start. You would only be able to see what game has already started. So we actually want to do the same thing Paul did with the calendar and move all of these entries farther forward. We will have each entry start at the time when the previous match ended. So for that, we have to decide how long a soccer match is, so I have decided that they are about 90 minutes. So I'm going to use a constant that we could change later if we discover we are wrong. So the match duration we will say is 90 minutes. And then we can write a method timelineEntryDateForMatch that goes and figures out for any given match where that should appear on the timeline. So what we will do is we will say, okay, match, what match is before you? And if there is one, then we are going to use the end of that match as the timeline date for this match. So if we have one, we will return its date incremented by the duration of a match. And if we don't, that means it's the first match in the schedule and I have pretty arbitrarily decided that I'm going to start displaying the first match six hours before it starts but you could obviously do something different here depending on your use case. So those are the two methods that are kind of doing the meat of the work. Now, we just need to go through and implement all of the different protocol methods that Paul described. So let's start from the beginning. So here we have the timeline configuration and the first method is getSupportedTimeTravel DirectionsForComplication. The default Xcode template here is saying that your timeline extends both forward and backwards and that's what we want in this case so we will leave that one alone. The next thing we have to think about is when our timeline starts. So because we have written these helper methods we can do that pretty easily. I will get rid of the handler with nil and instead we will figure out a start date which will be the entry date of the first soccer match. And let's not forgot to call the handler. And then the next thing we need to do is figure out the end date. So the end date of the timeline should probably be after the last match is over. So we will get the last match. We will get its date, and we will add the match duration to that date and that's going to be our timeline end date. We will call that with a handler. This next method is something we haven't actually gone into at all in the session, but it's an important method if your data is in any way sensitive. So when your user's Watch is locked, you don't want to be showing sensitive data on the screen because that's means they have taken it off their wrist and somebody else could have found it. If you are showing sensitive data in your complication, you can use this method to tell us to be sure to not show that data when the device is locked. Now, the schedule of a fictional soccer tournament is not particularly sensitive so I will leave this at default value of go ahead and show this on the lock screen no problem. So next we get to the timeline population. These are the really important ones. We need to give the current entry. We need to tell us -- we need to tell the clock how to extend the timeline backwards and we need to tell it how to extend the timeline forwards. I will start with forwards. So let's skip down to here. And what we want to do is construct an array of entries starting at the date we were given and going forward into the future from there. My strategy is going to be to make an array, and arrange to call the handler when we are done. And then we want to iterate through all of the matches until we get one that should start after the date at which point we are going to start creating the templates for that and sticking them in this array. So I will start with the first match in my tournament. And I have made this an optional, not because the first match might return nil, but because we are going to change this to represent each subsequent match and eventually we will run out of matches it will eventually take on the value of nil at which point we will know to stop. So next we are going to -- oops! While there is a match here, we are going to get the date that we should display that match at. That's the timeline entry date for this particular match. And now we are going to compare that to the date we were given, which is the date after which we are supposed to be populating timeline entries. If they are order descending we have gotten into the right section of matches and now we want to start giving back timeline entries for these. So we are going to populate a timeline entry. It's straightforward. We make a template for the match. That was our other helper method. We create an entry from the entry date that we computed and that template. We append it to our array, now we need to be careful not to make too many of these. We have been past a limit. We want to adhere to the limit. So we will just check now that we have added something to the array, did the count of the array reach the limit, and if so, we will stop. And finally, to make the loop work, we need to go and grab the next match after the match that we just dealt with. So that's going to populate the next N entries in the timeline after the date that we were given, and we can do something really similar to populate the earlier entries. So I'm just going to copy that code that we just wrote. And here I'm going to move up to the getTimelineEntries ForComplication before date method. Paste it. And we need to make three changes. Here we are going to do the exact same thing except we are going to start at the last entry and move forward or the last match, rather, and move back towards the first. So we will start with the last match and then down here where the loop happens we grab the previous one. Previous. And finally we only want to start using these when they get before the date we were passed. So we need to switch from order ascending to order descending. That's our extension methods. The last thing we need to write is the getCurrentTimelineEntry method and we can do something tricky and take advantage of the method we just wrote because getting the current entry is basically getting the entry before a particular date, namely now. That's the one we want to show currently. So what I'm going to do is call the GetTimelineEntries ForComplication before date method that we just finished writing. I'm going to pass now as the date. I'm going to pass a limit of one because we only need one entry here. And then when I get my handler invoked, I'm just going to grab the first entry in that array and pass it back to the handler for the getCurrentTimelineEntry method. So we can go ahead and run this, and what I'm going to do is I'm going to run it and then switch quickly over to the simulator, which as you can see is still running here and still showing us the placeholder template we populated earlier. So as soon as I run this, it will invalidate the timeline on the simulator, at which point it's going to go requery all of these methods again and now that we have implemented them, we will actually get values. So here we go. Run and then swap over. And then we should see our stuff populate. So there we have actual data. This is showing us - So this is showing us the game that started at 11:00, which is the right behavior if you remember, because we wanted to see the game that started at 11:00 until 12:30, 90 minutes later. If we Time Travel forwards and reach 12:30, we should start seeing this change to the next game. So Time Traveling is working if we get all the way up to 3:00, we should start seeing the one after that. Oops! I went way too far. Our Time Travel is working. All we had to do was basically fill out the three most important methods and we have a functional complication. So I will pass it back over to Paul who will tell us more about how to arrange for your complication to update as information changes in the world. PAUL SALZMAN: Thanks Eliza. So now that we are up and running with our complication. We want to make sure we are always showing something that is accurate to the world around us. So in watchOS 2 there is a lot of ways you can get contents from the surrounding world into your Watch extension. You can use the new Watch Connectivity APIs to talk to your companion iOS app and get data onto your Watch. Or use NSURLSession directly to talk to your web services and bring content onto the Watch. So let's say we have our complication timeline built up and we go out and talk to our web services and get a new piece of data. If that data has invalidated our content, we will need to tell the clock face we want to reload our timeline. So within our extension, we can get access to the CLKComplicationsServer object. That object is our interface into the clock face. We can make a request to the clock face to say please reload my data at which point we are going to throw away all of your existing content and start our communication channels over again by finding out your current timeline and flushing things out from there. You might already notice that this is a pretty destructive action. If you had a stocks complication, where all of your previous data is still valid. The clock face isn't querying you right now. It's actually your responsibility to let us know we either need to invalidate or possibly extend your content. So instead of getting rid of this content you can make a request to extend. At which point instead of asking you to reload everything, we are going to ask you to append data at the end of the most recent content we have available from you. So how does this look in your code? You can get access to the complication, the CLKComplicationServer shared instance. On the shared instance, you can actually query for all of the active complications. An active complication is actually visible right now on your watch face if you were to wrist up. And given a complication, you can actually make a request to the server to either extend the timeline or alternatively reload the timeline. So that's great. We know it's our responsibility to inform the clock face we need to update, but when do we actually have an opportunity to do this? Well, basically any time you are exception is running you can talk over the CLKComplicationServer to the clock face. This happens in a couple of instances, like when your watch application is foremost. But you have some opportunities to run in the background via a locally requested wake or even from your iOS companion application using some new Watch Connectivity APIs you can actually wake the extension from the phone so it can receive the data you have sent over. But because the two last calls allow you to run in the background we have to budget them. If you do a lot of expensive work in the background calls you can exhaust your budget and until your budget is replenished you may not have a chance to update your complication until later in the day. So learn more about the Watch Connectivity APIs please go to the Introducing Watch Connectivity session. There is also cool push functionality we have added in this release to support complication data. Let's talk a bit more about locally scheduling background wakes in order to get your complication data up to date via one more call on the complicationDataSource protocol. All you are going to supply to us is one date via the handler. And we are going to make this call across all of your complications, not by complication. When we receive this date, we are going to take this as a hint, and when budgetary constraints or system conditions are good we'll launch you in the background. At this point your data delegates from Watch Connectivity and NSURL can come in and it's your responsibility to verify if anything has changed and make any requests you need to make to the clock face to update your content. At this point that's wrapping up our session on complications. We hope you have learned that to be comfortable with building up a timeline and supplying us with templates and the appropriate providers and to take advantage of all of the hard work that went into the watchOS to actually form and fit your content in these text providers, to be comfortable refreshing your data as the world around changes you, you will get more opportunity to run if you are a good citizen and do less work in your background refreshes. For more information please refer to your documentation and sample code. We have good technical support and fantastic evangelists. There are great sessions to dig further into WatchKit, thank you!  RAV DHIRAJ: Good afternoon. Welcome to WWDC 2015. RAV DHIRAJ: And the first of two What's New in Metal sessions. So we have a lot of Metal content to cover this week across three sessions. In fact, we've added so much new stuff to the API, that we've decided to break the What's New in Metal session into two parts. So today, I'll be focusing on providing a high-level review of the Metal ecosystem over the last 12 months. Developers like you have already created some tremendous applications using Metal. I'll then talk about some of the new features that we're introducing this year, and we'll end with a specific example of how Metal integrates well with the rest of the system by describing a technology called app thinning. In the second what's new in Metal session, Dan Omachi and Anna Tikhonova will provide details on a great new support library that we're introducing in Metal this year or two great new support libraries rather, MetalKit which provides convenience APIs that allow you to create a great Metal application, and Metal Performance Shaders our highly optimized library of shaders that you can call directly from your application. And finally in the last session, Phil Bennett will dive into great techniques for taking advantage of -- for extracting the best possible performance out of your Metal applications. We'll be introducing our new GPU System Trace tool in this session, so be sure to check it out. We introduced Metal at WWDC last year for iOS 8. Our goal was a ground-up reimplementation of our graphics and compute APIs to give you the best possible performance on the GPUs in our platform. So we achieved this by getting much of the software between you and the GPU out of your way. To better illustrate this let's look back at an example that we showed last year at WWDC that describes the work done by the GPU and The CPU per frame. In this example the top bar represents the time spent by the CPU and the bottom bar represents the GPU time. So as you can see, we're currently CPU bound and the GPU is idle for part of the frame. So with Metal we're able to dramatically reduce the GPU API overhead and effectively make the GPU the bottleneck in the great frame. So the great thing is that this allows you to take advantage of this additional CPU idle time to make your game better. You can add more physics or AI for example, or you can issue more draw calls to increase the complexity of your scene. But we didn't just stop there. Metal also allows you to move expensive operations like shader compilation and state validation from draw time which happens many thousands of times per frame, to load time which happens very infrequently, and even better in some cases to build time, when your users don't see any impact at all. Additionally, with iOS 8 we not only introduced compute or exposed compute for the first time on our iOS devices, but we also provided you with a cohesive interoperability story between the graphics and compute APIs allowing to you efficiently interleave render and compute operations on Metal capable devices. And finally, with Metal, your application is able to make efficient use of multithreading without the API getting in your way, allowing you to encode for multiple threads. And the results have been stunning. So last year we showed you Epic's Zen Garden demo where they used Metal to achieve ten times the number of draw calls in the scene. We also showed you EA's Plants Versus Zombies technology demo where they used Metal to bring their console rendering engine to the iOS platform. Now this set a high bar for the development community. And over the last year we've witnessed the release of some truly astounding titles that have taken great advantage of the Metal API. Titles like the MOBA Vainglory by Super Evil Mega Corp that used Metal to achieve 60 frames per second in their game. Disney's Infinity: Toy Box 2, Metal enabled them to bring their console, graphics, and game play experience to iOS. Gameloft Asphalt 8, they were able to improve their gameplay by using Metal to render three times the number of opponents in the game. But it's not just about games. With the new version of Pixelmator for the iPhone they're using Metal to accelerate image processing in their powerful new distort tools. In fact, the response has been overwhelming, with a number of key content and game developers now adopting Metal on OS X. And much of this content has been enabled by our commitment to bring the leading game console engines to the iOS platform. This includes Unity, Epic's Unreal Engine 4, and EA's Frostbite mobile engine. Last year we also showed you how Metal fits in the big picture of how your application accesses the GPU. On the one side we have our high-level 2D and 3D scene graphs APIs that give you incredible functionality and convenience. And on the other side with Metal, we provided a direct access path to the GPU. So this gives you an amazing range to do what's right for your application, and if you choose to use one of the higher level APIs, the great thing is that we can make improvements under the covers and you can benefit from them without us changing a single line of code. Well this year, we're happy to announce that we've done just that, and we're bringing the power and efficiency of Metal to the system-wide technologies. We really believe that this is going to improve the user experience on our platforms. This is also a great year for Metal capable devices. The iPhone 5s and the iPad Air were the headliners at WWDC last year, and with the introduction of the iPhone 6, the 6+, and the iPad Air 2, we now have an incredible install base of Metal capable devices. But of course we didn't just stop there. We're happy to announce that we're bringing Metal to the OS X platform. We have broad support for Metal across all our shipping configurations. In fact, Metal is supported on all Macs introduced since 2012. This of course means that we have support for all three GPU venders: Intel, AMD, and Nvidia. And the other big news is that we're bringing all the tools that you're familiar with using on iOS to the Mac platform as well. This includes the Frame Debugger, the Shader Profiler, and all our API analysis tools. This is huge. We understand the challenges of debugging complex graphics and compute applications, and think that these will be invaluable in your development efforts on OS X. And of course, all of this is available in the seed build of OS X El Capitan that you can download today. So Metal on OS X is the same API you're familiar with using on iOS with a few key additions. With new APIs to support device selection, discrete memory, and new texture formats, Metal makes it incredibly easy for you to bring your iOS applications to OS X. And here are a few examples of developers who've done exactly that. So you heard in the keynote that we've been working with Epic to bring their iOS Metal development code to a Unreal Engine on the Mac. Well, Epic used Metal and their deferred renderer to create this amazing stylized look in Fortnite. Additionally folks at Unity brought up their engine and demonstrated their Viking Village demo in only a few weeks. It's really great to see this content on OS X on Metal. And we've been working with a number of additional Mac developers to enable them to access the power of the GPU through Metal. So you also heard in the keynote about digital content creation applications. Developers like Adobe, they're using Metal to access the GPU to accelerate image processing. The guys at The Foundry have also been using Metal to accelerate their 3D modeling application MODO. And here today to talk about their experience adopting Metal in OS X is Jack Greasley from the The Foundry. Welcome Jack. JACK GREASLEY: Thank you Rav. Hi. I'm Jack Greasley. I'm head of new technology at The Foundry. And at The Foundry we create tools for digital artists. Our software is used around the world in games, movies, TV, and film, including some photo real Peruvian bears, mutant monster hunters. But it's not just about the virtual. Some of our design customers like Adidas actually make things and if you ask a designer they'll tell you that any product is a result of thousands of little experiments. We understand this process and we create our tools specifically to support it. MODO is our premier 3D modeling animation and rendering system. It is used to make games, films, product design, lots of different things. Our users create stunning imagery and animations for things both real and imaginary. In our latest version of MODO 9.01, we revamped out GPU renderer, the aim was to provide a fluid interactive experience with a highest possible quality to designers. The benefit of this is that if your viewport is realtime you can make tens of decisions in the time it would take to do a single software render. We had already done some early work with Metal in iOS, but a couple of months ago we got a great opportunity to start working with Metal on OS X. So we put together a small team and set them a challenge, we gave them four weeks to see how much of the new MODO viewport they could bring over and get running on Metal. And we almost immediately got some stunning results. Although it's only a small triangle, it actually represents a huge milestone for us. Once we did that, we were able to very, very quickly make progress. And our plan of attack was to really work from the bottom up, and to start bringing the functionality from our new viewport over onto Metal. So here on day one we started with the environment. We added a few more triangles. A little bit of shading started to make this look a little bit more like a real car. Putting in the soft shadows and specular highlights really added a little bit of bling, and everybody loves shiny. And so here we are, four weeks later, and you remember that single triangle? We got some incredible results. Putting this all back into Metal gave us a fully functional view port running inside of MODO on Metal in just four weeks. One of the great things for us, is this gives us a standardized renderer across iOS and OS X, we created a WYSIWYG workflow between the two platforms. So, what did we learn? The first thing we learned is that working with Metal is fun. I've spent 20 years working with OpenGL, and I can tell you having a nice lean easy to use API is like a breath of fresh air. Secondly, the debugging and optimization tools in Metal are absolutely fantastic. As I have said, if you have done debugging on GPUs, you know why this is important. Metal can also be really fast. In some of our tests, we got three times speed-up, and that's using exactly the same data on the same GPU. Going forward we have some big plans from our new viewport and we're actually looking to integrate it into all of our tools across The Foundry, so hopefully we'll be seeing the Metal cropping up in interesting places very, very soon. So I'm going to hand you back to Rav, and thank you very much. RAV DHIRAJ: Thank you Jack. That was fantastic. Okay, so I'd like to now talk about the new features that we're introducing in iOS 9 and OS X El Capitan. And there is a lot of them. This is a just a selection of the features we've added this year. Now I don't have time to talk about every single one of these, so I'm going to focus on a subset, including GPU family sets, our new memory model, texture barriers, our expanded texturing support. Of course, as I mentioned before, you can learn more about MetalKit, Metal Performance Shaders, and our new Metal System Trace tool in the sessions later this week. So let's dive right into them. I would like to start with the GPU, our Metal feature sets. Metal defines collections of features that are specific to generations of GPU hardware. Metal calls these GPU families. So a GPU feature set is defined by the platform, iOS, or OS X, the Family Name, which is specific to a hardware generation, and a version that allows us to augment the feature set over time. It's really trivial to query the feature set, simply call supportFeatureSet on your Metal device to determine if that GPU family is supported. So here is our iOS feature set matrix. Now you'll notice that we have support for two major GPU families and versioning to differentiate between our iOS 8 and our iOS 9 features. On OS X the GPUFamily1 v1 feature set represents the features that we're going to be shipping in OS X El Capitan. This defines the base for a Metal capable device on the desktop platform. Now I would like to talk about two new shader constant updates APIs that we're adding. First a little bit of background. So for every draw that you encode into command buffer there's some constant data you need to send to the shader. Now it will be incredibly inefficient for you to have a separate constant buffer per draw so generally most Metal applications allocate a single constant buffer that they have per frame. They then append the constant data into the buffer as they encode their draws. So what does the code look like? Well, first we have some setup for the constant buffer and the data. Then just like in that diagram, within your draw loop, you send in the new constant data or you pen the new constant data into your constant buffer.