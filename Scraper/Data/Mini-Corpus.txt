 Hi, everybody. It's great to see you all here today. I'm Dave, and the next 40 minutes are about understanding and honoring what makes our programs actually work. There will be practical advice, but this is not a talk about tips and techniques or any specific algorithms even, though we will look at a few. It's about revealing something fundamental, the potential of which is already present in your code. I hope that for at least a few of you it marks the beginning of a new relationship to the practice of programming. Speaking personally, when I discovered this approach, it changed the course of my life and my career. It's the reason I care so much about software libraries, but it's also the source of the liability, maintainability, and performance in every piece of concrete code I've written since. But before we get into that, let me introduce you to a friend of mine. This is Crusty. Crusty is old school. He doesn't trust debuggers or or mess around with integrated development environments. No, he favors an 80 x 24 terminal window in plain text, thank you very much. Now, Crusty takes a dim view of the latest programming fads, so it can sometimes be an effort to drag him into the 21st century. He just thinks different. But if you listen carefully, you can learn a thing or two. Now, sometimes his cryptic pronouncements like "programming reveals the real; border on the mystical." And to understand him, I found it helpful to actually write some code. So, lately, I've been working on a little program called Shapes. I'm hoping to make it into a full-featured vector drawing program, but so far it lets you arrange shapes on an infinite canvas. Now, I want to tell you the story of the delete selection command because I learned so much from implementing this one feature. I think we've probably all gone through part of this progression as programmers as we learn how to remove things from an array. Everybody starts out doing something like this. That's the delete selection command. We loop from 0 to count, and when we find something to delete, we [inaudible] remove that, and then we continue with our loop until, ouch, we walk off the end. The array got shorter, but we picked the number of iterations when the loop started. Fortunately, you can't miss this bug if you Swift and test your code because it'll trap. But if you had to learn this lesson as a C-programmer, like I did, you might not be so lucky. Okay. So, we can fix it by replacing the for loop with a somewhat uglier while loop, which lets us examine the count at each iteration. But there's a subtle bug in this one too. If two consecutive elements are selected, it'll remove the first one and then immediately hop over the next one. Now, this bug is a little more insidious because it hides from you unless your tests happen to exercise it. But if we're lucky enough to notice it, we press ahead and we fix the implementation again by guarding the increment in an else block. So, are we done now? Are we sure this one is correct? I think I can prove to myself that it works. Anyway, having gone through this ordeal, what do we do? Well, of course, we lock this nine-line pattern into our brains so we can trot it out whenever we have to delete something. Now, I'm sure many of you have been holding yourselves back from screaming at me because that there's a much more elegant way to do it. I still remember the day I discovered this trick for myself because once you find it, you never do this nine-line dance again. The iteration limit and the index of the next item to examine kept shifting under our feet because remove(at: i) changes parts of the array after i. But if you go backwards, you only iterate over the parts of the array that you haven't changed yet. Slick, right? And this is the pattern I've used ever since because it's clean, and it never fails, until a few months ago. One morning, I had just finished my avocado toast, and I was idly fooling with my app when I tried deleting about half the shapes from a really complicated canvas. My iPad, it froze up for like three seconds. So, I took a sip of my half caf triple shot latte in its bamboo thermal commuter cup, and I considered my options. This was disturbing. I mean, it's a pretty straightforward operation, and my code was so clean, how could it be wrong? Profiling showed me that the hot spot was right here, but beyond that, I was stumped. So, just about then, Crusty walked up behind me carrying a can of off-brand coffee grounds from the local supermarket for his daily brew. "Stuck?" he said. "Yeah," I sighed, and I explained the situation. "Well, did you even look at the documentation for that?" Well, I hadn't, so I popped up the Quick Help for Remove At, and Crusty leaned in. "There's your problem, right there," he said, leaving a smudge on my gorgeous retina display. Now, I carefully wiped off the fingerprint with a handcrafted Italian microfiber cloth, and Crusty said, "What's that tell you, son?" "Well," I said, "it means that removing an element takes a number of steps proportional to the length of the array." And it kind of makes sense, since the array has to slide all of the following elements into their new positions. "So, what's that mean about your Delete Selection command?" he asked. "Uh," I said. That's when he pulled out a pack of mentholated lozenges and lined them up on my desk. "Try it yourself." So, I went through the process as I tried to answer his question. "Well, since Delete Selection has to do order n steps, once for each selected element, and you can select up to n elements, the total number of steps is proportional to n squared." Crusty added, "That's quadratic, son, whether you do it the ugly forward way or the fancy pants backward way." I realized then that for my little 10 to 20-element test cases, we'd only been talking about a few hundred steps, and since the steps are pretty fast, it seemed great. But the problem is that it doesn't scale well. Fifty squared is 2,500 and 100 squared is 10,000. So, if you do all your testing in this little region down here, you'll probably never see it, but scalability matters because people are using their phones and iPads to manage more and more data, and we keep shipping devices with more memory to help them do it. You care about this because scalability is predictability for your users. So, now, I understood the problem, but I still wasn't sure what to do about it. "Now what?" I asked Crusty. "You know, kid," he said, popping a lozenge, "there is an algorithm for that." I told him, "Listen, Crusty, I'm an app developer. You say you don't do object oriented. Well, I don't 'do' algorithms." You pay attention to your data structures in algorithms class because you know when it's time to get a job, your interviewer is going to ask you about them. But in the real programming world, what separates the elite from the newbies is the ability to wire together controllers, delegates, and responders to build a working system. "Bonkey," he said; I don't know why he calls me that, "What do computers do?" "They compute." "Now, where's the computation in all that?" "Well," I replied, "I guess I don't see anything that looks like an algorithm in my code." But Crusty wasn't having it. "Oh, your app is full of them," he said, dropping an old dead tree dictionary on my desk, "Look it up." After I gathered my composure, I carefully slid the book to one side and typed Define Algorithm into Spotlight, which Crusty thought was a neat trick. Hmm. A process or set of rules to be followed in calculations or other problem solving operations. Well, come to think of it, that did sound like most code, but I still wasn't sure. "You ever do a long division?" asked Crusty. "That's an algorithm." I started to type into Spotlight again, but he snapped, "On paper." And not wanting to embarrass myself, I turned the subject back to my code. "Hmm." I asked, "So, what is this magic algorithm that will cure my performance problem?" "Well, if you'll just let me at your TTY there for a minute," he said, "How do you work this thing? Oh, it's a track pad. I'll try not to touch that. So, first, you do away with this foolishness. Now, shapes.removeallwhere the shape is selected. Hmm. Try that on for size." Now, Crusty headed off to wash out the pitcher of his coffee maker, leaving me to figure out what had just happened in my code. First, I checked, and I found that the performance problem was fixed. Nice. And I didn't want another earful from Crusty about looking at the documentation, so I popped up Quick Help for removeAll(where: and I saw that its complexity scales proportionally to the length of the collection just like removeAt. But since I didn't have to put it in a loop, that became the complexity of my whole operation. Now, I want to give you some intuition for the kind of difference this makes. What are n means that the time the algorithm runs scales linearly with the problem size. The graph is a straight line. Now, the orange line is the shape of order n squared. As you can see, a linear algorithm may do worse on a small problem, but it's eventually faster than a quadratic algorithm. The cool thing is though, it doesn't matter how expensive you make the steps of the linear algorithm, if you keep looking at larger problem sizes, you'll always find one where the linear algorithm wins and continues to win forever. So, we're talking about scalability, not absolute performance. Well, my scalability problem was fixed, but I really wanted to see how the standard library had improved on my backward deletion scheme. Crusty reminded me that Swift is open source, so I could pull it up on what he calls "the hipster web." But the rest of us know as GitHub. Now, the first thing I noticed was the dot comment, which was the source of all that Quick Help, describing both what the algorithm does and its complexity. Next, it turns out that removeAll(where) isn't just some method on a reg; it's a generic algorithm, which means that it works on a wide variety of different collections. It depends on a couple of things, the ability to rearrange elements, which comes from mutable collection, and the ability to change the length and structure, which comes from range-replaceable collection. And it's built from a couple of other order n algorithms. The first is a half stable partition, which moves all of the elements satisfying some predicate to the end and tells us where that suffix starts. The half stable in its name, that comes, that indicates that it preserves the order of the elements that it doesn't move, but it can scramble the elements that it moves to the end. Now, sometimes, that doesn't matter though; the second algorithm removes subrange. It's just going to delete them anyway. Have we all seen this partial range notation? It's just a really convenient way of writing a range that extends to the end of the collection. Okay. Now, remove subrange is part of the library's public API, so you can find its documentation online, but halfStablePartition is an implementation detail. Now, we're not going to step through all of it, but there are a few things worth noticing here. First, it starts by calling yet another algorithm, firstIndex(where), to find the position of the first element that belongs in the suffix. Next, it sets up a loop variable j, and there's a single loop, and the loop index j moves forward one on each iteration. So, it's a sure bet that j makes just one pass over the elements. You can almost see the order and complexity just from that. Lastly, because this method needs to rearrange elements but not change the length or structure of the collection, it only depends on mutable collection conformance. So, this is the first lesson I learned. Become familiar with what's in the Swift Standard Library. It contains a suite of algorithms with documented meanings and performance characteristics. And although we happened to peek at the implementation, and you can learn a lot that way, it's designed so that you do not have to. The official documentation should tell you everything you need to know in order to use the library effectively. You'll even find a playground tutorial there. Now, I realize there's a lot in Swift, so it can look daunting, but you don't need to remember everything. Having an idea of what's there and how to find it will take you a long way. Now, before we move on, I want to point out something else that happened in my code when Crusty made this change. Which one of these most directly describes its meaning? Now, I actually have to read and think through the first one to know what it's doing. Hmm. Maybe I'd better add a comment. Okay. How does that look? Oh, even with that comment, the reverse iteration is kind of tricky, and I don't want somebody breaking this code because they don't understand it. So, I'd better explain that. Okay. While we're clarifying things, the After code actually reads better with a trailing closure syntax. Now, take a breath and look again. Which one is more obviously correct? Even with all these comments, I still need to read through the first one just to see that it does inefficiently the same thing as the second one. Using the algorithm just made the code better in every way. So, this is a guideline, an aspiration for your code first put forward by Shawn Perin. Every time you write a loop, replace it with a call to an algorithm. If you can't find one, make that algorithm yourself and move the loop into its implementation. Actually following this might seem unrealistic to you now, but by the end of the talk, I hope it won't. For a little motivation though, think back to the last time you were looking at spaghetti code. Was it full of loops? I bet it was. All right. Done and done. I have just made the code shorter, faster, and better in every way. I was ready to call it a day. "Thanks for your help, Crusty," I said, fastening the titanium carabiners on my bespoke leather messenger bag, but he looked at me suspiciously, and said, "You suppose you might have made that mistake anywhere else?" I sighed, put my bag down and started hunting down the loops in my code. There were lots in the file [inaudible]. Bring to front, send it back, bring forward, which sort of hops the selected shape over the one in front of it. Let's do that a couple of more times. Send backward with hops, under the shape, below the selected shape, and last of all, dragging in the shape list at the left. Now, these sound really simple until you realize that they all need to operate on multiple selected shapes that might not even be contiguous in the list. So, it turns out that the behavior that makes sense is to leave all of the selected elements next to each other after the operation is finished. So, when you're bringing shapes forward, you hop the front most selected shape in front of its neighbor, and then you group all the other ones behind it. And when you send shapes backward, you hop the bottom most selected shape backward behind its neighbor and group the other ones in front of it. So, if you didn't follow that completely, don't worry. We'll come back to it. But suffice it to say that I had some pretty carefully worked out code to get all of these details right. For example, this was bringToFront, and when I looked at it, sure enough, there was an order n loop over the shapes containing two order n operations, remove(at:) and insert(at:), which makes this, you guessed it, n squared. In fact, that same problem showed up in every one of my other four commands. All of the implementations here loop over an array, performing insertions and removals, which means they're all quadratic. Now, I was kind of discouraged at this point, so I asked Crusty if he'd look at them with me. He said, "Can't stay too late," he said, "I got my ballroom dancing tonight, but I guess we better get a move on." So, I pulled up bringToFront, and Crusty's first question was, "What does it actually do?" "Well," I said, "there's a while loop and j tracks the insertion point and i tracks the element we're looking at." "In words, not in codes," said Crusty. "Describe it." "Okay. Let's see. It moves the selected shapes to the front, maintaining their relative order." "Write that down in a dot comment and read it back to me." I'm a superfast typist. "Moves the selected shapes to the front, maintaining their relative order." "Sound familiar?" said Crusty. That's when I realized that this was a lot like half stable partition, except it's fully stable. I was starting to get excited. And what do you think this one is called? So, I had to guess, "Stable partition?" "That's right. One of my old favorites, and you can find an implementation in this here file from the Swift Open Source project." So, I pulled the file into my project, while Crusty mumbled something about how that comment we added should have been there all along, and I started coding, getting about this far before I had a problem. You see, stable partition takes a predicate that says whether to move an element into the suffix of the collection. So, I had to express bring to front in terms of moving things to the back. I looked at Crusty. "Visualize it," he said. So, I closed my eyes and watched as the unselected shapes gathered at the back, which gave me my answer. Now, I guess sendToBack was even easier because we just need to invert the predicate. We're sending the selected ones to the back. Now, I was just about to attack the bring forward command, and I figured that Crusty would be as eager to move on as I was, given his plans for the evening, but he stopped me. "Hold your horses, Snuffy. I don't want to miss the opening tango, but aren't you going to check whether it'll scale?" He had a point, so I popped up the Quick Help for stable partition, and I saw that it had (n log n) complexity. So, for a way to think about (n log n) take a look at log n. It starts to level off really quickly, and the bigger it gets, the slower it grows, and the closer it comes to being a constant. So, when you multiply that by n, you get something that doesn't scale quite as well as order n but that gets closer and closer to linear as it grows. So, order n login is rightly often treated as being almost the same as order n. I was pretty happy with that. So, we moved on to bring forward. Now, as we said earlier, bring forward bumps the front most selected shape forward by one position and gathers the other selected shapes behind it. But Crusty didn't like that way of thinking about it at all. "That little do-si-do at the beginning is making a line dance look like a Fox Trot." he said, "You don't need it." When I looked at him blankly, he broke out the lozenges again, and with five of his free digits, he executed the bringForward command. "See it again?" he asked. I felt a little like a mark in a Three-card Monte, but I played along. "Look familiar?" "No." He threw a hanky over the first few. "How about now?" That's when I realized it was just another stable partition. Okay. I got this, I thought. If we find the front most selected shape, then move to its predecessor, and isolate the section of the array that starts there, we could just partition that. "But how do you modify just part of a collection?" I asked Crusty. "Ain't you never heard of a slice?" he said, taking over the keyboard. "Shapes, starting with predecessor. There. Stick that in your algorithm and mutate it." Which I promptly did. So, human knowledge about how to compute things correctly and efficiently predates computers by thousands of years, going back at least to ancient Egypt, and since the invention of computers, there's been an explosion of work in this area. If the standard library doesn't have what you need, what you do need is probably out there with tests, with documentation, often with a proof of correctness. Learn how to search the web for research that applies to your problem domain. Okay. Back to the code. I was intrigued by this slice thing, and when I checked out its type, I saw that it wasn't an array. Since we had used stable partition on an array and bringToFront and sendToBack, and now we were using it on an array slice, I guessed out loud that it must be generic. "Of course, it is. What's stable partition got to do with the specifics of array?" "That's right, nothing. Speaking of which, Blonkey, just what does bringForward have to do with shapes and selection?" "Well," I said, "it works on shapes, and it brings forward the selected ones." "That's right, nothing," he went on without listening. "Can you run bring forward on a row of lozenges? Of course, you can. So, it's got nothing to do with shapes." Hmm. "Are you suggesting we make it generic?" I asked. "Isn't that premature generalization?" Answering a question with a question, Crusty replied, "Well, just how do you plan to test this method?" "Okay," I said, "I'll create a canvas. I'll add a bunch of random shapes. I'll select some of them, and then finally..." But I didn't even finish the sentence because I knew it was a bad idea. If I did all of that, would I really be testing my function, or would I be testing the initializers of Canvas and various shapes, the addShape method, the isSelected property of the various shapes if it's computed? I have to build test cases for sure, but ideally, that code shouldn't depend on other code that I also have to test. If I can bring the lozenges forward, it should be possible to do something casual with [inaudible] in a playground, like this, which brings forward the number divisible by 3. Now, at the other end of the spectrum, I should be able to throw vast quantities of randomly generated test data at it and make sure that the algorithm scales. Neither of those was going to be easy as long as the code was tied to Canvas and shapes. And so, I admitted to Crusty that he was right and started making this non-generic bringForward into a generic one. The first step was to decouple it from Canvas and move it to arrays of shapes. Of course, this array is the shape, so I had to replace shapes with self, and then I decoupled it from selection by passing a predicate indicating whether a given shape should be brought forward. And everything kept compiling. Awesome! At that point, I was pleased to find there were no dependencies left on shape, and I could just delete the where clause. Pretty cool, I thought. Now, I can bring forward on any array. I looked over at Crusty, who had been quietly practicing the cha-cha-cha in the corner, but he didn't seem to think I was finished. "What does bring forward have to do with arrays?" he asked. "Well, nothing," I sighed and started thinking about how to remove this dependency. Let's see, there's a stable partition here, which requires mutable collection conformance. So, maybe I'll just move it to mutable collection. Hmm. I thought, clearly the index type doesn't match Int. Okay. So, there's a simple fix for this, right? Have you done this? Don't do this. It compiled, but Crusty had suddenly stopped dancing, and I knew something was wrong. "What?" I said. "Rookies always do that," said Crusty, shaking his head. "First, you got that comparison with 0, which is going to be wrong for array slice. So, did you know that array slices, their indices don't start at 0? The indices of all slices start with the corresponding index in the underlying collection that they were sliced from. That relationship is critical if you want to be able to compose generic algorithms using slices, so it's really important." Well, this problem, I knew to fix. Just compare it with start index. But the real problem is "What does bringForward have to do with having integer indices?" I interrupted. "Yeah, I know." "Well, I do have to get the index before i, which I can do with subtraction." Crusty just sighed and broke out the lozenges again. And then, he laid two fingers down on the desk and walked them across until his right hand was pointing at the first green lozenge. "Not a backwards step in the whole dance," he said. And I realized that Crusty had just shown me a new algorithm. Let's call that one indexBeforeFirst. "Now, the trick here is to keep your focus by assuming somebody already wrote it for you." And he proceeded to hack away all the code that we didn't need, like that and that. "Predecessor is the index before the first one where the predicate is satisfied. Now, just look how pretty that reads." Now, if you look at it, you can see that he's right. Taking away all the irrelevant detail about shapes, selections, arrays, and integers had left me with clearer code because it only traffics in the essentials of the problem. "I already showed you how indexBeforeFirst works. See if you could write it," he said. So, I think I was starting to catch on now because I got it right the first time. I told you, I'm a superfast typist. All right. So, return the first index whose successor matches the predicate. I was pretty excited about how well this was going. "All right, Crusty," I said, "let's do the next one." "Ain't you forgetting something, Bonkey?" he asked. I didn't know what he was talking about. The code was clean, and it worked. "Semantics, son. How am I supposed to use these from other code if I don't know what they mean?" And that's when I realized every time we'd use the new algorithm, we had leaned on his documentation to draw conclusions about the meaning and efficiency of our own code. And because most algorithms are built out of other algorithms, they lean on the same thing. Recently, I was interviewing a perspective intern and asked him about the role of documentation, and he began with a phrase I won't forget. "Oh, it's incredibly important," he said. "We're building these towers of abstraction," and now I'm paraphrasing, "the reason we can build without constantly inspecting the layers below us is that the parts that we build on are documented." Now, as an app developer, you are working at the very top of a tower that stretches through your system frameworks, DOS, and into the hardware, which rests on the laws of physics. But as soon as you call one of your own methods, it becomes part of your foundation; so, document your code. The intern was hired, by the way, and he's sitting right there. So, I took the hint and documented Crusty's new algorithm, which meant that we could forget about the implementation and just use it, knowing that Quick Help had everything that we needed. Now, I also documented bringForward. Cool! Now, because it seemed to be solving all of my problems, at this point, I was really curious to see what was going on inside stablePartition. It turns out I was well rewarded because it's a really beautiful and instructive algorithm. The public method gets the collections count and passes it on to this helper, which uses a divide and conquer strategy. So, first, it takes care of the base cases when the count is less than 2, we're done. We just need to figure out whether the partition point is at the beginning or at the end of the collection. Next, we divide the collection in two. Now, at this point, you have to temporarily take it on faith that the algorithm works because we're going to stable partition the left half and the right half. Now, if you take a look at these two ends, you can see that everything is exactly in the right place, but this center section has two parts that need to be exchanged. Now, they won't always have the same length as they do in this example, and fortunately there's an algorithm for that. We call it rotate. Okay. I'm not going to go into rotate here, but it's actually quite beautiful, and if you're interested, you can find its implementation in the same file as stable partitions. Okay. Back to shapes. Now, this mess implemented the dragging in the shapes list, and it had always been one of my most complicated and buggy operations. The strategy had been to allocate the temporary buffer; then, loop over the shapes before the insertion point, extracting the selected ones and adjusting the insertion point, and then separately loop over the rest of the shapes without adjusting the insertion point, extracting the selected ones. And then, finally, reinsert them. Honestly, I was a little scared to touch the code because I thought I finally had it right. It had been almost a week since the last time I discovered a new bug. But I had gotten pretty good at this process now, so I tried visualizing the operation happening all at once. Hey, that looks familiar. Let's see it again. Hmm. Suppose we do this first, and then take care of the parts separately. That's right. This is just two stable partitions with inverted predicates. So, the generic algorithm collapsed down to this two-liner, and here's what's left in canvas. Now, let's just see that next to the old code. That's not bad, but we got a reusable efficient documented general purpose algorithm out of it, which is spectacular. Okay. So, this is a learned skill to look past the details that are specific to your application domain, see what your code is doing fundamentally, and capture that in reusable generic code. It takes practice. So, why bother? Well, the practical answer is that because they're decoupled from those irrelevant details, generic algorithms are more reusable, testable, and even clearer than their non-generic counterparts. But I also think it's just deeply rewarding for anyone that really loves programming. It's a search for truth and beauty but not some abstract untouchable truth because the constraints of actual hardware keep you honest. As Crusty likes to say, "Programming reveals the real." So, treat your computation as a first class citizen with all the rights and obligations that you take seriously for types and application architecture. Identify it, give it a name, unit test it, and document its semantics and performance. Now, I want to close by putting this advice you saw from Shawn Perin in its full context. "If you want to improve the code quality in your organization, replace all of your coding standards with one goal, No Raw Loops." Thank you.  Good morning. Welcome to Session 403, What's New in Testing. My name is Honza Dvorsky and I will share the stage today with my colleague, Ethan Vaughan. Today, we'll start by talking about the code's coverage improvements we made in Xcode 9.3. Then we'll look at the test selection and ordering features new in Xcode 10. And finally, Ethan will come up on stage to tell you about how you can speed up your tests by running them in parallel. Let's kick off with code coverage. We completely reworked code coverage in Xcode 9.3 and this resulted in many performance and accuracy improvements and it allowed us to add a new feature so that you have the granular control over which targets contribute to code coverage. We created a new command line tool called xccov and last but not least, we gave code coverage in the source editor a visual refresh. Let's look at all of these in detail. First, to give you a sense of how much better code coverage is now, we measured it on a large internal project at Apple. To understand the speed, we measured how long it took Xcode to load and show code coverage in the source editor. In Xcode 9, it took about six and a half seconds to do that. In Xcode 9.3, however, we got it down to less than a half a second, which is more than 95% faster. But we also wanted to make the coverage file smaller, as Xcode can potentially write out many of them, if you run your test often, as you should, so I'm happy to say that here, the improvements are just as dramatic. Xcode 9's coverage files were over 200 megabytes in size. And this is a lot but remember that we're talking about a large project with thousands of source files. But files written by Xcode 9.3 were less than a tenth of that. I'm sure you appreciate this if you maintain a continuous integration machine or if you're just running low on disk space. But the best part is that not only are the coverage files smaller and faster to read and write, they're also more accurate than ever before. One example of this is header files. Xcode 9 didn't correctly collect and show coverage in header files and this was a problem for a code basis that used C++ as in that language, you can have a good portion of your executable code in headers. So, if you're one of the people affected, you'll be happy to hear that Xcode now correctly gathers and presents code coverage for both implementation and header files. Now let's talk about code coverage features. The first one is target selection. This is a new option to control not only whether code coverage is enabled or disabled but when it's enabled, you actually control which targets are included. This can be important if your project builds third-party dependencies that you're not expected to cover by your tests or if you work at a company where another team supplies you with a framework that they already tested. You can customize the included targets in this scheme's test action under options. This means that it can continue to include all targets in code coverage or only hand pick some of them. Now in the name of more powerful workflows, we created a new command-line tool called xccov. It can be integrated into automation scripts easily, as it produces both human readable and machine parseable outputs. And at the high level, it gives you detailed view of your coverage data. So, I've mentioned the code coverage data a couple of times, let's look at how it looks under the hood. When tests are run with code coverage enabled, Xcode generates two files. First is the coverage report, or the xccovreport file extension, and that contains the line -- line coverage percentages for each target, source file, and function. The second one is the coverage archive and that contains the raw execution counts for each of the files in the report. And these coverage files live in your project's derived data directory and additionally, if you pass the result bundle path flag to Xcode build, these files will be placed into the result bundle as well. Let's look at an example. Here, we use xccov to view the coverage data of our Hello World app. We can see the overall coverage for each target but also detailed coverage for each file and even for each method. And of course, by passing the right flag identification, you can get the same exact information in JSON. This can make it much easier to integrate with other tools. So, as you can see, xccov is very flexible, so I would encourage you to explore its documentation. Now we talked about use -- viewing code coverage on the command line but the most convenient way is still going to be to see it right next to your source code. You control whether code coverage is shown by selecting editor, show, or hide code coverage. When you do enable it, you'll see the refreshed look of code coverage in the source editor. The whole line highlights when you hover over the execution count on the right-hand side. And this is just the best way to keep an eye on which of your code is already covered by tests and which still needs more work. Now let me show you all of these improvements in action. So, here I have a project called Dev Cast. It's a simple messaging app and I created the iOS version for this talk last year. So, this year, I wanted to create a Mac version. But I wanted to share the business logic between the iOS and Mac versions, so I put it all into a framework called DevCastkit and today, my goal is to explore and maybe improve the code coverage of this framework. So, right now, I'm not collecting code coverage, so I just need to turn it on. I'll do that by selecting the scheme, selecting edit scheme, go into the test action, go into options. And down here, I will just enable code coverage. Now I will run my test by going to product test. Right now, my framework, my test bundle and my Mac app are getting built, and my tests are being run. Now the test finished so let's look at the results. We'll go to the report navigator and to the latest coverage reports. Here, I can see that I have the -- both targets, the Mac app and also the framework. Now the Mac app only serves as a container for running tests, so I'm not really concerned about its low code coverage. In fact, I would actually not want to see it in this report at all. So, I can do that by editing the scheme again. And in the test action, here -- instead of gathering coverage for all targets, I'll change to just some targets. This gives me a hard cover list and I will add just the target that I'm interested in, which is the framework. Now I will rerun my tests and look at the updated coverage reports. So, now you can see that I only have the framework in the coverage report. This is what I wanted. So, now I can just focus on this one target. So, looking at the coverage percentage, is at 84%, which is not bad, but I know I can do better. So, to understand which of the files need more attention, I will disclose the target and see that the first file is only covered at about 66%. So, I'll jump to the file by clicking on the arrow here. And here, I'll look at my local server class. On the right-hand side, I can see the execution counts for each region of the code so I can see that all of the executable parts of my class are covered by tests. This is good. Unfortunately, my convenience function, get recent messages, hasn't been called for my tests at all, so I don't know if it's working properly. To fix that, I'll go to the corresponding test file and I will add a new test. This test just puts a couple of messages into a local server and then I call, get recent messages, on it to verify that it returns what I'm expecting. So, with the new test added, I will rerun my tests one more time. So, great, the test finished. I'll go to the coverage report again and now when I focus on my framework, I can see that it's covered at 100%. This is exactly what I wanted. So, we saw how I used the target selection feature to only focus on some of the targets. Then I used the coverage report to know exactly which file to focus on. And finally, I used the code coverage integration in the source editor to know exactly which part of my code still needs covering. So, this is the improved code coverage in Xcode. So, moving on from code coverage, let's talk about some new features in Xcode 10. First, we'll see how we can now better select and order our tests. Now why is this important? Well, not all the tests in your suite serve the same purpose. You might want to run all 1000 of your quick running unit tests before every single commit but only run your 10 long-running UI tests at night. And you can achieve this today by disabling specific tests in your scheme. The scheme encodes the list of disabled tests so that XE test knows which tests to skip. And this has an interesting side effect. Whenever you write a new test, it's automatically added to all the schemes that contain the corresponding test targets. But if that's not what you wanted, you have to go through all those schemes and disable the test there manually. So, in Xcode 10, we're introducing a new mode for schemes to instead, encode the tests to run. If you switch your scheme to that mode, only the test that you hand pick will run in that scheme. You control this mode in the scheme editors test action, where in the list of test targets, there's a new options pop-up, where you can declare whether the scheme should automatically include new tests or not. This way, some of your schemes can continue to run any new tests you write, while other schemes will only run a list of hand-picked tests. So, we've discussed how we can better control which of our tests run and when, but the order of our tests can be significant too. By default, tests in Xcode are sorted by their name. This means that unless you rename your tests, they will always run in the same order. But determinism can be a double-edged sword. It can make it easy to miss bugs, where one of your tests implicitly depends on another one running before it. Let's look at an example of when this can happen. Imagine you have tests A, B, and C. They always run in this order and they always pass. But when you look at your tests in detail, you realize that test A creates a database. Then test B goes and writes some data into it. And then finally, test C goes and deletes it. Now these tests only pass because they run in this specific order. But if you tried to shuffle them around, for example, by renaming them, then you try to run them again, you might have test B writing into a database that doesn't exist and your tests would just fail. So, to prevent issues like this, your tests should always correctly set up and tear down their own state. Not only will they be more reliable, but it will also make it possible for your tests to run independently of all the other ones and this can be really beneficial during development and debugging. So, to help you ensure that there are no unintentional dependencies between your tests, in Xcode 10, we're introducing the new test randomization mode. If you turn it on, your tests will be randomly shuffled before every time they're run. And if your tests still pass with this mode on, you can be more confident that they are really robust and self-contained. Randomization mode can be enabled in the scheme editor, just like the other features we've seen today. So, these are the new test selection and ordering features in Xcode 10. Now I'm really excited about what comes next. To tell you all about the new parallel testing features in Xcode, I would like to welcome Ethan Vaughan to the stage. Ethan. Thanks, Honza. So, many of you have a development cycle that looks like this. You write some code, you debug it, and then you run your tests before you commit and push your changes to the repository. By running your tests before you push, you can catch regressions before they ever make it into a build. However, one of the bottlenecks in this process can be just how long it takes to run your tests. Some of you have test suites that take on the order of 30 minutes to hours to run. And if you have to wait that long before you can confidently land your work, that represents a serious bottleneck in your workflow. We want to make sure that your tests run as fast as possible in order to shorten this critical part of the development cycle. So, last year, we introduced a feature in Xcode 9 to help you run tests faster, it's called Parallel Destination Testing. This is the ability to run all of your tests on multiple destinations at the same time. And you do this from the command line by passing multiple destinations specifiers to xcodebuild. Previously, if you were to run your tests, let's say on an iPhone X and an iPad, xcodebuild would run all of the tests on the iPhone X and then all of the tests on the iPad. Neither device would be running tests at the same time. But in Xcode 9, we changed this behavior so that by default, tests run concurrently on the devices and this can dramatically shorten the overall execution time, which is great. However, there are some limitations to this approach. First, it's only beneficial if you test on multiple destinations. If you just want to run unit tests for your Mac app, for example, this doesn't help. Also, it's only available from xcodebuild, so it's primarily useful in the context of a continuous integration environment, like Xcode Server or Jenkins. I'm excited to tell you about a new way to run tests faster than ever called Parallel Distributed Testing. With Parallel Distributed Testing, you can execute tests in parallel on a single destination. Previously, testing on a single destination looks like this, a continuous straight line with one test executing after the other. Parallel Distributed Testing allows you to run tests simultaneously so that testing now looks like this. In addition, it's supported both from Xcode, as well as xcodebuild, so no matter where you run your tests, you'll get the best performance. Now in order to tell you about how Xcode runs your tests in parallel, we first have to talk about how your tests execute at all, what happens at runtime. Let's start with unit tests. Your unit tests get compiled into a test bundle. At runtime, Xcode launches an instance of your app which serves as a test runner. The runner loads the test bundle and executes all of its tests, so that's how unit tests execute. What about UI tests? For UI, tests the story is similar. Your tests still get compiled into a bundle, but the bundle is loaded by a custom app that Xcode creates. Your app no longer runs the tests. Instead, the tests automate your app by launching it and interacting with different parts of its UI. If you'd like to learn more about this process, I'd encourage you to check out our session from 2016, where we go into even more detail. So, now that we understand how our tests execute, we can finally talk about how Xcode runs them in parallel. Just like before, Xcode will launch a test runner to execute our tests but instead of just launching a single runner, Xcode will launch multiple runners, each of which executes a subset of the tests. In fact, Xcode will dynamically distribute tests to the runners in order to get the best utilization of the course on your machine. Let's go into more detail. When Xcode distributes tests to the runners, it does so by class. Each runner receives a test class to execute and it'll execute that test class before going on to execute another one. And then testing finishes once all the classes have executed. Now you might be wondering why Xcode distributes tests by class instead of distributing individual test methods to the runners. There are a couple reasons for this. First, there may be hidden dependencies between the tests in a class, like Honza talked about earlier. If Xcode were to take the tests in a class and distribute them to different runners, it could lead to hard to diagnose test failures. Second, each test class has a class level set up and tear down method, which may perform expensive computation. By limiting the tests in a class to a single runner, XE tests only has to invoke these methods once, which can save precious time. Now I'd like to talk about some specifics to parallel testing on the simulator. When you run tests in parallel on the simulator, Xcode starts by taking the simulator that you've selected and creating multiple distinct copies or clones of it. These clones are identical to the original simulator at the time that they're created. And Xcode will automatically create and delete these clones, as necessary. After cloning the simulator several times, Xcode then launches a test runner on each clone and that runner then begins executing a test class, like we talked about earlier. Now the fact that your tests execute on different clones of the simulator has some implications that you should be aware of. First, the original simulator is not used during testing. Instead, it serves as a sort of template simulator. You can configure it with the settings and the content that you want and then that content gets copied over to the clones when they're created. Next, there will be multiple copies of your app, one per clone, and each copy has its own data container. This means that if you have a test class that modifies files on disk, you can't expect those file modifications to be visible to another test class because it could have access to a completely separate data container. In practice, the fact that class is executed on different clones will likely be invisible to your tests, but it's something to be aware of. So, where can you run tests in parallel? You can run unit tests in parallel on macOS, as well as unit and UI tests in parallel on the iOS and tvOS simulators. And with that, I'd like to give you a demo of Parallel Distributed Testing in action. All right, so this is the Solar System app, which you may have seen at some of the other sessions here at Dub-Dub. As one of the developers of this app, I want to run all of my tests before committing and pushing my changes to the repository. However, as I add more and more tests, this is starting to take longer and longer, and it's becoming a bottleneck in my workflow. So, let's see how parallel testing can help. I'll go ahead and switch over to the Xcode project and I already ran my tests and I want to see how long they took to run. All right, let's go back to the test report. So, here, you can see next to each method, we show exactly how long that method took to execute. Next, for each test class, we show the percentage of passing and failing tests, as well as how long the test class took to execute. And finally, in the top right corner, you can see exactly how long all of your tests took to run. So, here, we can see that our tests took 14 seconds. Now I'd like to enable parallelization, which I can do by going to the scheme. So, I'll select the scheme and choose edit scheme. Next, I'll click the test action and I'll click the options button next to my test target. Finally, I'll select the execute in parallel checkbox and that's all I need to do to enable parallel testing. So, let's go ahead and run our tests now by choosing product tests. So, I want you to look at the doc. Xcode launches multiple copies of our Mac app to run our unit tests in parallel. So, great, it looks like that finished. So, now, let's go back to the report and select the most recent report. So, whereas previously, our test took 14 seconds to run, now they only take 5 seconds. So, just by enabling parallelization, we've seen the greater than 50% improvement in test execution time. So, the Solar System app isn't just available for the Mac, it also has an iOS companion. And one of my responsibilities was to write a UI test suite to exercise the various screens of my iOS app. Now I already enabled parallelization for the iOS scheme, so I'll go ahead and just switch to that now. And then I'll run tests by choosing product, test. Now we'll go ahead and switch over to the simulator. So, here, you can see that Xcode has created multiple clones of the simulator that I selected, and these clones are named after the original simulator so that they're easy to identify. And on each simulator, Xcode has launched a test runner and each runner is executing a different test class in my suite. Now while my tests are executing, I'm going to switch back to Xcode and show you the test log. You can find the test log associated with the test report. The log is a great place to see how your classes are being distributed to the different runners. You can see an entry in the log for each runner and beneath a given runner, you can see the exact test class that it is currently executing. So, when the tests are completely finished, this is a great place to see how your classes were distributed, which gives you a picture of the overall parallelization. And with that, let's switch back to the slides. So, let's briefly recap what we learned in the demo. First, we saw how to enable parallelization in the scheme editor. Next, we saw how to view results in the test report, as well as how our classes are distributed in the test log. Then we saw Xcode launch multiple instances of our Mac app to run unit tests in parallel. And finally, we saw multiple clones of the simulator running our UI tests in parallel. Like I mentioned earlier, xcodebuild has great support for parallel testing as well. We've added some new command line options that allow you to control this behavior and I'd like to point out two of those now. First, we have parallel-testing-worker-count, which allows you to control the exact number of workers or runners that Xcode should launch during parallel testing. Normally, Xcode tries to determine an optimal number of runners based off of the resources of your machine, as well as the workload. This means that on a higher core machine, you will likely experience more runners. But if you find that the default number isn't working well for you, you can override it using this command line option. Next, we have parallel-testing-enabled, which allows you to override the setting in the scheme to explicitly turn parallel testing on or off. Now for the most part, all you need to do to get the benefits of parallel testing is just to turn it on. But here are few tips and tricks to help you get the most out of this feature. First, consider splitting a long-running test class into two classes. Because the test classes execute in parallel, testing will never run faster than the longest running class. When you run tests in parallel, you may notice a situation like this, where one test class dominates the overall execution time. If you take that class and divide it up, Xcode can more evenly distribute the work of test execution between the different runners, which can shorten the overall execution time. Now don't feel like you need to go and take all of your classes and divide them up. That shouldn't be necessary but if you notice a bottleneck like this, this is something to try. Next, put performance tests into their own bundle with parallelization disabled. This may seem counterintuitive, but performance tests are very sensitive to system activity so if you run them in parallel with each other, they'll likely fail to meet their baselines. And finally, understand which tests are not safe for parallelization. Most tests will be fine when you run them in parallel, but if your tests access a shared system resource, like a file or a database, you may need to introduce explicit synchronization to allow them to run concurrently. Speaking of testing tips and tricks, if you'd like to learn more about how to test your code, I'd encourage you to check out our session on Friday hosted by my colleague Stuart and Brian [assumed spellings]. You won't want to miss it. To wrap up, we started today's talk with code coverage and the new improvements to performance and accuracy. Next, we talked about new test selection and ordering features, which allow you to control exactly which tests run, as well as the order in which they're run. And finally, we talked about Parallel Distributed Testing, which allows you to distribute test classes between different runners to execute them in parallel. For more information, I'd encourage you to download our slides from developer.apple.com and come see us in the labs later this afternoon. Have a great WWDC, everyone.  Good morning. Welcome. I'm Michael, and this session is about what's new in Core ML. So introduced a year ago, Core ML is all about making it unbelievably simple for you to integrate machine learning models into your app. It's been wonderful to see the adoption over the past year. We hope it has all of you thinking about what great new experiences you can enable if your app had the ability to do things like understand the content of images, or perhaps, analyze some text. What could you do if your app could reason about audio or music, or interpret your users' actions based on their motion activity, or even transform or generate new content for them? All of this, and much, much more, is easily within reach. And that's because this type of functionality can be encoded in a Core ML model. Now if we take a peek inside one of these, we may find a neural network, tree ensemble, or some other model architecture. They may have millions of parameters, the values of which have been learned from large amounts of data. But for you, you could focus on a single file. You can focus on the functionality it provides and the experience it enables rather than those implementation details. Adding a Core ML model to your app is simple as adding that file to your Xcode project. Xcode will give you a simple view, describe what it does in terms of the inputs it requires and the outputs it provides. Xcode will take this one step further and generate an interface for you, so that interacting with this model is just a few lines of code, one to load the model, one to make a prediction, and sometimes one to pull out the specific output you are interested in. Note that in some cases you don't even have to write back code because Core ML integrates with some of our higher-level APIs and allows you to customize their behavior if you give them a Core ML model. So with Vision, this is done through the VNCoreML Request object. And in the new Natural Language framework, you can instantiate an MLModel from a CoreML model. So that's Core ML in a nutshell. But we are here to talk about what's new. We took all the great feedback we received from you over the past year and focused on some key enhancements to CoreML 2. And we are going to talk about these in two sessions. In the first session, the one you are all sitting in right now, we are going to talk about what's new from the perspective of your app. In the second session, which starts immediately after this at 10 a.m. after a short break, we are going to talk about tools and how you can update and convert models to take advantage of the new features in Core ML 2. When it comes to your app, we are going to focus on three key areas. The first is how you can reduce the size and number of models using your app while still getting the same functionality. Then we'll look about how you can get more performance out of a single model. And then we'll conclude about how using Core ML will allow you to keep pace with the state-of-the-art and rapidly moving field of machine learning. So to kick it off, let's talk about model size. I'm going to hand it off to Francesco. Thank you Michael. Hello. Every ways to reduce the size of your Core ML app is very important. My name is Francesco, and I am going to introduce quantization and flexible shapes, two new features in Core ML 2 that can help reduce your app size. So Core ML, [inaudible] why should you learn in models and device. This gives your app four key advantages compared to running them in the cloud. First of all, user privacy is fully respected. There are many machine-learning models on device. We guarantee that the data never leaves the device of the user. Second, it can help you achieve real-time performance. And for silicon [phonetic] and devices are super- efficient for machine-learning workloads. Furthermore, you don't have to maintain and pay for Internet servers. And Core ML inference is available anywhere at any time despite natural connectivity issues. All these great benefits come with the fact that you now need to store your machine-learning models on device. And if the machine-learning models are big, then you might be concerned about the size of your app. For example, you are -- you have your [inaudible] map, and it's full of cool features. And your users are very happy about it. And now you want to take advantage of the new opportunities offered by machine learning on device, and you want to add new amazing capabilities to your app. So what you do, you train some Core ML models and you add them to your app. What this means is that your app has become more awesome and your users are even happier. But some of them might notice that your app has increased in size a little bit. It's not uncommon to see that apps grow up, either to tens or hundreds of megabytes after adding machine-learning capabilities to them. And as you keep adding more and more features to your app, your app size might simply become out of control. So that's the first thing that you can do about it. And if these machine-learning models are supporting other features to your app, you can keep them outside your initial bundle. And then as the user uses the other features, you can download them on demand, compile them on device. So this -- in this case, this user is happy in the beginning because the installation size is unchanged. But since the user downloads and uses all the current Core ML functionality in your app, at the end of the day the size of the -- of your app is still large. So wouldn't it be better if, instead, we could tackle this problem by reducing the size of the models itself? This would give us a smaller bundle in case we ship the models inside the app, faster and smaller downloads if instead of shipping the models in the app we download them. And in any case, your app will enjoy a lower memory footprint. Using less memory is better for the performance of your app and great for the system in general. So let's see how we can decompose the size of a Core ML app into factors to better tackle this problem. First, there is the number of models. This depends on how many machine-learning functionalities your app has. Then there is the number of weights. The number of weights depends on the architecture that you have chosen to solve your machine-learning problem. As Michael was mentioning, the number of weight -- the weights are the place in which the machine-learning model stores the information that it has been learning during training. So it is -- if it has been trained to do a complex task, it's not uncommon to see a model requiring tens of millions of weights. Finally, there is the size of the weight. How are we storing these parameters that we are learning during training? Let's focus on this factor first. For neural networks, we have several options to represent and store the weights. And the first, really, is of Core ML in iOS 11. Neural networks were stored using floating-point 32-bit weights. In iOS 11.2, we heard your feedback and we introduced half precision floating-point 16 weight. This gives your app half the storage required for the same accuracy. But this year we wanted to take several steps further, and we are introducing quantized weights. With quantized weights we are no longer restricted to use either Float 32 or Float 16 values. But neural networks can be encoded using 8 bits, 4 bits, any bits all the way down to 1 bit. So let's now see what quantization here is. Here we are representing a subset of the weights of our neural networks. As we can see, these weights can take any value in a continuous range. This means that in theory, a single weight can take an infinite number of possible values. So in practice, in neural networks we store weights using floater 32 point -- Float 32 numbers. This means that this weight can take billions of values to better represent the -- their continuous nature. But it turns out the neural networks also work with lower precision weights. Quantization is the process it takes to discontinue strings of values and constrains them to take a very small and discrete subset of possible values. For example, here quantization has turned this continuous spectrum of weights into only 256 possible values. So before quantization, the weights would take any possible values. After quantization, they only have 256 options. Now since its weight can be taken from this small set, Core ML now needs only 8 bits of stored information of a weight. But nothing can stop us here. We can go further. And for example, we can constrain the network to take, instead of one of 56 different values, for example, just 8. And since now we now have only 8 options, Core ML will need 3-bit values per weight to store your model. There are now some details about how we are going to choose these values to represent the weights. They can be uniformly distributed in this range, and in this case we have linear quantization instead in lookup table quantization, we can have these values scattered in this range in an arbitrary manner. So let's see practically how quantization can help us reduce the size of our model. In this example, you are focusing on Resnet50, which is a common architecture used by many applications for many different tasks. It includes 25 million trained parameters and this means that you have to use 32-bit floats to represent it. Then the total model size is more than 100 megabytes. If we quantize it to 8-bits, then the architecture hasn't changed; we still have 25 million parameters. But we are now using only 1 byte to store a single weight, and this means that the model size is reduced by a factor of 4x. It's only -- it now only takes 26 megabytes to store this model. And we can go further. We can use that quantized representation that only uses 4 bits per weight in this model and end up with a model that is even smaller. And again, Core ML supports all the quantization modes all the way down to 8 bits. Now quantization is a powerful technique to take an existing architecture and of a smaller version of it. But how can you obtain quantized model? If you have any neural networking in Core ML format, you can use Core ML Tools to obtain a quantized representation of it. So Core ML 2 should quantize for you automatically. Or you can train quantized models. You can either train quantized -- with a quantization constraint from scratch, or retrain existing models with quantization constraints. After you have obtained your quantized model with your training tools, you can then convert it to Core ML as usual. And nothing will change in the app in the way you use the model. Inside the model, the numbers are going to be stored in different precision, but the interface for using the model will not change at all. However, we always have to consider that quantized models have lower-precision approximations of the original reference floating-point models. And this means that quantized models come with an accuracy versus size-of-the-model tradeoff. This tradeoff is model dependent and use case dependent. And it's also a very active area of research. So it's always recommended to check the accuracy of the quantized model and compare it with the referenced floating-point version for relevant this data and form metrics that are valid for your app and use case. Now let's see a demo of how we can use -- adopt quantized models to reduce the size of an app. I would like to show you a style transfer app. In style transfer, a neural network has been trained to render user images using styles that have been learned by watching paintings or other images. So let me load my app. As we can see, I am shipping this app with four styles; City, Glass, Oils and Waves. And then I can pick images from the photo library of the users and then process them blending them in different styles right on device. So this is the original image, and I am going to render the City style, Glass, Oils, and Waves. Let's see how this app has been built in Xcode. This app uses Core ML and Vision API to perform this stylization. And as we can see, we have four Core ML models here bundled in Xcode; City, Glass, Oils, and Waves, the same ones we are seeing in the app. And we can see -- we can inspect this model. These are seen as quantized model, so each one of these models is 6.7 megabytes of this space on disk. We see that the models take an input image of a certain resolution and produce an image called Stylized of the same resolution. Now we want to investigate how much storage space and memory space we can use by -- we can save by switching to quantized, models. So I have been playing with Core ML Tools and obtained quantizer presentation for these models. And for a tutorial about how to obtain these models, stay for Part 2 that is going to cover quantization with Core ML Tools in detail. So I want to focus first on the Glass style and see how the different quantization versions work for these styles. So all I have to do is drag these new models inside the Xcode project, and rerun the app. And then we are going to see how these models behave. First we can see that the size has been greatly reduced. For example, the 8-bit version already from 6 or 7 megabytes went down to just 1.7. In 4-bit, we can save even more, and now the model is less than 1 megabyte. In 3-bit, it is -- that's even smaller, at 49 kilobytes. And so on. Now let's go back to the app. Let's make this same image for reference and apply the Glass style in the original version. Still looks as before. Now we can compare it with the 8-bit version. And you can see nothing has changed. This is because 8-bit quantization methods are very solid. We can also venture further and try the 4-bit version of this model. Wow. The results are still great. And now let's try the 3-bit version. We see that there are -- we see the first color shift. So it probably is good if we go and check with the designers if this effect is still acceptable. And now, as we see the 2-bit version, this is not really what we were looking for. Maybe we will save it for a horror app, but I am not going to show this to the designer. Let's go back to the 4-bit version and hide this one. This was just a reminder that quantized models are approximation of the original models. So it's always recommended to check them and compare with the original versions. Now for every model and quantization technique, there is always a point in which things start to mismatch. Now we -- after some discussion with the designer, extensive evaluation of many images, we decided to ship the 4-bit version of this model, which is the smallest size for the best quality. So let's remove all the floating-point version of the models that were taking a lot of space in our app and replace them with the 4-bit version. And now let's run the app one last time. OK. Let's pick the same image again and show all the styles. This was the City, Glass, Oils, and big Wave. So in this demo we saw how we started with four models and they were huge, in 32-bit -- or total app size was 27 megabytes. Then we evaluated the quality and switched to 4-bit models, and the total size of our app went down to just 3.4 megabytes. Now -- This doesn't cost us anything in terms of quality because all these versions -- these quantized versions look the same, and the quality is still amazing. We showed how quantization can help us reduce the size of an app by reducing the size of the weight at the very microscopic level. Now let's see how we can reduce the number of models that your app needs. In the most straightforward case, if your app has three machine-learning functionalities then you need three different machine-learning models. But in some cases, it is possible to have the same model to support two different functions. For example, you can train a multi-task model. And multi-task models has been trained to perform multiple things at once. There is an example about style transferring, the Turi Create session about multi-task models. Or in some cases, you can use a yet new feature in Core ML called Flexible Shapes and Sizes. Let's go back to our Style Transfer demo. In Xcode we saw that the size of the input image and the output image was encoded in part of the definition of the model. But what if we want to run the same style on different image resolution? What if we want to run the same network on different image sizes? For example, the user might want to see a high-definition style transfer. So they use -- they give us a high-definition image. Now if I were Core ML model all it takes is a lower resolution as an input, all we can do as developers is size -- or resize the image down, process it, and then scale it back up. This is not really going to amaze the user. Even in the past, we could reship this model with Corel ML Tools and make it accept any resolution, in particular, a higher-resolution image. So even in the past we could do this feature and feed directly the high-resolution image to a Corel ML model, producing a high-definition result. This is because we wanted to introduce a finer detail in the stylization and the way finer strokes that are amazing when you zoom in, because they have -- they add a lot of work into the final image. So in the past we could do it, but we could do it by duplicating the model and creating two different versions: one for the standard definition and one for the high definitions. And this, of course, means that our app is twice as much the size -- besides the fact that the network has been trained to support any resolution. Not anymore. We are introducing flexible shapes. And with flexible shapes, you have -- if you have -- you can have the single model to process more resolutions and many more resolutions. So now in Xcode -- -- in Xcode you are going to see that this -- the input is still an image, but the size of the full resolution, the model also accepts flexible resolutions. In this simple example, SD and HD. This means that now you have to ship a single model. You don't have to have any redundant code. And if you need to switch between standard definition and high definition, you can do it much faster because we don't need to reload the model from scratch; we just need to resize it. You have two options to specify the flexibility of the model. You can define a range for its dimension, so you can define a minimal width and height and the maximum width and height. And then at inference pick any value in between. But there is also another way. You can enumerate all the shapes that you are going to use. For example, all different aspect ratios, all different resolutions, and this is better for performance. Core ML knows more about your use case earlier, so it can -- it has the opportunities of performing more optimizations. And it also gives your app a smaller tested surface. Now which models are flexible? Which models can be trained to support multiple resolutions? Fully convolutional neural networks, commonly used for MS processing tasks such as style transfer, image enhancement, super resolution, and so on -- and some of the architecture. Core ML Tools can check if a model has this capability for you. So we still have the number of models Core ML uses in flexible sizes, and the size of the weights can be reduced by quantization. But what about the number of weights? Core ML, given the fact that it supports many, many different architecture at any framework, has always helped you choose the right -- the model of the right size for your machine-learning problem. So Core ML can help you tackle the size of your app using this -- all these three factors. In any case, the inference is going to be super performant. And to introduce new features in performance and customization, let's welcome Bill March. Thank you. Thank you. One of the fundamental design principles of Core ML from the very beginning has been that it should give your app the best possible performance. And in keeping with that goal, I'd like to highlight a new feature of Core ML to help ensure that your app will shine on any Apple device. Let's take a look at the style transfer example that Francesco showed us. From the perspective of your app, it takes an image of an input and simply returns the stylized image. And there are two key components that go into making this happen: first, the MLModel file, which stores the particular parameters needed to apply this style; and second, the inference engine, which takes in the MLModel and the image and performs the calculations necessary to produce the result. So let's peek under the hood of this inference engine and see how we leverage Apple's technology to perform this style transfer efficiently. This model is an example of a neural network, which consists of a series of mathematical operations called layers. Each layer applies some transformation to the image, finally resulting in the stylized output. The model stores weights for each layer which determine the particular transformation and the style that we are going to apply. The Core ML neural network inference engine has highly optimized implementations for each of these layers. On the GPU, we use MTL shaders. On the CPU we can use Accelerate, the proficient calculation. And we can dispatch different parts of the computation to different pieces of hardware dynamically depending on the model, the device state, and other factors. We can also find opportunities to fuse layers in the network, resulting in fewer overall computations being needed. We are able to optimize here because we know what's going on. We know the details of the model; they are contained in the MLModel file that you provided to us. And we know the details of the inference engine and the device because we designed them. We can take care of all of these optimizations for you, and you can focus on delivering the best user experience in your app. But what about your workload? What about, in particular, if you need to make multiple predictions? If Core ML doesn't know about it, then Core ML can't optimize for it. So in the past, if you had a workload like this, you needed to do something like this: a simple for loop wrapped around a call to the existing Core ML prediction API. So you'd loop over some array of inputs and produce an array of outputs. Let's take a closer look at what happens under the hood when this -- when we are doing this. For each image, we will need to do some kind of preprocessing work. If nothing else, we need to send the data down to the GPU. Once we have done that, we can do the calculation and produce the output image. But then there is a postprocessing step in which we need to retrieve the data from the GPU and return it to your app. The key to improving this picture is to eliminate the bubbles in the GPU pipeline. This results in greater performance for two major reasons. First, since there is no time when the GPU is idle the overall compute time is reduced. And second, because the GPU is kept working continuously, it's able to operate in a higher performance state and reduce the time necessary to compute each particular output. But so much of the appeal in Core ML is that you don't have to worry about any details like this at all. In fact, for your app all you are really concerned with for your users is going from a long time to get results to a short time. So this year we are introducing a new batch API that will allow you to do exactly this. Where before you needed to loop over your inputs and call separate predictions, the new API is very simple. One-line predictions, it consumes an input -- an array of inputs and produces an array of outputs. Core ML will take care of the rest. So let's see it in action. So in keeping with our style transfer example, let's look at the case where we wanted to apply a style to our entire photo library. So here I have a simple app that's going to do just that. I am going to apply a style to 200 images. On the left, as in your left, there is an implementation using last year's API in a for loop. And on the right we have the new batch API. So let's get started. We are off. And we can see the new is already done. We'll wait a moment for last year's technology, and there we go. In this example we see a noticeable improvement with the new batch API. And in general, the improvement you'll see in your app depends on the model and the device and the workload. But if you have a large number of predictions to call, use the new API and give Core ML every opportunity to accelerate your computation. Of course, the most high-performance app in the world isn't terribly exciting if it's not delivering an experience that you want for your users. We want to ensure that no matter what that experience is, or what it could be in the future, Core ML will be just as performant and simple to use as ever. But the field of machine learning is growing rapidly. How will we keep up? And just how rapidly? Well let me tell you a little bit of a personal story about that. Let's take a look at a deceptively simple question that we can answer with machine learning. Given an image, what I want to know: Are there any horses in it? So I think I heard a chuckle or two. Maybe this seems like kind of a silly challenge problem. Small children love this, by the way. But -- so way, way back in the past, when I was first starting graduate school and I was thinking about this problem and first learning about machine learning, my insights on the top came down to something like this: I don't know -- seems hard. I don't really have any good idea for you. So a few years pass. I get older, hopefully a little bit wiser. But certainly the field is moving very, very quickly, because there started to be a lot of exciting new results using deep neural networks. And so then my view on this problem changed. And suddenly, wow, this cutting-edge research can really answer these kind of questions, and computers can catch up with small children and horse recognition technology. What an exciting development. So a few more years pass. Now I work at Apple, and my perspective on this problem has changed again. Now, just grab Create ML. The UI is lovely. You'll have a horse classifier in just a few minutes. So, you know, if you are a machine learning expert, maybe you are looking at this and you are thinking, "Oh, this guy doesn't know what he is talking about. You know, in 2007 I knew how to solve that problem. In 2012 I'd solved it a hundred times." Not my point. If you are someone who cares about long-lasting, high-quality software, this should make you nervous, because in 11 years we have seen the entire picture of this problem turn over. So let's take a look at a few more features in Core ML that can help set your mind at ease. To do that, let's open the hood once again and peek at one of these new horse finder models, which is, once again, a neural network. As we have illustrated before, the neural network consists of a series of highly optimized layers. It is a series of layers, and we have highly optimized implementations for each of them in our inference engine. Our list of supported operations is large and always growing, trying to keep up with new developments in the field. But what if there is a layer that just isn't supported in Core ML? In the past, you either needed to wait or you needed a different model. But what if this layer is the key horse-finding layer? This is the breakthrough that your horse app was waiting for. Can you afford to wait? Given the speed of machine learning, this could be a serious obstacle. So we introduced custom layers for neural network models. Now if a neural network layer is missing, you can provide an implementation with -- will mesh seamlessly with the rest of the Core ML model. Inside the model, the custom layer stores the name of an implementing class -- the AAPLCustomHorseLayer in this case. The implementation class fills the role of the missing implementation in the inference engine. Just like the layer is built into Core ML, the implementation provided here should be general and applicable to any instance of the new layer. It simply needs to be included in your app at runtime. Then the parameters for this particular layer are encapsulated in the ML model with the rest of the information about the model. Implementing a custom layer is simple. We expose an MLCustomLayer protocol. You simply provide methods to initialize the layer based on the data stored in the ML model. You'll need to provide a method that tells us how much space to allocate for the outputs of the layer, and then a method that does the computation. Plus, you can add this flexibility without sacrificing the performance of your model as a whole. The protocol includes an optional method, which allows you to provide us with a MTL shader implementation of your model -- of the layer, excuse me. If you give us this, then it can be encoded in the same command buffer as the rest of the Core ML computation. So there is no extra overhead from additional encodings or multiple trips to and from the GPU. If you don't provide this, then we'll simply evaluate the layer on the CPU with no other work on your part. So no matter how quickly advancements in neural network models may happen, you have a way to keep up with Core ML. But there are limitations. Custom layers only work for neural network models, and they only take inputs and outputs which are ML MultiArrays. This is a natural way to interact with neural networks. But the machine learning field is hardly restricted to only advancing in this area. In fact, when I was first learning about image recognition, almost no one was talking about neural networks as a solution to that problem. And you can see today it's the absolute state of the art. And it's not hard to imagine machine-learning-enabled app experiences where custom layers simply wouldn't fit. For instance, a machine-learning app might use a neural network to embed an image in some similarity space, then look up similar images using a nearest-neighbor method or locality-sensitive hashing -- or even some other approach. A model might combine audio and motion data to provide a bit of needed encouragement to someone who doesn't always close his rings. Or even a completely new model type we haven't even imagined yet that enables novel experiences for your users. In all these cases, it would be great if we could have the simplicity and portability of Core ML without having to sacrifice the flexibility to keep up with the field. So we are introducing custom models. A Core ML custom model allows you to encapsulate the implementation of a part of a computation that's missing inside Core ML. Just like for custom layers, the model stores the name of an implementation class. The class fills the role of the general inference engine for this type of model. Then the parameters are stored in the ML Model just like before. This allows the model to be updated as an asset in your app without having to touch code. And implementing a custom model is simple as well. We expose a protocol, MLCustomModel. You provide methods to initialize based on the data stored in the ML Model. And you provide a method to compute the prediction on an input. There is an optional method to provide a batch implementation if there are opportunities in this particular model type to have optimizations there. And if not, we'll call the single prediction in a for loop. And using a customized model in your app is largely the same workflow as any other Core ML model. In Xcode, a model with customized components will have a dependency section listing the names of the implementations needed along with a short description. Just include these in your app, and you are ready to go. The prediction API is unchanged, whether for single predictions or batch. So custom layers and custom models allow you to use the power and simplicity of Core ML without sacrificing the flexibility needed to keep up with the fast-paced area of machine learning. For new neural network layers, custom layers allow you to make use of the many optimizations already present in the neural network inference engine in Core ML. Custom models are more flexible for types and functionality, but they do require more implementation work on your part. Both forms of customization allow you to encapsulate model parameters in an ML model, making the model portable and your code simpler. And we've only been able to touch on a few of the great new features in Core ML 2. Please download the beta, try them out for yourself. Core ML has many great new features to reduce your app size, improve performance, and ensure flexibility and compatibility with the latest developments in machine learning. We showed you how quantization can reduce model size, how the new batch API can enable more efficient processing, and how custom layers and custom models can help you bring cutting-edge machine learning to your app. Combined with our great new tool for training models in Create ML, there are more ways than ever to add ML-enabled features to your app and support great new experiences for your users. After just a short break, we'll be back right here to take a deeper look at some of these features. In particular, we'll show you how to use our Core ML Tools software to start reducing model sizes and customizing your user experiences with Core ML today. Thank you.  [ Applause and Cheering ] Northern California's bay area. Here is the stage for a truly remarkable natural occurrence. Every year, a great migration is made by one of the world's most mysterious species. To reach this utopian destination, these unique mammals will instinctively embark on a nomadic journey from all corners of the world. This is the developer's [triterapsus], or as it is more commonly known, the developer. Eleven and a half months of hibernation has taken its toll, and the sun is harsh as these nocturnal cave-dwelling creatures must greet daylight. For most, this momentous migration is their first; while for a select few the pilgrimage is a time-honored tradition. Originating in the remote garages of Silicon Valley, developers evolved at an unprecedented speed. To understand how, we must observe them here, in their sacred temple. And so, it begins. As developers congregate at the event, access to the herd can be hard to come by. For some, blending in is easier said than done. To distinguish themselves, developers identify each other through decorative tribal symbols. These extraordinary coolidges [phonetic] are designed to entice and attract. Here, a pack of rogue younglings at play. The safety of numbers allows them to-- oh, wait a moment. They have now entrapped an indigenous silver-crested king developer. Although tempting, one may look at but not touch the mane. The feeding frenzy at the great gathering. Moving in like a pack of famished piranhas, this frugal breed will spare no morsel. Those who can, pull rank on the prime cuts. The event culminates in what is known as the keynote. As the doors open, a behavior [anomaly] can be found. Developers running at full speed, battling to secure a coveted front-row seat. Following the ceremony, the latest beta software is unleashed, it's nature at its cruelest. Only the strongest apps will survive. The many languages of the developer are a mystery to modern science. Just look at these letters, symbols, formulas, and mumbled jargon. Only these enlightened shamans can decode their true meaning. The creative bonds formed during the great migration will enable these geniuses to unlock our future world. The developer is no doubt a species scientists will continue to study for [millennia]. [ Applause and Cheering ] Good morning! Good morning. [ Applause and Cheering ] I hear the student developers. Good morning and welcome to WWDC 2018. It is great to be back in San Jose with everyone. We have developers here from all over the world this morning, from 77 countries. That's more than ever before. And I couldn't be happier to announce that we now have over 20 million Apple developers around the world. [applause] That's more than ever before. We've got 6000 folks in the hall this morning. Welcome. And many millions more watching online. Now the way the developers get to share their amazing work is through the App Store. Your creativity and hard work have made the App Store the best place to get the very best apps. Next month, the App Store turns ten. And in these ten years, the App Store has fundamentally changed the way we all live. It's enabled countless new companies, created tens of millions of jobs, spawned entirely new industries, and it has forever changed our lives. The App Store is the world's largest app marketplace [applause], and we now welcome over 500 million weekly visitors. This is mind-blowing. It's the most incredible app marketplace that the world has ever seen. We're also happy to announce that this week we're going to achieve another huge milestone. The money that developers have earned through the App Store will top $100 billion. This is just, this is beyond remarkable. The App Store is clearly the best place for you to be rewarded for your hard work and creativity. Now at Apple, we know that developers are a powerful group of creators, that they can achieve and they can develop anything that they can imagine with their code. We want more people to learn the power of code, and it all starts with Swift and Swift Playgrounds. We created Swift to make it easy, to make it so easy to learn to code that it was as easy as our products are to use. Swift is extremely popular. In fact, it's the fastest growing programming language out there. Now Apple developers are already using it in huge numbers. In fact, over 350,000 apps have been written in Swift on the App Store. Now [applause] we believe that coding is an essential skill and believe it should be offered by every school in the world. Learning to code has so many benefits. It develops problem solving and critical thinking skills. That's why created Everyone Can Code with free teaching and learning resources so that everyone could learn to code. It's been so successful and is now available to tens of millions of students around the world. Just imagine what this new generation of coders will create. Whatever it is, I'm sure that it's going to change the world. At Apple, changing the world and making it a better place is what it's all about for us. We aim to put the customer at the center of everything that we do. That's why, together with you, the developer community, we're working hard to provide new and better experiences for our customers to help them live a better day. And these experiences of course are expressed through our four amazing platforms. Today is all about software, and we've got some very exciting updates across all four platforms. We're going to get started with iOS. IOS embodies our philosophy of putting the customer at the center of everything that we design. Every year, we deliver a major iOS update that brings awesome new features that will impact the world. To tell you all about what we had planned for this year, I'd like to introduce Craig Frederique [phonetic]. Craig. Hey, good morning. All right. The next release of iOS is, you guessed it, iOS 12. Now, our customers, of course, are going to receive iOS 12 as a free software update. Now, it's easy to forget now, but iOS pioneered this approach of helping you get more out of the device you already own through free updates. And some of those updates have been really quite profound. Like the App Store, giving us a whole new way to discover and download apps. It itself was delivered via software update, as were the folders we used to organize those apps. And can you imagine living without Find My iPhone or iMessage with its end-to-end securely encrypted messaging. Or the revolution in iPad productivity with Slide Over, Split View, and Drag and Drop, or AR, changing the way we interact with the world around us, all delivered via software updates. And of course we want to get these improvements to as many of our customers as possible. IOS 11 supports devices that were introduced as far back as 2013, like the iPhone 5S. And we just love the way customers race to update to our newest releases. In fact, half of our customers upgraded to iOS 11 in just seven weeks. It's incredible. Now as we stand here today, 81 percent of our over a billion active iOS devices are running our latest release. And now, when you look at the competition, well it's hard to say they really have a software update model. So iOS has the fastest adoption of any operating system, but what's much more important to us is customer satisfaction, and we're thrilled to report that customer sat for iOS 11 is at 95 percent. Now, delivering all of these features across such a wide range of devices will maintaining high performance is a challenge we take really seriously, and so for iOS 12, we are doubling down on performance. We're working top to bottom making improvements, to make your device faster and more responsive, and because we want these changes to be available to the full range of our customers, iOS 12 will be available on all the same devices as iOS 11. This is the largest base ever supported by an Apple release, and we're focusing our efforts especially on the oldest devices. And while it's still early days, we're really pleased with some of the results we're seeing. And so I'd like to share some with you. And I'm going to use an example of a popular phone from a few years ago. This is the iPhone 6Plus. Now on that device, iOS 12 delivers a number of improvements across common operations. You'll see that apps launch up to 40 percent faster. The keyboard can come up up to 50 percent faster, and you can slide to take a photo at up to 70 percent faster. Now our deepest focus this year is optimizing the system when it's under load, and that's where you need performance the most and where iOS 12 really shines. Now, we've put iOS 12 through our stress test, and we saw in those conditions Share Sheet coming up twice as fast and apps launching twice as fast. These are big, big improvements. Now this took changes-- thank you very much. This took improvements in many, many places in the system, and I want to highlight just one. And it starts with the silicon. You know, our tight collaboration with our chip team has enabled us to optimize iOS across the full range of our A series silicon. Now CPUs traditionally respond to increased demand for performance by slowly ramping up their clock speed. Well now on IOS 12, we're much smarter. When we detect that you need a burst of performance, like when you begin scrolling or launching an app, we ramp up processor performance instantly to its highest states, delivering high performance and to ramp it down just as fast to preserve battery life. Now, these are just some of the improvements that are coming to not just our older devices, but the full range of devices, and that's a quick update on performance. Now if this is all we'd done in iOS 12, I think it would be a great release, but we've done more. Much more. And we have a lot to cover today, and it starts with augmented reality. Yeah. [applause and cheering] Now AR is transformational technology. By bringing experiences into the real world, it enables all kinds of new experiences, changing the way we have fun and the way we work, and in iOS 12, we wanted to make an easy way to experience AR across the system, and to do that, we got together with some of the greatest minds in 3D at Pixar, and together we created a new file format for AR. It's called USDZ, and it's a compact, single-file format that's optimized for sharing while retaining great 3D graphics and even animations. Now, you can use USDZ across the system from the files app, to Safari, even sharing them over messages and mail, and what's great is you can place these 3D objects into the real world. It's something that's like AR quick look. It's really awesome. Now, we want all kinds of creatives to be able to create content for AR, and so we're working with the leading companies for 3D tools and 3D libraries to bring their support for USDZ. Now one company that's been all in on USDZ and ARKit is Adobe, and to tell you about what they're up to, I'd like to invite Abhay Parasnis, their CTO, to the stage. Abhay. Thanks Craig. It's great to be here this morn. So at Adobe we believe augmented reality is an incredibly important technology. And with ARKit, Apple is by far the most powerful platform for AR. So earlier Craig talked about USDZ format. It's actually a pretty big deal. There is now a way to deliver AR experiences across the entire iOS experience. And so today we are actually announcing that we are going to bring native USDZ support to Adobe's Creative Cloud. With Creative Cloud, designers and developers will now be able to use familiar apps, apps that they know and love, like Photoshop or Dimension, to create amazing AR content and bring it easily via USDZ. And of course, we are not going to stop there. We are going to bring the power of immersive design to Creative Cloud with a new set of services and applications including a brand-new iOS application that's going to let all of you design amazing AR experiences quickly. So, you will be able to bring in images, videos, text, any object from Creative Cloud directly into a native AR environment. In fact, for the first time with Creative Cloud and iOS, you will have a what you see is what you get editing in AR. It's pretty cool. So, come join this afternoon State of the Union where we're able to give you a sneak peak of some of these new immersive design tools for the first time. Thanks. Back to you Craig. [applause] Thank you, Abhay. [applause] Now a critical part of enabling AR is accurate measurement, and fortunately the precise calibration of sensors in iOS devices and our tight hardware software integration mean that we do this really well, and we want to enable everyone to take advantage of this capability. So we're introducing a new app, and it's called Measure. It makes it really easy to measure objects, detect rectangles and get their dimensions, and measure lines along surfaces, and I'd like to show it to you now. [applause] Now, in looking for something to measure, I ended up digging through the attic and came upon my old suitcase from my traveling days in college. A lot of memories in here. Now I'm actually in the Measure app, and you see I can easily measure along this suitcase by just tapping and dragging out a line, like that, and check that out. It's a measurement. [applause] Now, what's really cool is I can extend these measurements so I can just tap, drag along another edge just like that, and even take it into full 3D by dragging down to the bottom like that. Isn't that cool? Now I mentioned that we could automatically detect rectangles and show you dimensions of objects. So I have a photo here actually. It's one my mom always wanted me to travel with of me as a baby, and so, you see, what I can do with Measure is it automatically detects the dimensions of that photo. I can just tap and get, oh, yeah, a cute little baby, wasn't I? And you can get measurements just like that. It's really fantastic. Now, I'd like to turn now to USDZ and its support throughout the system. You know, you can experience USDZ in so many places, and one of those places is news. Now here's an article. We're all accustomed to seeing images in news articles. But check this out. Here you see a USDZ asset. It can just tap on this little 3D control and jump right in and experience it. You can see the amazing animations that are possible, and of course it's fully interactive. So I can zoom in and pan. Isn't that amazing? [applause] Now USDZ is also great in the web. So here I am at the Fender website and actually let you configure your guitar with the kind of finish and pickguard that you want. So I can select a configuration option here, and then I can see the guitar I've configured. So I'm just going to tap in. Of course I can see the guitar here, but wouldn't it be cool if I could see it in the real world in its real size? Well, there it is. Check that out. [applause] I think I'm going to capture that for posterity. And that's a quick look at USDZ and Measure in iOS 12. Next-- thank you. [applause] Next, I'd like to talk about the key technology behind these augmented reality experiences, and that's ARKit. ARKit opens up the world of AR to hundreds of millions of users, making it the world's largest AR platform by far. And we're on a relentless pace of advancement with AR, and that continues today with ARKit 2. Yes. ARKit 2 delivers advances with improved face tracking, more realistic rendering, and support for 3D object detection and persistence, which enables launching into AR experiences associated with a particular object or a physical space, like starting a game built around a physical toy or having a physical classroom serve as the foundation for a lesson that was authored in AR. But probably best is the support for shared experiences. Now this delivers true, multiuser augmented reality. You and the people around you will be able to see your own perspective on a common virtual environment. And to help all you developers get started, we created this sample app written in Swift that you'll all be getting today. Now, check out how both players and even a third observer can all experience the same environment in real time. It's really fun. Now, we've also brought in a select few developers into our labs over the last couple of weeks to work with ARKit, and they love it. Now, one of them is Lego. And what they've done is so fun, you just need to see it. So I'm pleased to invite Martin Sanders, director of innovation of Lego, to the stage to give you a live demonstration. Martin. Craig, thank you very much. Creating and playing with physical Lego sets brings great joy to millions of children and Lego fans all over the world. And now, with ARKit 2, we get to expand those creative possibilities like never before and take things beyond the physical. What we try to do is combine physical and digital together to really open up those creative play possibilities, because our Lego sets are really the start point for all of those children's imaginations. And when we get a chance to really imbed ARKit 2, it takes it to the next level. Let me show you what I mean. Here we have Assembly Square. It's one of our Lego creator sets and already has so many great details it's awesome to play with. But wouldn't it be great if we could take things even further? Well now with 3D object detection, we get to recognize our models and bring them to life. And just look at all of those rich details we can now bring into our sets, because when we combine physical and digital together like this, it really opens up those creative play possibilities. And there's so much to do here. Did you see these icons of the people and objects? Well they represent missions and stories that we can explore. And with a world as rich and as immersive as this, who wouldn't want to play? So let's add a character. How about this little guy here? Welcome back, let's go on an adventure. Awesome. Let's go on an adventure. But going on an adventure with friends is often way more fun. Anders, why don't you pop in here. I'm always up for an adventure. Perfect. Because now with ARKit 2's multiuser support, we get to play with up to four friends in the same space. Let's go ahead and add a few things from our collection, Anders. Okay. I will add a bank over here. Very nice, I like it. How about taking this for a spin. Let's just look at all those rich details you can see on the outside of the building and even on the inside. Yeah, I think I'm going to add this guy. He's definitely going to keep the bank safe. Oh look, my character's hungry. Let's take him over to the bakery. Well let's see what they have. Because now with ARKit 2, we get to see inside our physical creations and check out all the details that were hidden before. We've got a ballerina, a little music session going on, and let's make a bathroom-- oops sorry. Moving on. Oh, another play trigger in the bakery. Let's see what happens when I click on this. Um, fresh pretzels. Oh, that doesn't look too good. Anders, I've got a situation. I'm going to need your help over here. Yeah, okay, okay. I've got a firetruck. Perfect. You put out those flames while-- oh, oh dear. Somebody's got themselves trapped on the roof, so I'm going to use this helicopter to go and pick up these clowns. [music] How are you getting on down there Anders? I'm almost done, almost done. Great. Those guys are safe. And the flames look like they're almost out. Perfect, we did it. Yeah. What? And we've even unlocked a new item for our collection. But the fun doesn't need to end there because with ARKit 2, we get to save our entire world back into our physical set and pick up where we left off. [applause] All right. That's awesome. So much fun. But what makes this truly amazing is how with just a single Lego creation and ARKit 2 it really opens up those creative play possibilities. So look out for more Lego AR experiences in the App Store later this year. Thank you very much everybody. Isn't that great? So that's our update on AR and iOS 12. Next, I'd like to turn to Photos. You know, over a trillion photos are captured on iPhone each year, and Photos is the best way to relive and share those moments. This year we're making photos even better, and it starts with Search. Search has powerful object and scene recognition. It lets you search for photos based on things like searching for cars or dogs or flowers, and that's a great way to explore your library, but in iOS 12, Search now starts working for you even before you start typing with Search suggestions. It'll highlight things for you like key moments and people that are important to you, places where you've taken some great photos and even categories of photos like hiking and water sports. And Search is much more powerful than ever. You can search for places by business name. So you could search for SFMOMA or even a broad category like museum. And, Photos indexes over four million events by time and place. Things like sporting events and concerts, and so you can search for them and find photos you took at those events. And search is super powerful. You can now search for multiple search terms like surfing and vacation and even get suggestions for additional search terms to help you find exactly what you're looking for. And now in iOS 12 we have an all new tab. It's called For You. And with For You, you have all of your memories, so those great memory movies, but more like featured photos highlighting a photo that you took on this day in past years. And Effects Suggestions, for instance, suggesting looping a live photo or applying a new portrait effect to one of your portrait photos. And we even highlight your shared album activity. Now when it comes to sharing, there are many ways you can share photos, but our focus is on sharing great photos with the people you care about most, and that's why this year in For You we've added Sharing Suggestions. So imagine you've gone out for a great dinner with some friends and you took some photos. Well afterwards in For You, you'll see a suggestion like this to share those photos. If you tap in, you'll see that photos is even recommending a set of photos from that set that you might want to share and suggest who you might want to share them with based in part by the people that appeared in the photos. And when you share them, they're shared at full resolution out of your iCloud photo library and when you're friend receives them, something really magical happens. Their phone searches their libraries for other photos they took at that event and suggest that they share them back to you, so you both can end up with a full set. Now this is built around iMessage so of course it's private, using end-to-end encryption, and all of those smarts are done with on-device machine learning. So that's your quick update on Photos in iOS 12, and next, let's turn to Siri. Now Siri is by far the world's most used digital assistant with over 10 billion requests processed per month, and because Siri works across all your devices, it's always there to help you through your day getting things done. Now we all know that Siri works with many third-party apps for things like messaging, ride sharing, and payments. But we wanted to make Siri able to do much more for you, and we're doing that by taking advantage of the power of apps with a new feature we call Shortcuts. Now with shortcuts any app can expose quick actions to Siri. Let's look at some examples. Now say you have the Tile app because you're always losing your keys. Well, the Tile app can expose the option to add a shortcut to Siri. And you can assign your own phrase, such as I lost my keys would be a good choice, and when you then say it, Siri will automatically activate Tile and show you right in the Siri UI start ringing your Tile just like that. It's really great. [applause] Of course there's so many uses for this kind of thing. You could say game time to get your team's schedule from TeamSnap or help me relax to kick off a meditation or order my groceries to order your usual. You know, with millions of apps, Shortcut enables incredible possibilities for how you use Siri. Now, as you know, Siri is more than just a voice. Siri is working all the time in the background to make proactive suggestions for you even before you ask, and now with Shortcut, Siri can do so much more. So, for instance, let's say you order a coffee every morning at Phil's before you go to work. Well now, Siri can suggest right on your lock screen that you do that. You tap on it, and you can place the order right from there. Or if when you get to the gym you use Active to track workouts, well that suggestion will appear right on your lock screen. And this even works when you pull down into Search. You'll get great suggestions. Like say you're running late for a meeting, well Siri will suggest you text the meeting organizer. Or when you go to the movie, suggest that you turn on Do Not Disturb. That's just being considerate. And remind you to call grandma on her birthday. Just tap, and it'll dial the call for you. Now, we think we're all going to really enjoy using Shortcuts, and so we went a step further. We wanted to let you create your own shortcuts as users, multiple steps across multiple applications, and we're doing it with a new Shortcuts app. So with the Shortcuts app, you could do something like create a shortcut for surf time, and it could go get you the surf report, look up the current weather, get you the ETA to the beach, and even create a reminder for you to put on sunscreen when you get there. Now, it's all done with simple drag and drop steps in the Shortcuts editor right here. It's really easy. Now to show you how Shortcuts can streamline your day, I'd like to invite one of our leaders from our Siri Shortcuts project, Kim Beverette [phonetic], to the stage to give you a live demo. Kim. Hey! I'm am so stoked to show you Siri Shortcuts. To do that, I'm going to walk you through my day. So imagine it's the morning. I'm headed to work, and I pick up my phone, and I see this suggestion from Phil's Coffee. Siri has learned that I do this most mornings, so now I can just tap on the suggestion and I see all the details I need to confirm my perfect Mint Monkey Dough right here on the lock screen without even going into the app. So let's get caffeinated. And I'm done. Fast forward a little bit, and I'm sitting at my desk at my office and I need to know when my next meeting is. I'll go to the Up Next widget, and it looks like I am running a little late for a rather important meeting, so I should probably let someone know. And it looks like Shortcuts is a few steps ahead of me. I could call into the meeting, or I could let the organizer know that I am running late. I should probably tell Arie what's up. That looks like just what I want to say, sorry Arie, let's send it. Perfect. I also want to show you how you can add a shortcut to Siri. So let's take a look at Kayak. I keep all of my travel details in Kayak. Most important is my post WWDC relaxation trip to Los Angeles. You can see I have got my flight, my hotel, all the details, everything I need, but what I really want is to be able to use this and get to this information with my voice while I'm on the go. So, let's head back, and I can just tap add to Siri, record my custom phrase, travel plans [beep], and I'm done. So now when I land at the airport and I'm about to get in a cab and I could really use that hotel address, I can just say, travel plans. Kayak says your hotel is at 929 South Broadway. You can check in after 3 p.m. Isn't that cool? [applause] It's pretty cool. So I would love to be on that vacation, but I should, I don't know, probably finish this demo, so let's head back to work, and I can show you how the Shortcut app can help me with my commute home. We start in the Gallery where there's hundreds of premade shortcuts that you can download, or we can hop over to the library, and I've got a bunch of shortcuts here, but I want to show you my heading home shortcut. You can see that it's just really a series of steps. It grabs my location and travel time, and it sends my ETA to my roommate. It sets my HomeKit thermostat to 70 degrees, and it turns on my fan. And last, it gets directions home with Apple maps with the best route to avoid traffic. Now this is already pretty cool, but I happen to be an NPR News junkie, so I should probably just add that to my shortcut to save me some trouble on the ride home. Let's tap search, and there's a bunch I can add here, but I can just tap Siri suggestions, and there it is, play KQED Radio, we'll drag this in, drop it, and we're set. I've already added the shortcut to Siri with a custom phrase, heading home, so now, whenever I leave work, I can just say, heading home. You will get there in one hour. I sent a message to Cheryl. Your thermostat is set to 70 degrees, and I turned on the fan. Playing KQED Radio. [music] Right? That's Siri Shortcuts in iOS 12. Thank you so much. Thank you, Kim. And that's Siri Shortcuts. It works on iPhone and iPad, and of course you can run your shortcuts from your home pod and your Apple Watch. And that's your quick update on Siri. Next, let's talk about apps, and to tell you about the latest, I'm going to hand it off to Susan. Susan. [ Applause and Cheering ] Hello. Thanks Craig and it is so great to be here. I'm excited to tell you about some great updates in some of our most popular apps, starting with one of my favorites, News. So News is a personalized feed where you can see all the stories you want to read pulled together from trusted sources. And, our top stories are handpicked by the Apple News editorial team to make a great collection of curated content. With our new browse tab, you can discover new channels and topics, and we've made it even easier to jump to your favorites because that's why they're your favorites, right? News shines on the iPad. We've added a new sidebar, and it's a great way to navigate. It makes it easy and I fun to dig into the areas you're most interested in. So that's news. Now, we've completely rebuilt the Stocks app, and it's got a beautiful new design. Of course, you can still see the stock prices and the changes at a glance, but we've added spark lines, those little charts, that show the stock performance throughout the day. And that's cool, but are you ready? We are so excited to announce we're bringing Apple News to Stocks. I'm really excited about that, and the top stories in Stocks features business news, right, curated by the Apple News editors. It's pretty terrific. You can tap on any stock to get a more detailed view, so you can see an interactive chart that now includes after hours pricing, and you see relevant headlines from Apple News curated by our Apple News editor. So it looks great. And if you tap on one of those headlines, you'll see the full article without leaving the app, and of course it's formatted to look gorgeous on the iPhone. Now, with iOS 12, we're bringing Stocks to iPad. It's pretty great, and we take advantage of the larger display, so you can keep your eye on your stocks on the left while you browse through your financial news. It's a pretty great experience. Next up, Voice Memos. We've also completely rebuilt Voice Memos to make it even easier to use, and we're bringing Voice Memos to the iPad for the first time. Importantly, we've also added iCloud support so your recordings stay in sync across all your devices. We think iPad users are just going to love this. And we think that iBooks is the best way to discover and experience eBooks as well as audio books, and with iOS 12, we're introducing an all new design, and we think the update is so great, we're calling it Apple Books, a new name. Very dramatic. We dropped the i. [applause] Apple Books has some great new features. For example, Reading Now, with a preview that makes it really easy for you to pick reading right where you left off. And there's so much more, including a stunning new store that makes browsing through your eBooks and audio books better than ever. We love these updates, and we think you will too, but we also have a smart and safe way to use your apps in the car. I think you know I'm talking about Car Play. Car Play already supports third-party audio and voice messaging, you know, voice calling and messaging apps, you problem know that. But what you might not know is with IOS 12, Car Play will also support third-party navigation apps. So now you have even more choices when you use Car Play. That is a really quick look at some of our app dates, and Craig, back to you. [applause] Thank you, Susan. [applause] Well, now I'd like to take a moment to talk about something that's on a lot of people's minds lately. You know, iPhone and iPad are some of the most powerful tools ever created for learning, exploring, and keeping in touch, but some apps demand more of our attention than we might even realize. They beg us to use our phone when we really should be occupying ourselves with something else. They send us flurries of notifications, trying to draw us in for fear of missing out. And some of us, it's become such a habit that, you know, we might not even recognize just how distracted we've become. Well, we've thought deeply about this, and today we're announcing a comprehensive set of built-in features to help you limit distraction, focus, and understand how you're spending your time and balance the many things that are important to you. Now it starts with Do Not Disturb. There are times of the day, or times when you just don't want to be disturbed, and one of those, of course, is at night. Sometimes you wake up in the middle of the night, and you look at your phone, maybe just to check the time, and you're confronted with something like this, a barrage of notifications that spin you up and keep you from falling back asleep. And so we're introducing Do Not Disturb during bedtime where all you'll see is this. Nothing to get you spun up. And in the morning, yeah. In the morning when you wake up, you're gently eased into your day. You can tap when you want to start confronting those notifications. Now we've all found ourselves in situations like this. [laughter] Now, rest assured, he stuck the landing on this one. But now Do Not Disturb can help, and we made it easier than ever to use Do Not Disturb because now we have a great new mode where when you press in to Do Not Disturb and Control Center, you can set an ending time for Do Not Disturb for when you leave a particular location or when an event ends on your calendar. So I think we're all going to be using Do Not Disturb a bunch more. Now next I want to talk about Notifications. Now, Notifications help keep us informed and connected to important things that happen throughout the day, and we'd like to give you more control over how many notifications you receive, and so we're enabling what we call Instant Tuning for Notifications right from the lock screen. You can press in to a notification, and from there, you can decide to send future notifications from that app directly to Notifications Center, bypassing your lock screen. Or turn them off altogether, and Siri will even help by suggesting that you turn off Notifications for apps that you're no longer using. Now, we also wanted to give you help managing large numbers of notifications. So I'm thrilled to announce that we're bringing to iOS support for grouped notifications. Notifications are grouped not just by app but also by topic and thread. It gives you a great overview of the notifications you've received. You can tap in and look at a particular group. But of course just as important with a single swipe, you can triage a whole group of notifications away. So that's Notifications. Now, in addition to these great features for helping you limit distractions, we wanted to go further, and it's with a feature we call screen time. Screen time empowers you with both insight and control over how you spend your time, and it starts with reports. Every week, you get a weekly activity summary that details how you used your iPhone or iPad. You tap in, and you get to view your full activity report. It's really detailed. You get deep insight on how much time you're spending, where you're spending it, and even how your use breaks down during the day or the night. You get a summary of the time you're spending in apps, how much time you're spending, how often per hour you're picking up your phone and what's drawing you in and what apps are sending you the most notifications. Now, equipped with this insight, you can make decisions about how much time you want to spend with your device each day. But we know there are people who would like a little extra help in managing their use of apps, and for them we've created App Limits. So if in your activity report you see an app where you might want to be spending a little bit less time, well you can set your own limit, and then during the day, when you're using the app, you receive a helpful notification letting you know time is almost up. And once you've reached your limit, instead of the app, you'll see this. It's time to move on. Now, we'll let you grant yourself and extension if you want, but we'll give you a reminder later to move along. Now, this is also in sync across your iPhone and iPad, so your limits apply to your total usage. And we think this is going to be helpful for many people, but especially for some kids. And we know this is something that can help families achieve the right balance for them. And of course it starts with providing your kids with great information so they get an activity report of their own, but as a parent, you get one as well on your device, and based on what you see, you have the option of creating allowances. Now you have many options. One of them is downtime, time when you want your kids to unplug altogether. For instance, at bedtime. And you can also limit your kids' time in apps by category or by individual app. Now, there's some apps you may want to always allow them to use. For instance, you may want them to be able to get at the phone at all times, so they can contact you. Or you may want to give them access to educational apps. And you can also limit access to only movies, apps, and websites that you deem age appropriate. Now, this works of course across their iPhone and iPad, and it's uses family sharing, so it's super easy to set up, and you can manage it all remotely from your parent parental device. And so that's are some-- Screen Time has great features to help you better manage your time. Now, next I'd like to talk about one of the most important uses of our devices, and that's communication. And we'll start with messages. Messages has given us fun ways to express ourselves with emoji and now Animoji, and one of the things that make Animoji so fun is how expressive they are, you know, from smiles to frowns, to nods of the head and blinking of the eye. Animoji does such an amazing job tracking our expressions. And this year, we're taking Animoji to a whole new level, the breakthrough new technology we call Tongue Detection. [applause] That's right. Now, you can make your favorite Animoji do this. We're all going to be sticking out our tongues to our phones in the near future. Now, we've also, we're also introducing some great new Animoji that I think you're all going to love, like ghost, koala, tiger, and T-Rex. But we wanted to take Animoji even further by making them even more personal. So I'm thrilled today to announce the arrival of the era of Memoji. [applause] That's right. With Memoji you can create your very own personalized Animoji. Now, these Animoji can look like you or the real you. And we've worked hard to build a deep set of customization options to let our customers create an incredibly diverse set of Memoji. It's really incredible what you can create. And we've designed a beautiful new experience to create these Memoji that makes the process fun and easy. Now, to tell you more about it, I'd like to invite one of the managers of our Messages and Animoji features, Kelsey Peterson, to give you a live demo. Kelsey. [ Applause and Cheering ] Good morning. I cannot wait to tell you what's new with Messages. Let's get started with Animoji. First, you need to meet the newest members of the team. We've got a new cat in town, our tiger. She's so cute. And now, my personal favorite, the koala. Just getting excited scrolling through here. They can't all be cute and cuddly though, so here's our T-Rex. And we've our very own friendly little ghost. So much fun. And if I swipe right, here's where I can create my very own Memoji. Let me show you just how easy it is. I recently chopped my hair, so I want one that matches the new me. So I've selected a skin color, and now I'm trying to figure out just the right amount of freckles. It's a real Goldilocks scenario. Yeah, these are just right. Okay. Onto the main event. There are so many hairstyles to choose from. First, I'm going to grab my color, and then like I said, I need to go a little bit shorter. All right. Nope, this is the one. Now that I'm all set, I can of course select my eye color, and what's really amazing is as I'm making changes, the character up above is coming to life. There are tons of options for me to customize. I could add earrings, but what I really want is a great pair of Sunnies, so I'm going to come over to eyewear and pick out some frames. Maybe not for today. I think I need two lenses. These are the ones. Now that I have frames, I'm going to tint my lenses to make a great pair of sunglasses. Perfect. Okay. I think I'm all set. I'll tap done, which saves my grandly created Memoji right here into the drawer alongside the rest of my team. And that's how simple it is to create your very own Memoji. Now I have a brand-new feature to introduce you to. This wasn't even in Craig's slides. We are bringing fun effects into the Messages camera. Let's take a look. I've a message here from my partner. It looks like he's informing me that he's bought our dog, Ferdinand, yet another tiny dog hat. This isn't even the first. I think this is the perfect opportunity for a response with this new fun camera. So I'll tap to pull up the camera, and then I can see this little star over here on the left. Tapping on that gives me a strip with all sorts of new effects. So I could add things like shapes or text. But let's check out these filters. Ooh. So this is comic book. It's really fun, but for a response about a tiny dog hat, I think I'm going to go in a different artistic direction. So what I really need to do is add a sticker from one of my favorite sticker packs. Ferdy looked really excited about that new dog hat, so I'm going to put him right here, and now we have an all new way for you to use Animoji. I can apply my favorite Animoji right here live. Here's the Memoji I just created, but I actually have just the one. It's of me in a very similar red hat, which is kind of perfect for twinning with my pup. So I'm going to set up my shot, snap, and send. And that's a demo of the fun new effects in messages. To you Craig. So that's Memoji and some fun new effects in the camera and messages. Next, let's talk about Facetime. Yeah. Facetime is the way that so many of us connect with the people in our lives and share some of our most important moments, and it's helped us deepen our connect with people important to us wherever they are, and of course it's a fun place just to hang out. Now this year Facetime is going to take a big leap forward because today we're introducing Group Facetime. Now, you'll be able to facetime with two people, three people, actually up to 32 simultaneous participants. Now, setting up a group call couldn't be easier. Just instead of typing one person's name, you can do many. You can ring them by tapping audio or video, but we also introduced a great new way because Facetime is now integrated into messages, so you can quickly go from a group chat you have going directly into a group Facetime, and members of the group can join in and drop out at any time. It's really great. Would you like to see it? [applause] Well let's do a demo. [applause] Well I think for our first live demo of group Facetime, I'm going to contact the folks back in Cupertino. I can just dive in to this conversation I have going with the members of the Facetime team, and it looks like actually they're already on a group Facetime call, so I'm just going to join right in. Hey everybody. Hey Craig. Check it out. So it's this beautiful Facetime UI with these big gorgeous tiles right up front where you see some of the leaders of the Facetime team, and down at the bottom there's and area we call the roster that contains everybody else. And of course I'm right there in the lower right-hand corner. Hey Craig. Wait, am I on the big screen? Yes, Lauren, this is not a test. You're in front of 6000 of your biggest new fans. Now, what you probably notice is when Lauren spoke, her tile automatically got larger to reflect her prominence in the conversation. This is totally automatic. Hey Roberto, how's it going back there in Cupertino? I'd say it's going pretty well, and Lauren, sorry for stealing your spotlight. So this works of course for people in the roster as well. When they speak, they come forward. Hey Christopher, you ready to make your big entrance? Finally, my moment has come. Hello world. Now, well done. Now you can control this too, so I want to bring Woody front and center, I just double tab. There he is. Woody, your baby is performing admirably here. Thanks Craig. It's exciting to finally be able to share Group Facetime with everyone. It sure is. Now, we have not only all of this, but we've also brought the fun effects to the Facetime camera. I can just tap in, and I have access to Animoji, filters, and all of my sticker packs, and everyone else on the call can apply them too. Now this is the future. Hey, Craig. Check this out, I'm a comic book koala, something I've always wanted to be. [laughter] I'm glad you've finally been able to express that side of yourself, Roberto. Hey, Tim, is that you? Yeah, it's me. I've signed up to help test Facetime. All right. [applause] Well, well thank you, Tim. Every little bit counts. Happy to help, and thanks to everyone on the Facetime team for making it a reality. I can't wait to start using it every Sunday night to follow the leadership team. Looking forward to that, Tim. Of course what I'm really looking forward to is getting Group Facetime to everyone on iOS 12. Thanks guys for a fantastic call. We'll see you back in Cupertino. So that's Group Facetime. It works on iPhone, iPad, and Mac, and you can even answer an audio on your wrist on your Apple Watch. So that's Facetime and Messages, and this is iOS 12. Improved performance, new AR experiences, Siri Suggestions, Screen Time, Memoji and fun effects in the Messages camera, and Group Facetime. I hope you like it. I'm going to hand it back to Tim. Thank you. Thank you, Craig. IOS 12 looks fantastic, and we can't wait for everyone to get their hands on it. Next up, we'd like to talk about the Apple Watch. Yeah. [applause] When we began development of the watch many years ago, we had a vision for just how impactful and essential it could become in our lives, so we worked very hard to create something that you would love and want to wear all the time, and customers do love it. In fact, Apple Watch is number one in customer satisfaction, [applause] and not just this year, but every single year since we launched in 2015. And growth has been off the charts. Apple Watch grew 60 percent last year. We're constantly hearing from customers about the many ways that the Apple Watch has changed their lives. And I'd like to share just one of them with you this morning. Mary Dovgen was boating with her husband, John, when due to a medical condition all of John's muscles went completely limp, and he fell into the ice cold water. With her arms wrapped around John to keep him from drowning, Mary could not reach her phone to call for help. But with her Apple Watch, she was able to call Siri, or to use Siri to call 911, rescuers soon arrived, and saved John's live. As Mary told us, if it wasn't for my Apple Watch, he really would not be here today. This is just one of the many stories that we've heard about about how Apple Watch is impacting people's lives. They range from getting people to be more active, to helping users live a healthier life, or even to alerting users to an elevated heart rate. Apple Watch brings such amazing capabilities right to the wrist, and of course at the heart of this is watchOS. We're excited to introduce watchOS 5 today, which brings even more ways for you to stay active and connected, and I'd like to hand it off to Kevin Lynch to tell you all about it. Kevin. Good morning. You know, stories like that, about how Apple Watch is supporting people around the world in even extreme situations like Mary and John's but also in their daily lives is really super motivating to us as a team. And there are things you do every day that shape your life. And Apple Watch has two [key areas] working to help you support those activities every day. First, staying active to increase your well-being and health and second being connected to the people and the information that you care most about, and we're moving watchOS in both of these areas. Let's start with health and fitness. Now much of the power in the health and fitness features that we've put in Watch are really empowered by the investment we do to make sure the data you see is accurate. The data from our custom-built heart rate sensor, Accelerometer, Gyro, GPS, are all thoroughly validated. In fact, from our fitness lab, we've studied over six terabytes of data where 12,000 study participants logged over 90,000 hours of sessions. And they actually burned 2.3 million calories doing this. We believe this is the largest biometric data collection of its kind. And we take this information and we work to integrate this seamlessly with the user experience so when you raise your wrist and you look at your activity rings, the information is not only accurate, but it's also meaningful to you. We really love hearing about your focus on closing these rings. And we really enable this through a number of ways in Apple Watch. First, of course, we do daily coaching so you can see what your goals are for the day. We also support celebrations when you achieve your goals, and we have special edition challenges like the most recent birthday challenge that we did. And on a monthly basis, we do monthly goals that are personalized to you. And of course the activity app also supports activity sharing, which has become one of the most popular features of the activity app. Many of you love the excitement of good old-fashioned competition though, so in watchOS 5, you can challenge any of your activity-sharing friends to a seven-day competition whenever you would like. And if they accept, you each try to win the week by closing your rings and earning points. You earn one point for each percent of a ring that you close. And while you're in the middle of a competition, your progress notifications are updated, do not only show you the progress your friends are making, but also where you stand in the competition. And when you win, you receive a new award. We're really excited about this edition, and if you're competition, it gives you a whole new way to enjoy the activity app. Now when you're doing [applause]-- now when you do want to go do a workout, there's a lot of workout types you can choose from, and they all have custom algorithms to measure things like calorie burn, pace, distance, elevation gain, and when you're swimming, it even counts laps and detects which swim strokes you're using. And with Gym Kit, your metrics are in sync with your favorite gym equipment. And we're really excited to bring more enhancements to workouts in watchOS 5, starting with a new workout type for yoga. Now this works primarily from your heart rate, and we calibrate this to your fitness through the rest of your day, so now you can more accurately track those yoga sessions including those intense Vinyasa sessions. New we've also added a new workout type for hiking. This takes into account pace and heart rate and elevation gain so you can more accurately get exercise credit while you're hiking on steep terrain or really long stages. And Apple watch has become a really great running companion especially now that we've added GPS and cellular and music streaming. And we're making this an even better experience now for training runs and races. In addition to current and average pace, you now have the option to keep track of your rolling mile pace, which is how fast you ran the immediately preceding mile. You can also now set a custom pace alert, so your Apple watch will tap you when you're above or below the pace that you've set. And finally, runners will now get cadence so you can see your current steps per minute. We're really excited for runners to try these out. Now-- Now there are sometimes when you forget to start a workout on watch, but you've started working out, and to solve this now, we're adding automatic workout detection. So your Apple Watch will now offer to start tracking your workout if it senses that you're beginning one. And even if you press start sometime after you began working out, you'll get retroactive credit for the workouts that you did. And [these start alerts] wills support all these great workouts on Apple Watch. Now when you reduce the intensity of your movement or your heart rate decreases but you forget to end your workout, of course Watch will also detect that and suggest that you stop. So, all these new features, activity competitions, the new yoga and hiking workout, new features for runners and automatic workout detection are all enabling you to more accurately track your workouts and stay motivated while you do. Now let's talk about being connected. Apple Watch enables you to remain in the moment while also easily connected to the people and information that you care about. And the introduction of cellular made this even better. You can stay connected even when you're going out for an evening, running some errands, or even going for a swim. Or, stay in touch when your phone might not be easily available to you. And staying connected with people you love is something that our customers love about Apple Watch. You can easily make or receive a phone call, and you can hear the emotion and tone at the other end of the voice as you talk in real time. Or you can use Messages to have impromptu, short conversations with loved ones in a Message threat. In watchOS 5, you'll have an entirely new way to communicate on your watch. That's real-time voice but with a spontaneity of short messaging. Did you steal my chips? Maybe. I cannot wait until you go to college. Introducing Walkie Talkie. This is a new app by Apple Watch. It's fun, easy way to talk with friends and family. Let's take a look at how it works. First you choose who you'd like to enable Walkie Talkie with, and it suggests some people that you often communicate with, so you could easily add them. Now the first time that you do this, your friend will receive a one-time request to allow a walkie talkie connection with you. If they accept, then you can speak to each other with walkie talkie whenever you like. And to do this, you just press to talk, and then your friend can hear your voice just like a walkie talkie. And they're going to feel a [haptic, hear a beep beep sound right before your voice comes out. And this new watch-to-watch connection works over cellular or Wi-Fi, and it has really high audio quality. And it's a lot of fun. We can't wait for you to try this out. That's walkie talkie. Now last year we introduced a Siri watch face, which presents the right information to you at the right time. And Siri does this using machine learning, so it's going to get better at predicting your actions over time by combining inputs like the time of day, your location, your daily routines, or which apps you use when. We're making some great enhancements to the Siri watch face now. First, we're adding new content. So now you can get live sports scores, you can get commute time home or to work, or you can see your heart rate, for example, after a workout or your resting heart rate. And we're also adding Siri Shortcuts. So the shortcuts you saw coming to iOS 12 are also going to be available in watchOS. So in addition to getting relevant information, you'll also receive predicted shortcuts right on the Siri watch face. So in a wrist raise, you'll be able to directly do things like turn on your leaving home scene or start an outdoor walk or play your favorite morning playlist. And these shortcuts app based on whether you typically do those actions at those times. So super easy now to just tap and do those actions. Also for the first time, you can now use third party apps on the Siri watch face. So now you can see both relevant content and shortcuts from your favorite apps. So if you always go running with Nike Plus Run Club at a certain time or you're logging your meals with LoseIt or you use City Mapware to find your commute home, you just raise your wrist and tap, it's that easy. So new content, support for Shortcuts, and third-party apps making for an even more powerful Siri watch phase. Now currently to talk to Siri you raise your wrist and you say, hey Siri. And when you think about this, it's quite a strong signal when you raise your wrist and talk to your watch. It's kind of like when someone's standing right in front of you, you don't need to say hey because you have their attention already, so we're bringing this same social cue to Siri on the watch so you no longer need to say, hey Siri. You just raise your wrist and talk to Siri. [applause] Now we're also bringing a number of improvements to Notifications, which are now more actionable and interactive. So, for example, with this notification from Quantis, you can check in and share your flight details right from the notification. After you finish a ride with Dee, you can scroll down. You can rate your ride and pay with Apple Pay right there in the notification. And with this one from Yelp that your table is ready, if you need a bit more time, you can extend the reservation out a bit just by tapping here. So super easy now to interact right in place inside Notifications. We've also improved our Message notification. Today if someone sends you a weblink, you aren't able to view it on your list. With watchOS 5, we've integrated WebKit, so now you have the ability to view web content and mail or messages. You can even tap on that link, yes, to easily view things like menus here. And while we think full browsing doesn't make sense on your wrist, there are times you get content. You'd like to see it right in the moment, and now you'll be able to do that and watch OS 5. And the contents can be formatted for the small screen, and the where reader made is available, it uses that on the watch. So WebKit on watchOS. Now with watchOS 4, we introduced an entirely new way to listen to music while you're on the go. You can stream 48 million songs on your wrist with Apple, or you can listen to automatically synced and created play lists. And now with watchOS 5, we're giving you even more to listen to. That's right. The Apple podcast app is coming to Apple Watch. Upcoming episodes from subscribed podcasts will be automatically synced to your watch, so you can get them right there, or you can just ask Siri to stream a podcast for you, and it'll start playing and playback resume across all your devices so you can continue just where you left off. So that's Podcast. Now those are the great new features coming in watchOS 5. So stay connected with people and information that you care most about, interactive notifications, web content, new content and shortcuts in the Siri face, podcasts and of course walkie talkie. Now we'd like to show you this stuff live in action, and we thought we'd increase the challenge a bit in our live demo here today to keep it interesting. So, Jules, who helps lead our fitness experiences, is actually going to do the demo while biking. So Jules has expertise in coaching, and she knows how to move and demonstrate and motivate all at the same time. Take it away, Jules. Thank you, Kevin. I've got some great stuff to show you that I think you're really going to love, and the bonus is, I get to work on closing my rings while I do it. So, first, I'm super excited to show you how easy it is to start a workout with GymKit. I just tap to connect and accept on the watch, and now I can control and start the workout right here from the console on the bike. All of my data between the watch and the bike is in sync, and all of my workout metrics are accurate. Now, I wouldn't typically be trying to demo things for you and sneak in a workout at the same time, but I want to show you activity competitions. And to do that, I need to keep moving. This morning I got this notification, and it says, your competition with Jay is down to the last day, and it's a close one. Have you got what it takes to win? Have I got what it takes? You bet I do. But if I scroll down, I can see that Jay has moved ahead of me by a couple of points, and that's not okay because this thing ends tonight. So, I think I'm going to add a little resistance to this workout here and work to earn as many points as possible and move ahead of Jay by the time this demo is done. I can simply tap here and send him a little smack talk to let him know I'm coming for him. Let me show you interactive notifications because I have a dinner reservation tonight that I made through Yelp. If all goes well here, I'm going to go out with my girlfriends from work to celebrate. And because we all know that Jay is going to lose this competition, maybe I should invite him along so it can be his treat. Now I can simply add to the number of guests and confirm right here in the app without ever having to open it. It's that simple. Now in watchOS 5, notifications from apps are smartly grouped together, and here are a couple messages from my friend Catherine. We're trying to get to a yoga retreat this summer, and it looks like she sent me a link. Now, I can tap it here and see if she's found somewhere great for us to go. Oh yeah, this looks beautiful. I'm already in, but I can continue to scroll through the site and maybe read a little bit more about the retreat, even check the dates to make sure that they work. And I can do all that right here from my wrist. Let me show you the enhancements that we've made to the new Siri watch face. At the top here, you see the workout that I'm in right now, and when I'm ready to get back to it, I can just tap. Next up is Glow Baby, because it's about this time each day that I check the app to see if my husband has logged our one-year-old's nap. Oh, good job, baby. You slept two hours, and you parents know that good naps equal good nights, so I'm looking forward to that. AR. As I scroll through my day, I can see my next meeting. I can see traffic and directions to places that I'm going later today. Here's that dinner that Siri found in my calendar, and waiting for me at the end of my day is a shortcut to my evening meditation or ten percent happier, nice. Now, I want to check back in on this competition, but I thought it might be a good idea to get a little moral support from my number one fan. And you heard Kevin say how walkie talkie is so great for staying in touch with close friends and family. So I set my daughter up with walkie talkie so that she can help demo what it's like to receive one and maybe give me a little bit of love up here. Oh, there she is. Mommy, I see you on TV. Isn't that fun? How am I doing? So good. I know you'll beat Uncle Jay, #mommyforthewin. Thanks Peach. I love you. And that's how walkie talkie works. AR. The time is now to check in on this competition and see if I have done enough to pull ahead. I'm going to tap into activity and swipe over to see the score. Oh, yes. Sorry Jay, but the day is still young. Thanks everybody, and back to you, Kevin. Way to go, Jules. So in watchOS 5 we've enabled even deeper integration on the watch for apps, enabling them to work right in the moment. And apps can include interactive controls within notifications so you can quickly do more without even opening an app like extend your parking with Pay by Phone. And with Shortcuts, there's also new opportunities for third-party apps to appear on Apple Watch right on the watch face. You can tap on a shortcut on the Siri face to order coffee, rent a bike, or pick up where you left off in a workout. You can even use your custom Siri commands that you created on your phone to speak on your watch. And for native apps building rich experiences, we've improved the workout API for greater performance, and we've added the ability for third-party apps to play background audio. This enables you to easily sync things, of course, like audio books, favorite playlists, guided meditations right to your watch and to be able to play continuously in the background. Now there are so many other features coming into watchOS including things like the ability to customize the button arrangement in your control center, or you can add an air quality complication to your watch face, but one really exciting one is student ID cards. You're going to have the ability to add your student ID card to wallet on your iPhone and Apple Watch. And this works by simply holding your watch near a meter anywhere you can use your student ID cards on and off campus. You can get access to places like your dorm or the library or events on campus. You can even pay for things like snacks or laundry or dinners, and this works with your watch or your phone, and it will be available this fall starting with these universities and will expand to more campuses over time. Now this is also Pride month, and we're really excited to introduce an all-new Pride education watch band. And we didn't stop there. We made a beautiful new face that matches perfectly with the band. And the face and the band are available today. And if you'd like to use the new watch face, you can choose to add it from the watch face gallery starting at noon today. That's just some of what's coming in watchOS 5. Thank you very much. Back to Tim. [applause] Thanks Kevin. Thank you, Kevin. WatchOS just keeps getting better and better, and we're excited for you to try all of these great new features to help you stay more active and connected. Next up is Apple TV. Last September, we introduced Apple TV 4K, and our customers love it. Since its introduction, the Apple TV business has grown an incredible 50 percent. Today, we've got some great new enhancements to Apple TV 4K and tvOS, and we think you're going to love them. So I'd like to invite Apple TV lead designer, Jen Fuls [phonetic] to the stage to tell you all about it. Jen. [applause] Thanks Tim. Apple TV is built on an incredible platform, tvOS. And tvOS is built specifically for the living room, to make it easy to enjoy your favorite TV shows, movies, apps, and games. Now as Tim mentioned, last September, we introduced Apple TV 4K with the goal of bringing the highest quality cinematic experience right to your home. It offers both 4K and high dynamic range, including support for HDR 10 and Dolby Vision. Time said, the new Apple TV is a high fidelity powerhouse. And to ensure you can enjoy this beautiful experience with as much content as possible, iTunes offers the largest collection of 4K HDR movies. And we've upgraded your previously purchased movies to 4K HDR for free on all available titles. There are also, thank you, there are also many 4K and HDR TV shows available from popular services like Amazon Prime video and Netflix. And we know what makes for an amazing cinematic experience is not just great picture quality. It's also incredible sound. So Apple TV 4K is bringing you the latest in audio technology, Dolby Atmos. With Atmos, you get room-filling sound. The perfect complement to the stunning visuals of Apple TV 4K. Now what makes Atmos so special is that unlike a traditional surround sound setup, where sound is assigned to channels like the left or the right, Dolby Atmos has the ability to completely immerse you. Atmos moves sound in three-dimensional space, creating an experience with powerful audio that flows all around you. It puts you right in the center of the action. All this with a home theater setup as simple as a Dolby Atmos enabled soundbar and an Apple TV 4K. In fact, Apple TV 4K is the only streaming player to be both Dolby Vision and Dolby Atmos certified. And this Fall, iTunes will be bringing you the largest collection of Atmos content anywhere. And just like with 4K HDR, your iTunes libraries will be upgraded to include Dolby Atmos on all supported titles for free. [applause] Now of course these movies and so much more are available in the Apple TV app. The Apple TV app is the center of your video experience, a single place to find and watch what you love across your favorite apps. It's on Apple TV, iPad and iPhone where you can find not only On Demand TV shows and movies but also live content including sports. The Apple TV app now offers a huge range of live sports. And we've added live news to make it even easier to stay current. The edition of live sports and news providers brings the coverage of the TV app to over 100 video channels, giving you a massive selection of content all in one place. With all this, Apple TV is the best box to connect to your TV. And that's now more true than ever as more and more cable companies fundamentally shift how video gets to your TV. This typical cable box is becoming a thing of the past as these companies embrace internet-based delivery. And many of them share our vision of Apple TV as the one device for live, On Demand, and Cloud DVR content. And we've already started working with partners around the world to make this a reality. In France, we're working with Canal Plus. Almost a third of French households subscribe to Canal services, and their subscribers can now choose Apple TV to access more than 100 live channels and some of the biggest sports including the French Open in 4K. In Switzerland, we've partnered with Salt. Salt just launched a new TV service with more than 300 live TV channels, and it's available exclusively on Apple TV. And I'm really excited to announce that here in the U.S., Charter Spectrum will be coming to Apple TV late this year. That means that up to 50 million homes will be able to choose Apple TV to access all their live channels and thousands of On Demand programs. And, they'll be able to use Siri and the TV app to get access to their TV service not only on Apple TV but on iPhone and iPad as well. Now with nearly any cable subscription, you can get access to dozens of video channels like ESPN, Showtime, NBC, and many more, but it used to be really painful to set up, having to authenticate each app individually. I'm sure all of you have seen that six-digit code you have to go online and type into a web browser. Well, last year we introduced single sign-on. Enter your cable credentials only once, and we unlock all of the apps your subscription supports. Well now we're making it even easier with zero sign-ons. So now if you're on your TV provider's broadband network, we'll securely and automatically unlock all the supported apps included with your TV service. No credentials needed. It just works. Charter Spectrum [applause]-- this is really awesome-- and Charter Spectrum will be the first to support zero sign-on and we'll be adding more providers over time. We're also giving you even more options for ways to control your Apple TV. For example, you can get quick access on your iPhone where Apple TV users will get the Apple TV remote automatically added to control center. And, for those that have installed home control systems, we're working with the leading providers so that you can use their remotes to control your Apple TV, even including support for Siri. At Apple, I love working with an amazing team designing for a product that I enjoy every day and on the biggest screen in my home. I get to watch all my favorite movies, TV shows and more. Over 40 million songs including the best metal on Apple Music. All of my photos, videos, and memories as well as thousands of apps and games that many of you have built. And, Apple TV brings some amazing views right to my living room with Aerials. Now, one of the questions I get asked the most about Aerials, especially by my family members is where is this? Well, this ball with just a tap of the Siri remote, you'll now be able to see every Arial location. And, you can even swipe between locations so you can see even more. And today, I have an incredible new location to share with you. For the past year, we've been collaborating closely with an amazing partner, filming some stunning footage from around the world with a very unique vantage point, and here it is. Earth. This is filmed by astronauts aboard the International Space Station. You can see Sicily just off the boot of Italy, and check out that Mediterranean Sea. The blue is just phenomenal. So here's a fun fact. The Space Station makes an orbit every 90 minutes, so that means they get 16 sunrises and sunsets every day. And with all those sunsets, nighttime can be particularly stunning the way those urban areas pop with light. Here we are flying over South Korea toward Japan. You can see Tokyo coming into view at the top, some distant stars, and even upper reaches of the Earth's atmosphere, which is that orange band enveloping the Earth. And here's one that might look a little more familiar. This is the Northern California coast heading southward. You can see Lake Tahoe surrounded by snowcaps and the San Francisco Bay. These are truly incredible and offer such a unique perspective on the world. I'd like to give a huge thanks to the International Space Station National Lab and CASIS for all their help in making this a possibility. These are going to look incredible at home in 4K HDR. So that's tvOS. Apple TV gives you amazing picture and sound with 4K HDR and now Dolby Atmos. And the Apple TV app is the center of your video experience. Available on Apple TV, iPad, and iPhone, it's the one place to find all your favorite TV shows, movies, sports, and news. And with that, I'll hand it back to Tim. Thank you. Thanks Jen. Apple TV 4K and tvOS look and sound awesome. You're really going to love it. Next up is the Mac. At Apple [applause]-- yeah. We love the Mac. The Mac was the first computer that made powerful technology so easy to use and put the customer at the center of the experience. And of course that remains at the core of all Apple products. For more than 30 years, the Mac has empowered people to create all kinds of amazing things, from the personal to the professional. Today, we're excited to take Mac a huge leap forward. The next version of macOS is chock full of new features inspired by pro-users but designed for everyone. I'd like to bring Craig back up to talk about it. Craig. Hello again. It's true that macOS is the heart of what makes a Mac a Mac. And we want as many Mac users as possible to have access to our latest software, and that's why six years ago with the introduction of OS 10 Mavericks, we began offering our macOS updates free for Mac users. And 2013 was also the year that we first introduced our California naming theme. Now after spending a year by the ocean, we not only modernized the look and feel of macOS, but we headed to the mountains with macOS Yosemite. Now, as you may be aware, our naming of Mac releases is handled by our crack marketing organization, and as you've probably noticed, they went on a four-year mountain-bound bender. [laughter] In El Capitan we added metal, our ground-breaking graphics technology. In Sierra, we brought Siri to the Mac and extended our capabilities and continuity. And last year with High Sierra, we focused on deep technology, preparing the Mac for future innovation. Well this year, we made some striking changes to macOS, and we've left the high country for a place entirely different but not less beautiful and here still in California, and I'd like to take you there now. Our next release of macOS is macOS Mojave. Now, Mojave is beautiful during the day, but what really captured our imagination was the beauty of the desert at night. And this inspired one of our most distinctive new features, and I'd like to show it to you now. So here we are live in macOS Mojave, and I'd like to show you a new side of Mojave. We call it dark mode. And as you see, dark mode is not just about the dock or the menu bar. It extends to your Windows Chrome, your sidebar, and even the content of the Windows, and it's so great for Pros. It makes photographic content absolutely pop off the screen. It's just gorgeous. It's so nice. And this is great not just for photography but when working on presentations or documents. It's also great doing ordinary things. Maybe you're working in a dark environment. Just look at calendar. Even mail in dark mode. It's so great. And I think some of us are going to want to run dark mode just because it's so cool. I mean your emoji look great. Your photos look great. I mean check out your album art and music or your For You feed in Apple Music. But I think one audience that's going to especially appreciate dark mode are some of you here in this room, our developers, because Expo looks fantastic in dark mode. Whether it's your source code or even interface builder and all of its inspectors, they just look fantastic in black. And that's a quick look at dark mode. Now, we were so inspired by this changing desktop wallpaper that we decided to add a new feature to Mojave that I think you'll enjoy. It's called dynamic desktop, and when you're using it, you're desktop actually subtly changes throughout the day from morning, to afternoon, to evening. It's really cool. Now, there's much more to Mojave that I'd like to share with you through demos, and it starts with the desktop. Now, the desktop is so crucial to how many of us use our Macs. When we have files that we're actively working on, we often put them on the desktop, but the result can be a desktop that looks something like this. And so now in Mojave we have a really great solution, and we call it Desktop Stacks. All of the contents of your desktop are automatically arranged into these stacks, and they can be arranged by kind, by date, or even by tag, and they're really easy to use. You just click on them. You can see all the contents in the stack. You can double click to open a document and put it away. And they stay organized. So for instance, if I bring forward mail, maybe I drag an image out, I want you to watch what happens because the image flies right into the right stack. Now you can also scrub your stacks. So, for instance, I'll just scrub across this stack. You see I can select between different photos, pick one up. Actually let me just hide mail here mid drag. I got a little excited with my, with all of my stack action. So I can just drag this out and drop it in just like that. And that's a quick look at Stacks. Now-- We've also brought some great new changes to the finder. I'd like to show them to you now. Now it starts with a new view. We all enjoy using icon view, list view. There's of course column view, but now we've added an all-new view called gallery view. It has a big preview up top, a set of thumbnails along the bottom, and it makes it easy to preview images, video, presentations, documents, spreadsheets, PDFs, and of course with images sometimes you want to know more detail about, for instance, how they were captured, and now the new sidebar in Mojave really helps because it now supports full metadata, so you can see around your photo, the camera you took the photo on, the kind of lens, the aperture settings, and so forth. It's really handy. And you'll notice also along the bottom there's this new area called Quick Actions, and Quick Actions that you act on the current photo. So, for instance, if I have a photo like this and I want to edit it, I don't have to open it and go through a new app. I can rotate it right here inside the finder. It's really powerful. Now this sidebar is available in other views of the finder. So for instance, I'm just going to bring up the preview pane here, and I'm going to do a multi selection of a PDF as well as several images, and you'll notice that the quick actions area is contextual, so it shows me create PDF as an option. I'm going to click create PDF, and it's going to assemble all of these photos into a PDF just like that. But what's really great is these actions are also customizable, so you can create automator actions and assign them to buttons here inside of Finder. So you'll notice that now that I have this PDF selected, I have an option to run a custom automator action that I've created called Watermark PDF. When I click it, my custom action runs, and my document is watermarked, just like that. And those are some quick enhancements to the finder. Now a tool that I think many of us love when working with files is Quick Look, and now in Mojave, we made Quick Look more powerful than ever by integrating Markup. Let me show you how it works. So you see down here I have a permission slip and it's a PDF document. I'm just going to hit tap spacebar to Quick Look it, and you notice now I have the option to invoke Markup. I click, and now I have access to my markup tools, including my ability to sign this document. I can just drag out my signature like this, and I'm done. Now this works for all kinds of files. So, for instance, with images I can rotate and crop, and with video I can even trim right here inside of Quick Look. And that's a quick look at Quick Look. Next, I want to talk about how we capture content on our Mac because one of the tools I think many of us use all the time is Screenshots, and we've made Screenshots more powerful than ever in Mohave. So let's take a look here at a webpage, and I'm just going to take a screenshot in the traditional way. I'm going to screen shot a selection of the page, and I want you to watch what happens in the lower right. I get a thumbnail instantly of that screenshot, and when I double click in, I get an accelerated workflow right into Markup where I have access to all of my tools. So, for instance, if I want to create a magnification here, I can just drag that out. Magnify. It's that easy. But now we've also made it easier to access a variety of tools, so when I bring up my screenshotting, you see I'm presented with this hud that tells me I can capture the entire screen, a selected window or selection, but we've also added screen capture for video right into screenshotting. Let me show you how that works. So I'm going to go to a webpage here that has an animation running. I'm going to bring up my screenshotting tools, and say record the selected area. I'll just make a nice selection here. Okay. And I'm recording just like that. Of course I can manipulate the app, so all will be reflected in my recording, and when I'm done, I can click the stop button right up here. And now you notice I have this thumbnail in the lower right. Well I can actually pick this up and drag it into a new space and incorporate it right into a document, just like that. Now, we've further enhanced the way you capture content, and that brings me to Continuity. So Mac users love Continuity for the way it lets us work across our devices with things like Air Drop or the ability to unlock your Mac using your Apple Watch. When it comes to capturing content, we'll walk around with one of the best content capture devices in the world in our pockets, our phones. And so we wanted to take advantage of continuity to bring that to the Mac with a feature we call Continuity Camera. Let me show you how it works. So here in my Keynote presentation, I have a space that's just waiting for a new photo, and on my phone right here. Well, when I select this object, I can choose to take a photo. And I want you to watch what happens when I select this to my phone. It automatically immediately lights up ready to take a photo. So I can take one like this, and when I do, I can select use photo, and it apps directly in my document. Isn't that cool? Now, this works as well for scanning documents. So here I have a place where I could use a scan. Once again, I'm going to select from the menu and this time scan document. Again, my camera lights up, this time right in my document scanner. I just scan like this. I can save it. And my scan goes immediately, and I think I forgot to push the save button. Sorry about that everybody. There you go, appears immediately in my document. Thank you. [applause] And so just like that I can take photos, stills, and even capture video, and this is a quick look at some great new features in Mojave. Next, I'd like to turn to apps. We are bringing News to the Mac. Now News has all of the stories you've come to expect from News on iOS, and it looks amazing on the Mac display. You get top stories picked by our editors, trending stories, you're personalized for you and that's not for me, and you also get this great new sidebar where you can drill in and jump right to the topics and channels you follow. News is going to be great. We also have Stocks coming to the Mac. Now you get your stock prices combined with high-quality business news delivered from Apple News. It shows you your watch list with your prices on the left, and you can drill in to this interactive chart to get more information and, of course, great news. We're also bringing Voice Memos to the Mac. Now Voice Memos is the most popular voice recorder on iOS, and so many of us use it to capture music recordings or lectures, and now that Voice Memos syncs via iCloud to your Mac, you can take those recordings that you make and for instance drag them right into Garage Band as the foundation for a song. Finally, thrilled to announce Home is coming to the Mac as well. You have all of your accessories here. You can run your scenes as well as monitor your video cameras, and of course with Siri you can command your home with your voice. So those are four great new apps coming to Mojave. Next, I'd like to talk about security and privacy. You know, one of the reasons that people choose Apple products is because of our commitment to security and privacy. And we believe that your private data should remain private, not because you've done something wrong or that you have something to hide but because there can be a lot of sensitive data on your devices, and we think you should be in control of who sees it. Now, to begin we protect your information on your devices using state of the art hardware and software, and this year we're adding greater protections about how apps can access that information. Today, Apple devices check in with you before granting an app access to information like your location. You could tell because you'll see an alert just like this one, and macOS already provides API-level protections for things like contacts, photos, calendar, and reminders, but now in Mojave, we're extending these protections to include your camera and your microphone as well as protecting sensitive parts of your file system, like your mail database, your message history, and your backups. And all of this is protected by default for any app that you run on the system, an important protection. Next, I want to turn to some great enhancements to Safari. Safari works really hard to protect your privacy, and this year it's working even harder. Last year, we introduced intelligent tracking prevention to dramatically reduce the ability for apps to track you across websites using cookies. This is the kind of thing where you look at a product on one site. Then you move to another site and another site and somehow this is just following you wherever you go. Well, we've all seen these, these like buttons and share buttons and these comment fields. Well, it turns out these can be used to track you, whether you click on them or not, and so this year we are shutting that down. Now if you do want to interact with one of these or one of these apps tries to access that information, you'll get this, and you can decide to keep your information private. Now next let's talk about Fingerprinting. You know, data companies are clever and relentless, and in addition to cookies, they use another method called Fingerprinting. And here's how it works. Just like you can be identified by a fingerprint, it turns out that when you browse the web, you're device can be identify by a unique set of characteristics, like its configuration, its fonts that you have installed and the plug-insulin that you might have on the device. And these data companies can use the set of characteristics to construct a unique fingerprint to track your device from site to site. With Mojave, we're making it much harder for trackers to create a unique fingerprint. We're presenting webpages with only a simplified system configuration. We show them only built-in fonts, and Legacy plug-insulin are no longer supported, so those can't contribute to a fingerprint, and as a result, your Mac will look more like everyone else's Mac, and it will be dramatically more difficult for data companies to uniquely identify your device and track you. Now, we're bringing all of these new protections to Safari on both Mojave and iOS 12. Next, let's talk about the mac App Store, and to do that, I'd like to invite to the stage Anne Tye, our product marketing manager for the App Store. Anne. Last year, we launched a completely redesigned App Store on iOS. Every day, we celebrate apps, games, and developers. We've written more than 4000 stories for the New Today tab, and hundreds of them have each been read by more than a million people. The response has been incredible, and we've learned a lot. We've got a bunch of great new features coming later this year that we'll cover in sessions. This year, we are turning our attention to the Mac App Store. [applause] Since it launched in 2011, it's changed the way we download and install software for Mac, making it easy with one click. It's the biggest catalogue of Mac apps in the world. It's also a trusted and safe place to download software. Entrusting where you get your apps from has become more important than ever. Developers can distribute their apps to 155 countries and get worldwide payment processing, and it offers seamless software updates from one place. This adds up to a great experience for our users. We've spent a lot of time thinking about what people do on their Macs and wanting to create a place organized around those themes. So we've redesigned an all new Mac App Store from the ground up, and we're thrilled to show it to you now. It's got a beautiful UI that should feel familiar but new and designed first and foremost to be a great Mac app. Starting with the new discover tab where each week you can find in-depth editorial about the best Mac apps through stories and collections and see what's most popular with top charts. Here's a story about musician and founder Karim Morsy. Learn about how he uses his app, Djay Pro 2, and get inspiration for your own set. Helpful videos auto play, so you can see what apps are capable of before downloading them. Visit the all-new create, work, play, and develop tabs where you'll find helpful recommendations and expertise around each theme. Here's the create tab where you can find apps that bring your artistic ideas to light. These tabs will also help you make the most of apps you might already have with tips and tutorials even the most expert users will find useful. The work, play, and develop tabs share the same beautiful design, and you can still browse by category on the categories tab. We've redesigned product pages too, bringing many features over from iOS based on our learnings there. It has more useful information like video previews available on the Mac App Store for the first time, an apps rank if it's charting and if it's been named editor's choice. Ratings and reviews are now front and center, and these are so important for app discovery. So we're introducing a ratings and review API for Mac apps. Now, it'll be easier than ever for people to leave feedback. We're really excited about the all-new Mac App Store. We've talked to some developers already, and they're really excited too. Like Microsoft, who will bring Office 365 to the Mac App Store later this year. And Adobe is bringing Lightroom CC. Panic is bringing Transmit. And Bare Bones is bringing BB Edit. [applause] And many more great names are coming to the all-new Mac App Store too. We can't wait for you to check out the new Mac App Store. Now, I'll hand it back to Craig. So we think the Mac App Store is going to inspire whole-new generations of apps, and so we want to talk about some of the technologies that will be behind some of that next generation. We spoke earlier about ARKit. I want to talk about two more. And let's start with Metal. Now Metal is the technology to get the highest performance graphics and computation from graphics processors. Metal was designed for modern GPUs. It's incredibly efficient, and that enables amazing console-level games like Fortnight from Epic to run great for the first time on mobile. But Metal also enables these games to scale to take full advantage of modern Macs. In fact, across iOS and the Mac, there are over one billion Metal-enabled devices, and we're constantly making Metal better. To bring the highest GPU performance in reach of all Macs, we've recently added support for external GPUs, and this is powered by Metal, and the results are truly mind boggling. For instance, running a filter in DaVinci Resolve, look at how it scales on the incredibly fast iMac Pro as you add up to four eGPUs. Now, the results are even more staggering when you take a 13-inch MacBook Pro and add eGPUs to it, achieving up to 8.5 times speedup. It's pretty awesome. Now, eGPUs also enable Macs to achieve all-new levels of performance and realism in 3D rendering and gaming. Now this beautiful forest you see here, this isn't a video capture. This is from Unity's new Book of the Dead interactive demo, and it's being rendered live right now on a MacBook with an eGPU powering this display. I mean doesn't this look amazing? Now, of course because this is rendered live, we can check what's up ahead, so let's start walking. Now Unity is using Metal's unified graphics and compute to generate real-time lighting and complex post-processing effects. So cool. And that's all rendered live on a MacBook running with an eGPU. It's pretty great. Now another place where we're doing incredible acceleration with Metal is in machine learning. And today ML specialists usually use one of these third-party libraries to train their models using servers, and now it turns out we can accelerate tools like this with our new Metal performance shaders. We're seeing speedups of up to 20 times and using the GPU with Metal instead of CPU-based training. And while speeding these tools up is great, we actually thing there's a better way for most developers, and that's training on the Mac you already have using a great new tool we call Create ML. Now, Create ML is designed to let you train without being a machine learning expert. You can train vision and natural language models. You can bring your own custom data, and it's really easy to use because it's all built in Swift. In fact, you can use Xcode Playgrounds to train your model. Just drag one in, you're training set, you can drag in your test set as well, and the training is all GPU accelerated, so it's incredibly fast. Now, as an example, we worked with Memrise. They're a developer who uses the camera to identify objects and speak them in multiple languages, and in the past, they would train their model with 20,000 images, and it would take them 24 hours to do so. Well now with Create ML, they can train that same model in 48 minutes on a MacBook Pro. And on iMac Pro, it's just 18 minutes. And what's even more incredible is that model in the past for them was 90 megabytes, and now it's just three megabytes. It's a huge difference. Now we also are making models run much faster on device using Core ML 2. Now Core ML is our technology for high performance on-device machine learning, and now it's better than ever. It's 30 percent faster in on-device processing using a technique called batch predictions, and you can reduce your model size by up to 75 percent using quantization. And so that's Core ML and Create ML. You no longer have to be an expert in machine learning to build those techniques into your app. Now, these technologies are redefining what's possible in apps, and they're common not just to macOS but also to iOS, and the fact that the mac and iOS share so much technology has led people almost every year to keep asking us the question, are you merging iOS and macOS? So I'd like to take a moment to briefly address this question. No. Of course not. We love the Mac because it's, and we love macOS because it's explicitly created that a unique characteristics of Mac hardware, like the ergonomics of the keyboard and the trackpad, the flexibility in displays and storage and because of the power it exposes. It makes the Mac able to accomplish almost anything. So we think that this question is actually coming from something else. You know, Mac users have access to a rich set of great native applications, apps that take full advantage of the power of Mac technologies. But Mac users also use apps based on other technologies. We routinely access web-based experiences like Netflix that build on WebKit, the standards-based web technology in Safari, and we also run sometimes cross-platform games built on technology like metal. And all of these platforms enrich the Mac user's experience. But we think there's room for one more, and so we'd like to give you a sneak peek of a multiyear project we have going on. Because we see a huge opportunity for the Mac to tap into the world's most vital app ecosystem. It's called iOS. I think you might be familiar with it. Now, there are millions of iOS apps out there, and we think some of them would be absolutely great on the Mac, and Mac users would love to have them there. And from a technical standpoint, it's actually a really good fit because from day one iOS and macOS have shared common foundations. But iOS devices and Mac devices of course are different, and the user interfaces are somewhat different and so the frameworks underneath are as well. And that makes today porting an app from one to the other some work. We wanted to make this much easier, and so we've taken some key frameworks from iOS and brought them to the Mac. And we've adapted them to specific Mac behaviors like use of trackpad and mouse, window resizing, integration of things like copy and paste and drag and drop into the system services on the mac. Now, phase one of this effort is to test it on ourselves. So this year in macOS, we've taken some of our own iOS apps, and we brought them to the Mac using those technologies to make sure it works well. You've actually heard about several of them earlier today, and it turns out they make fantastic Mac apps, and we're able to bring them to the Mac with very few code changes. Now, this is going to be coming to you developers next year. So you can easily bring your iOS apps to the Mac, and in the meantime, we hope you enjoy News, Stocks, Voice Memos, and Home in Mojave. Now we're really excited about Mojave. From desktop, Stacks, to Finder with gallery view to enhanced screenshots and markup, News and Home on the Mac for the first time. Stocks and of course the redesigned Mac App Store. And of course there's even more. Like APFS now supports Fusion and hard drives, and Safari tabs can now have favicons if you want them there. And, of course, Group Facetime. So that's macOS Mojave. I hope you like it. I'm going to hand it back to Tim. Thank you so much. Thank you, Craig. What a huge update to macOS and what an extraordinary morning. We got started with iOS 12 with all-new capabilities including taking AR further than ever before, bringing Siri to any app with Siri Shortcuts and ScreenTime and cool new communication features like Memoji and Group Facetime. And watchOS, now with Walkie Talkie, Activity, Competitions, New Workouts, New Siri Capabilities, and so much more. And Apple TV 4K now with Adobe Atmos, some great new partnerships, and the ease of zero sign-on. And you're going to really love those new Aerial screensavers. And macOS Mojave with all new dark mode, great updates to desktop and finder, enhanced privacy and security, and a completely redesigned Mac App Store. The updates will be available to our users this Fall, and there will be developer betas for each of them after the Keynote this morning. Thank you. Now, before we close, we wanted to celebrate you and the amazing work that you do, so we went out and talked to some of the most important people in your lives, the ones that know you the best, and we made a short video, and I'd love to run it for you now. I don't have any idea at all how to create an app. I don't understand any of it. Ask me another question. My son is a developer at Robinhood. My daughter, Jody, is a WWDC 2018 scholarship winner. My brother started and founded Yelp. All the apps. Timeless. One drop. Splitter critters. Homework. Homework, you talk to her. My brother Derek created the app Refugees and Immigrants the creative, the collective. Why did I say the creative. At Christmastime, when most kids maybe would want skis or something, he wanted computer books. He stayed in his bedroom, making games while the other kids were outside playing. And when she is really doing the coding almost always we have to tell my daughter to stop, but then my daughter will be like, just give me another ten minutes. I want to get this solved first. When I watch him code, his eyes move in a weird way, and he's like totally focused. Even when my friends are over, I'd say, we'll look at Christopher, look at him. And they'd go, wow. When he first came to us with the idea, we were like, okay, good luck. He started off in his small little one-bedroom apartment with next to nothing, and sacrificed eating a burger and just have ramen just so that he could, and I'm like just buy the burger. And he's like no, that's three bucks that I could put towards the app. Jeremy went out to build the first version of Yelp, and it was a total bust, a total failure. She tried really hard on a couple different titles, and you've never heard of them. And, you know, that's a really rough feeling. That was definitely a low point for Jeremy because it was like, hey we got to retool and try again. What Jeremy did notice in the data was that people loved writing comments, and that was really the genesis of the second version of Yelp, which worked. You have to be really okay with waking up to failure and then at the end of a whole bunch of failures is something that's great. Tessa didn't really think I want to make an app. Tessa found a problem, a food waste which you could use the app to help to solve. Emma's grandma was diagnosed with Alzheimer's. She even forgot my daughter's birthday. So Emma tried to search for an app that would help, but she couldn't find it. So she said, you know, why don't we just do it ourselves. My husband was a diabetic. That inspired Jeffrey to do something about diabetes, and it's an answer to a lot of people's prayers. Jessie won the iPhone game of the year. I couldn't believe that he made it. I mean to me it was remarkable. She pulled out her cell phone, and I saw his app on her phone, and I was like trying not to freak out, but oh my gosh. For my wife, it's not just creating, it's really wanting to be a part of something big. The one lesson that I learned from Jeremy every day is just that determination, tenacity, to focus. He foresaw the impact that the iPhone was going to have, and he bet everything on getting an app for it, and I think that it was one of the most important things that Yelp did. I think he's made a big difference, and so do thousands of other people. She wants to not just learn also teach others and it makes me really proud to see her blossom. As a parent, no better thing is letting your kids do what they want to do, because it isn't how much money you've made. It's really changes you've left behind. And she's hoping to leave a lot of change behind. I love that video, and I'm pleased that some of the developers are in the audience this morning. Their stories are great examples of all of your passion and your creativity. We love the work you do and the impact that it has on the world. It inspires all of us at Apple deeply every day. On behalf of everyone at Apple, thank you. And I'd also like to thank everyone at Apple who made today possible. Days like this only come from years of effort and hard work and great sacrifice, and so I thank them and their families. Let's have an incredible week together. Thank you.  Hello and welcome. I'm Lindsay Verity from Advertising Platforms and I'll be joined by my colleague, Carol Hsu, and we're here today to tell you what's new in Search Ads. Search Ads was first introduced to the App Store in the U.S. in October 2016 as an efficient and easy way for you to promote your apps directly in App Store search results so customers could discover your apps while protecting their privacy. Search Ads Advanced has since expanded to the App Stores in the U.K., Australia, New Zealand, Canada, Switzerland, and Mexico. In December 2017, we introduced Search Ads Basic to the App Store in the U.S., a minimal effort cost-per-install offering that's fast become a favorite of developers worldwide to promote their apps on the U.S. App Store. And the response to Search Ads from developers of all types has been incredible. Postsnap, a U.K.-based photocard app has used Search Ads to expand their offering to new markets and they told us Search Ads has given us a huge increase in high-value users in multiple regions. Now we're using Search Ads to help us launch other apps too. And JoyTunes, creators of instrument learning apps, are seeing incredible results. They say the quality of users is so good that the numbers upgrading from free to subscription is double that of any other pay channel. And, for Playtika's World Series of Poker, it's the quality and engagement of users that matters and they said Search Ads is an extremely powerful product that has brought high-quality users and engagement rates 10% higher than the average. It's been really great and wonderful hearing these stories and so many more like them. And so, today, I'm excited to announce that this summer, Search Ads Advanced is coming to six more app stores. Thank you. Japan, South Korea, Germany, France, Italy, and Spain, giving you even more opportunities to reach high-intent, high-quality users in more of the markets you've told us are most important to you. And, what's more, we'll be introducing Search Ads Basic to all 13 App Stores where Search Ads will be available. And we've worked hard to make sure that Search Ads Basic remains a simple, easy, effortless way for you to promote your apps across multiple app stores. Let me show you how easy it's going to be. Imagine I'm a developer and I want to start to promote my new app, NoMoss. I simply select the app from the drop-down list and note that I know see an ad example on the right and I'm also presented with a list of the app stores where Search Ads is available and also where my NoMoss app is available too. I want to permit my app in as many app stores as I can and so I'm going to leave all of those selected. Next, I enter my monthly budget, which is a single budget across all of the app stores where my app promotion will run. My monthly budget for NoMoss is $5000. And, finally, I input what is the maximum amount I'm willing to pay for an install. Now, note Search Ads is recommending a maximum cost-per-install of $1.50, which is based on what it knows about my app and also what similar apps are willing to pay for an install, so I'm going to take that recommendation. I then save and I'm taken to my dashboard where I'll check back later to see the results. I can now see confirmation that my new app promotion has been saved and it should start running soon. From my dashboard, I can also add another app that I want to promote and I can also make changes to any of my existing app promotions. So, I am currently running promotions in the U.S. App Store for my LightRight app and I'd like to expand that to more app stores. To do that, I simply click on the edit button at the top of the apps card and my LightRight app is only available in 10 of the app stores where Search Ads is offered, so I'm only presented with those 10 options. Now, I can pick markets individually, Canada and France, for example, or I can select all. I save. And I can now see that my app is being promoted in 10 of 10 available app stores. Now, because I'm promoting my app across more app stores, I also want to increase my monthly budget to enable me to reach more users and achieve more installs, so I'm going to change my monthly budget from $1000 to $3500. My change is saved and I'm done. It's really that simple and we can't wait for more of you to try it. And that was Search Ads Basic. So, let's talk about Advanced and, more specifically, new creative options. Developers have told us they love the ease of Search Ads Creative and how we automatically create the ads using the metadata and imagery you already provide for your App Store product page. And App Store customers respond really well to those ads because of their noninvasive appearance and because they're an accurate representation of what they can expect from the app's experience. Since February, you've had the option to add up to 10 app screenshots and 3 videos on your App Store product page. And just last week, we were excited to introduce our newest Search Ads Advanced feature, Creative Sets, giving you the ability to leverage these additional App Store assets to create more ad variations to help you better align your ad group keyword themes with specific audiences. Now, let me turn it over to Carol, who's going to tell you more about how Creative Sets work and give you a demo. Thanks, Lindsay. As Lindsay just showed, you can use Creative Sets to display different app previews in screenshots that align to keyword themes or specific audiences. Since Creative Sets uses App Store approved assets, this ensures the best user experience. I'm here to show you how it's done. Before we begin, imagine I'm a developer and these are all of the assets that I've uploaded to App Store Connect for my app, TripTrek. TripTrek is an app that lets users document every step of their travels and treks. The first three assets will be used in my default ad. The default ad matches what shows in organic search results; however, there are 2 specific keyword themes that I want to focus on with my Search Ads campaigns, fitness and mapping. I can use Creative Sets to choose the assets that best align to those themes. Let's go through the steps of how I do that by adding Creative Sets to my Search Ads campaigns and ad groups now. Here I am, already logged into my Search Ads account. And I'm on the campaigns view. Let's go ahead and create a new campaign together. I'll start by clicking Create Campaign, choosing my app, TripTrek, the storefront that I want to promote it in, and I'll give it a campaign name. Let's call this one Active Travel. I'll assign a budget and for now I'll skip over the daily cap and the campaign negative keywords. We'll go ahead and create the first ad group in this campaign. I'm going to call it fitness to match the keywords that I plan to bid on. I want this to run on both iPad and iPhone, so I'll leave that setting as is. And since, of course, I want this to run all day every day, I'll leave the ad scheduling alone for now. And I'll enter a default max CPT bid. I'm going to leave Search Match on so that Search Ads automatically matches my ads to relevant searches. And I'm going to choose from some of the recommended keywords some of the keywords that match my fitness theme. Maybe we'll go with hiking tracker, hiking trails, activity tracker, and step counter. And then I'll add a few more of my own. How about fitness tracker and fitness challenge, since that's one of the features in my app. Since I want my ads to show to all users, I'll leave the audience settings alone for now too. Here I can see a preview of the default text ad and the default image ad, which shows alongside the ads from my Creative Sets and matches the organic search result. But, since I want to use assets that are more closely aligned to my fitness theme, I'm going to create a Creative Set for that, so let's go ahead and add one now by clicking Add Creative Set. I'll name this Creative Set Social Hiking and choose the localization language. The set of languages depends on the storefronts that I'm promoting in and on the localized assets that I submitted to App Store Connect. I'll leave this as English for now. Next, I need to choose the right combination of assets to create an effective creative set. Here I can see all of the screenshots and app previews from the available set of assets on my app product page organized by display size and in the order that I submitted to App Store Connect. I'll only see the display sizes that have enough localized assets to satisfy a Creative Set. In App Store Connect, I had elected to use the 5.5-inch display, which is why I don't see any other iPhone display size assets here. If I'm having trouble remembering which devices are which display size, I can go to the upper right-hand corner and click on View Device Display Sizes to see that information. I'll go ahead and close this now. If I hover over an asset, an icon appears in the lower left-hand corner and I can click on that to get a closer look. I can also easily click through to see the other assets as well. Let me close this now and choose the assets for my Creative Set. For the iPhone 5.5-inch display, I have all portrait assets, so I need to select at least 3 in order to complete my Creative Set. I can see which of my assets are already being used in the default ads by this label here that says default on the first few assets. I'm going to choose assets that are different from the default ones and that are aligned with my fitness theme. Here we go. These look good. And then for the iPad 12.9-inch display size, I have all landscape assets, so I need to select at least one of these in order for my ads to show on iPad. I'll go ahead and pick this one for now. Once I've chosen assets for each display size, I click save and see that my creative set has been added to my ad group and now I'm ready to start my campaign. The Active Travel campaign now appears on my campaigns view and that's how you add a Creative Set to a new campaign and ad group. Next, let me show you how to add a Creative Set to an existing campaign and ad group. Let's go into my Navigation campaign. And from there, let's go into the City Maps ad group and I'll click on this new Creative Sets tab. This is a brand-new view in Search Ads where I'll be able to see not only how my Creative Sets are performing, but also how the default text ad and the default image ad are doing. I currently only have the default ads running and I don't have any Creative Sets yet for this ad group. So, I'll add one now by clicking Add Creative Set. Let's go ahead and name this Creative Set Mapping Features to align to my mapping theme. And I'll leave the localization language as English again. And for the iPhone 5.5-inch display size, I have all portrait assets, so again I need to select at least 3 to populate my ad. In this case, I'm going to choose more than 3 assets, including some that are already used in the default ads, and let Search Ads automatically create the various combinations of ads from all of these assets. Search Ads uses machine learning to continuously optimize to show the best performing ads. Since I'm only interested in testing ad variations for iPhone at this time, I'm going to elect to use default assets for iPad, which means only the default ads will show on iPad. I click save and now I've added my first Creative Set to this ad group and I can create up to 10 Creative Sets per ad group. I can return to this view at any time to check how this Creative Set is performing versus the default ads. So, that's how I added Creative Sets for my two keyword themes, fitness and mapping. I hope you'll give this a try yourself. Now, let me hand it back to Lindsay. Thank you, Carol. Thank you. So, to summarize, Creative Sets use App Store approved assets, so it continues ensuring a really great customer experience. And now with more ad variations, you can more closely align your ad images to specific keyword themes for audiences and our machine learning ensures you'll always achieve the best results. It's been great sharing these updates with you today. Just to recap, Search Ads Advanced is coming to 6 more markets this summer and Basic will be coming to all 13 App Stores where Search Ads is available. Creative Sets are available now, so if you're a Search Ads Advanced user, start experimenting with different ad variations and see how they can further improve the performance of your existing campaigns. And we really love to hear from you on your experiences with this new feature. As always, you can learn more at our website, searchads.apple.com. Thank you so much for listening. It's been really great sharing this with you today.  Hello. Welcome everyone. My name is Gaurav. And today we are going to talk about machine learning. Last year, we launched Core ML. And the response from developers, from you guys have been tremendous. We are just amazed by the apps you have made, the [inaudible] phenomenal. So let me first begin by saying thank you. Thank you for embracing Core ML. And we are -- we love seeing so many of you using it and giving intelligent features to our users. We are in this together. Thank you. It's an applause for the devlopers. Okay. So if you recall, Core ML gives you an easy way to integrate an ML model in the app. The idea is very simple. You get an ML model, you drag and drop in Xcode, and with just three lines of code, you can run state-of-the-art ML model with millions of parameters and billions of calculations in real time. It's just amazing. And you give -- your users get real time machine learning as well as privacy-friendly machine learning. All you have to do is to drag and drop an ML model in Xcode and Core ML takes care of the rest. I think the big question remains is where do I get these models from? So last year, we provided you two options. The first one was you could download some of these models, popular models from our website but, more importantly, we also released Core ML tools. Core ML tools allow you to tap the work which is done by amazing ML community. So the idea is, again, simple. You choose your favorite learning library, train your model in that training library, convert it into Core ML from that and then just integrate it into your app. When we released Core ML, we released with only five or six training libraries support for five or six training library but within a year, we have support for all the famous training libraries out there. We are enhancing our tools to even allow you more customization. And we are going to talk about more about Core ML tools in tomorrow's session. Another thing we did towards the end of the year, we released Turi Create, our open source machine learning library. We are going to talk about Turi Create in tomorrow's session. But this year, we want to give you something even more. We want to continue our journey. We want to give you something native, something Swifty, something that harnesses the power of our Xcode, something that puts the focus on you, our developers, something that just demystify machine learning for you. Hence, we are introducing Create ML -- Our machine learning framework in Swift. So Create ML completes the left-hand side of the equation. The idea is you make a model in Create ML and you run it in Core ML. You do complete end-to-end machine learning in Swift, our favorite language. So you are not dealing with language oddities where you are training in one language and then running in, for instance, another language. Create ML is simple and very powerful. It is tailored to your app. It leverages core Apple technologies, and you do everything on your Mac. So for this year we are going to focus on three very important use cases. The first one is images, second is text, and the third one is tabular data. These are the top use cases that we believe will benefit you. So you can do things like custom image classifier. Idea is that you make your own image classifier that can recognize product from your product catalog. You can do things like text classifier so you can make your own sentiment analysis, topic analysis, domain analysis. And you can also do classical regression and classification on tabular data. For example, let's just say you want to predict the wine quality using its chemical composition. The possibilities are endless, and we are going to discuss them in detail in the next 30 minutes. However, before we do, let's take a look at common workflow. First, let's just say you are trying to enable an experience in your app, make sure that machine learning is the right thing to do there. So don't just blindly apply machine learning. Make sure machine learning is the right thing to do there and define a machine learning problem. Second, collect data. Make sure this data reflects the real usage of your app. So, for example, if you're making a custom image classifier that is going to be used by users on their iPhone, so collect pictures from your iPhone. Do not collect -- collect less screenshots but have more iPhone pictures. Then you train your model. Finally, an important step here is to evaluate this model. The model evaluation is done on a separate handout set. If you're happy, you write out the ML model. But let's just say the results are not good. You should either retrain your model with different parameters or you collect more data. Create ML actually helps you across all four stages of this workflow. We have powerful in-built data [inaudible] utilities, data source and data table that we will talk in the remainder of the presentation. You can actually train your model using only one line of code. And the training is done hardware optimized. There are built-in evaluation metrics, so you don't have to write your own precision and recall and confusion metrics calculation. Use them. And finally, when you're happy, just write out the model. Now we will take a deeper look in all three use cases: images, text, and tabular data. So let's start with images. And to do that, I will invite Lizi Ottens, Senior Engineer in Machine Learning team. Thank you. Thank you, Gaurav. Since enabling image-based experiences are some of the most powerful and interactive ones that you can add to your apps, today we'll take a look at how to train custom image classification models. Image classification is the problem of identifying what label out of a set of categories you'd like to apply to an image. Depending on the type of training data, you can target domain specific use cases to enable in your apps. The first step is to collect training data. In doing so, we'll take a look at a fruit classifier and see how you would do so. First, you'd want to gather many varied types of images that reflect the true data that you'll end up seeing and then label them. First, you can do this as a dictionary with the string label corresponding to arrays of images. Or what we've noticed is many popular data sets are organized in hierarchical directory structures such that the label is the name of the folder that contains all images within it. There are also other data sources such as single folders that contain labeled filenames. And in the Create ML API, we've provided conveniences to extract these structures. Now training is the more complex part of the equation. So once you have your data, this is what you will get next. And what you can do is you can start training a very complex model from scratch on your input images. And for this you need lots and lots of label data. You need big compute and you need a lot of patience. But another well-established technique in the industry is transfer learning. And since Apple has lots of experience in training complex machine learning models, we already have one in the operating system that you can take advantage of. So what we do is we apply transfer learning on top of this model that already exists in the OS, and we augment it, retraining the last few layers to your specific data so you no longer need millions of images. You can train a good classifier using the amount of data that you have. This results in faster training times. And for developers that we've worked with, we've seen them go from hours of training down to minutes for thousands of images or for small data sets, even seconds. This also results in much smaller models going from hundreds of megabytes down to just a few megabytes for thousands of images or even kilobytes. The goal of Create ML is to abstract much of this and make it simple and easy to use. But to prove it, let's take a look at a demo. First, to set up the problem, I started by running an app that's using a state-of-the-art image classification model that's already in the industry. This one, though, is quite large. It's 100 megabytes in our app. And if we run it, we have some fruits but it's not quite what I was looking for. I'd really like it if, instead, we could classify these particular ones. So what we can do is we can switch to a new playground and import CreateMLUI and walk through how to do this using the UI for it. We can define a builder. Initialize it. And to enable drag-and-drop training, we can show the builder in the live view. This brings up a prompt in the live view to drag in images to begin training. And here I set aside some photos of fruits. Here's some blueberries and other types. And you can drag them in and automatically an image classifier model begins training on the Mac. All of this is accelerated by the GPU on however many categories you end up training on. It automatically tells you what the accuracy is on the training data set, but what's more helpful is to try this on new images that the model hasn't seen before to predict how it will do on real use cases. So I can drag in this other folder containing unseen images. And now the model is evaluating all these new types of fruits. And if you scroll, you can see what the true label is of each type as well as with the predicted one was by the model. And if you're happy with this accuracy, what you can do is you can take the model and drag it into your app. I'll add it here. And if we take a look, this model is 83 kilobytes. It's a huge savings down from hundreds. So we can delete the old model that we were using before. And in the view controller, we can initialize this new one, ImageClassifier. We can then re-run the app, bring up the simulator, and see how it does on some of those fruits. On the raspberry, it can now correctly predict it since we trained the model to recognize raspberries. We can even see if it can distinguish from strawberries and it can now. But there are other workflows you can use. Perhaps you want to do this programmatically or perhaps you want to automate it. We can also walk through how to use Create ML to do so. So now we can switch to another playground and import Create ML. Since we'll be using URLs, we also can import foundation. And since, on our desktop, we still have these folders of fruits, we can say where they are and also say where the testing fruits are. And then the next step is to actually train the model. So we can define a model, and we can initialize an image classifier. And now if we take a look at what auto complete shows to us, we can see we can provide training data in the form of a dictionary of labels to arrays of images or we can use a data source or even specify model training parameters if we want to. Let's use a data source. And we'll use label directories since that's how our data is organized and specify the training directory. And since we're running in the new [inaudible] mode of Xcode playground, I just need to hit shift enter and the model begins training right away. You can even pull up the console and see output of one, its extracting features and how many iterations it's running through. Afterwards, you can also open quick looks and see the name of the model and how many instances it's trained on. Now we might want to evaluate on the testing data that we've set aside. So what we can do is we can call evaluation on another data source since that folder is organized the same way, specifying the URL of the testing data. You can hit shift enter and now the model is evaluating testing images. Once it's complete, we can also look at the quick look and see how many examples it evaluated on as well as how many classes were in that folder altogether and the accuracy. If we're happy with that, we can write it out. And say that I want to write it to the desktop with the name fruit classifier ML model. Once I do, you can see this new model appears on the desktop. We can double-click it and take a look and see it's exactly the same. This is also 83 kilobytes. Furthermore, we can integrate it back into our app the same way. Let's recap. We saw two ways of training image classifier models in Create ML. One was with the UI which makes it super simple to drag-and-drop your training data and evaluation data to produce an ML model. The other way was with the Create ML API. If we walk through some of this code, we can see the first thing we had to do was import Create ML. The next was to specify where our training and testing data was and then actually begin training the model by specifying how our training data was laid out. We can then evaluate on the testing data and finally save ML model. If you want to automate this, you can also turn these into scripts, which is a very popular way of saving what you've done and re-running it whenever. You can then change permissions on the file and run them like so. Or for other workflows, you can always use Swift command line [inaudible]. So we've seen today how to train image classification models using a few different workflows. But next, I'd like to pass it off to Tao to talk about natural language. Thank you. Thank you, Lizi. Hello everyone. My name is Tao. I'm an engineer here at Apple working on the Core ML team. You just saw how easy and intuitive to train an image classifier with just a few lines of code. Now I'm going to show you the same can be done for natural language. In this year's release, we're going to support two natural language tasks: text classification and word tagging. Today, I'm going to focus on text classification. For details on word tagging, please join the natural language session that happens tomorrow. Text classification can be used in a few machine learning applications. For example, sentiment analysis. The energy of developers is amazing. That's a positive note. You want your app to know it. Spam analysis. If you saw this message in your mailbox, you know it's very likely it's spam. So you want your app to know that as well. Topic analysis. The Warriors just had an amazing comeback win. That's a sport post. You want your app to be able to classify that. So to train such a classifier, the first thing you do is to collect some training data. With Create ML, we support a few different ways for you to organize your training data. For example, label directories. Here you have two folders. One named positive, the other one named negative. Within each folder, you have a number of articles with just raw text whose truth label is simply the name of the folder. Alternatively, you can prepare your training data using simple CSV where you prepare your raw text and the truth label separated by comma. We also support JSON formatting the training data and know that we just talk about the training data organization and you can actually organize your test data in the exact same way. Now with your training data and test data ready, what other steps involve to train such a text classifier? A typical workflow would look something like this. You start with your raw text. You do a language identification to figure out which language it is in. You convert that into tokens. And then you convert that into some feature values and then you can apply a machine learning model that gives you some predictive value that you have to map to some desired label. And then you can compare that label to your truth label and start iterating on it. With Create ML, though, we took away all these complexities so that all you need to do is to prepare raw text with their truth label and start training immediately. Now let me give you a concrete example like how you can train such a classifier and use it. For example, we have this simple app called Stay Positive whose purpose is to encourage positive post. If a user entered I hate traffic, the background turns red and it will disable the post button. I love driving my car at five mile per hour just chilling in traffic. That's a positive post. We encourage you to post it. Just imagine what our Internet would look like with this app running on everybody's phone? Now, in order to do that, let me give you a live demo. So to train such a classifier, the first thing I do is collect some training data. On my desktop, I have a train folder and also a test folder. In train folder, we have two folders. One is named positive, the other one negative, and there are a number of articles in each folder. And test folder is organized in a very similar way. So the first thing I do is to import Create ML. Now I need to tell the [inaudible] where to find my training data. For that, I'm simply using a URL capability and then I can start training my model using the label directories that Lizi just showed you. Look. The training has started. As you can see on the bottom there, there is some progress report for you to check. Looks like training has finished. Now you can check some basic performance numbers on this model. For example, model.trainingMetrics that shows you this model has been trained on over 2000 examples and accuracy is 100%. But how does it perform on some unseen data? So I'm going to do the same to define test data and then evaluate that model on the test data. As you can see, we have 77 test examples, and we are achieving over 94% accuracy, which is very good. I'm sure you want to iterate on that if you want to see like even a higher number, but this number is pretty good enough for my app so let me just test it out. So to save out the model, what I need to do is define a URL where it's saving to and then write out a model to my desktop. Looks, that model has been saved. So now I need to switch back to my app. Just drag and drop it. There you go. Now I can start use it. I will do let model equal to textClassifier which should auto complete. And then I'm going to insert some basic inference code. In this inference code, as you see, the first line I do is using model.prediction to get prediction. And then in order to hook up with this simple app UI, I just convert that into some double value. Let's give it a try. Yeah. Let's try some example we have showed you. I hate traffic. Negative. I love driving my car at five mile per hour just chilling in traffic. Positive. Let's try something different that'll be fun. Machine learning is hard. Create ML makes it so easy. Positive. So that's how you train your customized text classifier and drag it into your app to use it. Here's a recap. So to train such a classifier, the first thing you do is to specify your data. You specify your training data as well as your test data and then you can create your model on the training data. To evaluate its performance, you evaluate a model on the test data. Finally, to use your model in your app, you simply save it out using this write API. To summarize, with just a few lines of code, you can train your customized text classifier simple intuitive. With that, I'd like to hand back to Gaurav who is going to talk about tabular data. Thank you. Thank you, Tao. Besides images and text, another common source of data that occurs very frequently when you're solving a machine learning problem is tabular data. What I mean by tabular data, I mean the data is in special format or in a table format. This kind of data occurs fairly frequently. For example, let's just say you're trying to predict house prices using number of beds, number of baths, or square footage. Generally the data is arranged in a tabular format. You want to predict the quality of wine using its chemical compositions. Chances are data will be arranged in table format. Or something even simple like where to hop, which bar to hop tonight using happy hour or its price, the data will be in tabular format. To handle the data which is in tabular format, we actually introduce a new data structure which we call as MLDataTable. MLDataTable is based on [inaudible] technology that we will discuss in detail tomorrow. There's something interesting about these data tables. The rows contains the observations or examples. So here, house number two has four bed, three bath, and 500K price. The columns contains what we call as features. So the beds are features, baths are features, square feet, etcetera are features. There is one special column that we want to predict, in this case price, and this column is known as target or response variable. The whole idea behind tabular data is that we want to predict target variable as a function of one or many of these features. So what are the common sources that we support? Well, CSV, JSON as well as you can actually have code. So let's talk a little bit more about MLDataTable. First, you can read data simply by using CSV. What is more important that you can access the column using a subscript notation. So all you do is house or the price and you get an entire column of price. You can add two columns, subtract two column, multiply two column, divide two columns. And the way you do it is in very natural looking syntax. So you just simply say house or the price divided by house or the square foot to get price per square foot. Behind the scenes, this calculation is done using [inaudible] evaluation and through vector operations. It can also do some of the other interesting things. For example, you can split data table in training as well as you can even do filtering. So for example, if you're only interested in large houses, you can create an indicator variable and filter it out. There a lot of operations that data table support. I urge you to try it out in Xcode playground. They're fun. Now once you have data in data table, you would like to do the training on it. Create ML supports a large number of algorithms such as Boosted Tree Regression, Random Forest, etcetera. And all of these algorithms are represented by their class. In order to train your model, you only have to write one line of code. Basically, you tell what is the target and where you're getting the data and which is the algorithm you are instantiating. So in this case, let's just say you are running Linear Regression or Regularized Linear Regression, you just actually tell it that the data is house data and the column is price. If you do Boosted Tree Regression, just replace Linear Regression with Boosted Tree and you're all set. Now Random Forest like that. Plus we also provide a high level abstraction MLRegressor that automatically runs all these algorithms and choose the best one for you. This is in line with our philosophy that you should focus on task. So the task is to predict the price. You should not focus about nitty-gritty details of the algorithm. Having said that, in case you're an expert, you can actually use Boosted Tree and change its parameters also. So a complete end-to-end would look like this. It follows exactly the same pattern as image and text. First, you specify the data. Second, you just create your model. Third, you evaluate the model. And once you're happy, you save it out. So tabular data, image data, or text data, they all follow the same pattern. So let's just take a quick summary of what we saw in this session. So Create ML is our ML framework insert. It's very simple to use and it is very powerful and it leverages core Apple technologies. You do end-to-end machine learning in Swift on your Mac. We also discussed about our workflow. Once again, you start from an experience. What is the experience you're trying to enable? Then define the problem. Then collect the data. Make sure this data is reflective of the real-world usage of your scenario. Then you train the model. And finally evaluate it. And once you are happy, you just save it out. Create ML is in Swift. And it's available on macOS Mojave. You can use it in Xcode Playground, Swift Scripts and [inaudible]. So please try it out. We would love to hear from you. We are here to receive your feedback, and we hope that you will love it as much as we do. We will be in the machine learning get together as well as the labs. So there is -- tomorrow there is a get together. We will be in labs also, so please give us your feedback. There are also related sessions in the WWDC App. We have Core ML session tomorrow morning and ML session tomorrow afternoon, Vision sessions on Thursday. And we have labs on Wednesday and Friday. Thank you.  Good morning. Wow. Thank you. Thank you, very much. And once again, welcome to your first session at WWDC. My name is Joaquim. I don't think I have to reintroduce myself. Karan, Dongyuan, and I are really excited to talk to you about internationalization, at long last. So, what are we going to talk about? I'm going to do a brief introduction, like I started doing. And we're also going to do some deep dives on making your app's layout adaptive. And also, cover some really fun discussions on text, as well. So, why is this important? This applies to every single app. Internationalization is a crucial concept and we're going to go over some of the details as to why it applies to every single app, regardless of the set of languages that you support. And once again, this is all about reaching out to more people. It's enabling everybody to learn about and use your app. Regardless of the language they speak or the country that they live in, or the country that they grew up in. And it's important to note that those last two might be different, as well. And that's something that's worth keeping in mind. So, internationalization covers a wide range of topics, and potentially, every single aspect of your app. And if you're new to the topic, probably, the best way to think about it is that internationalization is best applied as a constant process. It's a discussion that belongs in every single stage of your app. Whether it's the initial inception, its design, to its implementation, and even distributing it on the App Store. Things like your App Store screenshots and even your app's name should be open for discussion in an international context. And like I said before, this covers a wide variety of topics. At Apple, we have a lot of firsthand experience about this. So, we estimate that over 70% of our customers are outside of the United States. And this means that when somebody downloads your app they expect that app to match your language preferences. And not only that, but also, be aware of any details that might be related to their region settings, as well. We support exactly 38 different written languages and many, many more keyboards and input methods for people to type and text. And what this means for you, is you have a rock solid foundation to get started with. Every single one of our API's supports these languages and input methods. And this means that you can get started right away by using these and you're off to a great start in providing really great international support to your app. So, for example, this might entail presenting dates and times. This wouldn't be an international talk if I didn't talk about dates and times. But it's important to think about, right? Because people have different expectations around the world. This is the [inaudible] Sao Bento in Portugal. It's a lovely train station. But my focus today isn't so much the train station as much as the way the times are presented in the train station. And if you go there, you might notice that everything is in 24-hour time. And this is true, not just in train stations, but it's commonplace everywhere in many countries. And so, your app should know about these and present time accordingly, depending on the region. And you can do this using date formatter. It's a set of APIs we provide that lets you do this. And it really does all the heavy lifting for you. Another example is calendars. There are many different calendrical systems around the world. And it's also important to note that many people use more than one on a daily basis. They might use one for religious or festive events and another for business. And these all have different properties like leap months and leap years and different numbers of days and different numbers of months. And so, being precise about these and knowing how to present these appropriately means being internationally aware. This is an example of the iOS Calendar app showing the Chinese Lunar calendar on top of the Gregorian calendar. And we have a set of API's in Calendar and alongside Date Formatter that help you do this. Could also talk about units and measurements. Whether it's presenting things in the metric versus the imperial system, or even temperatures like Celsius or Fahrenheit. Your app should be aware of the appropriate regional defaults for these, and also, respond to changes in the user's preference. And not only that, whether they're units or not, even numbers themselves. There are so many things to cover there from the decimal separator to the thousand separator. And even, the entire class of digits used to represent and talk about numbers. These can change between regions. And if you use number formatters, your app gets this behavior for free. And this is important not only for consistency, but also, understandability. Text is, also, another great topic with many different areas. And depending on the language or languages that you grew up learning and writing, and the scripts that those language are written in. You might have thought that there are some properties that are completely immutable across other scripts and other languages. And that might not, necessarily, hold. One example, probably the biggest example, is script directionality. So, some languages, like Arabic and Hebrew, and in this case Urdu, are written and read from right to left. Which, is completely the other way from English, from what you might be used to. And taking this further, many books, most books published in traditional Chinese and Japanese are presented in vertical and right to left. And so, knowing about these formats and knowing about the context in which it's appropriate to present these and adapting your layout for these could be an important aspect of your app. And are why frameworks alongside our text frameworks like TextKit and CoreText can help you make sure you're doing the right thing in every single one of these cases. Going a little bit higher level, even things like names. Depending on where you grew up, you might have a first, middle, and last name. Or a family name that comes before your given name. And again, knowing how to present these, and even asking your user about these could be a crucial part of your app. And this is something that users expect that you get right and know about the details and intricacies of formatting names. And we have PersonNameComponentsFormatter API that help you do this. Okay. So, throughout all of these concepts it's also important to note that many of the users might interact with and speak more than one language on a daily basis. Whether it's because they moved countries, or simply because that region, in and of itself, uses more than one language. And taking this example further, even if your app only supports one language it's very likely that your customers are using your app to publish and consume content in their own native language. And so, regardless of the set of languages that your app supports, these are all still very important topics to consider and talk about throughout the design and implementation of your app. So, if all of this is new to you, worry not. Like I said before, we have a great set of APIs that really do all the heavy lifting for you and take care of all these different aspects. And are aware of all the different details between languages and regions. These are some examples of the Formatter APIs that I talked about. And we've, also, got some great sessions that go into great detail on how to further tailor these for your app, as well. So, let's start our first deep dive in to layout and some of the goals related to internationalization when it comes to adaptive layouts. Because, really, the core goal of an adaptive layout is to present all kinds of different information. And when you're adapting your application for other languages, probably, the biggest guarantee that you'll get is that these translation lengths are going to be different. They're going to be much shorter or much longer, depending on the language. And this is something that your design and layout should adapt for. On top of this, there's also directionality, like I mentioned before. Because some languages are written in right to left this has some design considerations, not just in the text that you have in your app, but also, the way you present information. Especially, horizontally flowing information and how that general flow should adapt for both script and writing directions. So, a great starting point to help you do all of this is Auto Layout. You might have heard of Auto Layout, before. This is a powerful technology that is at the core of our layout engine. And Auto Layout, instead of describing explicit frames or positions of your controls and labels, describes constraints or relationships between these views. And therefore, how they're positioned relative to each other and how they're allowed to grow relative to each other. On top of this constraint-based system we have the idea of leading and trailing constraints. And what this means is that this, essentially, describes properties that are left and right in a language like English that automatically evaluate to right and left, respectively, in a language like Arabic and Hebrew. And this means that with Auto Layout you can create these adaptive layouts that flow depending on the writing direction without having to write special code for either one of the writing directions. Another great starting point is to use high level components and containers that we provide in our UI frameworks. Because these use all of these concepts and are aware of how to be adaptive. Some examples are collection views and stack views that we provide, both in UIKit and AppKit. And you can create really complex layouts and even embed these within each other to, potentially, even just create the whole layout of your app just using these. Just as an example for Stack View itself, I could arrange these horizontally in a Stack View. So, I have the city name on the left and the time on the right. And this is, this could just be put into a horizontal stack view. And because this flows left to right in English and because I'm using a StackView if I run my app in Hebrew, this is the result I would get. Because that information would adapt automatically. Because StackView is aware of these concepts and uses Auto Layout under the hood. If you'd like to learn more about Auto Layout and details on how to adapt your application for different writing directions, these are some great starting points to check out, as well. So, when it comes to the text in your app and the content in your app the key goal here and, obviously, the number one recommendation is to simply not assume fixed widths. If you don't do that you're off to a great starting point in terms of allowing your app to adapt to, not only different changes in length horizontally. But even, potentially, allow your labels to grow vertically and adapt to multiple lines if necessary. And this is something you'll have to decide in your app and your app's design on what you want to prioritize in terms of what is allowed to grow and use up the real estate of your app. If your labels and your controls are either in stack views in these high-level components that I mentioned before, or positioned using Auto Layout itself, this work is pretty much done for you. You're already allowing your labels to grow and your controls to take up the content that they need to take up. And then, it just becomes a matter of prioritizing how you want to let them grow relative to each other. So, this is great. Let's say you've done all this. And how, exactly, do you make sure that your app is checking off all the right boxes in terms of adaptive layout and responding to all of these things? Well, the good news is that Xcode provides a number of features for you to test. And not only that, it allows you to test early and find these out quickly in the development, and early on in the development of your app. One example is pseudolanguages. Pseudolanguages are great, especially, when you haven't yet added language support to your app. So, what you can do from within Xcode is run your application in a pseudolanguage and this changes a few details of the layout and text of your app. One example is the bounded string pseudolanguage. This adds a few characters at the beginning and end of every single UI string that you display. And then, you can see, make sure that there aren't truncations or unexpected clippings in the content of your app. So, this is a really useful one. And we have many more pseudolanguages that can help you out, as well. Specifically, for Auto Layout we have Auto Layout warnings built into Xcode. So, Xcode can tell you about common antipatterns right within an interface builder. And this can be things like fixed width constraints or too few constraints on a control or label that might, potentially, introduce issues at runtime, especially, in other languages. So, with that, I'd like to hand it over to Dongyuan, who's going to give you a demo of all of this in action. Thank you, so much. And hope you have a great week. Thank you, [inaudible]. Thank you, Joaquim. Hi, I'm Dongyuan. Let me show you some techniques to [inaudible] and some common pitfalls to avoid in real world. Here's an app called Vacation Planet, which is the first interplanetary travel agent that allows you to book travel for not only other countries, but also, as you can see in this table view, other planets. So, I wanted to go to the moon for quite a few times. So, I can do that, this time. And here are all the locations available on the moon and their distance from Earth. Let me choose the location like the Clavius Base. And here is our Travel Details page. It seems pretty cheap for one trip. So, I'm going to buy more tickets. Let's buy three of them. And there we go. As you can see, because we design and developed the app in English our layout works great in English. However, we still want to make sure it can adapt for other languages. Because we are still early in the development cycle and we haven't localized the app, yet, we can use the pseudolanguages in Xcode. To do that, I'm going to go through the Current Scheme, click Edit Scheme. And in the scheme editor I have an Application Language selector. I can select one of the pseudolanguages like the Bounded String Pseudolanguage. It's very useful for exposing potential clipping or truncation issues. Let's run the app in this configuration. As you can see, this pseudolanguage adds a few special characters at the beginning and end of every UI strings. Our tag line here is still good. But the Browse button is truncated. Let's try to fix that. When I select the Browse button I can see that there's a wording in Xcode. The wording shows that we have a fixed width constraint and that may cause clipping. If I want to know more information, I can click on the I button. I'm going to fix the issue by click the growth, sorry. By click the Warning sign. Here are three options. I'm going to select the first one, which is simply remove the constraint to allow the button to be wider when there's more content. Let's do that. Okay. No other layout issues. And then, I'm going to show you a very simple way to verify the change that was made. I'm going to go to the Assistant Editor at the top right corner, the middle button here. Here I can select Preview and Main tell Storyboard. The Preview pane allows you to view your layout in multiple screen sizes and different languages without running the app. On the bottom right corner, I'm going to select the first, the same Bounded String Pseudolanguage we just used. Great. Because we don't have the fixed width constraint, anymore, our Browse button can now accommodate a slightly longer string. Another very useful pseudolanguage is the Double Length Pseudolanguage. It's for verifying your layout against potential languages that have longer string length, like German, Finnish, or Russian. They can sometimes be two times longer than English. Let's see if that works. Now, the Browse button is still okay. But our tag line is clipped by the screen boundaries. When I select the label, I can see that we only have a center X constraint. We don't have any leading or trailing space constraint, so that the label can overflow. Let's add a leading space constraint to fix that. Again, I'm holding down the Control key, drag from the label to [inaudible] view, and select Leading Space to Safe Area. I can select the constraint to adapt at just its value. Let's put something reasonable, like 20, for the margin. And here, I can see instantly that my label is no longer clipped. However, it's still truncated and it's a very important message we want to show to all of our customers. That's not ideal. Instead of truncation, I can allow the label to wrap into multiple lines when necessary. Here, when I select the label I can see that we have Lines property set to one, which means we only allow one single line for the label. If I change the value to zero I can allow the label to wrap into any number of lines, when necessary. I can see that my label is now three lines in this Double Length Pseudolanguage. If I switch language back to English my label is still one line, which is exactly what we expected. I, also, encourage you to check the app down here, where you can select different screen sizes. I encourage you to check your layout in the smallest device, like iPhone SE, because clipping and truncation are more likely to happen in the smaller devices. Now, I wonder if our layout can adapt for right to left languages, like Arabic or Hebrew. As I said earlier, we don't have localizations for those languages yet. We can run another pseudolanguage. Let's open the Scheme Editor, again, this time in application language. And I'm going to select the Right to Left Pseudolanguage and run the app in the simulator. Okay. Browse. As you can here, our table will, now flows from right to left without us making any changes. That's because we used UITableView and other standard UIKit components. The system had done the hard work for us. All the section titles are now at the right side, which is the leading side for right to left. The chevrons here are at the left side, which is the trailing side. I'm going to go to Jupiter, this time. And you may notice that the Back button is now at the top right corner, instead of top left. That is, actually, very natural for right to left languages. Let's select a location on Jupiter. So, here's our Travel Details page. Everything seems to be okay, except the stepper and the Traveler label. The first issue is that the stepper should be at the trailing side, which is the left side for right to left. The Traveler label should be at the leading side, which is the right side, here. And also, there's an unnecessary spacing for the Traveler label. Let's fix that in that Interface Builder. Let me locate our trouble zone cell, is here. Okay. Zooming in a little bit. When I select the Traveler label and the stepper I can see that there's no constraint on them. That's not good. And of course, Xcode has a warning for that. So, one way to resolve the issue is to add a leading space constraint for the Travelers and a trailing space constraint for the stepper. Leading and Trailing will translate to right and left for right to left languages. However, here I'm going to show you an even simpler way, which is to use UIStackView, a high-level container view that uses Auto Layout under the hood. By using Stack View I can get right to left support for free. Let me select the two views. And click the Embed down here at the bottom right corner. Select Stack View. Now, my two views are inside this UIStackView. The only thing left is to add constraints to the Stack View itself. I'm going to select Stack View, click the Add Constraints button. I'm entering four zeros, here, because I want the Stack View to fill the table view cell as much as possible. And I like to select the Constrain to margins here, because I want some default margins for the Stack View. And I want the leading edge to align with the cell separators. And add four constraints. There you go. And I'm going to verify my change. Yep. Browse. This time, I'd like to visit Earth. And I'd like to go through Lisbon, Portugal, to visit [inaudible]. Do that. Now, as you can see, because our Traveler label and the stepper is now inside a UIStackView, we get right to left support for free. Let's book this travel. Great. Now, let me summarize what we talked about. So, to make our app layout great for our global audience, there are a few simple steps. First, is to use high level containers like StackView whenever possible, as they provide a lot of heavy lifting for you. And believe me, they are just simpler to use. For finer control, remember to use Auto Layout and make sure to use leading and trailing constraint so that you can adapt for right to left languages. For early testing without even localizing your app, you can use the pseudolanguages in Xcode Scheme Editor. And please, don't ignore the Auto Layout warnings in the Interface Builder. They are very useful for avoiding clipping, truncation, and overlapping issues. Now, over to Karan to talk about text. Thank you, Dongyuan. Good morning, everyone. Let's talk about text. At Apple, a high quality typographical experience is a key part of our design process. And it's very important to us how text looks on screen and how it's designed. And also, the way that it translates to other languages. So, let me walk you through some key aspects that we keep in mind when we localize our own apps into other languages. And how you can take advantage of these things to make your apps look great in other languages. Now, I'm going to talk about three main topics, set up some fundamentals with languages and scripts. And then, dive into typefaces and styles. Let's talk about languages and scripts. What is a script? So, when I'm talking about script, I don't mean a Bash script or a Python script. I'm talking about the way that a language is written. So, the writing system. The letters that you use to write a language. These are some of the scripts that we support that are written from left to right. We, also, have scripts that are written from right to left. The key thing to note about all of these scripts is that they're multilingual. So, each script supports a huge variety of languages that are written in it. And as you can see, the Latin script here supports everything from English to Vietnamese. And this is true for other scripts, as well. For example, the Cyrillic script supports a variety of different languages. And if you look at right to left scripts, we see that the Arabic script, not the Arabic language, but the Arabic script supports a variety of different languages, such as Arabic and Persian, Urdu, etcetera. So, what you're seeing on screen, yes, there's a lot of visual variety. But this is not just for show. There's, actually, a lot of implications when you develop your apps and different scripts because some concepts don't map across scripts as easily as others. So, let me show you a few examples of this. Let's talk about typefaces. So, here you see the Health app in English. So, you see a lot of labels. Now, you see the same app in Catalan. And lastly, you see the same app in Vietnamese. Now, the thing that I'd like to call your attention to is that all the text here on screen is rendered using our system font, San Francisco. And the other thing I'd like for you to notice is that everything is rendered beautifully. That's because San Francisco has support for a huge variety of different languages. And when you use the system font in your apps, you're guaranteed to get that support for free. Now, any text label that you create in Xcode will get San Francisco by default. But if you want to go one step further, use a text style. We support a variety of different styles in our OS that are meticulously implemented to support a variety of different use cases. And also, to map across different languages, really well. So, when you use a text style you guarantee a consistent high quality experience to your users. You can also go one step further. If you set your label to automatically adjust its font size, it will do so with respect to the user's text size setting. And this is really handy for people who use smaller or large sizes for their text. Like myself. So, I highly encourage you to use that and this will ensure that your text styles scale appropriately. But let's say, that like us, we're making this Vacation Planet app to go to Jupiter, to go to Mars. And we wanted to have this fun vacation look to it, which it doesn't right now, because everything is in the system font. So, we decided that the title of the font should evoke the personality of the app. And so, we looked at a few different fun fonts to choose from. We looked at this one, for starters. But as Joaquim mentioned from the start, we like to keep localization as a central part of our development and design process. So, the first thing we did was to check does this work for all the languages we need to support? Well, we already had a French localization. So, we tried it out. As it turns out, it doesn't. So, we kept looking at our short list of options. And we looked at this other typeface. And when we tried it out in French, voila! It works because it supports all the characters needed for French. So, that's great news. So, the next step for our app was to expand to Vietnamese. But we didn't have a Vietnamese localization, yet. So, one great tool that you can use is FontBook, which comes installed on every Mac, even if you're not a developer. It comes installed on every Mac. And in FontBook you can easily search for the name of a language, like Vietnamese, here. And we see here, that in the font that we chose Vietnamese is in the set of supported languages. In fact, this font also supports Cyrillic and Greek script. So, we are somewhat sure that if we expand to Russian and Ukrainian and Greek later that this font will work for us. Now, I should advise some caution here. Just because FontBook says that a font supports a given language doesn't mean that you don't have to, actually, test your app in that language. You still need to make sure that the font really works for that language, by trying it out. So, we've got our beautiful fun font for our Vacation Planet app. And now, we're going to expand even further. So, we want to do more languages. Specifically, we want to localize into Chinese. Okay. So, we sent off all our strings for translation and they've come back. And our app is now perfectly localized into simplified Chinese. Cool. But it's not really, though. Because look at what happened to the title. So, in English, we have this fun font, but in Chinese we are using the system font. That's because our font only supports the Cyrillic, Greek, and Latin scripts. So, of course, the way to fix this is to repeat the same process for Chinese. And here now, we have out fun font in Chinese. It's as simple as that. And unfortunately, I can't really help you with that. That's a stylistic thing and you have to choose your own fonts. But I can show you how this is done in code. So, this is really straightforward. First of all, you start with your font for your development language, normally. Such as, in our case, it's English. So, we choose our Latin script font and get our font here. And the key concept that I want to introduce you to, here, is called a Cascade List. So, a Cascade List says if I'm looking to render this Chinese character and this first font doesn't have it, what font should I use after this to look for this character? Now, if you don't specify a Cascade List you're going to get the system font. But if you do have a Cascade List, then you can specify other fonts to try before falling back to the system font. So, in this case, we create a Cascade List with a font descriptor for our font for Chinese that we hand-picked. Now, if your app supports multiple scripts you add multiple things here and that's as simple as it works. And once you've got a Cascade List you create a new font descriptor. And then, you create a new font. Pretty straightforward. And also, make sure that if your app used dynamic type, which it really should, then your font should adjust to that, as well. And again, that's as simple as an API call. Let's see some examples. So, this is the new Word of the Day screensaver in macOS Mojave. And as you can see, for this new design we've chosen a rounded style. And of course, it wouldn't make sense if we didn't, also, choose equivalent rounded styles for all the languages that the screensaver supported, which includes Japanese. And also, new in macOS Mojave, simplified and traditional Chinese. Another example is the Messages app, in which you can respond to a message using a tap back reaction. Now, here you'll see that the ha-ha in English has been not only translated to other languages. But has, also, been matched in style, so you get the same bubbly fun look in all the languages that we support. That's it for typefaces. Let's talk about styles. Again, I should start with the definition. What do I mean by a style? So, broadly speaking, I'm referring to aspects of your text that you choose once you've chosen a typeface. So, let's say the font weight, like how bold it is, or whether it's italicized, and what the size of the font may be. So, the key thing to keep in mind is that some aspects translate better to other languages and others don't. So, let's take a look at an example of where something might not translate. So, here we have a simple string in English. We've italicized Mars and 2 Travelers to indicate that they're variables, so they can change. And this is how it translates into traditional Chinese. There are a couple of issues here. Mars is italicized in English. It's not italicized in Chinese. Why? Because italicization is not a concept in Chinese. And it's not a concept in, actually, most scripts outside of Latin, Cyrillic, and Greek. So, a design that uses italics, probably, won't work. The other thing to note is that in Chinese there are no spaces between words, and also, there is no concept of uppercase and lowercase letters. So, you kind of lose a natural distinction that you get in English when you translate it into Chinese. A couple of small things to note, also, is that the word order is different. And also, because the 2 still comes from the Latin script and is still italicized, it doesn't look great. So, how can you fix this? Well, the keys to realizing that you're doing emphasis, not necessarily italics. And emphasis can be done in multiple different ways. This is a great way to do emphasis that works across several different languages. In fact, it works for all the languages that we support, which is to bold a given word. Lastly, let's talk about emphasis at a character level or sub-word level. Let's say we have an app with a Search feature and we want to highlight the part of the results that matched to make it clear to the user what's happening. Now, this works really well for English, by using a bolder weight for the matched segment. It works really badly for Hindi. So, what you're seeing here, so anybody who can read Hindi will tell you that everything on the right-hand side looks completely broken. Any time you see a dotted circle inside a Hindi word, something has gone very, very wrong. And the reason this is happening is because even though it's the same font family, different weights within that font are, actually, a different font. And you can't have the appropriate joining behavior for languages like Hindi if you have two different fonts. So, one easy way to solve this problem is use a different kind of emphasis at a character level. So, at a character level you can use something like using a different color. For example, here we use black color for the math segment and gray for the remainder of the word. This works really well. Now, you'll see some dotted circles on the keyboard. That's perfectly fine. They're dependent marks. And here's the same example in Arabic. Again, the same approach is used all over iOS and macOS, and it works really well for many purposes. And I should also mention that this is really easy to do with attributed strings. Finally, let's go over everything that we've talked about. First off, it's important to start planning early in your apps, not only the development, but also, the design phase. Once you know the languages you're going to localize into and plan to extend to in the future, internationalize as you go. Technologies like Auto Layout, StackViews, and dynamic type are very simple to adopt while you're in the process of building your feature. But if you finish your project and try to come back and use them, then you may have to completely rearchitect your app. And that will be a lot of work. If you're doing something for which you feel, oh, yeah, there should be an API for this, right? There probably is an API for it. So, make sure that when you're doing something like formatting any kind of data, to make sure to look to see if there's a formatter class. And also, make sure to look to see if there is an API for anything you're doing with text before trying to ruin your own implementation. And lastly, ensure that nothing is lost in translation. So, the key thing to realize here is that every localization of your app is a unique experience. And you need to make sure that your intent that you specified in your development language actually makes it over to all the other languages that you support. And that nothing is lost along the way. Thank you, very much.  Hey, guys. Thank you. Thanks for coming by, guys. Welcome to Designing Fluid Interfaces. My name is Chan. And, I work on the human interface team here at Apple. And, most recently, I worked on this, the fluid gestural interface for iPhone 10. So, me, Marcos, and Nathan, we want to share a little bit about what we learned working on this, and other projects like this in the past. So, the question we ask ourselves a lot is, what actually makes an interface feel fluid? And, we've noticed that a lot of people actually describe it differently. You know, sometimes, when people actually try this stuff, when we show them a demo, and they try it, and they hold it in their hands, they sometimes say it feels fast. Or, other people sometimes say it feels smooth. And, when it's feeling really good, sometimes people even say it feels natural, or magical. But, when it comes down to it, it really feels like, it's one of those things where you just know it when you feel it. It just feels right. And, you can have a gestural UI, and we've seen lots of gestural UI's out there, but if it's not done right, something just feels off about it. And, it's oftentimes hard to put your finger on why. And, it's more than just about frame rates, you know. You can have something chugging along at a nice 60 frames per second, but it just feels off. So, what gives us this feeling? Well, we think it boils down to when the tool feels like an extension of your mind. An extension of your mind. So, why is this important? Well, if you look at it, the iPhone is a tool, right? It's a hand tool for information and communication. And, it works by marrying our tactile senses with our sense of vision. But, if you think about it, it's actually part of a long line of hand tools extending back thousands of years. The tool on the left here was used to extract bone marrow 150,000 years ago, extending the sharpness of what our fingers could do. So, we've been making hand tools for some time now. And, the most amazing thing is that our hands have actually evolved and adapted alongside our tools. We've evolved a huge concentration of muscles, nerves, blood vessels that can perform the most delicate gestures, and sense the lightest of touches. So, we're extremely adapted to this tactile world we all live in. But, if you look at the history of computers, we started in a place where there was a lot of layers of extraction between you and the interface. There was so much you had to know just to operate it. And, that made it out of reach for a lot of people. But, over the last few decades or so, you've sort of been stripping those layers back you know, starting with indirect manipulation, where things were a little bit more one-to-one. A little bit more direct all the way to now, where we're finally stripping away all those layers back, to where you're directly interacting with the content. This to us is the magical element. It's when it stops feeling like a computer, and starts feeling more like an extension of the natural world. This means the interface is now communicating with us at a much more ancient level than interfaces have ever done. And, we have really high standards for it. You know, if the slightest thing feels wrong, boom, the illusion is just shattered. But, when it feels right, it feels like an extension of yourself, an extension of your physical body. It's a tool that's in sync with your thought. It feels delightful to use, and it feels really low-friction, and even playful. So, what gives us this feeling? And, when it feels off, how do we make it feel right? That's what this presentation's all about. We're going to talk about four things today. And, we're going to start with designing some principles, talking about how we build interfaces that feel like an extension of us. How to design motion that feels in tune with the motion of our own bodies, and the world around us. And, also designing gestures that feel elegant and intelligent. We're also going to talk about that, now that we've built this kind of stuff, how do we build interactions on top of it that feel native to the medium of touch, as a medium? So, let's get started. How do we design an interface that actually extends our mind? How do we do this? Well, we think the way to do it, is to align the interface to the way we think and the way we move. So, the most important part of that is that our minds are constantly responding to changes and stimulus and thought, you know? Our minds and bodies are constantly in a state of dynamic change. So, it's not that our interfaces should be fluid, it's that we're fluid, and our interfaces need to be able to respond to that. So, that begins with response. You know, our tools depend on the latency. Think about how hard it would be to use any tool, or play an instrument, or do anything in the physical world, if there was a delay to using it? And, we found that people are really, really sensitive to latency. You know? If you introduce any amount of lag, things all of a sudden just kind of fall off a cliff in terms of how they respond to you. There's all this additional mental burden. It feels super disconnected. It doesn't feel like an extension of you anymore. So, we work so hard to reduce latency. Where, we actually engineered the latest iPhone to respond quicker to your finger, so we can detect all the nuances of your gestures as instantly as possible. So, we really care about this stuff, and we think you should too. And, that means look for delays everywhere. It's not just swipes. It's taps, it's presses, it's every interaction with the object. Everything needs to respond instantly. And, during the process of designing this stuff, you know, oftentimes the delays kind of tend to seep in a little bit. You know? So, it's really important to keep an eye out for delays. Be vigilant and mindful of all the latencies or timers that we could introduce into the interface so that it always feels responsive. So, that's the topic of response. It's really simple, but it makes an interface feel lively and dynamic. Next, we want to allow for constant redirection and interruption. This one's big. So, our bodies and minds are constantly in a state of redirecting in response to change in thought, like we talked about. So, if I was walking to the end of this stage here, and I realize I forgot something back there, I could just turn back immediately. And, I wouldn't have to wait for my body to reach the end before I did that, right? So, it's important for our interfaces to be able to reflect that ability of constantly redirecting. And, it makes it feel connected to you. That's why for iPhone 10, we built a fully redirectable interface. So, what's that? So, the iPhone 10's an actually-- it's pretty simple two-axis gesture. You go horizontally between apps. And, you go vertically to go home. But you can also mix the two axes, so you can be on your way home, and peek at multitasking and decide whether or not to go there. Or, you can go to multitasking and decide, actually, no, I want to go home. So, this might not seem that important, but what if we didn't do this? What if it wasn't redirectable? So, what if the only gestures you could do was this horizontal gesture between apps, and then a vertical gesture to go home, and that's it. You couldn't do any of that in-between stuff I just mentioned. Well, what would happen is that you would have to think before what you did, before you performed the gesture, you'd have to think what you want to do. And so, the series of events would be very linear, right? So, you'd have to think, do I want to go home? Do I want to go to multitasking? Then you make your decision, then you perform the gesture, and then you release. But, the cool thing is when it's redirectable, the thought and gesture happen in parallel. And, you sort of think it with the gesture, and it turns out this is way faster than thinking before doing. You know? Because it's a multi-axis gestural space. It's not separate gestures. It's one gesture that does all this stuff. Home, multitasking, quick app switching, so you don't have to think about it as a separate gesture. And, helps with discovery. Because you can discover a new gesture along the path of an existing gesture. And, it allows you to layer gestures at the speed of thought. So, what does that last one mean? So, let me show you some examples. And, we've slowed down the physics on the simulation, so you can actually see a little bit what I'm talking about. So, I can swipe to go home, and then swipe to the next page, or springboard while I'm going home. I can layer these two gestures once I've internalized them. Another example is that I can launch an app and realize, oh, actually I need to go to multitasking, and I can interrupt the app and go straight to multitasking, while the app is launching. Or, I can launch an app and realize, oh, that was the wrong app. And, I can shoot it back home, while I'm launching it. Now, there's one other one where I can actually just launch an app, and if I'm in a hurry, I can start interacting with the app as it's launching. So, this stuff might not seem really important, but we've found it's super important for the interface to be always responding, always understanding you. It always feels alive. And, that's really important for your expectation and understanding of the interface, to be comfortable with it. To realize that it's always going to respond to you when you need it. And, that applies as well to changes in motion, not just to the start of an interaction, but when you're in the middle of an interaction, and you're changing. It's important for us to be responsive to interruption as well. So, a good example is multitasking on iPhone 10. So, we have this pause gesture where you slide your finger up halfway up the screen, and pause, and so we need to figure out how to detect this change in motion. And so, how do we do this? How do we detect this change in motion? Should we use a timer? Should we wait until your finger has come below a certain velocity for a few amount of time, and then [inaudible] bring in the multitasking cards? Well, it turns out that's too slow. People expect to be able to get to multitasking instantly. And, we need a way that can respond as fast as them. So, instead we look at your finger's acceleration. It turns out there's a huge spike in the acceleration of your finger when you pause. And, actually the faster you stop, the faster we can detect it. So, it's actually responding to the change in motion, as fast as we know how, instead of waiting for some timer. So, this is a good example of responding to redirection as fast as possible. So, this is the concept of interruption and redirection. This stuff makes the interface feel really, really connected to you. Next, we want to talk a little bit about the architecture of the interface. How you lay it out, conceptually. And, we think when you're doing that, it's important to maintain spatial consistency throughout movement. What does that mean? This kind of mimics the way our object persistence memory works in the real world. So, things smoothly leave and enter our perception in symmetric paths. So, if something disappears one way, we expect it to emerge from where it came? Right? So, if I walked off this stage this way, and then emerged that way, you'd be pretty amazed, right? Because that's impossible. So, we wanted to play into this consistent sense of space that we all have in the world. And so, what that means is, if something is going out of view in your interface, and coming back into view, it should do so in symmetric paths. It should have a consistent offscreen path as it enters and leaves. A good example of this is actually iOS navigation. When I tap on an element in this list here, it slides in from the right. When I tap the back button, it goes back to the right. It's a symmetric path. Each element has a consistent place where it lives at both states. This also reinforces the gesture. If I choose to slide it myself to the right, because I know that's where it lives, I can do that. It's expected. So, what if we didn't do this. Here's an example, where when I tap on something, it slides in, and then when I hit back it goes down. And, it feels disconnected and confusing, right? It feels like I'm sending it somewhere. In fact, if I wanted to communicate that I was sending it somewhere, this is how I could do it, right? So, that's the topic of spatial consistency. It helps the gesture feel aligned with our spatial understanding of the world. Now, the next one is to hint in the direction of the gesture. You know, we humans are always, kind of, predicting the next few steps of our experience. We're always using the, kind of, trajectories of everything that's happening in the world to predict the next few steps of emotion. So, we think it's great when an interface plays into that prediction. So, if you have two states here, initial state and a final state. The object-- and you have an intermediate transition. The object should transition smoothly between these two states in a way that it grows from the initial state to the final state, whether it's through a gesture or an animation. So, good example is Control Center actually. We have these modules here in Control Center, where as you press they grow up and out towards your finger in the direction of the final state, where it actually finally just pops open. So, that's hinting. It makes the gestures feel expected, and predictable. Now, the next important principle is to keep touch interactions lightweight. You know the lightness of multitouch is one of the most underrated aspects of it, I think. It enables the airy swipes and scrolls, and all the taps and stuff that we're all used to. It's all super lightweight. But, we also want to amplify their motion. You want to take a small input and make a big output, to give that satisfying feeling of moving or throwing something and having a magnified result. So, how does this apply to our interfaces? Well, it starts with a short interaction. A short, lightweight interaction. And, we use all our sensors, all our technology, to understand as much about it. To, sort of, generate a profile of energy and momentum contained within the gesture. Using everything we know, including position, velocity, speed, force, everything we know about it to generate a kind of, inertial profile of this gesture. And then we take that, and generate an amplified extension of your movement. It still feels like an extension of you. So, you get that satisfying result with a light interaction. So, a good example of this is scrolling, actually. Your finger's only onscreen for a brief amount of time, but the system is preserving all your energy and momentum, and gracefully transferring it into the interface. So, what if it didn't have this? Those same swipes, well, they wouldn't get you very far. And, in order to scroll, you'd have to do these long, laborious swipes that would require a lot more manual input. It would be a huge pain to use. Another good example of this is swipe to go home. The amount of time that your finger's onscreen is very light. And, it's-- ends up making it a much more liquid and lightweight gesture that still feels native to the medium of multitouch. While still being able to reuse a lot of your muscle memory from a button, because you move your finger down on the screen, and back up to the springboard. And, it's not just swipes, it's taps too. It's important for an interface to respond satisfyingly to every interaction. The interface is signaling to you that it understood you. It's so important for the interface to feel alive and connected to you. So, that's the topic of lightweightness and amplification. The next one is called rubberbanding. It means we're softly indicating boundaries of the interface. So, in this example, the interface is gradually and softly letting you know that there's nothing there. And, it's tracking you throughout. It's always letting you know that it's understanding you. What happens if you didn't do that? Well, it would feel like this. It would feel super harsh and disconcerting. You kind of hit a wall there. It would feel broken, right? And, you actually wouldn't know the difference between a frozen phone, and phone that's just at the top of the edge of the screen, right? So, it's really important that it's always telling you that you've reached the edge. And, this applies to transitions, too. It's not just about when you hit the edge, it's also when you hand off from one thing to another thing. Tracking. So, a good example of this is when you transition from sliding up the dock to sliding up the app. It doesn't just hit a wall, and one thing stops tracking, and then the other thing takes over. They both smoothly hand off in smooth curves, so that you don't feel like there's this harsh moment where you hand off from one thing to another. Next one is to design smooth frames of motion. So, imagine I have a little object here moving up and down. It's very simple. But, we all know this object is not really moving, right? We're all just having the perception of it moving. Because we're seeing a bunch of frames on the screen all at once, and it's giving us the illusion of it moving. So, if we took all of those frames of motion, and kind of, spread them out here. And we see the ball's in motion over time, the thing that we're concerned about is right around here, where there's too much visual change between the adjacent frames. This is when the perception of the interface becomes a little choppy. You get this visual strobing. And, this is because the difference between the two frames is too much. And, it strobes against your vision, so. Here's an example of where you have two things both moving at 30 frames per second. But the one on the left looks a bit smoother than the one on the right, because the one on the right is moving so fast, that it's strobing. My perception of vision is, kind of, breaking down. I don't believe that it's moving smoothly any more. So, the important thing to take away is that it's not just about framerate. It's what's in the frames. So, we're kind of limited by the framerate, and how fast we can move and still preserve a smooth motion. So, this one's in 30 frames per second. If we move it up to 60 frames per second, you can see that we can actually go a little bit faster, and still preserve smooth motion. We can do faster movement without strobing. And, there's addition tricks we can do too, we can do things like motion blur. Motion blur basically bakes in more information in each frame about the movement, like the way your eyes work, and the way a camera works. And you can also do-- take a page from 2D animation and video games by stretching, this-- this technique called motion stretching stretches the content in each frame to provide this elastic look as it moves with velocity. And so, in motion, it kind of looks like this. So, each of the different techniques, kind of, tries to encode more information visually about what's going on in the motion. And, I want to focus a little bit on this last one here, motion stretching, because we do this on iPhone 10, actually. You know, when you launch an app, the icon elastically stretches down to become the app as it opens. And, it stretches up in the opposite direction as you close the app. To give you that little bit extra information between each frame of motion to make it a little bit smoother-looking. Lastly, we want to work with behavior rather than animation. You know, things in the real world are always in a state of dynamic motion, and they're always being influenced by you. They don't really work like animations in the animation sense, right? There's no animation curve prescribed by real life. So, we want to think about animation and behavior more as a conversation between you and the object. Not prescribed by the interface. So, to move away from static things transitioning into animated things, instead think about behavior. So, Nathan's about to dive deep into this one. But, here's a quick example. So, in Photos, there's less mass on the photos, because it's conceptually lighter. But, then when you swipe apps, there's more mass on the apps. It's conceptually heavier, so we give more mass to the system. So, that's a little bit about how to design interfaces that think and work like us. In-- it starts with response. To make things feel connected to you, and to accommodate the way our minds are constantly in motion. To maintain spatial consistency, to reinforce a consistent sense of space, and symmetric transitions within that space. And, to hint in the direction of the gesture. To play into our prediction of the future. And, to maintain lightweight interactions, but amplify their output. To get that satisfying response, while still keeping the interaction airy and lightweight. And, to have soft boundaries and edges to the interface. That interface is always gracefully responding to you, even when you hit an edge, or transition from tracking one thing to tracking the other. And, to design smooth dynamic behavior that works in concert with you. So, that's some principles for how to approach building interfaces that feel like an extension of our minds. So, let's dive in a little deeper. I'm going to turn it over to Nathan de Vries, my colleague, to design motion-- to talk about designing motion in a way that feels connected to motion, to the motion of both you and the natural world. Thanks, Chan. Hi everyone. My name's Nathan, and I'm super excited to be here today to talk to you about designing with dynamic motion. So, as Chan mentioned, our minds and our bodies are constantly in a state of change. The world around us is in a state of change. And, this introduces this expectation that our interfaces behave the same way, as they become more tactile, it shifts our expectations to be much higher fidelity. Now, one way we've used motion in interfaces is through timed animations. A button is tapped on the screen, and the reins are, kind of, handed over to the designer. And, their job is to craft these perfect frames of animation through time. And, once that animation is complete, the controls are handed back to the person using the interface, for them to continue interacting. So, you can kind of think of animation and interaction as being-- as moving linearly through time in this, kind of, call and response pattern. In a fluid interface, the dynamic nature of the person using the interface kind of shifts control over time away from us as designers. And, instead, our role is to design how the motion behaves in concert with an interaction. And, we do this through these continuous dynamic behaviors that are always running, that are always active. So, it's these dynamic behaviors that I'm going to, really focus on today. First of all, we're going to talk about seamless motion. And, it's this element of motion that makes it feel like the dynamic motion is an extension of yourself. Then, we're going to take a look at character. How, even without timing curves, and timed animations, we can introduce the concept of playfulness, or character, or texture to motion in your interfaces. And finally, we'll look at how motion itself gives us some clues about what people intend to do with your interface. How we can resolve some uncertainty about what a gesture is trying to do by really looking at the motion of the gesture. So, to kick things off, let's look at seamless motion. What do I mean by seamless motion? So, let's look at an example that I think we can all familiarize with. So, here we have a car, and it's cruising along at a constant speed. And then, the brakes are applied, slowing it down to a complete stop. Let's look at it again, but this time we'll plot out the position of the car over time. So, at the very start of this curve it's, kind of, straight, and pointing up to the right. And, this shows that the car's position is moving at a constant rate, it's kind of unchanging. But then, you'll notice the curve starts to bend, to smoothly curve away from this straight line. And, this is the brakes being applied. The car is decelerating from friction being introduced. And, by the end of the curve, the curve is completely flat, horizontal, showing that the position is now unchanging. That the car is stopped. So, this position curve is visualizing essentially what we call seamless motion. The line is completely unbroken, and there are no sudden changes in direction. So, it's smooth and it's seamless. Even when, actually, new dynamic behaviors are being introduced to the motion of the car, like a brake, which is applying friction to the car. And, even when the car comes to a complete stop, you'll notice that the curve is completely smooth. There's this indiscernible quality to it. You can't tell when the car precisely stopped. So, why am I talking about cars? This is a talk about fluid interfaces, right? So, we feel like the characteristics of the physical world make for great behaviors. Everyone in this room finds the car example so simple because we have a shared understanding, or a shared intuition for how an object like a car moves through the world. And, this makes us a great reference point. Now, I don't mean that we need to build perfect physical simulations of cars that literally drive our interface. But, we can draw on the motion of a car, of objects that we throw or move around in the physical world around us and use them in our own dynamic behaviors to make their motion feel familiar, or relatable, or even believable, which is the most important thing. Now, this idea of referencing the physical world in dynamic behaviors has been in the iPhone since the very beginning with scrolling. A child can pick up an iPhone and scroll to their favorite app on the Home screen, just as easily as they can push a toy car across the floor. So, what are some key, kind of, characteristics of this scrolling, dynamic behavior that we have? Well, firstly it's tapping into that intuition, that shared understanding that we all have for objects moving around in the world. And, our influence on those objects. The motion of the content is perfectly seamless, so while I'm interacting with it, while I'm dragging the content around, my body is providing the fluidity of the movement, because my body is fluid. But, as soon as I let go of the content, it seamlessly coasts to a stop. So, we're kind of maintaining the momentum of the effort being put into the interface. The amount of friction that's being used for scrolling is consistent, which makes it predictable, and very easy to master. And finally, the content comes to an imperceptible stop, kind of like the car, not really knowing precisely when it came to a stop. And, we feel that this distinct lack of an ending kind of reinforces this idea that the content is always moving, and always able to move, so while content is scrolling, it makes it feel like you can just put your finger down again, and continue scrolling. You don't have to wait for anything to finish. So, there are innumerable characteristics of the physical world that would make for great behaviors. We don't have time to talk about them all, but I'd like to focus on this next one, because we personally find it incredibly indispensable in our own design work. So, materials like this beautiful flower here, the natural fibers of this flower have this organic characteristic called elasticity. And, elasticity is this tendency for a material to gracefully return into a resting state once stress or strain is removed. Our own bodies are incredibly elastic. Now, we're capable of running incredibly long distances, not because of the strength of our muscles, but because of their ability to relax. It's their elasticity that's doing this. So, our muscles contract and relax once stress and strain is removed. And, this is how we conserve energy. Makes us feel natural and organic. The same elasticity is used in iPhone 10. Tap an icon on the Home screen, and an elastic behavior is pulling the app towards you. Bring it exactly where you want it to be. And, when you swipe from the bottom, the app is placed back on the Home screen in its perfect position. We also use elasticity in scrolling. So, if I scroll too far and rubberband, like Chan was talking about, when you let go, the content uses elasticity to pull back within the boundaries, helping you get into this resting position, ready for the next time you want to scroll. So, let's dig in a little deeper on how this elasticity works behind the scenes. You can think of the scrolling content as a ball attached to a spring. On one end of the spring is the current value. This is where the content is on the display. And, the other end of the spring is where the content wants to go because of its elasticity. So, you've got this spring that's pulling the current value towards the target. Its behavior is influencing the position of the content. Now, the spring is essentially pulling that current value towards the target. And, what's interesting about a spring is, it does this seamlessly. This seamlessness is, kind of, built in to the behavior. And, this is what makes them such versatile tools for doing fluid interface design. Is that you, kind of, get this stuff for free. It's baked in to the behavior itself. So, we love this behavior of a value moving towards a target. We can just tell the ball where to go, and we'll get this seamless motion of the ball moving towards the target. But, we want a little bit more control over how fast it moves. And, whether it overshoots. So, how do we do that? Well, we could give the ball a little more mass, like make it bigger, or make it heavier. And, if we do that, then it changes the inertia of the ball, or its willingness to want to start moving. Or, maybe its unwillingness to want to stop moving. And, you end up with this little overshoot that happens. Another property that we could change is the stiffness of the spring, or the tensile strength of the spring. And, what this does, is it affects the force that's being applied to the ball, changing how quickly it moves towards the target. And, finally, much like the car, and the braking of a car, we can change the damping, or the friction, of the surface that the ball is sitting on. And, this will act as, kind of, a brake that slows the ball down over time, also affecting our ability to overshoot. So, the physical properties of a ball and a spring are, kind of, physics class material, right? It's super useful in a scientific context, but we've found that in our own design work they can be a little bit overwhelming or unwieldy for controlling the behavior of objects on the screen. So, we think our design tool should have a bit of a human interface to them. That they need to reflect the needs of the designer that's using the tool. And so, how do we go about that? How do we simplify these properties down to make it more design friendly? So, mass stiffness and damping will remain behind the scenes, they're the fundamental properties of the spring system that we're using. But, we can simplify our interface down to two simple properties. The first is damping, which controls how much or little overshoot there is from 100% damping, where there will be no overshoot to 0% damping where the spring would oscillate indefinitely. The second property is response. And, this controls how quickly the value will try and get to the target. And, you might notice that I haven't used the word duration. We actually like to avoid using duration when we're describing elastic behaviors, because it reinforces this concept of constant dynamic change. The spring is always moving, and it's ready to move somewhere else. Now, the technical terms for these two properties are damping ratio and frequency response. So, if you'd like to use these for your own design work, you can look up those terms, and you'll find easy ways to convert them. So, we now have these two simple properties for controlling elastic behaviors. But, there's still an infinite number of possibilities that we can have with these curves. Like, there's just hundreds, thousands, millions of different ways we can configure those two simple properties and get very different behavior. How do we use these to craft a character in our app? To control the feel of our app? Well, first and foremost, we need to remember that our devices are tools. And, we need to respect that tools, when they're used with purpose, require us to not be in the way, not get in the way with introducing unnecessary motion. So, we think that you should start simple. A spring doesn't need to overshoot. You don't need to use springy springs. So, we recommend starting with 100% damping, or no overshoot when you're tuning elastic behaviors. That way you'll get smooth, graceful, and seamless motion that doesn't distract from the task at hand. Like, just quickly shooting off an email. So, when is it appropriate to use bounciness? There's got to be a time when that's appropriate, right? Well, we feel if the gesture that's driving the motion itself has momentum, then you should reward that momentum with a little bit of overshoot. Put another way, if a gesture has momentum, and there isn't any overshoot, it can often feel broken or unsatisfying to have the motion follow that gesture. An example of where we use this is in the Music app. So, the Music app has a small minibar representing Now Playing at the bottom of the screen, and you can tap the bar to show Now Playing. Because the tap doesn't have any momentum in the direction of the presentation of Now Playing, we use 100% damping to make sure it doesn't overshoot. But, if you swipe to dismiss Now Playing, there is momentum in the direction of the dismissal, and so we use 80% damping to have a little bit of bounce and squish, making the gesture a lot more satisfying. Bounciness can also be used as a utility, as a functional means. It can serve as a helpful hint that there's something more below the surface. With iPhone 10, we introduced two buttons to the cover sheet for turning on the Flashlight, and for launching the Camera. To avoid accidentally turning on the flashlight by mistake, we require a more intentional gesture to activate the Flashlight. But, if you don't know that there's a more intentional gesture needed to activate it, when you tap on the button, it responds with bounciness. Has this kind of playful feel to it. And, that hint is teaching you not only that the button is working, but that it's responding to you. But, it's kind of teaching you that if you press just a little bit more firmly, it'll activate. It's like teaching you. It's hinting in the direction of the motion. So, bounciness can be used to indicate this kind of thing. Now, so far we've been talking about using motion to move things around, or to change their scale, change their visual representation on the screen. But, we perceive motion in many different ways. Through changes in light and color, or texture and feel. Or even sound. Many other sensations that we-- our senses can detect. We feel this is an opportunity to go even further, go beyond motion, when you're tuning the character of your app. By combining dynamic behaviors for motion with dynamic behaviors for sound and haptics, you can really fundamentally change the way an interface feels. So, when you see, and you hear, and you feel the result of the gesture, it can transform what was otherwise just a scrolling behavior into something that feels like a very tactile interface. Now, there's one final note I want you thinking about when you're crafting the character of your app. And, that's that it feels cohesive, that you're staying in character. Now, what does this mean? So, even within your app, or across the whole system, it's important that you treat behaviors as a family of behaviors. So, in scrolling for example, when I scroll down a page, using a scrolling behavior, and then I tap the status bar to scroll to the stop of the page, using an elastic behavior. In both cases, the page itself feels like it's moving in the same way, that it has the same behavior, even though two different types of behaviors are driving its motion, are influencing its motion. Now, this extends beyond a single interaction like scrolling. It applies to your whole app. If you have a playful app, then you should embrace that character, and make your whole app feel the same way. So, that people-- once they learn one behavior of your app, they can pick up another behavior really easily, because we learn through repetition. And, what we learn bleeds over into other behaviors. So, next up, I'd like to talk a little bit about aligning motion, or dynamic motion, with intent. So, for a discrete interaction like a button, it's pretty clear what the intent of the gesture is. Right? You've got three distinct visual representations on screen here. And, when I tap one of them, the outcome is clear. But, with a gesture like a swipe, the intent is less immediately clear. You could say that the intent is almost encoded in the motion of the gesture, and so it's our job, our role, to interpret what the motion means to decide what we should do with it. Let's look at an example. So, let's say I made a FaceTime call, a one-on-one FaceTime call, and in FaceTime, we have a small video representation of yourself in the corner of the screen. And, this is so I can see what the person on the other end sees. We call this floating video the PIP, short for picture in picture. Now, we give the PIP a floating appearance to make it clear that it can be moved. And, it can be moved to any corner of the screen, with just a really lightweight flick. So, if we compare that to the Play, Pause, and Skip buttons, like, what's the difference here? So, in this case, there's actually four invisible regions that we're dealing with. No longer do we have these three distinct visual representations on screen that are being tapped. We kind of have to look at the motion that's happening through the gesture, and intuit what was meant. Which corner did we intend to go to? Now, we call these regions of the screen endpoints of the gesture. And, when the PIP is thrown, our goal is to find the correct endpoint, the one that was intended. And, we call this aligning the endpoint with the intent of the gesture. So, one approach for this is to keep track of the closest endpoint as I'm dragging the PIP. Now, this kind of works. I can move the PIP to the other corner of the screen, but it starts to break down as soon as I move the PIP a little bit further. Now, I actually need to drag the PIP quite far, like past halfway over the screen. Pretty close to the other corner. So, it's not really magnifying my input. It's not really working for me. And, if I try and flick the PIP, it kind of goes back to the nearest corner, which isn't necessarily what I expected. So, the issue here is that we're only looking at position. We're completely ignoring the momentum of the PIP, and its velocity when it's thrown. So, how can we incorporate momentum into deciding which endpoint we go to? So, to think about this, I think we can set aside endpoints for a moment, and take a step back. And, just really simplify the problem. Ultimately, what I'm trying to do here is move content around on the screen. And, I actually already have a lot of muscle memory for doing exactly that with scrolling. So, why don't we use that here? We use scrolling behaviors all the time, so we have this natural intuition for how far content goes when I scroll. So, here you can see that when I scroll the PIP instead, it coasts along, and it slows down, using this familiar deceleration that we're familiar with from scrolling. And, basically by taking advantage of that here, we're reinforcing things that people have learned elsewhere. That the behavior is just doing what was expected of the system. Now this new, hypothetical, imaginary PIP position is not real. We're not going to show the PIP go here in the interface. This is what we call a projection. So, we've taken the velocity of the PIP, when it was thrown. We've, kind of, mixed in the deceleration rate, and we end up with this projected position where it could go if we scrolled it there. And so, now instead of finding the nearest endpoint to the PIP when we throw, we can calculate its projected position and move there instead. So now, when I swipe from one corner of the screen to another with just a lightweight flick, it goes to the endpoint that I expected. So, this idea of projecting momentum is incredibly useful. And, we think its super important. I'd like to share some code for doing this with you, so that you can do this in your own apps. So, this function will take a velocity like the PIP's position velocity, and deceleration rate, and it'll give you the value that you could use as an endpoint for dynamic behavior. It's pretty simple. If we look at my FaceTime example of the pan gesture ending code, you can see that I'm just using the UIScrollView.DecelerationRate. So, we're leaning on that familiarity people have with scrolling and how far content will go when scrolled. And, I'm using that with my projection. So, I take the velocity of the PIP and the deceleration rate, and I create that imaginary PIP position. And, it's this imaginary, projected position that I then use as the nearest corner position. And, I send my PIP there, by retargeting it. So, this idea of using projection to find out the endpoint of a position, is incredibly useful for things being dragged or swiped, where you really need to respect the momentum of the gesture. But, this projection function isn't just useful for positions, you can also use it for scales, or even for rotations. Or, even combinations of the two. It's a really versatile tool that you should really be using to make sure that you're respecting the momentum of a gesture, and making it feel like the dynamic motion in your app is an extension of yourself. So, that's designing with motion. Dynamic motion. Behaviors should continuously and seamlessly work in concert with interactions. We should be leaning on that shared intuition that we have for the physical world around us. The things that we learn as children about how objects behave and move in the physical world, apply just as readily to our dynamic interfaces. You should remember that bounciness needs to be purposeful. Think about why you're using it, and whether it's appropriate. And, make sure that as you add character, and texture, that you're balancing it with utility. And finally, remember to project momentum. Don't just use position, use all of the information that's at your disposal to ensure that motion is aligned with the intent of where people actually want to go. And then, take them there. So, to talk a little bit more about how to fluidly respond to gestures and interactions, I'd like to introduce my colleague, Marcos, to the stage. Thanks for having me, everyone. That was great. Thanks, Nathan. Hi everyone. My name is Marcos. So far, we've seen how important fluidity is when designing interfaces. And, a lot of that comes from your interaction with a device. So, in this section, we're going to show you how touches on the screen become gestures in your apps. And, how to design these gestures to capture all the expression and intent into your interfaces. So, we're going to start by looking at the design of some core gestures like taps and swipes. Then, we'll look at some interaction principles, that you should follow when designing gestures for your interface. And then, we'll see how to deal with multiple gestures, and how to combine them into your apps. We're going to start by looking at a gesture that is apparently very simple, a tap. You would think that something-- you would think that a tap is something that doesn't have to be designed, but you'll see how its behavior has more nuances than it seems. In our example, we're going to look at tapping on a button, in this case, on the Calculator app. The first thing to remember is that the button should highlight immediately when I touch down on it. This shows me the button is working, and that the system is reacting to my gesture. But, we shouldn't confirm the tap until my touch goes up. The next thing to remember is to create an extra margin around the tap area. This extra margin will make our taps more comfortable, and avoid accidental cancellations if a touch moves during interaction. And, like my colleague Chan was saying, I should be able to change my mind after I've touched down on the button. So, if I drag my finger outside the tap area, and lift it, I can cancel the tap. The same way, if I swipe it back on the button, the button should highlight again, and let me confirm the tap. The next gesture we're going to talk about is swipe. Swipes are one of the core gestures of iOS, and they're used for multiple actions like scrolling, dragging, and paging. But, no matter what you use it for, or how you call it, the core principles of a gesture are always the same. In this example, we're going to use a swipe to drag this image to the right. So, the interaction starts the moment I touch down on the image with intention to drag it. But, before we can be sure it's a swipe, the touch has to move a certain distance. We learn to differentiate swipes from other gestures. This distance is called hysteresis, and is usually 10 points in iOS. So, once the touch reaches this distance, the swipe begins. This is also a good moment to decide the direction of the swipe. If it's horizontal, or vertical for instance. We don't really need it for example, but it's very useful in some situations. So, now that the swipe has been detected, this is the initial position of a gesture. After this moment, the touch and the image should stay together and move as one thing. We should respect the relative position, and never use the center of the image as the dragging point. During the drag, we should also keep track of the position and speed up the touch, so when the drag is over, we don't use the last position. We use the history of the touch, to ensure that all the motion is transferred fluidly into the image. So, as we've seen, touch and content should move together. One-to-one tracking is extremely important. When swiping or dragging, the contents should stay attached to the gesture. This is one of the principles of iOS. You enable scrolling, and makes the device feel natural and intuitive. It's so recognizable and expected that the moment the touch and content stop tracking one-to-one, we immediately notice it. And, in the case of scrolling, it shows us that we've reached the end of the content. But, one-to-one tracking is not limited to touch screens. For instance, manipulating UI on the Apple TV was designed around this concept. So, even if the touch is not manipulating the content directly, having a direct connection between the gesture and the interface puts you in control of the action, and makes the interaction intuitive. Another core principle when designing gestures, is to provide continuous feedback during the interaction. And, this is not just limited to swipes or drags. It applies to all interactions. So, if you look again at the Flashlight button on the iPhone 10, the size of button changes based on the pressure of my touch. And, this gives me a confirmation of my action. It shows me the system is responding to my gesture, but it also teaches me that pressing harder will eventually turn on the flashlight. Another good example of continuous feedback, is the focus engine on the Apple TV. So, the movements on the Siri remote are continuously represented on the screen. And, they show me the item that is currently selected, the moment the selection is going to change, and the direction the selection is going to go. So, having our UI respond during the gesture is critical to create a fluid experience. For that reason, when implementing your gestures, you should avoid methods that are only detected at the end of the gesture, like UISwipeGestureRecognizer. And, use ones like the actual touches, or other gestureRecognizers that provide all possible information about the gesture. So, not just the position, but also the velocity, the pressure, the size of the touch. In most situations though, your interfaces must respond to more than one gesture. As you keep adding features to your apps, the complexity and number of gestures increases, too. For instance, almost all UIs that use a scroll view will have other gestures like taps and swipes competing with each other. Like in this example, I can scroll the list of Contacts, or freely touch on one of them to preview it. So, if we had to wait for the final gesture, before we show any feedback, we would have to introduce a delay. And, during that wait, the interface wouldn't feel responsive. For that reason, we should detect all possible gestures from the beginning of the action. And, once we are confident of the intention, cancel all the other gestures. So, if we go back to our example, I start pressing that contact, but I decide to scroll instead. And, it's at that moment that we cancel the 3D touch action, and transition into the right gesture. Sometimes, though, it's inevitable to introduce delay. For instance, every time we use the double-tap in our UIs, all normal taps will be delayed. The system has to wait after the tap, to see if it's a tap or a double-tap. In this example, since I can double-tap to zoom in and out of a photo, tapping to show the app menu is delayed by about half a second. So, when designing gestures for your applications, you should be aware of these situations, and try to avoid delays whenever possible. So, to summarize, we've seen how to design some core gestures, like taps and swipes. We've seen that content and touch should move one-to-one, and that is one of the core concepts of iOS. You should also provide continuous feedback during all interactions, and when having multiple gestures, detect them in parallel from the beginning. And now, I'd like to hand it back to Chan, who will talk about working with fluid interfaces. Thanks, everyone. Nice job. Alright, I'm back. So, we just learned about how to approach building interfaces that feel as fluid, as responsive, and as lively as we are. So, lets talk about some considerations now that we're feeling a little bit more comfortable with this, for working within the medium of fluid interfaces. And that begins with teaching. So, one downside to a gestural interface is that it's not immediately obvious what the gestures are. So, we have to be friendly and clever about how we bring users along with us in a way that's friendly and inviting. And so, one way we can do that is with visual cues. So, the world is filled with these things, right? You can learn them once, and you can use them everywhere. They're portable. And so, when you see this, you know how to use it. So, we've tried to establish similar conventions in iOS. Here's a couple examples. So, if you have a scrolling list of content, you can clip the content off the bottom there, to indicate that there's more to see, that invites me to try and reveal what's under there. And, if we're dealing with pages of content, you can use a paging indicator to indicate that there's multiple pages of content. And, for sliding panes of content, you can use an affordance, or a grabber handle like this, to indicate that it's grabbable and slidable. Another technique you can use is to elevate interactive elements to a separate plane. So, if you have an interactive element, lifting it up to a separate plane can help distinguish it from the content. So, a good example of this is our on/off switch. We want to indicate that the knob of the switch is grabbable, so we elevate it to another plane. This helps visually separate it, and indicate its draggable nature. So, floating elements, interactive elements like this, above the interface can help indicate that they're grabbable. Next, we can use behavior, you know, to show rather than tell to use-- how to use an interface. So, we can reinforce a dynamic behavior with a static animation. So, an example of this is Safari. In Safari, we have this x icon at the top left to close the tab, and when you hit that button, we slide the tab left to indicate it's deleted. This hints to me that I can slide it myself to the left. And, accomplish the same action of deleting the tab through a gesture. So, by keeping the discrete animation and the gesture aligned, we can use one to teach the other. And, there's another technique we can use, which is explanations. This is when you explicitly tell users how to use a gesture. So, this is best when used sparingly, but it's best when you have one gesture that's used repeatedly in a bunch of places, and you explain it once up front, and then you just keep using it, and keep reinforcing it. Don't use it for a gesture that's used only intermittently. People won't remember that. Now, I want to talk a little bit about fun and playfulness. Because this is one of the most important aspects of a fluid interface. And, it only happens when you nail everything. It's a natural consequence of a fluid interface. It's when the interface is responding instantly and satisfyingly. When it's redirectable and forgiving. When the motion and gestures are smooth. And, everything we just talked about. The interface starts to feel in sync with you. And, something magical happens where you don't feel like you need to learn the interface, you feel like you're discovering the interface. And so, we think it's great when we allow people to discover the interface through play. And, it doesn't even feel like they're learning it, it feels fun. So, people love playing with stuff. So, we think it's great to play into our natural fiddle factor. You know, play is our mind's internalizing the feel of an interface. So, it's great when we're building this stuff, when we're prototyping it, just to build it. You know, play with it yourself. See how you fiddle with it. Hand it to others see how they play with it. And, think about how you can reinforce that with something like an animation, or behavior, an explanation. And, it's surprising how far play can go, and having interface teach itself to people. Let's talk a little bit about fluidity as a medium. How we actually go about building this stuff. You know, we think interfaces like this are a unique medium, and it's important that we approach it right. So, the first thing is to design the interactions to be inseparable from the visuals, not an afterthought. The interaction design should be done in concert with the visuals. You shouldn't be able to even tell when one ends and another begins. And, it's really important that we build demos of this stuff. The interactive demo we think is really worth a million static designs. Not just to show other people, but to also understand the true nature of the interface yourself. And, when you prototype this stuff, it's so valuable for you because you get to almost discover the interface as you're building it. You know, this technique is actually how we built the iPhone 10 interface. And, it's really important because it also sets a goal for the implementation. We're so lucky here at Apple that we have this amazing engineering team to build this stuff, because it's really hard to build. And, it's so important also to have that kind of magical example that reminds yourself and the engineering teams, and yourselves that what it can feel like, you know? And, it's really important to, kind of, remember, remind yourself of that. And, it makes-- when you actually build it, it makes something that's hard to copy, and it gives your app a unique character. So, you know, multitouch is such an amazing medium we all get to play in. We get to use technology to interface with people at an ancient, tactile level. It's actually really cool. You know, all those principles we talked about today, they're at the core of the design of the iPhone 10 gestural interface, you know, responsive, redirectable, interruptible gestures, dynamic motion, elegant gesture handling. In a lot of ways, it's kind of the embodiment of what we think a fluid interface could be. When we align the interface to the way we think and move, something kind of magical happens. It really stops feeling like a computer, and starts feeling like a seamless extension of us. You know, as we design the future of interfaces, we think it's really important to try and capture our humanity in the technology like this. So, that one of the most important tools of humankind is not a burden, but a pleasure and a delight to use. Thank you very much.  Hey everyone. Welcome to our session on what's new in App Store Connect? My name is Daniel Miao. And I'm an engineering manager on the App Store Connect team. So what's new? Well, first off, you may have noticed we changed our name. This is really just a small part of our ongoing commitment to building the best tools and services focused on you, our developers building apps for the App Store. Now along these lines, we've also been undergoing a few renovations on our App Store. So last year, we went back and we redesigned the iOS App Store. And following the success of that redesign, this year we also redesigned the Mac App Store. Now with this Mac App Store redesign, we've added support for a few new features including app subtitles and app previews. You can take advantage of these features by signing into App Store Connect or by using the XML feed if you use that for automation today. Now for today's session, we're going to focus on enhancements we've made to App Store Connect. And we're going to talk about it within the context of your app lifecycle. This lifecycle typically begins with the design phase. This phase can be both visual or in code. And as you make your way around this lifecycle, you eventually end up with your app on the App Store followed with an analysis of how your app is doing on the App Store. Now more specifically, we're going to focus on a few key areas of this lifecycle. We're going to talk about changes to provisioning, user management, delivering your app into our system, beta testing the builds, and analyzing your biggest business drivers. Now looking at all these different areas, you can imagine how something like automation could bring all these together quite nicely. And we tend to agree which is why we're very excited because this summer we'll be releasing an all-new product focused around automation and we call it the App Store Connect API. Now the App Store Connect API is simply a RESTful interface into your app management experience. To authenticate with the API, you'll be using industry-standard JSON web token authentication. Some of you may already be familiar with this with other APIs you've worked with. In designing the API, we've designed it around strict convention. This means whether you're looking at something like naming or error formats, no matter what corner of the API you're in, everything should feel familiar. And finally, we've been writing extensive documentation for this API. You might be looking for references around resources and their attributes or maybe you're simply looking for a guide around how to best use this API for your use cases. Well, either way and for many other scenarios, this documentation will be the place to go. We'll be launching the API this summer. And we will be launching with support for the areas of the lifecycle we highlighted earlier. Now as we add more features and support for other areas, we will be delivering those features to you. So going back to our lifecycle, let's talk through some enhancements and some API integrations. We'll begin by talking about provisioning. Now today, many of you rely on Xcode to manage your provisioning automatically. But for some of you who are looking for a little more control, today you sign into the Apple Developer website to generate the resources you need to sign your apps. Well, with the API this summer, you'll be able to do a lot of these things directly. The API will support generating provisioning profiles, creating and revoking signing certificates as well as managing your devices and your app bundle IDs. This should make it a little bit easier for you to integrate your provisioning activities into your automation process. So early on in the lifecycle while you're thinking about provisioning, you're probably also thinking about managing your users. Now the API is going to support inviting new users to App Store Connect, modifying which apps your users can see, managing your user's roles as well as updating profile details. Now we didn't want to stop at simplifying your management experience with just the API. We also wanted to take a look at how we could simplify where you go to manage your users in the first place. Today, you go to both App Store Connect and the Apple Developer website. But starting this summer, you'll be able to go to just one place. And that's App Store Connect. Coming to App Store Connect, you'll be able to think about all of your users within the context of just one set of roles. And you can manage just one account for each of your Apple IDs. Now we know that App Store Connect and the Apple Developer website, they're very different systems. So you might be wondering how is all this going to come together? Well, that's why we've built a way for you to see how your user's permissions will be changing once this process is complete. Once we have this preview available, we'll let you know via Developer News and on the App Store Connect home page. From here, you can click into a page that looks sort of like this. Here you'll see a list of all of your users as well as a summary of how their roles will be changing. Of course, some of your users won't be changing at all. Now if you click into a single one of these users, you'll be able to pull up a modal giving you much more information about that user including what their permissions will look like going forward. We encourage you to come in to App Store Connect and take a look at each and every one of your users and make any necessary changes before we began this process. And once this process completes, we encourage you to come back one last time to make sure your users are in a state that makes sense to you and after that you simply come back to App Store Connect for any future changes. All right. So while some of you were thinking about provisioning and user management, others were working on your apps. Now a natural part of the app development process is, of course, spinning up builds. At some point, you reach a build that you're ready to deliver, that way you can distribute it to your customers. Now many of you today are using our tool called Transporter. For those of you who aren't familiar, Transporter is our command line tool that does many things and one of those is delivering your builds into our system. For those of you who use Transporter today, you know that macOS is a supported platform. Well, we also know that many of you use Linux for things like continuous integration, which is why this summer we'll be adding Linux as a supported platform for Transporter. Now you'll continue using Transporter the same way you do today. And Transporter will continue allowing you to validate your builds before you deliver them. This will save you a lot of time because you'll know whether your builds are in a good state before you push us all of the bits. And once we launch the API this summer, in addition to the username and password authentication that you can do today, you'll also be able to use the same API tokens that you use with the rest of the App Store Connect API. Okay. So now that we have a few builds in the system, it's time to beta test them. And, of course, beta testing means TestFlight. We've got a few enhancements to TestFlight. And I'd like to invite Tommy McGlynn up to the stage to tell you all about them. Thanks, Daniel. I'm really excited to be here again talking to you about TestFlight. We've got a new feature to tell you about, and it deals with tester acquisition. Today, all you need to invite someone to TestFlight is their e-mail address. You simply enter their e-mail and they'll be sent an invitation. Tapping on the invitation will launch TestFlight where they can install your beta app. It's pretty awesome, but if you're dealing with a lot of testers, it can be cumbersome. And if you're working with testers who don't have an e-mail address, there isn't an easy way to invite them. We think we can do even better, and we call it TestFlight public link. This is really exciting. Public link is a unique URL that represents an open invitation to your beta app. You can share it anywhere you'd like. And it can be used by anyone to become a new tester. So public links can be shared anywhere that a regular link can be shared. This means you can send a public link directly to someone, or you could share it on social media to reach a wider audience. If someone wants to become a tester, they simply tap on a link. This would launch TestFlight where they can install your beta app and instantly become one of your testers. It's really easy. You don't have to collect any information. You simply make the link available and testers can join. So let me show how we can create a public link. In TestFlight, groups allow you to organize your testers and decide which builds each group of testers should have access to. We'll need to create a group before we can generate a public link. And we can do that by clicking new group in the left nav and then giving the group a name. Now before we create a public link and start bringing testers in, we want to make sure there is a build that they can test. So first we'll add a build to this group. We do that by going to the builds tab and clicking the plus button next to build. This will show all the builds we've uploaded. And we can choose one that we want to distribute. Now that this group has a build, we can create a public link for this group. We'll go back to the testers tab and click enable public link. This generates a unique URL that we can share anywhere to reach new testers. And testers who join through that link are automatically added to the group. So when you're ready to deliver a new build, you simply add the build to the group. You can see how this is a much faster way to bring in up to 10,000 active testers. If you want more control over the number of testers or you're not quite ready for your beta app to go viral, you can easily set a custom tester limit on each public link. This would put a cap on the number of testers who could join through that link. You also have the ability to disable the public link at any time at which point new testers would no longer be able to join. If someone tries to open a link once it's been disabled, they'll receive a message letting them know this beta isn't accepting any new testers. I also want to show what happens when someone taps on a public link and doesn't have TestFlight installed. They'll land on a localized page that explains how to install TestFlight and get started as a beta tester. This will make it a lot easier for brand-new testers to begin testing your app. I'd also like to mention that you'll soon be able to do all of this without using the App Store Connect UI at all. This will be possible using the new App Store Connect API. You'll be able to automate the creation of groups, assign builds to groups, manage public links, add and remove testers, and update test information. All of this can be automated using our new REST API. Thank you. We have an entire session devoted to the new App Store Connect API. And I strongly encourage you to check it out tomorrow at 3:00. That's TestFlight public link. And I'm really excited to see what you're able to do with this new feature. Thank you. Thanks, Tommy. Well, TestFlight public links and the new TestFlight APIs are just two more ways for you to streamline your beta distribution process. So now that we've gone through a successful round of beta testing, prepared our App Store metadata, gone through app review, and now our app is in the hands of our customers. Now it's time to take a look at some hard numbers to see how our customers are actually responding to our apps. So the first place we can look at this is in sales and trends. Just this week we launched an all new sales and trends overview page. This page gives you a summary of your biggest business drivers like app units, in-app purchases, and sales. As you scroll down this page, you'll see your top apps and how they contribute to each of these metrics. Here, when you look at these sales numbers, these numbers include not just app sales but also in-app purchase sales from within those apps. As we scroll down towards the bottom, we'll see these same metrics broken down by territory and by device. After looking at all this information, you'll know better where to focus your efforts. Now another place we can look is in your generated reports to see how your app is doing. Today, in order to download your generated reports, you use a reporter tool but beginning this summer you'll be able to do the same thing directly with the API. And we'll be supporting the download of both financial reports and sales reports. This will make it easier for you to integrate reporting in to your process. So we've been talking about a lot of features. And so far they've mostly been centered around your web and your desktop experience. So now we'd like to move on and talk about our all new mobile experience. And I'd like to invite Alex Miyamura up to the stage to tell you all about it. Thanks, Daniel. We know that the involvement with your apps doesn't stop the moment that you step away from your Mac. You may have a version of one of your apps that you've been working on hard for quite a long time that you're just itching to release and it's now in app review. You may just have released a version of one of your apps, or you may just want to see how your apps are doing, check out their sales and trends or their ratings and reviews. And for that reason, we've created a brand-new App Store Connect experience for iOS. This experience is centered around giving you the ability to access your apps data on the go as well as empowering you to take certain quick actions and letting you know exactly when you can take them. Now let's start with sales and trends. When you tap into your trends tab, you'll see this gorgeous graphical summary of your app's performance over the past 7 days. You'll see your app's units and proceeds. Scrolling down, you'll see sales, updates, and then in-app purchases and app bundles. Now we know that you implement diverse modernization strategies across your apps. Some of you have paid apps, free apps, apps with in-app purchases, or you may leverage subscriptions. Now we know that and we wanted to give you the ability to select exactly which of these graphs are most relevant to you. So we've added this edit function where you can select the graphs you'd like to see. So let's say that, in my business, I only have free apps. I'll go ahead and unselect proceeds, sales, in-app purchases, and app bundles. Hit done and, voila, now I just see units and updates, exactly what's relevant for my business. Next, we're going to take a look at the time selection control underneath the trends title. In iTunes Connect mobile, we allow you to see the status from 7 days all the way to 26 weeks. Now this was amazing if you are interested in longer-term trends, right? But what about an app that you just released? You might not even have 7 days' worth of data. And if you want to compare those 7 days to the previous 7 days, well obviously you're not going to have that. So in App Store Connect, we introduce the 1 day view. In this view, you can see how your apps are during day over day even just a few days after launch. Of course, some of you are still going to be interested in longer-term trends, right? So we still offer you the ability to view your apps data over 2 weeks, 5 weeks, 13 weeks, and 26 weeks. You can also drill down into each one of these graphs in more detail. So we're going to take a look at units right now. When you tap into one of these graphs, you'll see a breakdown across your free apps, your paid apps, and in-apps for the iOS/tvOS App Stores as well as the redesigned Mac App Store. You can also take a look at a deeper dive into any one of these cells just by tapping on them. So let's go ahead and tap the free iOS/tvOS app cell. Now you can see data about each one of your apps in more detail and also your apps top territories. If you're interested in seeing more territories, you can tap the show more button. Now you can see the territories that your apps are available in worldwide. You can also take a look at your apps data individually as opposed to this aggregate view by taking a look at the my apps tab which we'll do next. When you tap into the my apps tab, you'll see a list of your apps, obviously. But what if you're a member of more than one development team? So do we have anyone in the audience that is, indeed, a member of more than one development team? Cool. So it's actually quite a bit of you. So that's awesome. And that's exactly why we've introduce the ability to select which development team you'd like to see within App Store Connect. So we'll tap the little account button. That brings up settings. And you'll see the account cell. We'll tap into that, select the team that we'd like to see within App Store Connect, and dismiss this. Now we see exactly the apps that we're interested in right now. Let's go ahead and take a look at one of the apps within our apps list, Mountain Climber. When we tap into Mountain Climber, you'll see four sections. The first section that you see is the iOS app section. And here you'll see your iOS versions. If your app has a tvOS app version, you'll see the tvOS app section and, obviously, the tvOS app versions underneath. Now we also have App Store information. And remember that I noted that you can see your apps trends data individually. Well, all you have to do is tap into that cell and you'll be taken there. Now, finally we have notifications. And we're going to do something a bit unorthodox here. We're actually going to start from all the way at the bottom with notifications. There are two types of notifications that you can receive within the App Store Connect app. The first one is for app status changes. And this is something like when your app is in review and then goes to pending developer release when it's approved. The next one that you can take is for reviews with a certain star rating. And so we're going to go ahead and select the five-star review because we want to thank users that are enjoying our app as well as the one-star review because we want to engage users that, for whatever reason, are not having the best of experiences. Now that we've selected our notifications, we're going to go back to the app view. The reason why we started with notifications is because one of the key tenets of the App Store Connect experience on iOS is giving you the ability to take a quick action around your apps and letting you know when you can do so. So we know that you've taken -- you've spent countless hours working on your apps. You've distributed them to beta testers via TestFlight. You've taken those testers feedback, iterated upon that with your apps, and finally you have a build that's ready for release to the world. Now one of the last things you have to do before releasing your app to the world is, of course, submitting your build to app review. One of the notifications that we just set up in the App Store Connect app was for app status changes. This means that when your app is approved by app review, you'll receive a push notification that tells you this happy news. The second that you received one of these notifications, you can open App Store Connect wherever you are. And we'll go to the version view. You'll now see that our app Forest Explorer is pending developer release. We'll scroll down and you'll see two buttons. The first one that you'll see is this big red button. It's really obvious. Reject this binary. We don't want to do that, but now we see release this version and that's exactly what we want to do. We'll tap there. We'll get a confirmation dialog and we'll hit release. Now we sent that build off to the world. Sometimes you don't get the happiest push notifications and this is one of them. Right. Sometimes your app may be rejected by app review. And, obviously, that's something that no one wants to see. Later in our session, my colleague Daniel is going to go over some tips and tricks from app review to make sure that this never happens to you. But what should we do now? We need to make sure that we can get this app approved and out there to the world. Now what we're going to do is open up App Store Connect. And under App Store information, you'll see this warning badge next to resolution center. Within resolution center, you'll be able to see app reviews feedback for your app. You can tap into one in more detail and, of course, reply to it on the go. Now once we send our response back to app review, they'll get a chance to review it and hopefully our app will be approved at once. Now once your app has been submitted to app review, once it's been approved, and you've used App Store Connect to release it, the next thing that you'll be receiving his customer feedback. And here's one of the notifications that you may receive, a five-star review. Now recall we set up notifications for one and five-star reviews within App Store Connect. Since we set these up, you'll know the very minute a customer submits one of these reviews. You can access all customer feedback within App Store Connect by accessing the ratings and reviews section where you'll be able to see your current rating across any one of the territories you've selected or, obviously, all territories. You can also take a look at your reviews, read them in more detail, and reply to them. Now this is a satisfied customer so we're going to thank them for the positive feedback. That covers App Store Connect notifications and the quick actions that you can take from them but there's one more thing. We've also optimized App Store Connect for iPad. So let's take a quick look. As I mentioned earlier, you can view a certain apps sales and trends data. And here is sales and trends for Forest Explorer. You can see units. And we'll scroll down to updates and also drill down into the territories view. Now I'm sure you want to try this out on your own. And our app is actually available now. So if you haven't downloaded it already, please go ahead and do so. We've worked hard to bring an experience -- an improved mobile experience to you. And we hope you'll enjoy using it as much as we enjoyed creating it. And with that, now back to Daniel. Thanks, Alex. So that's a beautiful new app that'll give you even more power to manage your apps on the go. So next we'd like to talk about a guideline change we made just this week that will let those of you with paid apps offer time-based free trials for your customers. Now those of you who are using subscriptions today know that free trials are a great way to attract new customers to your services. But we also know that subscriptions aren't the model that works for every app which is why we're happy to now have a path for paid apps. So we're going to take a look at what this setup looks like. So let's say we start with a paid app. The first thing you would do is turn your paid app into a free app. Now on top of this free app, you're going to add two non-consumable in-app purchases. The first is a free tier 0 priced non-consumable. And you'll present this to your customers when they launch the app so that they can opt-in to the free trial. Now the second non-consumable is the in-app purchase that you use to unlock your app functionality for that customer for good. You can present this at any point; in the beginning when they launch the app or when their free trial is complete. Now in order to use this setup, there are a few guidelines to look out for and to follow. First, make sure you name your trial in-app purchase with this naming convention. And next, you'll want to make sure that it's very clear to the customer exactly what they are signing up for. So please make sure that they know how long the free trial is going to last, how much it'll cost to unlock the functionality, and what kind of features and content won't be available anymore after the trial if they decide not to continue. All right. So we talked about a number of new features today. So let's take a second to recap them. First, we're launching an all-new App Store Connect API this summer. We're unifying where you go to manage your users. We've extended app Transporter to support Linux. We're launching TestFlight public links, which will make it a lot easier for you to invite large numbers of testers. We've launched a brand-new sales and trends overview page, a new App Store Connect for iOS page, and a path for you to offer trials to customers on your paid apps. Now these are features that will be launching between now and the end of the summer. But we also have two significant features we launched over the past year that we'd like to highlight. And these are intro pricing and pre-orders. So introductory pricing, for those of you who use subscriptions, is a great way to incentivize new customers to sign up for your subscriptions. Now you do this by offering your customers a discounted introductory price at the beginning of your subscription. There are three different pricing models we offer for this. First, we have free trials. Now with free trials, your customers sign up for free. They use your services for the introductory period. And at the end of the period, they move on to your regular subscription price. Next, we have a model that we call pay upfront. With this model, your customers sign up and they pay one time at the beginning of your introductory period. And, again, at the end of the period, the regular subscription price kicks in. And finally, we have pay as you go. With pay as you go, your customers pay a recurring discounted price during the introductory period. Now once the entire period has been completed then they move on to the regular subscription price. Now we have two related sessions that happened yesterday that have more information around the StoreKit side and best practices around both intro pricing and subscriptions. They are Best Practices and What's New with In-App Purchases and Engineering Subscriptions. So check those out if you have a chance. So next we have pre-orders. Now pre-orders are a great way for you to drum up excitement for your app before your app is actually available for download or purchase. In order to set up a pre-order, you can take any app that has not yet been released on the App Store and you can enable it. After you submit it to app review and that app has been approved, you can release that app to the App Store as a pre-order. Once that pre-order period is over and your customers have signed up for your pre-order, your app will move into ready for sale state where your customers can now download or purchase that app. It's also at this transition that customers that have signed up for your pre-order are charged for the price of your app. Now let's take a look at how you might set this up in App Store Connect. We begin here on the pricing and availability page. Again, if this app has never been released to the App Store, you'll see a section in the middle here labeled pre-orders. Here, you can enable your pre-orders and set a release date. This release date is a date on which your app goes from being available as a pre-order on the App Store to being available as a download or purchase on the App Store. In other words, this is when it goes from pre-order ready for sale to ready for sale. Now once you've gone through app review and your app is in pending developer release state, you'll see a button in the top right corner. This is the release pre-order button. You click this button to make your app available on the App Store as a pre-order. Once you do this, you'll see a banner at the top of the page telling you when your pre-order started and when it completes. This completion date is simply the release date that you set for your app. Now if you'd like to release this app before the release date or more immediately, you can click the release app now button in the top right corner. And that's intro pricing and pre-orders in a nutshell. So we have to close out this session with a few best practices from app review. These are a few tips that'll help your apps to get through review a little more smoothly. First, please enter contact information and keep it current throughout the review process. Sometimes app review needs to reach out to maybe ask a few questions to clarify how your app works. This will help us to get your app through review a bit more quickly, but we can't reach out to you without contact information. Next, please submit demo account information if your app requires a log in. There's a section on the version page where you can enter this and keep it updated. Please make sure the account -- this account information is current throughout the review process. And if there is a server side to the log in, which usually there is, please make sure that the server side account is also enabled throughout the process. You may be wondering does app review ever look at the notes? Well, yes they do. They look at it with every single review. So if there is any information that might make it clearer for app review or maybe tells them about any kind of non-obvious features of your app, be sure to include that information in these notes. When you submit screenshots, please make sure to include customer experience. We want to see what the customer sees in these screenshots. And finally, if you're asking your customers for permissions around things like their location or maybe access to their photo library, please include, in that permission modal, why you're requesting that information and how it's going to be used. And those are a few notes from app review. For more information about anything we've talked about today, please visit this link. A recording of this video will be posted there along with related documents and other things, more information about what we've talked about. We also have a few related sessions. Tomorrow, as Tommy mentioned, we have our Automating App Store Connect session at 3:00 where we're going to take you into a deep dive of the API and show you how to use the API at a much deeper level. We also have two labs coming up, one tomorrow and one the day after. So if you'd like to come and talk to us, we'd love to talk to you. And finally, there is an App Store lab that happening, as many of you know, on the other side of this building. So if you have any questions, feel free to sign up for that. Or if you just have a quick question, we now have a walk-in table this year that you can just drop by with a question for. Thank you so much for spending your time with us and we hope you enjoy your dinner and the rest of this conference. Good morning, everyone. Wow. Thank you. Welcome to Advanced Dark Mode. I'm Matt Jacobson. I'll be joined later on stage by my colleague, Jeff Nadeau. We're engineers in the Cocoa Frameworks group at Apple. We are super excited to talk to you today about the awesome Dark Mode in Mojave. Now, in the intro session yesterday, you learned all the things you need to get started adapting your app for Dark Mode, like rebuilding on the macOS 10.14 SDK. Making use of dynamic colors instead of static or hardcoded colors. Making correct use of template images and materials. And, most of all, making use of the new features in Xcode 10 to define custom color and image assets specifically for Dark Mode. Now, if you need a review on any of those topics, I highly recommend going back and watching the intro session on video later. Now, most UI will look great in Dark Mode using just those techniques. In fact, some of our system apps required no other changes. It was great. But we know some cases will require a little bit more work and that's what we're going to get into in this session. We're going to cover six main areas today. First, the appearance system, how it works, and how you can make use of it in your custom views. Second, materials, what they are, and how you can best make use of them in your UI. Then I'll hand it over to Jeff and he'll talk about vibrant blending, which is an awesome way to make your views look great. As well as reacting correctly to selection using something called background style. Finally, he'll wrap it up with some discussion on how to back deploy your app to older versions of macOS while still supporting Dark Mode, as well as some general tips and tricks for polishing your apps for Dark Mode. All right. Let's get started. So, in Mojave, your app will need to look great in light and dark. And the way you'll do that is using something called NSAppearance. NSAppearance is the theme system used throughout Cocoa and the key part about it is you only have to maintain a single view hierarchy and NSAppearance will help it look great in light and dark. Now, in addition to being at the core of Dark Mode, we've already been using NSAppearance for several years and it's underlied [phonetic] such features as the high contrast mode of macOS as well as the touch bar UI designed specifically for that awesome piece of hardware. Now, previously we've had one main appearance, one main light appearance for aqua windows and we called aqua. And, of course, in 10.14, we're introducing a second appearance for aqua windows for Dark Mode called darkAqua. These objects contain all the assets that views draw with. So, any time you use system dynamic colors or standard effects or named images or even just standard Cocoa controls, this is where all that stuff is coming from. And AppKit will automatically provide appearances for all of you views and windows based on the user's light/dark preference in system preferences once you link on the macOS 10.14 SDK. So, here's our beautiful Chameleon Wrangler app that Rachel and Taylor created in the intro session and you can see once we linked it on the macOS 10.14 SDK, AppKit went ahead and automatically gave it the darkAqua appearance. Now, that's great, but what if we want to change the appearance? For example, what if we wanted to change the appearance of this notes view? We might think that in the dark appearance we still might want the notes view to appear light. Well, you can do that using something called NSAppearanceCustomization. Now, this is a protocol, but it's not a protocol you have to go off and adopt in your applications. It's already adopted by NSView and NSWindow and, in Mojave, NSApplication conforms as well. It's a pretty simple protocol. It just adds two properties. First property is appearance and this is where you can override the appearance for a particular object. Now, it's an optional NSAppearance because, if you set it to nil, the object will simply inherit its appearance from its ancestors. There's also effective appearance and this is a read-only property that you can use to find out what appearance a view will draw with. And of course, to use this, you'll have to get the right NSAppearance object and you can do that pretty easily using the NSAppearance named initializer. Just pass aqua or darkAqua based on which appearance you want and then you can go ahead and just assign that to the appearance property of the object that you'd like to customize. So, in this case, we'll assign the aqua appearance to the appearance property of the text view and now it uses the light appearance. All right. That was pretty easy, so let's take a look at another case. You might have a window that kind of hangs off of a particular view. And you probably want its appearance to match the view it hangs off of. Now, we could just assign the aqua appearance to this window just like we did to the view, but what we really want is something a little stronger. We want its appearance to inherit from the view and we can do that-- first of all, AppKit will automatically do this for us for a number of common windows, like menus, popovers, tool tips, and sheets, so you don't have to worry about it in those cases. But, for custom cases like this, there's new API in Mojave that you can use to do this. It's called Appearance Source. Now, this is a property that takes any object that conforms to that NSAppearanceCustomization protocol-- so, views and windows-- and you just assign it to the appearanceSource property and the window will inherit its appearance from that object. So, in this case, we'll assign the text view to the appearanceSource property of that child window and now it's appearance will always inherit from that view no matter what it is. In fact, you should think of the appearance system as a sort of hierarchy. Similar to the view hierarchy you're probably familiar with, but extending to windows and the application as well. And when we ask AppKit for a view's effective appearance, AppKit will simply walk up this hierarchy until it finds an object with a specified appearance and that's the appearance we'll use. OK. So, now that we know how objects get an appearance and how the appearance system works, let's talk about how you can use it in your custom views and controls. Here's an example. Let's say I wanted this custom header view here to use a different color in light and dark appearance. Now, we already know in Xcode 10 I can go into the asset catalog editor and specify specific color assets for light and dark. But then how do I use that in my custom view? Well, here's one way that seems tempting but won't work and I'll show you why. First, we'll add an NSColor property to our view. And in init, we'll use that color to populate our layer. And if the color changes, we'll go ahead and update our layer there too. Let's try that out. OK. It looks pretty good in light, but if we switch to dark, we can see our color didn't actually change. And that's because even though our NSColor is dynamic, the CG color that we get from it is static. It won't change for the appearance. And, since we configured our layer in our initializer, we didn't get a chance to run any code when the appearance changed. Now, the key takeaway from this is you need to do your appearance sensitive work in specific areas. Specifically, the update constraints, layout, draw, and update layer methods of NSView. Now, AppKit will automatically call these methods as needed when the appearance changes. And if you need to trigger them manually, of course you can always use the needsUpdateConstraints, needsLayout, and needsDisplayProperties and AppKit will automatically call them. So, let's go back to our example. Instead of overriding init, we'll implement updateLayer and there we can go ahead and safely populate our layer by asking our NSColor for a CG color. And if our color changes, instead of updating our layer right there, we'll just set the needsDisplay property to true. AppKit will come back around automatically and call updateLayer. So, let's run it again. Still looks good in light. And now it uses the correct color in dark just like we wanted, so that's great. Now, what if we want to do something a little more complicated that might not be expressible just with dynamic colors or images? For example, maybe I would like to add this nice white glow behind Chloe's beautiful face here, but only in Dark Mode. How would I do that? Well, for cases like that, we have new API in Mojave that you can use to match against your view's appearance. Let me show you how it works. So, in this view, I'll override the layout method and I'll switch on effectiveAppearance bestMatch(from:, I'll pass an array with all of the appearance names that my view happens to know about. In this case, aqua and darkAqua. Then it's just a matter of implementing behavior for each of those appearances. So, for the aqua appearance, I'll simply use my imageView with Chloe's face as a subview. And for darkAqua, I'll not only use that imageView, but I'll also through my glowView behind it. Finally, I'll implement a default case and this is for appearances my view doesn't know about and that includes potential appearances Apple might come out with in the future. OK. Let's take a look at what it looks like. So, there it is in light, no glow, that's what we wanted. Switch to dark and we have that glow. That's great. All right. Let's talk for a minute about high contrast. So, I said before that we've been using NSAppearance for the high contrast mode of macOS. And one of the nice side effects of doing all this work to support Dark Mode is it makes it really easy to support high contrast really well as well. As a reminder, high contrast is enabled through the increase contrast checkbox in system preferences. And in this mode, colors are changed so that control bounds and other kinds of boundaries are more easy to see. Now, in this mode, AppKit automatically replaces the aqua and darkAqua appearances with high contrast counterparts. Now, these high contrast appearances inherit from their normal contrast versions. So, what that means is any code you've written to take advantage of Dark Mode will automatically apply in high contrast Dark Mode. But you can go even further. In Xcode 10, if you check this high contrast checkbox in the asset catalog editor, it'll allow you to specify color and image assets specifically for the high contrast versions of the appearances. Now, you can also use those appearance names in code. You might be temped to think, well, great, I'll just pass them to NSAppearance themed and I'll get the NSAppearance object and I'll do something with that, but that won't work. Those appearances are only available through system preferences. But what you can do is pass them to bestMatch(from:) just like we did before for Dark Mode to implement custom programmatic behavior. OK. Let's talk for a minute about sublayers. I know a lot of you out there have views that manage their own sublayers and there are important things to be aware of for Dark Mode. Primarily, you need to know that custom sublayers will not inherit your view's appearance automatically. Now, the easiest fix for this is to switch them from being sublayers to subviews. If you do that, AppKit will automatically handle the appearance inheritance for those views, just like any other view. Otherwise, you'll have to manage those layers manually using a couple techniques that I'll talk about now, viewDidChange EffectiveAppearance and the concept of the current appearance. So, first viewDidChange EffectiveAppearance. This is a new method on NSView that you can override to find out when your view's effective appearance changes. Now, this is a good time to perform any custom invalidation you might need to do or drop any caches that are no longer relevant. But remember you don't need to invalidate the view itself here, AppKit will do that for you automatically. Second, the concept of the current appearance. Now, this is a thread local variable that you can access through a class property on NSAppearance. If you're familiar with concepts like the current NSGraphics context or the current NSProgress, you already know what I'm talking about. If not, just remember that this is the appearance used to resolve dynamic colors and images. AppKit will set up the current appearance automatically for you before we call any of those special NSView methods we talked about before, like updateConstraints, layout, draw, and updateLayer, but you can also set it up yourself where necessary and let's take a look at an example why you might do that. So, here's a custom that maintains some sublayers. I'll override this new viewDidChange EffectiveAppearance method and I'll set my sublayer needsDisplay. Now, if I didn't do this, my sublayer wouldn't update when my view's effective appearance changed. It would just stay the same. And then in my layer delegate routine, I'll save off the current appearance for later and then I'll go ahead and set the current appearance to my view's effective appearance. Then I can go ahead and update my layer. Now, if I hadn't set the current appearance before this, this code wouldn't be using my view's appearance and so it would end up looking wrong. Finally, when I'm done, I'll just restore the old current appearance. Here's another thing to be aware of if you're managing layers. You might have code that looks like one of these two examples. Either you're setting the contents of a layer to an NSImage or you're using the layer contents for content scale API to create layer contents from an image for your layer. If you have code like this, you should know that the image will not automatically inherit the appearance. As before, the best fix is to switch to views. In this case, NSImageView. NSImageView will take care of this detail as well as a bunch of others automatically, so do that if you can. Otherwise, you'll need to create a CGImage from your NSImage for your layer. And you'll do that using the cgImage(forProposedRect:, context:, hints: API on NSImage. And you'll have to be careful to do this at a point where the current appearance is correct. So, a good place to do it is in your updateLayer method. All right, so that's appearance. Now let's talk about materials. Now, you've probably heard that materials are one of the building blocks of the modern Mac UI, but you may have wondered to yourself, well, what exactly is a material, so let's start with a definition. Materials are dynamic backgrounds that make use of effects like blurs, gradient, tinting, translucency, and they provide a sense of depth or context to your UI, as well as just a bit of added beauty. Here's a pretty typical Mac desktop and you can see all the different places where we're using these material effects-- actually, this isn't even all of them. Now, AppKit automatically provides materials in a number of common places, like the title bars and backgrounds of windows, table views, sidebars, popovers, menus, and even other places as well. But you can create a material yourself and add it to your UI using a view called NSVisualEffectView. If you're not familiar with NSVisualEffectView, quite simply, it's a view that shows a material. And if you want to use one, you'll need to be aware of three main properties that you'll have to set up and I'll go through these in order. The state, blendingMode, and material properties. So, first, the state property. This controls whether the material uses the active window look. Now, by default, the material will just match its containing window and so when that window is active it'll look active. When the window's inactive, the material will look inactive. But you can also specify this specifically to be active or inactive if you'd like to control it manually. Second, the blendingMode property. This property controls whether the material punches through the back of the window. Let me show you what I mean by that. Here's a preview using two different materials. For one, this title bar material, if we peel it back, we can see that it's blending the contents within the window, including that color image there. So, it's not punching through the back of the window. There's also the sidebar material and if we peel it back we can see it's blurring the contents behind the window, so it's punching through the back so it can see to the windows behind it as well as the desktop. So, by default, a visual effect view will be in behind window mode, but you can control that using the blendingMode property. Finally, the material property. This property encapsulates the material effect definition. What do I mean by that? That means the exact recipe of blur, translucency, gradience, tinting-- that all depends on the material property. Now, when we first started using materials in Yosemite, we had two main materials, the light and dark materials, and those served us really well at the time, but since then we've really expanded our use of materials across the system. And now with Dark Mode, it no longer really makes sense to specify a material just as light or dark. Instead, we have something called semantic materials. Now, if you're familiar with semantic colors, you know that they're named after where they're used, not necessarily what they look like. Same thing for semantic materials. The menu material, for example, will always look like system contextual menus, regardless of light versus dark. And in Mojave, we're introducing a bunch more semantic materials so that you can always use the right one for your specific use case. In fact, these semantic materials are now our preferred way of using materials and we're deprecating these non-semantic materials like light, dark, medium light, and ultra dark. If you're using one of these materials, now is a great time to go ahead and switch over to a semantic material that's right for your use case. Just to give you an idea of where we're using these semantic materials across the system, here's the Finder using the title bar and sidebar materials. Here's Mail using the header view and content background materials. Here's our Chameleon Wrangler app using the underPageBackground material. And here's system preferences using the window background material. Now, of course this window background material, as you've probably heard, is one of these special desktop-tinted materials new in Mojave. And the way these work is they pick up a slight tint from the desktop picture based on the window's location onscreen. And the idea here is to help your window blend in with the windows on the rest of the system. Again, the easiest way to get one of these desktop-tinted materials is to use the automatic support in NSWindow, NSScrollView, NSTableView, and NSCollectionView. The default configurations of these objects will come with this desktop-tinted effect. You can also configure NSBox to get these materials by setting its type to custom and selecting one of these fill colors. It'll use the corresponding NSVisualEffectView material. Here's an example. I'll set my box's type to custom and then I'll set its fillColor to the underPageBackgroundColor. Of course, I can also use NSVisualEffectView, I can set it's material property to the underPageBackground material. Now, the advantage of using NSBox is it's back-deployable actually all the way back to Leopard. VisualEffectView, on the other hand, gives you a little more flexibility and I'll give you an example of that later. So, just as a reminder, these materials will show their untinted color in light. And in dark, they'll show that desktop-tinting effect. But remember that the tint effect can be disabled. Let me show you why. So, in Mojave, you can choose an accent color for the system. And if I switch this over to graphite, you'll probably first notice that all the controls lost their colored accents, but those desktop-tinted materials also lost their tint. So, just make sure you're not depending on that tint being there in any way. Now, VisualEffectView by default will show its material in its frame rectangle like this. And that's pretty great, but what if I wanted to show a custom UI element with this material, like say a chat bubble. How would I do that? Well, here's one way that seems tempting, but won't work, and I'll show you why. We'll first implement the draw method on NSView and then I'll go get my custom chat bubble BezierPath. And then I'll fill with the controlBackgroundColor in that path. Now, if you do that, you'll find it looks something like this and it looks pretty good, but if we zoom in closely, you'll see that the bubbles are not getting that desktop-tinting effect that we want. It's just a plain gray. So, what went wrong? Well, this effect is provided by the Quartz window server like a lot of our other material effects. And what this means is it updates asynchronously from your application and this is great for performance, but it also means that you can't directly draw with that color or get it's RGB values. Instead, you can use the maskImage property of VisualEffectView to do something very similar. maskImage is an optional NSImage on VisualEffectView that VisualEffectView will use to mask its material, the material that it shows. And in addition to using standard art-based images, you can use drawing handler images to simulate drawing with the material. Let me show you an example. So, I'll go back to my view, I'll override layout, and I'll go ahead and add a VisualEffectView. I'll set its material to the contentBackground material, and then I'll create a drawing handler image using the NSImage size flipped initializer that takes a block. In it, I'll set the white color-- this color doesn't really matter as long as its opaque. And then I'll go ahead and fill with my path. Then I'll set that image ask the maskImage on my VisualEffectView. All right. Let's look at it now. Looks a lot better. It's desktop-tinted. And, if we look side by side, we can really see the difference. So, this technique works with any material, but just remember that only the alpha channel of the image is used for the mask. This is similar to template images. And the mask only masks the material, not any subviews or other descendent views of the VisualEffectView. A common technique is to provide a resizable image for this-- for the maskImage using the capInsets and resizingMode properties of NSImage. And this is really good for performance. OK. With that, I'll hand it off to Jeff, who is going to talk about vibrant blending. Jeff. All right. Thank you, Matt. So, now that we've had a look at our great materials, I want to cover the things that we draw in front of those materials, particularly the materials that we use that pull in part of the background and provide that really awesome blur effect. So, if we revisit our Chameleon Wrangler application, we have this UI here, it's our mood-o-meter. It's where we go to record how our various reptiles are feeling. And it's in a popover, which means that it's automatically getting that awesome popover material backing. And what we want when we're drawing over this backing material is for our content to really stand out on top of that varied background. Something like this. And we do that with an effect that we call vibrancy. So, what is vibrancy? It's a blending mode that we apply to the content that uniformly lightens or darkens the content behind it. It's very similar to a color dodge or burn that you might have seen in your favorite photo editor or design tool. But let's take a closer look. Here we have a glyph that's drawing in about a medium gray, about a 50% gray, but at 100% opacity. And when we apply the vibrant blending effect that we use against dark materials, which we call a lightening effect, we can see that it's not that the opacity has dropped on our glyph, but we're actually lightening the content behind it using the lightness of that gray value. And in fact, when we look at how this works on a range of gray values-- here we have swatches going from 0% to 100% gray, all totally opaque. When we apply our lightening effect, we can see a number of interesting things have happened. Down on the bottom right side, we have 100% light, and because we have added the lightness of white to the content behind, it just remains white. There is nowhere further to go. But on the top left where we were drawing black, there was no lightness to add, which means that it completely disappears. In fact, you wouldn't be able to see it if I didn't have an outline there. And in-between we can see that we have varying degrees of lightening which we can use to establish a hierarchy of content in our application. But where does this effect come from? Well, it's our old friend NSAppearance, it turns out. We have two special vibrant NSAppearance objects, vibrantDark and vibrantLight and these are a complete package. Not only do they include the exact formula that we use for that lightening or darkening effect, but they also have a set of control artwork and color definitions that have been designed to work great with that blend mode. But how does your code use it? Well, it's very simple. In your NSView subclass, you can override the allowsVibrancy property to return true and the blending effect is going to automatically apply to your views drawing and also the drawing of all of its descendants. Typically, when you're drawing in this vibrant context, you want to use one of the built-in label colors, depending on the prominence of your content. Both vibrantDark and vibrantLight have great definitions for all four of these colors that allow you to establish that nice hierarchy. However, you don't have to use these colors. You can use any color that you'd like, but we prefer to use non-grayscale colors. Avoid non-grayscale colors because, if you use them, the blending effect is going to impact the design intent of your color and it's going to wash it out in a way that is not desirable. I'll show you an example of that later. So, revisiting our application, we can go ahead and override allowsVibrancy on our view and in this case we're going to just set it on the view that contains our entire meter in the entire popover. And let's see what that looks like. Well, our slider looks pretty good. It's exactly what we expected. But what happened to the faces? They're all washed out. And what happened here is that when we set allowsVibrancy on the overall meter view, not only are we getting the vibrant blending on that view, but also both of these subviews. And the fix here is pretty simple. If we localize our definition of allowsVibrancy to just the part that's drawing the slider, we get exactly what we expected. Our slider is drawing vibrantly and the colors in our face buttons look exactly the way that we wanted. When you're drawing vibrantly, typically you'd want to apply vibrancy to only the leaf views that are drawing the content that you actually want to have vibrant. And, if you have views that are drawing a mix of content, that means that you probably want to break your drawing out into separate sibling views that you can use to apply vibrancy at the granularity that you want. Further, you should avoid overlapping vibrant and non-vibrant views. If you do this, the blending modes can clash and you might find that some of your content is drawing with a blend mode that it didn't expect. Further, don't subclass Cocoa controls just to override allowsVibrancy. I mentioned earlier that the vibrantLight and vibrantDark appearances have been designed with control artwork and colors that were designed specially for the blend mode and if you remove that blend mode, the contrast on that artwork is not going to be what you expected because we're using the blend mode to provide a lot of that pop against the material, so you should only override allows vibrancy if you're actually overriding drawing and you know what vibrant blend mode or non-vibrant blend mode is appropriate for the drawing that you're doing. That's vibrancy. Next, I want to talk a little bit about background styles, specifically the ones that we use for selections. So, here we have a pretty typical situation in a aqua Cocoa application. In this case, it's a message from the Mail application and we can see that when we have a selection state, we need our content inside of this table row to invert to look good against that blue selection. But when we add darkAqua into the mix, we can see that we can't just naively invert our content anymore. That's not going to work uniformly. And so we need to describe these states semantically. Now, if you're familiar with Cocoa, you've probably seen the NSView.BackgroundStyle enum and that includes a couple of cases, including light and dark, and NSTableView sets this automatically on the TableRowView, TableCellView, and also all of the controls that are immediate subviews of your TableCellView. Now, traditionally, we have set the light background style on unselected rows and the dark background style on selected ones. But, in the face of this brand-new, beautiful theme where the background is effectively always dark, these names don't make sense anymore and so we've renamed them to normal and emphasized, respectively. And these are just more semantic descriptions that better match the way that these enum cases are used in a modern Cocoa application. We also have some additional updates with background styles, including that TableView will now automatically set that background style recursively on all of the controls in your table row, not just the ones that are immediate subviews of your CellView. And so, if you've been catching that background style and trying to forward it along to all these subviews because you wanted to use a stacked view or something for layout, you no longer have to do that on Mojave. That's the applause of somebody who's done this manually. Thank you, I agree. Further, all four of our label colors now automatically adapt to the background style, which means that you can just set up your content hierarchy once, describe it semantically, and it's going to look great in both of these contexts. You can also use these emphasized variants manually and I'll give you an example. So, here we have something that looks a little bit like the icon view in Finder. And we've got two labels that are ascribed with label color and secondary label color. And we want to draw a custom selection behind them, so we've got this custom Bezier path-based selection, maybe we're filling it with alternate selected control color, and we want our labels to match the primary and secondary variants in this emphasized style. And to get that is very simple. All we have to do is set the background style to emphasized on both of our text fields and they're automatically going to provide this nice emphasized variant. And the great thing is that now that we've described it this way, when we switch into Dark Mode, everything just works. We don't have to do anything special to support that. One final note on selections. The selection material that you commonly see in sidebars, menus, and popovers now follows the accent color preference on Mojave. And what that means is that if you're drawing a custom blue selection, it's not going to fit in. Instead, you should use NSVisualEffectView. It has a special selection material just for this and when you use this it's going to automatically follow the preference, as you expect. Now, before I get into the exciting part, the tips and the tricks, I want to say a couple of words about backward deployment because we know that many of you, especially on the Mac, like to deploy your applications back to previous releases of macOS and it was important to us to make sure that you could adopt Dark Mode without necessarily compromising on your backward deployment. And so I'm going to step through a couple of APIs and just examine them for backward deployment, starting with system colors. So, here's a sampling of the system colors that we support that are dynamic for the appearance. And what I want to highlight here is that the ones highlighted in green have been available since at least 10.10 Yosemite, many of them actually far further back. And that means that we think that you have a great vocabulary of colors available to you to describe more or less any UI that you'd like and that all supports backward deployment out of the box. For custom colors, our modern preferred solution for defining them is asset catalogs and these are available back to 10.13. Now, when you do specify dark variants for any of your assets, when you back deploy them, those dark variants are safely ignored on previous versions of the operating system, so that's a-- that's a solution that has backward deployment built right in. But if you want to deploy back further than 10.13, you can use a technique like this where you write a custom color property. And here we just encapsulate the availability check to use our asset catalog color on operating systems that support it and then we can go ahead and put in a hardcoded fallback color for those older operating systems. Desktop-tinted materials is another great new thing in Mojave and if you want to address those materials directly with VisualEffectView, of course that's only available starting in 10.14, but we've been providing-- but we have classes that are providing these materials automatically, including Window, ScrollView, and TableView, which have been available since essentially the beginning of time. In fact, some of these predate macOS 10.0. And so, if you configure them correctly, they're going to on previous operating systems show that special NSColor which looks exactly the way that you would expect in previous versions and then when you run it on Mojave you're going to get that material automatically. And of course NSBox, the custom style that allows you to set a fill color, deploys back to Leopard 10.5 and so does NSCollectionView. And this works whether you're using the legacy NSCollectionView API or the modern one, although we'd prefer that you use the modern one. Finally, enabling Dark Mode is generally gated on linking against the 10.14 SDK, but, as you can see, really, the tools that you need to develop a great Dark Mode application aren't necessarily specific to the 10.14 SDK and you could have developed one just using the 10.13 SDK that you have today. And so, if you have a situation where you can't necessarily update your SDK, we have an Info.plist key that you can use to opt-in to Dark Mode. It's called NSRequiresAquaSystemAppearance and if you set that to NO explicitly, then that's going to enable Dark Mode even if you're linking on an earlier SDK, although we very strongly prefer that you update your SDK. It's a far better solution. You can also set this key to YES to disable it temporarily-- and I want to emphasize temporarily. This is a key that you can use to give yourself time to really build a great polished update for supporting Dark Mode. Finally, some tips and tricks. First of all, when you're updating your application, one of the greatest things that you can do is just audit your use of NSColor just by searching through your code base and seeing where you're using it. And you're going to find a couple situations that you can use to upgrade to make your Dark Mode experience a lot better. And so, for example here, we can find places where we're using named colors that are not dynamic and also colors that have hardcoded components. And when we encounter these kinds of situations, we can look at these and decide one of two things. One, maybe there is a built-in system color that describes what I'm going for and is fully dynamic for the appearance. Or, two, this is a custom color that I think is really important to be specific to my application. And so the first case is pretty straightforward. We were using black color for this label and we can just switch that to labelColor and that's going to be fully dynamic and do what we expect. But in the second case, we might decide that this color is actually really special to our app and that's a really great candidate for moving into the asset catalog. Not only does this clean up our code because we get all of these magic numbers out of our code and into a data-driven source, but we can also then set a dark variant for that color and so we get great Dark Mode support built in. Another common source of issues is offscreen drawing. To do offscreen drawing, you have to make sure that you're being sensitive to the appearance and also other drawing conditions. One really common case of this is using the NSImage lockFocus API to try and draw custom NSImages. In this case, we're going to go ahead and try and draw this badged image where we have a base image and we're applying a badge because something new is happening with our lizard. And, in this case, we're creating an NSImage, calling lockFocus on it, and then doing our drawing. And the problem with this is that once we've used lockFocus, we lose a lot of the semantics. We just have a single bitmap representation. And so if the appearance changes or if many other conditions change, including say the backing scale factor because you've moved your window from a Retina display to a non-Retina display, suddenly this drawing is going to be stale. So, a better solution is to use the block-based image-- image initializer, NSImage size flipped drawing handler. And we can just do the exact same drawing that we were doing before, but inside of this block. And when you assign this kind of image to an NSImageView, you're automatically going to have this block rerun when the appearance changes, scale factor changes, color gamut changes-- anything changes, essentially. And so that's great news because if our, say, badge fill color is a dynamic color, it's going to always resolve against the correct appearance. There are a couple of other ways that you might be doing offscreen drawing. You might be making custom bitmap graphics contexts using NSGraphicsContext or CGBitmapContext. And, depending on what you're doing, these might also be great candidates for replacing with a block-based NSImage. Further, if you're using the NSView cacheDisplay in Rect method to cache your image to a bitmap rep, just be aware that this method is not going to capture some of our more advanced rendering techniques like materials and blurs and it's also just another way that you can produce drawing that goes stale when the appearance changes, so be aware of that. Here's another situation that you might find yourself running into. If you have an NSAttributedString or NSTextStorage and you're manipulating those attributes manually-- say I am in this case, I've just set my attributes to just be a dictionary with a font in it-- you might find that this happens. Your text is drawing black even when you switch into Dark Mode and what has happened here? Well, we're missing a foreground color attribute and when the text drawing engine encounters a range of attributed strings that doesn't have a foreground attribute, it defaults to black. And this is what it has always defaulted to and it's going to continue to be the default for compatibility. So, one way to fix this is to set a foreground color explicitly to one of our dynamic system colors, and that's going to do what you expect. But a better alternative is that if you're doing manual attributed string drawing, you should switch to a Cocoa control, like an NSTextField, which does this for you automatically, or, if you're manipulating the storage of a textView, we have new API called performValidatedReplacement on textView that does a nice thing for you. If you go ahead and replace a string with an attributed string in your textView, it will fill in any missing attributes with the typing attributes from the textView, so that way you can go ahead and specify your new attributed string without having to manually merge all your attributes together. Here's something else that we've encountered in a couple places, which is appearances that are set in Interface Builder. So, if you're going ahead and building and debugging your application and you find that there's some part of your app that just isn't switching, you might have this in your Interface Builder. A hardcoded aqua appearance. And it's easy to miss, because before today, essentially, you were always running under aqua, so you didn't notice it. And the fix for this is easy. If you set this back to the Inherited option in the pop-up menu, your view's going to automatically inherit from its ancestor. An extra special case of this is NSVisualEffectView. It's very likely that if you have a VisualEffectView in Interface Builder or even in code, you're setting one of the two vibrant appearances on it and the great news is that in macOS 10.14 this is no longer necessary. NSVisualEffectView will automatically pick the right vibrant appearance based on the appearance it's inheriting. So, if it inherits darkAqua, it's going to choose vibrantDark and if it inherits aqua, it'll choose vibrantLight. And so the fix for this is easy. In Interface builder, you can set this to inherited and then in code you can set the appearance to nil or just delete your override. Interface-- speaking of Interface Builder, Interface Builder is a great tool for designing and previewing your views visually. And so, for example, here I have a view that is actually a custom view using IB designable. So, I'm rendering a gradient here and I can see it right here in the canvas. And, by default, my canvas is previewing my custom designable view using the canvas's appearance, in this case dark. But down at the bottom, we have a new toggle that lets you go ahead and set it to the light appearance so that you can preview the way that your view looks in either appearance. And thanks to Interface Builder's great support for asset catalog colors, we can actually use our custom asset catalog colors, which have dark and light variants, and we can preview them in the canvas live. And if you see there's a little arrow button built into that pop-up button and you can use that to follow it and go straight to the definition in your asset catalog, so you can see live as you're changing it. And you can do this all without even building and recompiling. When you do build and run, you're going to see a new item in your Debug Bar and it produces a menu that allows you to choose the appearance for your application. And this is really handy for previewing your app in various appearances without having to go and reconfigure your entire system. Not only can you choose light and dark, but you can also choose the high contrast variants and test those as well. And, if you have a Touch Bar Mac, this appears in the expanded Debug Bar as well, so you can do this without even leaving your app to go back to Xcode. Finally, I want to talk about one last tool in Xcode that is really great for debugging your Dark Mode applications. So, here we have our app and really things are looking pretty good. There's nothing out of place, but I find that when I scroll and rubber band a bit, oh, I'm revealing something that I didn't expect. There's a light background hiding back there somewhere, but it's hard to see without doing that little scroll gesture. And this is a great case for using the View Debugger. Using the View Debugger's expanded 3D view, the view that's drawing unexpectedly is really easy to spot. And in this case, we can see that although our collection view was drawing the background that we expected, the scroll view behind it still has a light background for some reason. And when we select it, we can use the Inspector to see how it's being configured. And in this case, we can verify that, yeah, it's just drawing a hardcoded white color and that's a really easy fix. The View Debugger has made a number of enhancements in Xcode 10 that are great for debugging Dark Mode applications, including colors. They can now show you the names of colors, both dynamic system colors and your asset catalog colors, so you can identify where these RGB components are coming from, and it'll show you the configuration of your view for NSAppearance, including the appearance that it's going to draw with as well as whether there's any local overrides of NSAppearance on that object. So, we have covered an awful lot of content and so let's rewind and make sure that we remember it all. We started off with NSAppearance and leveraging it effectively to draw your custom views that adapt based on the theme. Then we learned how to add depth and beauty to our UI using our new and updated palette of NSVisualEffectView materials. We talked about drawing in a couple of interesting contexts, both vibrancy and selections, and then we walked through some of the great ways that Xcode can help you design and debug your Dark Mode applications. As always, you can go to developer.apple.com to re-watch the video for this talk and see any related resources and we have labs today. We have a special Cocoa and Dark Mode Lab, it's at 2:00, and not only will we have Cocoa engineers onsite to help you with your code, but we'll also have human interface designers onsite to help you with your design questions as well. So, go get lunch, think about Dark Mode the entire time, and then come see us. And then, finally, we have an additional Cocoa Lab on Friday at 11:00 as well. All right. Thank you very much.  Good afternoon, everyone. Good af-- alright. I hope you've been enjoying WWDC so far. I know it's been an exciting week filled with announcements, features, updates, developer tools. My name is Shloka Kini, and I work in developer publications, which means-- to borrow a phrase from Cardi B., I don't just code now, I write docs too. Specifically, the docs that'll help you to write amazing applications. Today, I'm privileged to call out some great features in Safari and WebKit. So, if you develop websites, and want to make use of the latest web technologies, and the latest versions of Safari, this talk is for you. And, if you're a native app developer that uses web views, or extensions developers, this talk is for you, too. And, even if you're not in any of these categories, you should still stick around, because the latest version of Safari has some great features that will improve your browsing experience. Now, there've been a lot-- a lot-- of new improvements since we last had a What's New talk. But, today I'm going to highlight a few that can really help you get secure, performant apps, and use the latest web technologies for a rich experience. And, many of them you can get for free. So, let's kick things off with security. And, a few announcements. WKWebView. Now, I know what you're thinking. WKWebView has been around since 2014, so it's not technically new. However, it's worth mentioning again because we are now officially deprecating UIWebView. So, if you're starting a new app, or a new project, and would like to display web content that's not easily put into a native view, use the WKWebView. And, even if you've used UIWebView in the past, switching might be easy for you. It can definitely save you time in development, if you're developing apps for both macOS and iOS, because WKWebView works on both platforms. Unlike UIWebView for iOS, and WebView for macOS. So, you can share a lot of code between the two versions. WKWebView also runs in a completely separate process from the rest of your app. So, unlike UIWebView, even if your web process is compromised, it won't compromise your app. If your web view has complex content, you can't accidentally cause your app to stall. And, even if WKWebView crashes, it's confined to the web view, not the app. WKWebView can provide security benefits while keeping your apps performant and reliable. So, whether it's hard or easy, the benefits you get using WKWebView are worth the switch. The next announcement involves extensions, but extending Safari-- I mean, it's evolved a lot over the years. So, let me start with a quick recap of the history of Safari extensions. Now, in 2010, before we had a platform concept of app extensibility, we had legacy Safari extensions. Now, these were the Safari EXTZ files you could build in Safari Extensions Builder. They could be distributed through the Safari Extensions Gallery, or in some unusual cases, by developers directly. These legacy extensions were incredibly powerful, because they had access to all your browsing data, which made them popular, especially for fraud and malware. We needed to create a safeguard, so that's why we didn't just leave it at these Legacy Safari Extensions. So, the next milestone in our story came in 2014, when we introduced app extensibility for macOS and iOS. App extensions, though, are a way to extend apps, not Safari. However, this move greatly changed how we thought about extensions in Apple platforms. Here, you could clearly extend the system while users are interacting with other apps. And, like apps, they could be built in Xcode. Because of this better extensions model, we wanted to apply some of these concepts back to those legacy Safari extensions, and at the time, the most popular ones were adblockers. So, we introduced content blockers in 2015. Content blockers were a type of app extension built in Xcode, that worked on both macOS and iOS. They have a special architecture that makes them fast. So, any content blocker is faster at blocking than any legacy Safari extension. They don't have the power to slow down browsing, and they're private, because the extensions never see what web pages your users visit. And, by this point, the app extension model offered so many performance benefits we thought, maybe we can bring all these concepts back to the legacy Safari extensions, so we can get the best of both worlds. An extension that extends Safari's functionality, but also extends your app to talk to Safari. So, in 2016, the modern Safari app extensions for macOS were introduced. A way to extend Safari that could be built in Xcode. And, unlike previous extensions, you get them through the App Store, which means they can be free, or you can charge for them. Either way, you don't have to do your own billing. So, compared to those legacy extensions in 2010, content blockers and Safari app extensions have great benefits. So, the best thing for you to do, is if you have a legacy Safari extension, switch over to a Safari app extension. And, if it happens to be an ad blocker, use content blockers. And now that we've done all this work, what can we do about the use of legacy Safari extensions for fraud? Starting with Safari 12, we're officially blocking support for legacy extensions distributed outside of the Safari Extensions Gallery. Legacy extensions will still work in Safari 12 as long as they're in the Gallery. The only exception are those extensions using the deprecated Can Load API, which we turn off by default. We'll continue to accept submissions to the Gallery until the end of 2018. However, we will be coming up with more updates in the following year, and will eventually transition entirely to Safari app extensions. So, the best thing for you to do is learn how to develop extensions in these two models. And, to learn how to do that, check out the docs, courtesy of yours truly and Developer Publications. Now that we've covered the two biggest announcements for native developers using WebViews, and extensions developers, the remainder of these features are primarily going to be about web development. So, let's start with subresource integrity. Now, as a developer, you may serve your content over an HTTPS connection to your user. And, that resulting content may also include content distributed over a third-party server, like a content delivery network. Now, both connections may be secure, both may use HTTPS, which means you maintain the confidentiality, authentication, and integrity of the data transferred. But, what happens if that third party itself is compromised? It could happen. And, in this case, while HTTPS secures the connection, it doesn't secure against a compromised server. It can modify the scripts, and styles you serve to users if that third-party server is compromised. Subresource integrity ensures that you don't serve compromised scripts to your users. So, how does it work? Well, with hashing. First, you add the integrity property for a script or link element in your markup. The value for this property is a hash that you create using a secure hash algorithm. When a user fetches the files, then another hash is calculated. The two are compared, and if they don't match, your script will fail to execute. This process ensures that scripts won't execute if they're compromised. Unless they match what you intended, your scripts will not execute. And, to make sure you don't lose functionality, you can also provide a fallback to reload a resource from your server, in case the third-party script fails to execute. Now, keeping compromised resources from executing keeps users secure. And, intelligent tracking prevention can keep the browsing experience private. Now, I'm sure you heard about intelligent tracking prevention in the Keynote. It's a Safari feature that reduces cross-site tracking by limiting cookies and website data for domains with tracking abilities. And, in previous versions, cookies were kept according to two rules. One, cookies could be used in third-party context for 24 hours after user interaction in a first-party context. And two, for 30 days of Safari use, including that initial 24 hours, those cookies would be kept in isolated storage before being purged. But, now we're tightening the belt a little. And, we're removing the 24-hour general cookie access window for domains with cross-site tracking. But, by default, all cookies are kept in isolated storage, and as developers, I know authenticated embeds are already important to many of your workflows and interactions with web content. So, how do you allow authenticated embeds? Using the Storage Access API. With the Storage Access API, every time a domain with cross-site tracking would like to access cookie in a third-party context, you'll need to request storage access. If the user has not granted access previously, a prompt appears, asking the user whether to permit cookie access or not under this website. By enabling users to provide explicit consent for cookie access, we're empowering them to take control of their cookies, and what websites can track, keeping their browsing experience more private if they choose. Now, next, we'll move on to authentication with automatic strong passwords. Now, I'm sure you saw this in the State of the Union and session earlier this week. Automatic strong passwords is a great way to guarantee that users will always select and save a password that's strong when signing up for a new account. And, this is good for everyone. I mean, I like to think of myself as someone who chooses strong passwords, but give it a little bit of time, and I'll realize that password wasn't as strong as I thought. And, I probably used it in a couple places. For most developers, you won't need to do anything to get this feature, because heuristics will determine if you're on a sign up or login page. But, to guarantee this works, regardless of login flow, add the AutoComplete attribute to the appropriate input fields. Now, the strong passwords we choose are by default 20 characters in length, including upper case, lower case letters, digits and hyphens. Now, while this was designed to be compatible with most services, we acknowledge that sometimes your passwords need to have specific requirements to be compatible with the back-end system. For this reason, there is a passwordRules attribute that you can add to your text elements to specify those requirements. And, on the developers site, there's a password validation tool, to help you test compatibility with automatic strong passwords, and develop your own password rules. Another feature mentioned in the State of the Union, security code AutoFill. Another feature most of you will get for free. This is one I'm going to be making good use for, because I find it tedious to switch between my app and the website, and the messages, and then find out those numbers for the code, and input it and try to remember it. So, having Safari figure out when I have to input a security code, and then suggesting it in the quick type bar? Makes this much more convenient. And, just like before, you get this feature for free, because it uses heuristics, but to ensure that these heuristics work, and you get that quick type suggestion, mark your input fields with the one-time code value in the AutoComplete attribute. For more details, I encourage you to check out the Automatic Strong Passwords and Security Code AutoFill session online. So, that's security. Switch over to WKWebView, more over to content blockers in Safari app extensions, subresource integrity is a failsafe to ensure you don't serve compromised scripts to users, and intelligent tracking prevention improves privacy with the Storage Access API. And, with automatic strong passwords, and security code AutoFill, you get features that are secure and convenient for your users. Whew. You all still with me so far? OK, moving right along, let's talk about performance features, starting with font collections. Now, for those of you who may not have caught it at the top of this talk, my name is Shloka Kini. And that ain't no Anglo-Saxon name. And so, here's my first and last names, using the Devanagari script in the Hindi language. Multiple fonts, different weights and styles, but the same character set. New this year, we support font collections, WOFF 2 and TrueType collections. Bundling related fonts together inside a single collection file can eliminate duplicated tables for character maps. For example, one of our built-in fonts, PingFang has an 84% reduction of file size from using a collection. Font collections can substantially reduce the size of your font files, because the fonts share a table for the same character set. Now, this next feature, font-display, requires no change for most developers. Essentially if you have web content that uses custom fonts, if they don't display for your user for whatever reason, by default we leave a blank placeholder for the text for up to three seconds, before your font displays, to maintain the position of all the content on the screen. But, if this default behavior isn't quite right for you, and you want to have more control over what happens instead of those three seconds, you can use the font-display descriptor. Using different values, you can specify another font as a fallback, or check if the browser has that font in the cache. Now, one cool trick you can use to improve the performance of animated images is using video. Now, I love the colored dust explosion background on my Mac. It's really great, but it's static. I mean I want this thing to bam, pop! I mean, I want motion. I want a GIF. But, animated GIFs take much longer to load, they use more battery power, and give lower performance than a video file showing the exact same thing. Now, in Safari, MP4 video files are supported in image elements, making use of Apple's built-in support for hardware video decoding. My content loads faster, uses less battery, gets better performance, but I can also use MP4s in the CSS background image property. Now, if you adopt this technique, in the simplest way, you could come up with a version that isn't compatible with older browsers. Older browsers don't support MP4s and image elements. Luckily, using existing technology, you can specify a fallback image to display if the MP4 doesn't work. Now, listen up, because now we're going to move on to event listeners. Yes? No. Another feature that has some great defaults, and some customizability in special cases. When any user tries to navigate a web page with a touch screen, they're going to need to scroll. And, for every touch to scroll, a touch event listener can fire, which can interrupt scrolling and cause it to jump a little. Take a look at these two examples. Now, the one on the left is interrupted much more than the one on the right. I mean, it's barely moving. So, what's the one on the right doing right? It's using passive event listeners. By default, we enable passive event listeners on the document, window, and body elements, so any touch events on these elements indicate to the browser to continue scrolling, and not be interrupted waiting for the event listeners to finish. If there are additional elements with event listeners that you want to make passive, you can set the passive property to "true" for those event listeners. Essentially, without preventing default event handling, this flag tells the browser not to wait for event listeners to finish, and lets your users continue scrolling smoothly. Next, we move on to asynchronous calls, with async image decoding. Now, typically, images are decoded synchronously. So, the main thread is blocked. All the images are decoded, and then they display. By blocking the main thread, this blocks user interactions. But, with asynchronous decoding, the operations happen in parallel, and on a separate thread, which means the interactions aren't blocked. And now, new this year, async image decoding happens by default on the first page load, which can cover most cases for web content. However we know that some of you may have special cases. Say, you have a tiled map on your webpage that loads after the initial page load. And, if it has lots of images, some of the tiles may be delayed in their display. Or, maybe you have a carousel of images in your app that you want to fade into each other, but when you try to advance the slides, if the images are decoded synchronously, they might not be ready for display. And, they abruptly switch. But, on the right, asynchronous decoding gives you a smoother fade. Now, if you want to fall into one of these special dynamic cases, you have two options. One, you can add the decoding async attribute to your images elements in markup. Or, you can use the JavaScript API's HTMLImageElements.decode method, which returns a promise, making sure that you know when an image can be added to the dom without causing a decoding delay on the next frame. And, continuing with asynchronous calls is support for the Beacon API. We know, as developers, you want to send data on an unload event. Perhaps to track outgoing links. And normally, asynchronous requests on unload are ignored, so you're stuck using a synchronous request, which can stall the next page load. However, we now support the Beacon API. So, as long as Safari is running, you can send your data to the server and forget about it, with the guarantee that it will be delivered. But, you've heard me talk enough. I mean, I'm sure you want to see some of these security and performance features in action. So, I'd like to call Jason onto the stage to show you how they all work. Jason? Hi, everyone. My name is Jason Sandmeyer, and I'm a developer on Apple.com. In my free time, I enjoy doing arts and crafts, like building birdhouses, and I recently started this blog to share some of my projects and inspire others. I spent a lot of time picking just the right fonts, the right colors. I'm pretty proud of it. But, you know what, I don't just pride myself on good design, I also pride myself on providing a good, secure, and performant experience for my users. So, I'm really excited about all these new performance and security features in WebKit and Safari, and I really want to take advantage of them on my own site. I'd love to show you how easy that can be. So, I have my site loaded on my MacBook Pro here. And-- whoa. OK. Dude! Jason, what did you do? Yes, this isn't the elegant blog I was just bragging about, is it? Let's see-- that's the right URL. You know, I think I know what happened here. When I first started this site, my friends, they warned me that the lifestyle blogging industry can be pretty cutthroat. Clearly, this is sabotage. Someone's replaced my style sheet on my content delivery network. But, luckily, I have a backup, fortunately. And, we can use subresource integrity to add a little bit of an extra layer of security, and ensure this doesn't happen again. So, I'll start by adding the new integrity attribute to my link tag. I should also mention this works on scripts, but we're going to make some changes later, so we'll add that later on. So, the value of this attribute is the hashing algorithm that was used to generate the checksum for the file that I expect my users to see. I've already prepared a hash with SHA256. Next, a hyphen, and then a base64 representation of the hash. Now, let's save this, go back to our page, reload. And, we'll see there's no styles. Because the hash for the downloaded file doesn't match the hash in the HTML. So, Safari has blocked it from being loaded. Now, let's connect to my CDN. And, here's my backup on my desktop. Let's drag in my backup to the CDN, replace the compromised file. And now, when we reload, that looks a lot better. Thanks. So, with subresource integrity, I'll be more confident that my visitors will see the styles and scripts that I expect them to get. Now, let's switch gears a little bit and talk about some performance improvements we can make. I found it insightful to know which links are being clicked on my site, and which ones aren't. It helps me make more informed design decisions. So, I have this click handler that reports which links are being clicked to a server that I control that aggregates that data so I can take a look at it later. But, notice this delay when I click on this Woodworking link that goes to a page that showcases other woodworking-related sites on my page. I'm going to click the link now. Took about a half a second to a second, and this is happening because I'm making a synchronous request in the click handler, which blocks Safari from navigating to the next page. Making a synchronous request ensures the browser doesn't cancel the request when navigating to the next page. But, this is waiting for my server to respond, which can take a while. And, the thing is, I don't really care about the response, I just want to make sure that that data hits my server. So, the Beacon API is actually a perfect replacement for this. I'm going to start by checking that the Beacon API is available in the browser by looking for the sendBeacon method on the navigator object. If it's not available, I'll continue doing what I was doing before. Then, we can just use it. Passing in the endpoint I want to hit, along with the data. Let's save that. We'll go back, reload to get the new script. And now, when I click this link you'll see it's nearly instant. I'm going to click the link right now. And, there we go. So, compared to the XML/http request this is even less code, and it's just as reliable. And now, it'll be much faster for my users to navigate around my site. Thanks. So, next I want to take a look at a problem that I've noticed is more apparent on my iPad here. So, I've organized each step for building this birdhouse as a slide in this crossfading carousel. Tapping the right-facing arrow advances this to the next slide. But, you may have noticed that brief moment of a blank white space where the image should be. Let me go through a few more slides. Let's take a look at some of the code for this carousel, and see what's going on. I think this can be a lot smoother. So, here's my carousel class. I want to focus on this method here, setCurrentSlide. This is the method that's called when the button is clicked to transition to the slide at the given index. Because each slide isn't immediately visible on page load, my carousel only loads the next slide's image when the user taps the button to advance to it. The problem that we're seeing is that the transition is happening immediately. It's not waiting for the image to load. And, after the image has loaded, it still needs to be decoded before it's ready to be displayed on the screen. So, what I really want to do is wait until the image has been loaded and decoded, and I'm sure that we can show the image. And, I can use the new decode method on the HTML image element to make this a lot better. So, I have my image-- a reference to my image element here. The decode method will asynchronously decode the image, and return a promise that resolves when the image has been loaded and decoded. So, I'll just pass my transition function in as the callback for the promise. Now, let's switch back to the iPad. And, we'll refresh to get the new script. And now, when I advance, you'll see this is much smoother. No flashing. It's really great. Thank you. Now, let's switch back to the Mac. Now, finally, at the bottom of my page, I have this animated GIF of a bird furnishing its new birdhouse. This image is-- this video's pretty large-- well, it's a GIF. Seven-- it's roughly a little over 7 megabytes. And, honestly the quality isn't that great. But, I happen to have the original H264-encoded MP4, and now I can just use that directly on my page. So, let's go back to my HTML, and find that image. Here it is. So, I can just change the extension to point to the MP4 file. Reload. And, now I'm using the actual video. The quality's a lot better, and this is only about a megabyte. Plus, it's a little bit longer than the animated GIF. And, as Shloka mentioned, this can also be used in the source attribute to provide a fallback image for browsers that don't support this. So, that's just four of the many new security performance features in Safari and WebKit. I hope you'll take advantage of them on your own site, and I think your users will thank you for it. Now, I'd like to welcome Shloka back up on the stage to tell you about even more new and exciting features. Thank you. Thank you, Jason. And, I had no idea it was that brutal in the blogosphere. You stay safe out there. And, thank you so much for that great demo. To recap performance, using font collections can reduce font file sizes. The font-display property lets you have more control over what happens with custom fonts. Using videos in image elements can help with performance instead of GIFs. Passive event listeners can improve scrolling, and using asynchronous calls, both with the Beacon API, and with image decoding keeps the main thread from stalling. Last, we move onto rich experience. Some cool new features that can really improve your users experience. Starting with drag and drop. Now, first, some general improvements to drag and drop, thanks to some API updates, now you can drag and drop entire directories of files to upload them to the server. No compression or zipping required. And, we support reading and writing MIME types for rich HTML, plain text, and URLs to the system pasteboard. And, specifically for iOS, we've made some new updates to the data transfer API, so now you can customize drag and drop with the getData and setData methods. So, for example, if I wanted to drag groceries into my online grocery shopping cart, I can customize the drag and drop behavior. So, dragging an image element will drop the name of that element and its price into my cart. Now, you can specify what happens with drag and drop behavior, which lets you implement richer user interactions. Next, we move into the API section of this talk, starting with the Payment Request API and Apple Pay. So, let's talk about Apple Pay. Apple Pay's not just a way to pay. It's a way to rethink electronic payments entirely. With Apple Pay, vendors won't directly receive credit card information of your customers, which keeps them more secure. Now, we know that many of you have been requesting a way to support Apple Pay using a standard API. And, I'm pleased to tell you, we listened, and with collaborative efforts, Apple Pay now works with the W3C Payment Request API. So, while you have the option to use this API, remember that to get the benefits of Apple Pay for you and your customers, you will need to make a few changes. For example, adding an Apple Pay button to your interface, rather than adding Apple Pay as an extra option in existing checkout flow. And, at the moment there are a few features incorporated in the Payment Request API, like granular error-handling, handling cobranded cards, and phonetic names. Features that only appear in Apple Pay JS. So, if you need those specific features for Apple Pay, use Apple Pay JS. The next API we're supporting is the Service Worker API. And, if your user's network connection isn't ideal, maybe-- I don't know, they have poor connectivity, or they're completely offline, you want to make sure that you handle that situation gracefully. And Service Workers can do that. A Service Worker is registered by a unique origin, and it can cache offline interactions, and intercept requests made by scripts associated with that origin. Now, every page in your domain can share the same Service Worker instance. So, you can have multiple tabs open at the same time, and all those requests will be intercepted by the same script. So, you can keep a persistent store of resources. Service Workers makes your web page, whether its a web app, or whether you're using SF Safari viewController, more resilient to variants in network connectivity. And, the last of the APIs is the Fullscreen API for iPad. Now, you can customize fullscreen controls for the iPad. Any arbitrary element in Safari. And, clicking on that elements will bring up a complete fullscreen experience. Now, for videos we auto-detect the content, and a Cancel button appears. And, after a short delay, if the content is playing, the button will disappear. Now, if you're presenting content that ends up being blocked by this Cancel button, you can use the CSS Environment Variable fullscreen-inset-top to avoid it. You can also have your content hide at the same time as the button, by using the fullscreen-auto-hide-delay environment variable. Last, a couple of cool callouts, starting with AR. Oh, you've heard so much about AR at this conference so far. And now, you can add AR models to your UI with image thumbnails. So, your websites can take advantage of the brand-new AR Quick Look. And, the code's fairly short. You start with an anchor tag, set the del attribute to "AR" and set the HREF link to your USDZ file, then you file format for AR models. You add a single child, either an image or a picture element containing an image of the model. So, the resulting image looks like this. In the top corner of the image, a small icon appears, indicating an AR model is available if you click on the image. It's a great way to add more depth to the content in your websites. And, for more details on Quick Look, you can check out the session online for Integrating Apps and Content with AR Quick Look. And, last, watchOS. You can already view websites on the MacBook, and the iPad, and an even smaller screen with the iPhone, a screen that can fit in your pocket. But, now we're going to downsize one more time. We've brought you websites on watchOS. Now, I'm personally really excited about this one, because I receive recipes from my mom all the time. I cannot cook, and I see them in Messages and emails and now, when I get that recipe I can see it right there on my wrist while I'm following along. Now, if you use responsive design, great. We do all the work for you and your websites are going to look great on watchOS. But, if you would like to further optimize your webpages for Apple Watch, there's a video for Designing Web Content for watchOS in the WWDC app. Excuse me. Designing, yes. And now, I bet Jason's birdhouse blog could really up the ante with some of these great new rich experience features. So, I'd like to call Jason back on the stage to show us how some of them can be used. Jason? Thanks again. So, I've been thinking about ways to make it more fun for my readers to get started with their birdhouse project. Let's switch back to the iPad. So, I have this list of all the supplies that my readers will need to get started. And, I thought it'd be convenient if I could actually provide them with a way to add to their shopping list, the things they might need, and maybe even purchase some of these directly from my site. Plus, I figured I can make a little extra cash in the process. So, I have the ability to drag and drop the supplies from the left onto this shopping list. And, this works great now on my iPad as well. So, let's take a look at some of the code that's used to achieve this. Doesn't actually take a lot of code to do this. So, for each supply, I add a dragStart eventListener, which stores the element's text, using the Data Transfer API. Then, in my drop zone, which is my shopping list area, I have a drop event listener, that retrieves the previously stored text from the Data Transfer API. And, appends that to the shopping list element. Note that you do also need to add a dragOver eventListener, and for the area where you want the element to be dropped, to prevent the default event, and indicate that a drop is allowed on that element. So, with very little code, I was able to create this fun shopping UI that works great on my Mac, and now on my iPad as well. So, now that I can place supplies in my shopping list, I need a way for my users to actually make a purchase. Let's take a look at how we can provide a great Apple Pay experience with the Payment Request API. So, I've already added the necessary HTML and CSS to my site to display an Apple Pay button, but I've hidden it by default. You should only show the Apple Pay button if the user's device is capable of using Apple Pay. so, let's check for that, using the ApplePaySession.canMakePayments method. If Apple Pay is available, we can show the button. Let's add an eventListener to the button. Now, inside this function is where we'll create a new paymentRequest instance to initiate the transaction. If paymentRequest isn't available, we should consider using Apple Pay JS instead. Here's the constructor for the Payment Request API. It accepts three arguments. We'll start by adding the paymentMethod data object. This contains the Apple Pay paymentMethod identifier, along with some options specific to Apple Pay. Following that, are the payment details. This is where we specify details about the transaction, such as my website's name, the total amount, and each line time. I kept things simple, and decided that I'm just going to charge $5 for everything on this list. Finally, the options argument specifies what information I need to collect from my user to complete the transaction. Let's switch back to the iPad and add some supplies to my list. So, now that we've passed all the information in, we actually need to call another method to show the sheet. And, that's the show method on the paymentRequest. And, this method returns a promise that resolves with a payment response when the user authorizes the transaction. So, [inaudible] with Face ID or Touch ID. In here, is where you would process the transaction. And then, finally, you'll call complete with a value of success or failure, depending on the state of the transaction. Alright, now let's check that out on the iPad. There we go. So, there are a few additional steps you'll need to take, like obtaining a payment session from the Apple Pay server. To learn more about that, please see the sessions page on the Apple developer website for links to those additional resources. Now, finally, I realize I haven't given a glimpse of my readers-- what they're actually building. So, I want to add an image near the top of my page of the final product. But, why stop at a static image? Wouldn't it be great if you could actually see the birdhouse in your own environment, get a sense of its size? So, with the new AR Quick Look feature in iOS 12, we can do this with just a few lines of code. So, we'll go into my HTML. And, I think this is a good place of it. So, all I'm doing here is adding an image. And, linking to a USDZ file, that is the model of my birdhouse, with a rel attribute of AR. Switch back to the iPad, and there's our finished product. That looks pretty nice, but now my users can also tap on this Quick Look, AR Quick Look icon in the corner here. We can see the model, move it around, and I can also place it in the real world, and actually get a sense of what I'm going to build. So, it's actually really easy to do this. Please check out that session if you have the chance. I'd like to bring Shloka back up on stage to wrap things up. Thank you. Thank you so much, Jason. And, the AR model looks really, really cool. And, I think its inspired me to try to build a birdhouse. Not making any promises. So, you can add custom drag and drop features. And custom fullscreen controls for the iPad. You can use the Payment Request API to support Apple Pay, and the Service Worker API to support offline experiences. Or, you can add AR models to your content to give it depth. And now your websites can be viewed in Apple Watch. I've called out several sessions that you can reference for individual features, but if you have any questions right after this talk, stop by the Safari, WebKit, and Password AutoFill Lab. And, check out the link to this session for, of course, documentation resources and related sessions. Now, there are a lot, a lot of features when it comes to Web, and I hope that this quick overview gives you a taste of how Apple constantly improves Safari and WebKit support. So, web developers, native developers, and extensions developers can always offer the best experiences possible for their users. Thank you for so much for enjoying us for this-- for joining us for this session. Hope you enjoyed it. And, enjoy the rest of your afternoon at WWDC.  Hi, everyone. I'm Vicki Murley and I'm the Engineering Manager for the MapKit JS Team. This is Session 212, Introducing MapKit JS. So, when iPhone came out more than ten years ago, it really changed the game, and it introduced us all to what we now kind of know as this app ecosystem. And it's funny to think about, because for so many of us, apps are such an integral part of our daily lives, but before there were apps, there were websites. If your company was founded before iPhone, your whole company may have started as a website, and even today, if you have an app, you might be using a website to reach a larger audience or even just a different audience of people. So, lots of developers out there, just like you, have a website and an app. And the WWDC site is a great example. We have this site and there's an accompanying app, which you're probably using to find your way around the conference center, as you're on the go, here this week. So, at Apple, we've been really lucky that we have been able to show Apple Maps on our own websites for a little while now. This is a page on the WWDC site, which is showing you other events around the convention center. If you've ever used Find My iPhone of iCloud.com, you've seen Apple Maps on a website. And if you've ever searched for a retail store, again, an example of Apple Maps on a website. So, most of you out there are probably used to using MapKit inside your apps, and today, we're making MapKit JS available to you for your websites. So, this week at WWDC, we're making a MapKit JS Beta available, and this is the same library that we have been using on our production websites. Every web mapping library out there, has a free tier of usage. Like some number of requests that you get for free, and MapKit JS Beta is no different. So, you -- as part of the MapKit JS Beta, you're getting 250,000 map initializations, and 25,000 service requests. That includes geocoding, search, search auto-complete, and directions. And often these free usage tiers are specified over some period of time, like per year, per month, per week. For the MapKit JS Beta, we're making this amount of free usage available to you per day. So, that's a good amount. And -- but if you need more for your particular use case or your business, you can contact us. There's a form online. Just fill it out and submit it, and we will receive that request. To use MapKit JS, you need a key, and you can get that by signing into your developer account, and go to the Certificates, ID's, and Profiles Panel. And you can get a key there, just like you would for any other service in your developer account. There's a limited number of keys available for the MapKit JS Beta, so I suggest you go and get one as soon as possible. Once you get a key and start using MapKit JS, I mean, I'm biased because I work on it, but I think there's a lot of things that you're going to really like about it. The first thing is, it lets you unify on one map provider, for both your app and your website. So, we've noticed lots of you are using Apple Maps inside your apps. Now, you'll be able to use Apple Maps everywhere. The MapKit JS APIs, are really inspired by the APIs that you're already using inside native MapKit, but they're using conventions that are familiar to web developers. So, they should be easy for you to adapt. Also, MapKit JS really brings the Apple Maps experience to web pages. It's localized and in 32 languages with full right to left support. It's accessible via keyboard access and with voiceover. It has the beautiful cartography that we all know and love in Apple Maps. And it has support for native gestures like Pinch to Zoom, Two-Finger Rotation, and also Shift, and Two Finger Scroll to zoom in. It also uses something that's called Adaptive Rendering Modes. So, what that means is, there's a couple different modes in which the map can be rendered. The first is Client-Side Rendering, and this is a full WebGL engine that is rendering the map on the client. And this allows us to do lots of things, like these very, smooth transitions as users pinch to zoom or shift two-finger scroll to zoom in. You'll see the labels fading in and out as we transition between zoom levels, just like on native. Also, Client-Side Rendering gives us full control over the labels on the map. So, we can enable features such as Rotation, where I'm rotating the map with two-fingers, but the labels are staying horizontal as I rotate. Finally, with CSR, we have the marker style annotations that you're used to seeing on iOS. And the title of this annotation is My Marker. If I select this annotation, the subtitle will appear, but it would overlap that label that is beneath it. So, with Client-Side Rendering, and full control over the labels, we can just collide that label out and remove it so that the labels on the map never overlap the text for your annotations. And even as users are zooming in and out, and the map labels are changing all over the place, the map labels are never going to overlap your annotation labels. In fact, if you have a draggable annotation and the user is picking it up and moving it all over the map, labels are never going to overlap your annotation labels. So, this is one of the rendering modes, Client-Side Rendering. But web code can run anywhere, and not all devices are created equal. So, for those times when your users may be running on a low performance configuration, maybe an old device, we have a different mode that we call Labels Only Client-Side Rendering. And what this is, it's a grid of image tiles that show the base map, with a layer of WebGL labels over the top of it. So, we can still give as many users as possible these great features like rotation and label collisions, in a high-performance way. In your users' configuration does not support WebGL at all, they get something that we call Server-Side Rendering, which is a grid of tiles, tile images, and in this case, the labels are actually part of the image. So, adaptive rendering modes are pretty cool because there is an ideal mode for every client configuration out there. And we choose the best mode for your user automatically. So, you don't have to decide whether to use the WebGL version, or the tile version, or whatever. We run a quick test and give that user the best mode for their configuration. In our experience so far, most users are getting Client-Side Rendering or Labels Only Client-Side Rendering. So, that's a quick introduction to MapKit JS. And now, to tell you more about using the API, I'm going to hand it off to Julien Quint. Thank you, Vicki. Let's get down to the technical details of using MapKit JS for your website. We will see how to set up a map. We will see how to navigate an annotate this map so that you can show your own data. And finally, we'll see how to enable rich interactions with additional services. To set up your map, you need to go through these four steps. First step is to import MapKit JS. Here you see a script tag that you can add somewhere on your page. And the URL of the MapKit JS script includes a version number. We follow the semantic versioning system where the first number is the major version number, which is updated if breaking changes are introduced. The second number is a minor version number, so when bugs are fixed, when new features are added or when performance improvements take place, we update this number. And finally, for urgent patches, we update the patch number. You can specify and X instead of a specific number for the minor or the patch version, and we recommend that you use this Version 5.0.X to get started so that you will get urgent bug fixes on our first public release. In order to be able to show a map on your website, you will need to tell your page -- to tell MapKit where to show that map. So, in order to do that, you can use an HTML element that will be the container for your map. So, in this case, I'm using a development which is usually a very good candidate for that. I give it an ID so that I can refer back to it easily. And one thing that is very important is to set its size, because the map doesn't have a size of its own, so it will just use the size of the elements that it is part of. So, if you have a development that I represented here with a gray background, a map will be shown inside that element. If your element size changes, then your map size will change as well. So, I have set up an element for my map. The next step is to initialize MapKit JS. So, we just saw that you need a key to be able to use MapKit JS, and in initializing MapKit, you get your authorization callback so that you will be able to use the services that MapKit JS provides. And most importantly, to show the actual map on your websites. And the last point is to create a map object, here using the Map Constructor from MapKit. I give it the idea of the container where the map will show. And this is what happens. We have a map showing on our sites. This is the different map. We didn't pass any other parameters, so it will show a different region and the different control. You can see the controls in the corners. You can see the Compass for rotation. The Zoom control. The Map Type Control on the top, etcetera. This was a pretty large area, but if you're on a page where you want to show only a very small map, these controls can use a lot of valuable space, so by default, when your map size is below a certain width or height, we will toggle these controls off automatically. On touch devices, the situation is a little bit different. We don't really need to show the -- some of the controls like the Zoom or the Compass, because we have gestures to change the rotation of the map or to zoom in. So again, we don't really need to use up that space by showing these controls, and we turn them off by default. Note that the Apple logo and the legal text are always present. These are the difference for the controls, but you can also set these control visibilities yourself. Some controls are so-called Adaptive Controls. This includes the Compass and the Scale. This means that depending on the platform, they will behave in a slightly different manner. So, on iOS, the Compass control is not shown by default, but if the map rotates, then we will show it indicate where North is pointing. And the Scale is an indicator of the units of distance on your map. And we show it only when the user is zooming in and out. So, here, we can see the -- when we have zooming and rotation, we can see the Compass appear and disappear in the bottom right corner. And the Scale appear and disappear in the top left corner. Of course, we can also select these controls to be always visible or always hidden. The rest of the controls have a binary state, so they can be hidden or set to be visible by setting a Boolean property. In this example, we show the controls that do not show by default on desktop, such as the User Location Control which appears in the bottom right of the window. Here, I've selected it. So now, the user location is tracked, and you can see the user location in the center of the map. And the Scale is shown at the top let, because I marked it to Always Visible. We can go further in configuring these controls. In order to match better the color scheme of your page, you can set the tint of the map. And this changes the color of the user controls. So, you can see, for instance, the -- all of the controls of the map. You can see the user control is now highlighted in red because I set this tint color to a red color. You can use any CSS color value here. The MapKit will use your [inaudible] language configuration, so that it will adapt itself to the language that your user expects. You can also set that language into the initialization code or you can even set it on the fly using the language property. So here, I've set it to Japanese so that I'm sure -- so that I can show the map, as well as the controls in Japanese. So, you can see the North indicator for instance in the Compass has been replaced, and the units are not in -- using the metric system which is more traditional in Japan, than using miles or years. If you set your language to a right to left language, such as Hebrew in this example, or Arabic, the controls are also mirrored to match your users' expectations. So, the controls are the way -- are some of the ways that your users can interact with the map, but again, also in turn directly. As I said, we can use touch events on touch enabled device. On this stuff, you can use the Track Pad for some of the gestures, and you can also use the mouse to click and pan the map, or double-tap for instance, to zoom in on the map. You can also disable these interactions directly, by setting the Zoom Enabled, Scroll Enabled, or Rotate Enabled property to be false. So, for instance, in the case where you have this very small map as I showed earlier, you might want to make sure that the user doesn't accidentally move the map around to be sure that it always shows the right area. And you would set these properties to false, which would make the map static. Now, that we know how to set up the map on our page, we want to make sure that we show some interesting content. So, we can do that by navigating around the world to show a specific area, but also to annotate the map to call attention to the region that we are showing. We'll see how to set the center in the region, how to add annotations to the map to mark locations, and how to cover the -- some areas of the map with overlays. So, starting with the center and the region of the map, again, this is the default map. And it shows the default region, which is zooming out at the minimum possible level. And centering the map on, zero, zero, latitude and longitude. You probably want to show a more specific area of the world. So, for this example, I want to focus on Rio de Janeiro in Brazil. And to do that, I will set the center of my map to the coordinate of Rio de Janeiro. So, you can see on the right, that the map center has moved to show Rio at is center. But setting the center of the map does not change the scale. So, at this scale, we are always still not very much in focus because we can see all of South America, for instance. So, let's zoom in a little bit. And in order to do that, I will set a region. So, a coordinate region is made up of a center, which is represented by these dots at the center of the map, as well as a latitude and longitude span. So here, I have represented the region as this dotted box. But we will not that the map -- the region that is set ultimately on your map, will be bigger than the region that you asked for because we need to make sure that it fits entirely within the bounds that you have set for your map. So, in this case, I need to add some padding, north and south of the region to be able to show the -- all of the region that was specified. So, in terms of putting the MapKit JS framework -- the center is set to a MapKit coordinator object which is a latitude and longitude pair. In the Coordinate Region, to set the region of the map, is an object that contains two member, a center which is also a coordinate and a coordinate span which is a latitude delta and a longitude delta there. How did I come up with the region or with the coordinates that I showed you in these screen shots? There are many ways to decide on regions and coordinates. You can look them up on Wikipedia and help put them in your code. You could get them from some database that has some geographical information. But one way that you can also get this information, is to use MapKit Services that includes Geocoding and Search. So, Geocoding lets you look at a place and will return your coordinate in a region. So, in these previous examples, I used the geocoding from MapKit JS to look at the coordinate in the region for Rio de Janeiro and this is what I set on my map. You can also use search to search for places. And it will return not only the coordinates of these places, but also bounding region that you can set on your map, to make sure that it encloses all of the results. Another way to quickly set a region on your map that is very convenient, is a method called Show Items. We will see now how to add annotations and overlays on your map, and by calling Show Items, we will make sure that we have a region that encloses these items so that they are all visible to your user. I will note that region changes may be animated, so that you can provide a nice animation. Or you can set the region instantly. Because these things, changing the regions, changing the center, etcetera, can be done from your code but can also be done by the user as a result of interactions you want to be able to react to these interactions. And we -- and the Map Object allows you to listen to user events such as region changes, when they begin, when they end, since they can be animated. And also, when gestures such as coding, zooming, rotations begin and end. We follow the model of DOM Event Handling, so you will have the familiar method such as Add Event Listener and Remove Event Listener, the name of the event, and you get past an event object with the parameters of the event. So, for instance, if I wanted to refresh my map with some new information every time the user has mapped to a different area, I can listen to region change and events, and that will let me know that the map has settled to a new place, and I can now use the region of that map to compute which information I wanted to show at this stage. Now, that we know how to set up a map, let's add some our own information on it. And we can do that with annotations and overlay. We'll start with annotations. And MapKit JS provides three kinds of annotations. The first one, you've had a quick peek at it, which is the Marker Annotation. You can customize annotations further than the -- what the marker annotation provides by using images, or even DOM Elements that will have completely customizable appearance. The marker annotations are the most useful annotations that MapKit JS provide as they also have lots of rich interaction built-in. And there's lots of styling options. So, they are analogous to the marker annotations that that you may have seen already on iOS. They have a built-in animation for selection and deselection. And they automatically collide out the underlying map labels as we've seen before. So here, when I select my annotation, the label for the exit of the station is hidden to make room for the subtitle. Marker annotations -- the appearance of marker annotation also adapts itself automatically to the rendering mode that has been selected by MapKit JS for your map. So, we've seen that if we have Client-Side Rendering Mode or Label Only Client-Side Rendering Mode, we are able to hide labels, so we can display the title and the subtitle of your annotations on the map. And this is what is shown on the left-hand side. If you have Server-Side Rendering, the labels are built into tiles and we cannot hide them. So, if you see the map on the right-hand side, we see that we have many more labels because none of them have been hidden. So, in order to be able to show our annotations legibly, we only show the annotation balloon and the labels are not shown on the map. We can still see the title and the subtitle of these annotations when we select them by using a callout bubble as in this example. And custom annotations, will also make use of these callout bubble to show the title and subtitle. Finally, the market annotations provide a lot of standing options. So, this is the default for market annotation. You create an annotation with a coordinate and some properties such as title and subtitle, and this is the basic default rendering. You have glyph image which is a pin shown in the balloon, and it comes up with a red color. You can change that color to any CSS color value. So, here, I've used a green color for my balloon. You can also change the color of the glyph. So, the glyph can only have one color. So, in this case, I switched from the default white to a yellow color. You can change the image of the glyph. So, from the default pin that is provided by MapKit JS, to any raster image that you can provide. So, in this case, I've provided this image. You can give several sources for your images for different pixel ratios. When the annotation is selected, it becomes bigger, so you have more room to show more details in your glyph. So, in this case, we can also specify a selected glyph image, which is different from the original one. Finally, instead of an image, you can change the glyph by using some text. We recommend that you stick to very, short strings such as one to three characters, and in this case, I'm just using the Letter M. When the glyph text and the glyph image are both specified, then the glyph text is shown. So, now that we know how to display marker annotations, we can go crazy and put a lot of annotations on our map. So, this is the result of doing searches for cafes and bars in Paris. And I've styled them slightly differently to distinguish them from one another. But the problem is that there is a very dense number of annotations in the same area. Fortunately, we have two ways in MapKit JS to deal with this sort of clutter. The first one is to set a display priority property on annotations. What this means is that when two annotations collide, the annotation with the highest display property, which is a number, will collide out the annotation with the lowest display priority. So, in this case, we can see that many annotations have been collided out. When you zoom in and out, these collisions run again, so that zooming in for instance, will let you discover more annotations. And when they have the same display priority, we will use the one that appears at the bottom of the screen as the more -- as the one with the highest priority. Another way to deal with clutter, is to use clustering instead of these display priorities. So, in this case, when two annotations that have the same clustering identifier, which is a property that can be set on marker annotations, and is a simple string, when two annotations that collide and have the same clustering identifier, they are replaced by a single annotation that will represent both of them, or more than two of them of course if more than two annotations collide. So, in this case, we see that some of the annotations can be shown by themselves because they have no neighbor, but in the dense areas in the center, we have clustered several annotations together. When a cluster is created, we use a marker annotation to represent it and we use the glyph to show how many annotations are in the cluster. You can also change the behavior -- not the behavior, the appearance of your clusters, by specifying a new style or a new kind of annotation. And now, we'll do a small demo to show these annotations in action. So, in this demo, which should appear here -- in this demo, we will use a map and the data that we will display is Bigfoot Sightings that Have Occurred in this Area. So, after this session, or maybe this weekend when you are done with the conference, you can maybe go explore the woods of Northern California and see if you can find Bigfoot. I have created a map which I have initialized in the manner that I've displayed in the previous slides. And what I have done, is I have gathered a list of Bigfoot annotations -- of Bigfoot sightings, sorry, including the location, the year in which they occurred, and some other information. So, what I will want to do is I will want to use these annotations, show them on the map, with marker annotation. So, let me start by creating a function to translate a sighting object which has all these properties, into a marker annotation. So, when I get a sighting, I want the coordinate of the sighting to be used as the coordinate of my marker annotation. And these sightings have several other properties, such as the year of the sighting, which I will display in the title. This comes from a database put together by the Bigfoot Field Researchers Organization. So, I will show the idea so that if you are interested in more details about that sighting, you can go look at that database and find even more information. And finally, I'm ready to create a marker annotation with that coordinate and these options that I've created. So, now that I know how to set up a marker annotation for a sighting, I will get the sightings that I've prepared. And with this list of sightings, I will create an annotation for each of them. I will then make sure that my sightings are shown on the map. My annotations are shown on the map. And I will use the Show Items method, which is very convenient because not only it ensures that the items are visible, but also that they are added to the map. And since I'm doing a webpage, I can also add some extra information on my page such as how many sightings there were in this area. So, let me make sure that I save and by reloading, I now see a map with 60 sightings on it. So, all of these annotations are sightings and we run into the problem that I've just highlighted which is that in this park area, I can see lots of sightings happening, and they all overlap each other. And even as I zoom in, I can see that there's still lots of clutter, so I will clean that up using the Display Priority Property on Annotations. So, in order to choose a display priority, what I will use is I will use a very important property of the sightings which is whether they were clear sightings. So, if someone really, really saw bigfoot, or maybe they just heard it or saw some traces of it. So, I have this clear flag which I will use to set my display priority. Display priorities a number and these priorities can be pretty arbitrary. So, we provide predefined values, such as required, meaning that your annotation must always be shown on the map, but also, high and low display priority. So, when the sighting is clear, I will set a high display priority. When it's not, I will set a low display priority. And finally, I will also encode that information as the color of my annotation so that the users can understand better why an annotation was shown or now. So, when it's brown, it's a clear annotation and when it's great, it was not so clear. So, let's see the difference that this makes. So, now you can see that the map is much cleaner because all of the collisions have been a result. You can see that some of the annotations are brown. So, these are the clear sightings. These are probably the ones that are the most interesting ones to visit first. But you can also see that if I zoom in on the map, then new sightings are being revealed. So, your users can explore the map and find out about where they can hope to see Bigfoot. I will conclude by this demo, by adding, another piece of information to my map. I will change the glyph to be either an icon of Bigfoot which I've prepared. So, this is a simple PNG image which I will use for the clear sightings, and in case the sighting is not so clear, I will make that more explicit by setting the glyph text to be a question mark. And now, what I can see, is that these questionable sightings are a question mark. And the Bigfoot sightings that are really clear, have the Bigfoot icon. So, this includes the first demo. Thank you. So, in this demo, we show how to create marker annotations from JavaScript objects, how to set the display priority of annotations to unclutter our map, and how to style these annotations with colored glyph image, glyph text, so that we can translate the information that we have onto the map. Sometimes, market annotation is very convenient, but may not be exactly what you want to use on your map. For instance, if you want to use the logo of your company and it has more than one color, then you cannot choose a glyph image because it's restricted to just one color. Sometimes the shape of the annotation is not really the one that you are going for, so in this case, you may provide images to represent your annotations instead. So, in this case, I have the small sticker look to mark places where -- which I have visited recently. So, I can use a raster image for these annotations. The title and subtitle are shown in a callout bubble, just like in Server-Side Rendering for marker annotations. And to create an image annotation, this is very similar to a marker annotation, but with the additional property that much be provided which is the URL for the images. And here, you can see two different URLs for two different pixel ratios. Another way that I could represent a notation is using the classic pin example. And pins usually come up in many different colors. And the problem is that if I want to provide a lot of different colors, I will need to provide a lot of different images. And this can quickly become unmanageable. So instead, what I can do is I can use a custom annotation. And in this case, instead of an image, I will use any DOM Element to represent my annotation. And these elements are created on demand for your annotations. So, let's see an example. Here, if I wanted to do just some color for any pin that shows up on my map, then I will create an annotation with a coordinate [inaudible]. And a third parameter which is a function that we will return a DOM Element for that annotation. So, in this case, I will use a Canvas Element. I create a Canvas Element. I get .Context. I drew a pin image in that Canvas. I changed the head of the pin to the color that I want, and I returned to Canvas. And this is the Canvas that is shown on the map for this annotation. So, these were the three kinds of annotations that you could use. But sometimes you want to show more than just a single location or a set of single locations. Sometimes you want to call -- to show complete areas on your map. And to do that, we have overlays and we have -- we provide three different kinds of overlays. And here are some examples of how you could use them. With a circle overlay, you can show all of the distances -- or the distances from a point. With a polyline overlay, we can show a route between two points. And you can highlight geographical areas such as a state, the boundaries of a country or a city, etcetera, using a polygon overlay. So, here is an example of a circle overlay. So, now I'm in Brussels and I've centered my map on the Manneken Pis. And this is right in the center of the city and this is a very nice place to walk around. So, by using concentric circles around my location of increasing radius, I can see walking distances. So, here I've created overlays with their coordinates, and the second parameter that you need to give to a circle overlay, is a radius expressed in meter. So here, every overlay has a radius starting from 400 meters, increasing by 400 meters. So, 400, 800, etcetera. So, I can see in the right, that there is a museum for comic strip art. That sounds interesting. And it looks like it could be between 1200 and then 1600 meters. So, about 10 to 15 minutes' walk. One important thing that I can do with overlays, is I can also style them. So here, to represent this style of overlay, I can use a MapKit Style Object, which has several properties such as the line width, the line depth, the stroke color which again, can be any CSS color. By default, circle overlays are filled with blue color, but in this case, I want them to be transparent. So, I set the field color to be null. I have decided that I want to go visit this comic strip museum, and MapKit JS as we will show, has services that can give you walking and driving directions between two points. So, in this case, I've asked for directions from where I was, the Manneken Pis, to where I want to go. I receive as a result, a route which I represent on this map using a polyline overlay. And a polyline overlay is a list of points. So, these are all coordinates linked to each other. I can style them by setting the Stroke Opacity for instance, or a thick line width to make sure that it shows up on my map. And the last example is a polygon overlay. This is very useful to do for instance data visualization. And here I have a map of the United States where every state is an overlay of its own. A polygon overlay is defined again by a list of points, but this -- in this case, the shape is closed and filled. You can even specify lists of lists of points to -- for more complex scenarios. So, for instance, if you have enclaves and exclaves, which is often the case with real geographical areas. Annotations and overlays can also have an extra piece of information added to them, which is this data property. So, in this example, I want to show the population of every state of the United States. I can show it as a color, which gives me a rough idea of the population. But I can also add some additional data so that if the user selects one of the overlays, so in this case, the State of Texas, I can show that the actual population is over 20 million people. And overlays are selectable just like annotations. To show overlays such as the route that I showed you or to show all of the states, we need a lot of data, we need a lot of points, so for the states, we had thousands and thousands of points. So, a convenient way to get that data into your program, is to use GeoJSON Import. So, if you have a GeoJSON data file, then you can import it automatically using MapKit JS. Annotations and overlays are created. So, in this example, this was a different data file, which contained a lot of UFO sightings. And this is just a detail of the map. So, you can see there are lots of these sightings. And all of these annotations and overlays are created automatically and can of course be customized using some delegate methods. Finally, these -- since these annotations and these overlays can be selected by users, you want to listen to events from them, as well as you could from the map. So, you can listen to selection and the selection events. And we've also seen earlier that annotations could be dragged, so you can listen to dragging events. And again, this uses the Add Event Listener Method. So, for instance, you can use Add Event Listener to listen to select events from the map, which will tell you when an annotation or an overlay has been selected. And we will see an example of that in the demo. The next section is Enabling Rich Interactions with Services, and for this section, I will give the mic to Melody. MapKit JS provides you with access to three services, Geocoding, Search, and Directions. I'll step through examples of each of those, but before that, I'll tell you what they have in common. You can use the services in a similar way, with four easy steps. You first create the service object, then you specify the request parameters and options, then you make the request, and finally, handle the asynchronous response via a callback function. So first, let's talk about the Geocoder. Here's an example of how you can use the Geocoder, and it has two methods: Lookup and Reverse Lookup. So, this is if have you an address or a place, and you want to find the coordinate, or if you have the opposite. You have a coordinate and you want to find the address. So, the first step is to create your object, which you can optionally provide at Gets User Location parameter. This will allow you to provide more context to the geocoder. This is useful in the case where there are places with the same name in multiple locations. For example, if you're using the geocoder here, you'll be most likely to return Brisbane, California instead of Brisbane, Australia. Next, you build the requests. Here, we're using this convention center. And then you can handle the response. As we mentioned before, the geocoder can easily be used to place marker annotations. So, here we're adding a marker annotation to the map. And then here we have an example of the reverse lookup where we have a coordinate, and we want to find the address for the place. So, that's the geocoder. Next up we have service context. So, as I mentioned before, you can provide a Gets User Location parameter, but you can also provide coordinate and region to search. This is useful to provide the most relevant results to your user. So, that's where places are in multiple locations with the same name, or you can provide your search results, closer to the user. Here's an example of how you can use the Search Service. It's an example for finding coffee nearby the user. So first, you create your service object. Then you build your request using query like coffee. You can also do something more specific like a specific name of a coffee shop that you have. And then you can handle the results, which can easily be displayed using annotation since sometimes you return multiple results. And you can use the coordinate as well as the title to populate the fields of annotation. And then finally, we simultaneously add and display the annotations using the Map Show Items method. So, this is useful if you have your full query that you want to send in the request, but if you have a Search Bar that you want to respond to user input, you can use the Search Autocomplete Method. With Search Autocomplete, you can provide a partial query to the user - to the service, and then save your user some keystrokes by providing suggestions which you can then use to complete a search. And the last service is directions. Similarly, to the other services, you create your directions object. You can choose to provide a language which will then return the route steps in that provided language. If you don't provide a language, the language provided to the MapKit initialization function will be used. And if that's not provided, the browser's preferred language will be used. Then you build your request using two required fields: the origin and the destination. These can be coordinates, addresses, or places. By default, the transportation type is by automobile. You can switch this using the transport enum to walking directions. We won't keep that, because that's a really far way to walk, but we will use Request Alternate Routes. You can use this to provide multiple options to your users. This way we can display different routes instead of the default of one optimal route. And then finally, you can display your results on the map. Here we have polyline overlays as you've seen earlier. And you also get a set of route steps as well as the distance and the travel time. So, those are the services. Now, that you're more familiar with them, Julien will come back up for a demo. Thank you, Melody. So, we will pick up where we left off and we will continue building our Bigfoot Finder. And there's one things that's really missing from this finder, is that even though it can tell us where we are, and it can tell us where Bigfoot is, it doesn't yet tell us how to get there. So, I'm going to add some directions from All Current Location, which is the McEnery Convention Center, to these Bigfoot locations. I will use the MapKit JS direction services to do that. I will use polyline overlays to draw them. But the first thing I need to do is I need to make sure that the user - I need to find a way so that the user can tell me which one of these sightings they want to see. So, well, this one is selected, so that's probably the one that the user wants to see, right? So, this is what we will do. We will use the Select Event Listener from the map to listen to annotation selections from the user. And when an annotation is selected, that means they might be interested in knowing how to see that specific location. So, we will request a route and draw it so that they can see how to get there. And then finally, we will let them also select one of the routes from the options that are returned by Direction Services and display more information about each of these routes. So, let's start by setting up our Event Listener. So, we listen to the select events from the map, which tells us when an annotation or an overlay is selected. So, this is the property of the event, object that I received. And if annotation has a value, then that means that this annotation was just selected. So, there is only one annotation overlay selected at a time, in MapKit. So, there is no ambiguity here. This is the one that we want to go to. So, I will show routes to that annotation. Showing our route means drawing it on the map. So, let's set up a polyline overlay to draw a route on the map. So, first off, I will start by defining a style that I will use for all my routes, setting some opacity and some line width to make sure that the route stands out. The default color is blue, which is fine for the moment, so that's what we will use. And just like we did a function that created annotations on demand for citing objects, here we'll create overlays on demand for routes. So, a route that is returned by the MapKit JS Direction Service, contains a lot of information including a path which is a list of steps that you can take to go from A to B. And each list of steps, itself contains a list of points. And if you remember to create a polyline overlay, I want a list of points. So, now that I have a list of - list of points, I will reduce it to a single list so that I will put all of - the path steps one after the other. This is the list of points that I want to create my polyline overlay. This is the style that I just defined above, that I want my overlay to show up as. And later, I will let the user select this overlay to see more information. So, I will keep around all of the route object, in the Data Properties so that I can display the distance of the estimated travel time. I know how to draw these overlays, so now I know - I need to request them from the service. So, here we go. This is a little bit longer but not so bad. I first need to create a directions object. I will just use the current language for the directions. So, the default is fine. So now, when I'm requesting a route to a specific annotation or rather, routes to a specific annotation, I will clean up my map first because maybe I've shown routes to a different place before. So, if I query the other list property of my map, it tells me which overlays are currently added to the map, and I can remove them by using the Remove Overlays Method. So, now I have a clean map, and I can display new routes. I'm building my request for the origin. I will hot code this place, the McEnery Convention Center. Again, I could put coordinates for instance, instead, but this is much more convenience. For the destination, I will use coordinate because I have them. They are the coordinate of the annotation that was given as a parameter. And I want to show all of the possible - not all of the possible but the several possible routes to this point. So, I make sure that My Request Alternate Routes Property's set to true. Now, that my request has been created, I can execute it on the Direction Services using the Route Method. Things can go wrong when you do these kind of queries. So, we must make sure that we have a way to handle errors when they occur. So, in this case, I will fall back to show a default route that I've created and it's just a straight line. So, if we see a straight line, then we know we're in trouble. But let's focus, let's be positive and focus on the cases where the request succeeds, and in this case, I will have a list of routes and I will create a new polyline overlay for every one of them. I will use my trusty Show Items Method on the map to add these routes and make sure that they are being shown by setting the region to enclose the routes. And let me introduce a couple more options in that method. The first one is Animate, because I wanted to make sure that there is a smooth animation from the current region of the map to the one where the routes are shown. And finally, I will also add some padding so that I can keep some of the context and not constrict the region too much around the routes. So, I'm now ready to request routes from the - this convention center to an annotation. So, let's select this one again. And now that my annotation has been selected, the request has been made. I see that I have three different results. So, they share a part of the route, but then they diverge in the end. And they start from our current location, the convention center. So, I think this is right. Yes, that looks right. I have got three results. So now, I might be interested in knowing more about what's the difference in travel time or distance. So, I can use selection to do that. But if I select a route, the only thing that happens is that my annotation has been deselected because there is only one selection at a time. The problem here is that there is no default behavior for overlays being selected. So, let's implement our own. What we will do is we will highlight these overlays and then display their information on the side bar. So, in order to highlight these overlays, I will create a new style for highlighting overlays. So, let's put it here, along with my old style. So, this style will keep the same opacity. We will make the line figure and we will make it purple so that it stands out from the other ones. But now, we need to apply this style to my overlay once it's selected. Fortunately, I already have an event handler for selections. And up to that point, we were only interested in listening to annotation being selected. But now we will just add a new -- we will add plans to this function. Oops. So now, if annotation is selected, I still do the same thing that I was doing, but if an overlay's selected, I will change its style to be the selected route style that I've just defined, so that it is highlighted. So, let's see this in action. We can use a different one. I'm feeling lucky. And now, we can see that we can select the routes to - from the list of routes that have - that has been provided. I haven't implemented showing the data yet. Let's see if I select another route. What happens is that there is nothing happening by default when an overlay's selected, but there is nothing happening by default when an overlay is deselected. So, I have to implement that part of the behavior as well. So, in this case, I will add a new event listener for deselect events. So, when an overlay is deselected, then I will switch its style to the default one. So, this should fix my issue. So, let's try again. I have a route here. And if I select again, I do the same place because here we have two overlays on top of each other. This will select the other overlay. And the deselection event handler is working, so, my previous highlighting has disappeared, and now, the new selection is shown correctly. So, this seems to be working pretty well. Here is another example. So again, I can select these routes. But now I want to know more about them. I want to know what are the driving directions, how long it takes to use one of the routes versus another, etc. So, this is very simple to do because all of the - everything that I need is already in place. I have saved data on my overlays, and I have Event List now for Selection and Deselection. So, when the overlay's selected, I will show the data in the sidebar. So, this is just classic. Now, we are really into web development land where we have a bunch of the time we went to -- format it into some HTML, so I won't go over the details. But the detail that we have is the one that is associated with the overlay. And it gets shown when the overlay's selected. And I will not make the same mistake about this this selection, so I will directly hide the data when the overlay's deselected so that I don't have several routes appearing at once. So, let's try again. Let's go back. I like this example. So again, I will select this route. And now that the route is selected, we can see on the right-hand side that we have some additional information. This is a quick name for the route. So, this is the route that goes from Mount Hamilton Road. It's 70 kilometers long and under ideal driving conditions, it should take me an hour and 32 minutes to reach there. And here are all the steps that I can follow to get to my destination. I could see what this one looks like. It's actually longer in distance but it should get me there faster. So, there we are. We have built a Bigfoot Finder and -- that lets us select where we want to go and tells us where we are going to go. Thank you. Quick recap. We've seen how to react to user events. So, in this case, selection and deselection. We've got driving directions from the MapKit JS Service. We've used polyline overlays to render these driving directions. And we've added our own behavior for the selection of overlays since there is no specific behavior provided by default. So, there is plenty more to talk about MapKit JS, but we only have one hour. So, I will stop here. But I hope that we have made a good case for MapKit JS, delivering a top-notch map experience for the web. That it lets you unify on a single map provider with a familiar and flexible API. So, if you have on your team people who have used the native MapKit API, this would feel right at home, because the same concepts apply. And web developers will be happy to see that the APIs are also tailored to match JavaScript. So, I will remind you that you need the MapKit JS Key to be able to use MapKit JS. So, please go to the developer portal, get your key, and try it out today. If you want to know more about MapKit JS, you can find the documentation and sample code at this URL. And if you want to know more about this session, you have this URL for the session itself. We have accompanying video that will take you step-by-step through getting and using a MapKit JS Key. And finally, we have labs happening on Wednesday morning and Friday morning, so you can come to talk to the whole MapKit JS team to ask us questions about MapKit JS. Thank you very much.  Good morning. Good morning, everyone. How's it going. It's nice to see everyone today. Hopefully you've enjoyed the conference this week. We're really happy to be here to start off the last day. So, my name is Jon Dascola, and later I'll be joined by my colleague Heena Ko. We're both designers on the human interface team here at Apple. And we're going to talk about designing great notifications. So, this year in iOS 12, we introduced a lot of great feature to really enhance the notification experience, both for you and everyone that uses your apps. And we think these features are really powerful, and could create a meaningful, but also mindful experience for everyone. But, before we get into what's new, I think it would be helpful to take a trip down memory lane and see how the notification experience has evolved over the years, because knowing how we got here will help us make a better future. So, when iPhone launched, every alert was a blue box, model alert. Do you guys remember this? Every notification, every message, every invitation, it was an interruption that needed to be dealt with immediately. Nothing would be on the lock screen, so at times, it was pretty cumbersome when you'd unlock the phone, right? Whatever missed messages you received were backed up and would appear a single box at a time. So, you could either ignore them, one at a time, or if you wanted to reply, you launched into the app to take action. Then, in iOS 4, as more apps were sending push notifications, we started to queue them up on your lock screen. So, now people could see all of their missed notifications at a glance and choose the specific one they wanted to interact with. A swipe on that row would launch into the app but all the other ones would be dismissed from the lock screen. And they all went up to notification center. And that was a way to access all the other things you had missed. So, because so many notifications were being accumulated, this became a really useful place to go and to see all the other activity on your phone. Next, we introduced rich notifications. As the quantity of notifications was increasing, we wanted to increase their quality. Rich notifications are a great way to provide more context and information around each notification. That way you can see one, understand what it's for and get the extra information that you need to take action. And it all happens right in line. You don't need to launch into the app and lose the context of what you're doing. And that leads us to where we are today and all the work that we've done to enhance the notification experience in iOS 12. So, we're going to talk about some features that make the world better for you, as app developers. Ways to make your notifications more valuable, useful, and organized. And then, we'll talk about how those features can create a better experience for everyone. But before we get going, can we take a moment and be candid? Can we talk about the current state of notifications? Maybe not always the greatest. You know, people are getting a lot of interruptions. And they're not always for the right reasons, right. And we have way more apps than ever before. And they're sending more notifications than ever. And because people are seeing so many notifications, it's so important to make sure that we're creating a really great experience for everybody that uses your app. Because the best notifications, they're for connecting and communicating with people. I mean, that's what makes iPhone great, that real human connection. And then, you know, we have social notifications. I totally understand how these are important, right? People want to stay on top of their digital lives. Everyone likes their likes. And then there's even the occasional notification about new sneakers or something, that could be super fun and really useful. But not all of these things all of the time. Again, all things in moderation. So, as more apps are continuing to send more notifications and taking more of our attention, we really have to make sure we're doing the right thing. We need to remember that the best notifications are for connecting people and delivering meaningful information. Now, I know none of you here would ever do any of this, but there are some app developers that maybe go a little bit overboard. And this year, we've created a UI that makes it easier for people to control how their notifications are delivered. And there's even an option to turn off all the notifications from an unhelpful app. So, in iOS 12, when you receive a notification on the lock screen, you could swipe left and access some actions, and you'll notice this year, we've added a manage button. Tapping that brings in our newly designed quick tuning UI, which allows you to configure how the notifications from that app are being delivered. So, at the top, the first option is to deliver quietly. And these quiet notifications will be send directly into your notification center without interruption. It did not appear on the lock screen. It did not play a sound or haptic, or present as banners. And I think this is a really great option for an app that's sending you notifications that you want. Content that you're interested in, but you don't necessarily need to be interrupted by for every new post, right? So, you can access these notifications now on your own terms. But also, right here in the card is a button to turn off all notifications from that particular app. And we think having that shortcut is really handy. But having to use it, it's not a great experience for anyone. We don't want people to have to turn off notifications because they're annoyed or frustrated. And as developers you don't want to lose the privilege to reach out to those individuals, right? So, by the time we're done here, we want to make sure that you have all the tools and information you need to make sure you're sending only the most meaningful notifications that you can. Because if you send great notifications, you'll be, making everyone who uses your app happy. And if you've got happy people using your app, that's going to make you happy. And if you guys are all happy and they're happy, that's going to make Heena and I real happy today. And being happy is good, right? So, let's get into it and talk through the notification journey and how to create the best notification experience. We're going to start with the first run prompt, in that initial agreement and talk about all the best ways for you to make sure your notifications are allowed and delivered. Then we'll talk about the best ways to provide value with all the notifications that you're sending. Better ways to organize your content with notification grouping. And finally, how to make the most of rich notifications to create a really useful and holistic notification experience. So, let's get started and talk about that first prompt in asking for permission to send notifications. This is a familiar screen, right? It's your one and only opportunity to receive permission to send notifications. And it's an important moment. And you should remember that you're asking someone to make the difficult decision here. I mean put yourself in their shoes right. They just downloaded your app. They're excited to give it a run for the first time. And then all of the sudden, they're interrupted. And then they're asked make this decision about receiving notifications. How are they supposed to know, right? Especially if it's presented without any context and you're not letting them know why notifications are valuable, and if they've really had any time to experience your app. I mean why should they tap allow? So, this happens, right? They don't. And that was it. You had one shot, one opportunity to send notifications. And that was your only way to ask for permission. Well, in iOS 12, that's no longer the case. We've created a new feature that allows you to send notifications directly and only into notification center, without running that initial prompt. It's your choice. You can continue to show the prompt and ask for the privilege to send notifications to the lock screen with the risk of being denied. Or, you can choose to deliver your notifications quietly and directly in the notification center. So, they'll appear like this in your notification center list. And if we look a little more closely, you'll see the content within the notification platter is presented as it always is. Your information comes through as a normal notification. But at the bottom, we extend down the platter and we add some buttons. Now the prompt is integrated within a notification. It's less disruptive. It's a non-model experience. So, you won't be stopped when you're in the middle of using that app and being asked about notifications. You see the prompt right when you're looking at your notifications center. And the prompt now provides more information. It's presented with in an actual notification. So, I can see what sort of content the app will provide. And now, I'll have a better understanding of their purpose so I can better judge their quality, and be better informed to make that decision. So, let's look at some different categories of apps and talk about how they might go about requesting permission. So, we have social apps, news apps, games. And let's focus on a news app for a moment. Let's imagine we're creating a new news service from scratch and we're working out our notifications. Well, should we go to the directed notification center route? I mean that seems like a good idea, right? I mean there's a group of people out there that are interested in news. Maybe they're curious about what's going on in the world. But they don't feel the need to be interrupted for every single article that's posted. You know, especially if they've just downloaded the app and they're unsure of the content they might receive. I think the directed history route makes a lot of sense. But, you know I also see a very clear use case to ask for lock screen access too, right? People love breaking news. They want to be on top of what's going on in the world, as soon as it happens, right? I get it. So, what do we do? Well, if we start with the new world and talk about the cases that could be sent directly to notification center. So, if your app is sending content that can be consumed passively, and doesn't require critical or timely responses, then I think the directed notification center route is your best approach. It ensures that your content is delivered. It won't interrupt people. And most importantly, it gives them an opportunity to see your content and try your notifications before they have to commit to it. So, if our news app is sending longform articles, I think that makes sense, right? Or, social apps. I think they can deliver their likes and comments quietly. Games that are sending out promotional notification. Examples like that, I think they make sense to go direct to notification center. Now let's take a moment and talk about the traditional route. What sort of use cases make sense here? Well, if your apps need to deliver notification content immediately, if people need to see your notifications the moment that they're posted. Or your app requires an urgent response to notifications, ask for permission. You know, maybe that news app is breaking news. Or your social app has a big messaging component, or games where you need to see your friends online. If you've got an app like that, then go for it. I think that makes a ton of sense to ask for lock screen access. But if you do, there's a few things to keep in mind while you're designing that experience. Because if you do create a great experience here, I think it will greatly increase the probability that your notifications are going to be allowed. So, don't prompt or send this alert as soon as people launch your app for the first time. You know, give them a moment to experience your app, what you've built and what you have to offer. And find a place within your app to explain why your notifications are going to be valuable. Let them know why you're sending notifications. And more importantly, why they need to appear on the lock screen. And finally, that prompt, well post it at the right time. Present it when someone understands the reason why they'll be receiving notifications from your app. Like if you're a delivery service, wait for someone to complete their order and explain the notifications will update them on its progress. Or if you're a travel app, notifications will be for flight delays and gate changes, right. Because in either scenario, with the direct to notification center route or this one we want you to have the best opportunity to deliver your notifications. So, that's what's new when asking for permission to send out notifications. Remember, it's up to you. Both approaches are valuable and both should be considered. It's a decision that you're going to need based on your app and your app's particular content. So, now that you've taken all the right steps to make sure people will be receiving your notifications. Let's talk about how to make sure you're setting the best content in making notifications like extremely valuable for everyone. And why is it so important that we're sending out great notifications? It's because our attention is valuable. Our attention is precious. Right? Interrupting people, it's a real privilege. And we need to respect that. I mean you've all felt this right? You're trying to focus on something, you're in the zone. I mean maybe it's happened this week, you're updating all your apps to adopt these great new APIs and then all the sudden [ding]. You're interrupted. And you stop. And you lose focus. And when the wrong thing comes at the wrong time it's frustrating. So, you've got to be extra considerate when we're sending notifications. First off, we need to make sure we're sending great content and providing the highest quality of information. Everything you said in a notification has got to be meaningful. Every notification should provide a specific purpose. Each notification should have a specific message to communicate or a task to complete. Notifications are not just a reason to get people to launch into your app. So, Dark Sky, it's a really lovely weather app that sends particularly meaningful notifications. See, I think the interesting thing about the weather is you really need to know about the weather once it's different. You know, if there's a change in conditions. And that's when Dark Sky sends these notifications, right? Rain is starting soon. That's what I need to know. Just really smart. Door Dash is another great example. You get your notification when your food arrives. I mean everybody gets excited when their dinner is here, like what more do you need to know? It's perfect. And HQ does the right thing with their notifications. For a live game that requires you to be online at a specific time, a notification like this, it makes a ton of sense. So, what do they all have in common? Well, all those notifications, they served a clear purpose. They were presented for a specific reason with real information. They weren't just empty invitations to launch into your app. So, now that you're sending out really great content. It's important to consider when you're sending that content. So, be considerate when you deliver notifications. And reminders is great. You can choose whether you want your notification to be delivered by time, or by location. So, you get the appropriate content whenever you need it. And Headspace is great, they're one of my favorite apps. It's a meditation app that really helps with focus and clarity. So, I think it's appropriate that they let me choose when a reminder comes through. Because it would be terribly ironic if they were the ones sending me frustrating notifications at inopportune times throughout the day. Headspace is really great. And here, I love this screen with CNN, right. After you allow alerts, they ask you to choose the alert frequency. So, it's up to me to decide how often I would like to receive their content. It's such as smart way to manage and configure your notifications you know. Because as a user, now I have some expectations around what that notification experience will be. And here's a notifications from Duolingo. So, I was trying to learn a new language. I got a bit distracted here working on this presentation, so I missed a few sessions. And rather than continuing to send me notifications that I'm not interacting with, they decided to pause them. And I just think that's really considerate. Because I could totally understand the argument right, why they might want to send me more notifications at a time like this and really encourage me to get back and re-engage with the app. But with the new tuning features we announced, if I get annoyed by those notifications, I can easily turn them all off. So, for them, doing the right thing in the short term, while it might seem like the harder decision to make, I think it's going to pay off in the long run. So, in all cases, it's really important to consider how and when you're delivering notifications. And finally, we want to make sure your people are in control of the content that they're receiving. I mean there's a real trust relationship that's created when someone allows notifications from your app. They're making space for you to interrupt them with your content, right? And as we've discussed, it should be really valuable. But not all things have that same level of value or importance to all people. So, let's talk about giving everyone control on how their content is delivered, right? So, we've redesigned settings this year to more clearly visualize the different delivery methods of notifications. We hope that with more clear and graphic UI, it will make it easier for people to adjust how they're receiving your different notifications. And if you notice at the bottom here, we've also added a link to your third-party custom notification settings. And providing that link is even more important now because of our new tuning UI. Because if someone decides to tap turn off, we don't just immediately stop all the notifications from that app, but we present a confirmation step. So, in this action sheet, you could choose to turn off all notifications from the particular app, or tap a button, to go into that app's particular custom notification settings. And again, we want people to be in control of the types of content that they receive. So, while we hope that turn off everything button is a last resort, we want to encourage people to get into your app settings and customize the notification categories you have in a more granular way. So, with this extra visibility you know it's really important to make sure these settings pages are well-designed. So, let's look at a few examples. ESPN is an app with really detailed settings. And they've done a great job with their design. There's like a great sense of hierarchy and all of their content is tailored to my interests. And when I drill down and see the screen, they have specific details for each type of notification that you can receive. You know, basketball is different than baseball. Right. I have all the controls that I need to receive exactly the right notifications from ESPN. And "The New York Times" app is great. I have the options to configure the different categories or different sections that are alerting me. You know I might not find sports or politics helpful, but I can still receive breaking news and top stories. And I think this is really smart for "The New York Times" to have implemented. Because not everyone may appreciate interruptions every single category that they offer. So, for allowing this topic level control, it assures people to configure the experience to their liking without resorting to turning off all notifications from that app. So, it's really important, this set of control is supported in your app in a well-designed way. All right. So, those are the important things to remember when creating notifications. Now, you're going to be sending out some really valuable information. The next thing is to talk about notification grouping. And this is a new feature in iOS 12 to help everyone keep their notifications, you know all those great, and valuable content you'll be receiving, more organized. So, in iOS, we've always appreciated the convenience of seeing notifications on the lock screen. The chronological list is really helpful to organize. And it's great to see the content of notifications without having to interact with the device. You just pick up the phone and everything is there, it's all visible. But you know, there gets to be a point where that long chronological list starts to break down. Whenever there's a lot of different content coming on the screen, right? Especially multiple messages, or if you have group chats happening all at the same time, it's really difficult to follow. So, that's when we decided to start grouping notifications. Now, all your related content is organized together. A simple tap on each group expands it open and you can interact with each notification individually. And by default, notifications will group together by app. And for most cases, that make sense. But you know there's some circumstances when sorting them out in a more detailed way can be helpful. And messages is a perfect example. I don't think it would be the most useful thing to see every missed message from every conversation thread and group chat you're having all lumped together into a single group. So, to more clearly organize everything we create a new group for each conversation. And we call each individual group a thread. So, let's take a moment and talk about notification threads and what the best ways may be to organize your content. So, notification grouping uses the existing thread identifier API. This API was introduced as part of the notification content extensions. We expanded its use to create notification groups. The thread identifier can be any string that you want and the notifications with the same thread identifier are all grouped together. That's it. Super simple. So, threads make a lot of sense and you have separate conversations to group together. But what about other types of content? How should we handle grouping in those cases? Well, let's look at news. Each source is broken out into a separate thread. You can see there's a group from "The New York Times." A group for Quartz, and a group from "The Washington Post." And it's a really helpful way to find and organize content delivered from the news app. And let's look at another example. Podcasts. They really do the smart thing with their notifications. They create a special thread that groups together all of your new episodes. And what they do is they resisted the temptation to declare each individual Podcast a separate thread. Because remember, threading is about consolidating and organizing your content. So, when scrolling through your notification history, I think it's a much better experience to see all new Podcast episodes in a single group, rather than a bunch of discrete groups separated out and mixed together amongst all of your other missed notifications. Because, if there's a single group, a tap will expand it open and reveal all of the related content. Everything is together. Everything is organized. And everything is easy to find. So, while threads can be incredibly valuable, it's really important to not create too many. They should be used to highlight and distinguish meaningfully different types of content. So, remember it's okay to leave the default behavior of grouping all of your apps notifications together. Often that's going to be the best experience for people to find and interact with your content. All right, so that's notification grouping, and I think we're doing pretty good so far. You know, people have agreed to receive your notifications now. You're respecting their attention. You're sending good content at relevant times. If it makes sense for your app, you're grouping your content together by a few relevant threads. We're moving. Now, we need to make sure the rich notifications are rounding out the experience. And it's just so important, it's really important to create useful rich notifications. Because as I mentioned earlier, rich notifications are a way to provide more context and information to each notification. Now, we want people to take action on them, without losing the context of what they're doing. I say, like every notification should be like a little self-contained package of information to allow me to complete a specific task. I shouldn't have to launch into the app to find value in a notification. And photos is a great example. I can see that my friend added a new image to a shared photo stream. And with 3D touch, when I touch on it, I can see the big, full-sized version of the image that was added. And below there's a description. And with the quick action buttons, I can either like it, or leave a comment right in line. I don't need to leave the context of what I'm currently doing to take action on the notification. I tap like. I swipe it away and I'm back to the lock screen. Calendar is another great example. I see I have an invitation to an event on my lock screen. And I use 3D touch and press on it, I see my availability right in the notification. Again, I don't need to launch into Calendar to see my day. And then there's some great quick options below. I can accept the invitation right in line. So, if we look at messages, with our new notification grouping UI, my conversation with Heena is all grouped together. Now, when I press on the notification group, I'm seeing all of that group's content together. All of the individual platters that were stacked in the group are consolidated together in this view. I can read the entire thread right here. And now, in iOS 12, we've added interactivity to the rich notification views. So, you can double tap on a specific messages bubble and access our tap back UI. So, I tap like, and I can send that acknowledgment right back to Heena. So, as we mentioned, if you've created individual threads for your content, you can also have a threaded or consolidated rich notification. So, if we look at Podcasts. All of their new episodes are grouped together here on the lock screen. When I press on that stack, I get a single rich notification that summarizes each of the new Podcast episodes that have been released. There's a custom design with each show's artwork. The episode title and a short description. And because we can have discrete tap regions, there are separate play buttons for each episode. I mean, I just think this example is great. It checks all the boxes for making a great rich notification. There's detailed content, nice images, custom controls, and rich interactions. It's a great way to finish the notification experience. So, to summarize, when it comes to that first run experience, you have the question, did you ask for lock screen access, or deliver quietly in the notification center. Well, I think it depends on your content. Is it timely? Does it require an urgent response? Then go on, ask for permission to send notifications to the lock screen. If you're sending passive content that doesn't require an immediate response, then delivering directly to notification center could be the right approach. But either way it's your decision to make. This should be based on the needs of your app. Next, we need to make sure we're really providing value with our notification content and sending just like the highest quality notifications. So, they need to be meaningful content. We should be sending specific information. Notifications are not just a reason to launch into the app. And we've got to have a well-designed settings and configuration UI so people can easily tailor their notification experience from within your app. Notification grouping it's a great way to organize your content. So, by default, all notification from your app will group together. And you can use the threat identifier to create threads, if you need more nuanced grouping. But remember, only create threads when necessary. Don't overdo it. And finally, rich notifications should be created to provide that extra bit of content around the notification. Each notification should be a specific task to complete. You can add images, video, audio, and custom content. And now, interactivity to create a holistic notification experience. So, I mean that was a lot, right? That was a lot to take in, a lot to do, a lot to consider. But, it's not everything. There's still something else to consider. And for a lot of people it's the most important. And it's definitely the most personal part of the notification experience. The Apple Watch. So, I'd like to invite my fellow designer, Heena Ko here, to talk about making great notifications on the Apple Watch. Thank you. Thanks, Jon. Okay. So, we just heard about the importance of notifications on the phone. So, why think about notifications on Apple Watch? Well, we consider Apple Watch to be our most personal device. It states unlocked on your wrist, so you stay connected. But because it's so lightweight, you can stay focused on what you're doing. And particularly for Apple Watch, notifications are really great and incredibly effective. They're glanceable and the interactions are lightweight. And great notifications, they send you valuable and timely information. Or, they can encourage you to reach your health goals. And in some cases, they tell you critical information, like if you've had an elevated heart rate. Notifications are an essential part of the Apple Watch experience. In fact, we'd go as far as saying that notifications are the primary way people interact with apps on Apple Watch. There's another important reason why you may want to think about notifications on Apple Watch, and that's because they can be pushed to either device. You see, we coordinate alerts. So, we send them to the device that's most accessible to you. So, if your phone is locked, it's in your bag, in your pocket, which is a lot of the time, then that notification gets sent to the watch. So, you want to make sure that that notification looks great in both places. Okay, so here's a picture I took on a hike on Mount Tam the other week. It's really beautiful up there. While I was going on a hike without my phone, I was able to get this notification from Dark Sky about an upcoming thunderstorm. So, now, with Apple Watch Series 3 with cellular, notifications on the watch are more important than ever. I can go hours without my phone. And still receive notifications and stay connected. Okay, so you may ask, how do I make them look great on both devices? Well, we try to make it as easy as possible for you by giving you some stuff for free. So, let's take a look. Take this notification from one of my favorite Podcast Apps Castro. When that phone notification is pushed to the watch, some elements come with it with minimal work. Like, the image attachments, as well as the title, the body, and any relevant quick actions. So, here I'm able to add this Podcast app to my queue from my watch, so it's right in my phone when I want to listen to it. By simply adding a few additional elements, the watch's notification experience is so much better. Okay, so there are additional ways you can customize your watch notifications. You can add a sash color. You can add a background color. You can add images, icons and even inline video to make your notifications more visually rich. Okay, and now if you have a WatchKit app you can create interactive notifications. Interactive notifications are a new feature in WatchOS 5. They allow for more interactivity right in the notification, so you don't even have to launch the app. We're really excited about this one. Okay, so here's a notification from a fictitious ride sharing app. You guys know this. We all get these after every single ride. And occasionally I'll open the app and rate the ride right after the ride is over. But sometimes, actually a lot of times, I forget to do it. So, now in WatchOS 5 apps can create interactive notifications. Here's one from DiDi, I ride sharing app. So, they've included the ability to rate and pay right in the notification. I just have to rotate the digital crown, tap the stars, hit pay, and I'm done. So, this is a great example of an interface that clearly communicates the purpose of that notification, which is to encourage me to submit a rating after my ride. Pay By Phone is an app that allows you to pay for your parking spot remotely. It's really convenient when I'm still really far from my car and I need to extend my time. Here's a notification from them, letting me know that I only have 10 minutes left on my parking spot. It allows me to extend my time right in the long look. I just have to rotate the digital crown, and I tap steppers, and that's all I have to do to extend my time. So, this is a great example of a quick interaction. I was able to extend my parking time with just a couple of taps. So, amongst my friends, I'm oftentimes the person that chooses the restaurant and makes dinner reservations. And because of traffic and weather, you name it, people are always running late. So, here's an interactive notification from Yelp, letting me know that my table is ready. And these new notifications, they allow me to extend my checking time and in this one, for up to 45 minutes right in the notification. So, we don't need to give up our table if people are running late. Okay, so rich notifications are particularly good for moments of quick data input. Here's a notification from a fictitious medication reminder app. It's reminding me to take my medication before the end of the day. So, it not only reminds me to take my medication at the right time, but it also provides a range of ways to respond. I can say I took a single medication, or I can tap the take all button to say that I've taken both. So, this is really great, because this is something that I had to do every single day. Okay, so what do these notifications have in common? Well, they were informative. At the same time, they're succinct. They were visually rich. These used images, videos, and icons to make the notifications much visually richer. They're actionable. I was able to accomplish a tone of things without even having to open the app. And lastly, interactions on the watch are best when they're quick. We're going to make notifications richer, but we don't want to recreate the app experience. Okay, something you can do to make notifications even more effective is to get to know your audience. And tailor notifications to individuals. It can make a huge difference in how they experience your app. My Weather is an adorable app that sends me forecast notifications every morning and it's customized to my location. I really love receiving notifications from Wallet on my watch. It's especially great, when you're at the airport, and you're dealing with lots of luggage. Or you're at the store dealing with lots of groceries, they're also really handy when going to concerts. So, here's a concert ticket I got from Wallet. It arrived right when I got to the venue. It also contained a full-screen QR code that allowed me to enter the venue and I didn't even have to pull my phone out. This is a great example of customizing the timing of when I received a notification. It arrived right when I needed it and provided me with everything I needed for the event. Okay, so Qantas Airlines has really great interactive notifications, and they allow you to share your flight time with that clutch friend that's going to pick you up from the airport. Here, my friend Gabriel has just sent me his trip information through the Qantas app. It includes his ETA, as well as the option to set up a pickup reminder, which I'm totally going to use. Later in the evening, I'll receive the pickup reminder, along with a suggested time to leave. And it shows me exactly where I can pick him up, as well as the options for directions and it includes the option to send him a message letting him know that I'm going to be late. So, must know that I'm in LA. This is an excellent example of customizing notifications along an entire journey. They utilize time and location as well as just simply providing helpful tools to make sure people have a great experience from start to finish. Okay, so, at this point, everyone here should be a notification expert. But we covered a lot. So, let's do a quick recap. Okay the first run prompt. Notifications are oftentimes sensitive. But if they're not considering sending notifications directly to notification center. You won't be interrupting people and folks can read them on their own time. Providing value and sending great content. Remember notifications are about making human connections and conversation. They're also about delivering valuable information. Consider delivery. Consider providing ways for people to customizing notifications and incorporate things like time and location when sending them. Okay, notification grouping. So, iOS and WatchOS will group notifications by app by default. And most of the time, that should be totally okay. But consider threading related content to have discrete meaningful groupings. People are really excited about this one. Okay, rich notifications. Consider creating rich education so people can accomplish more right in the moment. And last, but not least, consider notifications on both devices. You'll be providing a great experience in any circumstance. So, the next time people hear this sound [ding], people will be delighted because you value their attention, and sent them something really great. Thank you guys. So, for more information about notifications, check out these related talks.  Hi, everybody. My name is Ari Weinstein, and I'm really excited to be here along with Willem Mattelaer to tell you about Siri Shortcuts. So, two years ago, we announced the first version of SiriKit, which enables your apps to work with Siri, and it has great domain-specific knowledge for things like sending messages, requesting rides, making payments, and more. But we know that you want to do even more with SiriKit, and so that's why, this year, we're so excited to announce Shortcuts. This central idea behind Shortcuts is that it lets you expose the key capabilities of your app to Siri, and this is really great, because it enables people to use the features of your apps in new ways and in new places. And it's a really powerful way to engage your users. So, exposing a shortcut opens up a ton of new possibilities for how people can use your apps, so let's explore all of the places where Shortcuts can be used. Shortcuts takes Siri's suggestions to the next level by surfacing what you want to do next, not just with your apps, but inside of your apps, and it does it by surfacing them in search at just the right time. And Siri can suggest shortcuts on the Watch, so you can use them right from your wrist on the Siri Watch Face. When Siri is confident that it's the exact right time for a Shortcut, it will be surfaced on the lock screen. And, when you tap on a shortcut, you can use it right in line with a rich custom UI specific to that app. And you can also add shortcuts to Siri, so you can use them with your voice just by asking Siri. And in Siri you'll see the same custom UI that you see in search and on the lock screen. And apps can provide a great voice experience in Siri by providing custom response dialog that Siri will read out loud to tell you things like how long it will take for your coffee to be ready. So, when the user adds a shortcut to Siri, they get to pick a custom personal phrase, so the user chooses what to say to Siri to invoke your shortcut. And, as a developer, you get to suggest what phrase to use, so, in this case, a suggestion is "coffee time." And, once a shortcut has been added to Siri, it works across all of your iOS devices, and on Apple Watch, and even on HomePod. So, we also have the new Shortcuts app, and with the new Shortcuts app, anyone can build their own shortcuts, and you do it just by dragging and dropping a series of steps together. And those steps can even include the shortcuts exposed by your app. So, today, we're going to talk about how to adopt shortcuts for your apps, and then we'll talk about how to optimize your shortcuts for great suggestions. And we'll also cover some privacy considerations that are important to keep in mind, and we'll talk about how to make great shortcuts for media playback. So, first, we'll talk about how to adopt the new Shortcut's APIs. There were three steps for creating a shortcut, and the first step is to define your shortcut. That means you have to decide what you want to expose as a shortcut and define each one so Siri knows everything that your app can do. Next, you'll need to donate your new shortcut. That means you need to tell the system every time the user does something in your app that you expose a shortcut for, which will let Siri learn when and where is the right time to suggest your shortcut. And the third step is to handle your shortcut. That means, when the user wants to go and use your shortcut in Siri, on the lock screen, in search, you need to be ready for your app or app extension to be invoked and be handed the shortcut to handle. So, before you define shortcuts, you'll need to decide what exactly it is that you want to expose. And you should start by thinking through what are the most important things that people like to do with your apps? Because those are the things that you might want to consider exposing shortcuts for. So, every shortcut that you expose should accelerate the user, to perform a key function of your app. That means it should take something that the user already wanted to do with your app and help them do it even faster. The acceleration that you offer should be substantial, so you shouldn't just expose a shortcut that does about the same thing as opening your app normally. If you expose a shortcut that doesn't provide very much acceleration to the user, it won't be suggested as frequently. And, next, every shortcut that you expose should be of persistent interest to the user, so that means it should be something that the user might want to do multiple times. It's not a good idea to expose a shortcut for something that the user might only want to use once or twice. And you should also expose only shortcuts that are executable at any time, because you can't rely on the user being in some particular state before your shortcut will be ready for use. So, once you've decided what shortcuts to expose, you're ready to check out the shortcuts' APIs. There were two APIs that we support for adopting shortcuts. The first is NSUserActivity. NSUserActivity is a lightweight way to represent the state of your app, and it integrates with other Apple features like Spotlight and Handoff. And the second is intents. Intents are a way of representing, in detail, a type of action that your app can perform, and Siri includes a bunch of great built-in intents that support a range of capabilities that apps can use to integrate with Siri. And, this year, we're introducing something really cool, which is the ability for you to define your own custom intents for use with Shortcuts. So, for every shortcut you expose, you'll need to decide whether it should be an NSUserActivity or an intent. So, let's talk about how to make that decision. Now, NSUserActivity is a great choice for building a shortcut. If you're just building a simple shortcut that opens something in your app, or if you're exposing a shortcut for something that you already index in Spotlight search, or that you already offer in NSUserActivity for, for example, for Handoff. But for the best Shortcuts experience, you'll want to adopt intents, and intents-based shortcuts are really cool, because they can run inline without launching your app. And they can include custom voice responses and custom UIs like we saw earlier. And they can also include granular parameter predictions, which Willem tell you a little bit about later. So, once you've decided how to adopt, you're ready to dive into the implementation, so let's do that now. First, let's go over how to adopt shortcuts with NSUserActivity, and the first step is to define your shortcut. For NSUserActivity, that means you need to declare a UserActivityType in your app's Info.plist file to register your activity type with the system. Next, you need to donate your shortcut. For NSUserActivity, that means every time the user is looking at the screen in your app that you want to provide a shortcut to, you should make an NSUserActivity object available. So, there's one key new thing here, which is the isEligibleForPrediction flag. Setting this on any user activity turns it into a shortcut, and what's really cool is if you have an existing user activity in your app, you just have to set this to true and that user activity will become a shortcut automatically. Now, note that the isEligibleForSearch flag has to be true in order for isEligibleForPrediction to have an effect. And, by the way, you might also want to consider the isEligibleForHandoff flag, which defaults to true. So, by default, all of your user activities will be able to be handed off between devices. Now, when you're creating your user activity, you want to make sure to include all of the information in your user info dictionary that you'll need to restore the activity later on. And the last step is to mark your activity current by attaching it to a UI viewController or a UI responder object that's onscreen. Now, the last step is to handle your shortcut, once you've defined and donated it. And every time the user uses an NSUserActivity from your app, it will be opened in your app. And so you need to handle it by implementing the app delegate method for handling NSUserActivity, which is called continueUserActivity. So, all you have to do is implement this method, check if the activity type matches the one that you registered, and, if it does, then you want to restore the state of your app to what it was when the user activity was saved. So, that's everything you need to do to get Siri to start suggesting shortcuts for your apps with NSUserActivity. Now, let's talk about how to expose Shortcuts through intents. The first step, again, is to define your shortcut. And so with intents, you'll need to start by deciding what type of intent you'd like to adopt. And, as you know, Siri includes many great built-in intents, like for messaging, workouts, lists, and more. And now we're introducing the ability to define custom intents in Xcode for Shortcuts. And so if the shortcut that you want to build corresponds to one of the default built-in SiriKit intents, then you should adopt that. But, otherwise, you can define your own. And whether you want to define your own or adopt an existing one to customize it, you want to get started by creating an intent definition file in Xcode. So, believe it or not, in my spare time, I've been working with a couple friends on a new app, and it's a soup delivery app, and it's called Soup Chef. It's the easiest way to order soup, and I'm really excited about the potential of hooking it up to Siri. So, let's step through how to use the new Intent Editor to build an intent for Soup Chef. I'm going to start by going to File, New File in Xcode, and I'll choose the SiriKit Intent Definition File type. And then I'll see the new Intent Editor. So, let's get started by clicking the plus button in the bottom left. Next, I want to give my intent a name. And, so in this case, I'm making an intent for ordering a soup, so I'm going to call it OrderSoup. Then I want to fill out my intent's metadata. So, let's look at that one step at a time. The first piece of metadata is category and defining a category helps Siri know how to talk about your intent and how to display it in the UI. So, for example, in this case, I'm choosing the Order category, and when I choose Order, Siri will say something like, "Okay, I ordered it," when I use it with Siri. And it'll display a button confirmation title like Order. So, we offer several different categories, and you should choose the one that matches your intent's purpose the most closely. Next, you want to fill out your intent's title and description, and that will be used in all the places where people can discover which shortcuts your app supports. And then there's the User confirmation required checkbox, which determines whether or not we'll ask the user for a confirmation before using your shortcut. So, Siri might say something like, "Are you ready to order with Soup Chef?" That's really great for my app, because I want to make sure that people aren't accidentally ordering soup [laughs]. So, then, you'll see the intense parameters. And these define all of the parameters that are passed to your shortcut. For example, in my case, I have two parameters. They are the list of items being ordered and the location to deliver to. Now, parameters support a short list of types, and those types include things like string, number, person, and location. But if you're building a parameter that represents an object that's in your app that's not on the short list of types then you can choose the custom type. Once you've defined your parameters, you'll want to look at shortcut types. These define all of the types of shortcuts that you'd like to be suggested to the user. Each type includes a particular combination of parameters that will be predicted, and formats for how to display the title and subtitle of each one, filling in the values for your parameters. So, if your shortcut can't be performed in the background because it needs to launch your app when you use it, you should uncheck the Supports background execution checkbox. Now, in some cases, one intent may have multiple shortcut types, where some support background execution and some don't. Now, in the case of Soup Chef, that's especially relevant, because we can support background execution when both the items and delivery location are predicted, because we have all we need to place an order. But if we add a shortcut type that only includes delivery location, Siri may predict that, and we won't actually have enough information to place an order. So, we'll need our app to open to ask the user what location to deliver to, and, in that case, we'll want to not support background execution for that shortcut type. Now, you should specify shortcut types for every variation of your shortcut that you think will be useful so that Siri can make the best predictions. And, for the best experience, you should-- on all the shortcut types, you can-- support background execution. Shortcut types that support background execution provide more acceleration to the user, and so they'll actually be suggested more frequently. You can provide up to 16 shortcut types for every intent that you define. Now, once your intent is defined, Xcode will automatically code generate an intent class and an intent handling protocol. In my case, we've generated the OrderSoupIntent class and an OrderSoupIntentHandling protocol with properties that correspond to what I just defined in my intent definition file. Because Xcode is doing code generation, it's really important to consider in which targets Xcode is doing this code generation, because you don't want to end up with duplicate instances of the same classes that conflict with each other. So, let's look at the targets in the target membership section of the inspector on my intent definition file. Your intent definition file should be included in every target in which your intent is used, so you should check the box under Target Membership. But you'll want to make sure that if you have a framework, you don't do code generation in multiple targets that are included from each other. So, if you have a framework, set the target membership to only generate intent classes for that framework by choosing intent classes and choosing no generated classes for every other target. But if you don't have any frameworks in your app because you haven't separated your app into frameworks yet, you should check the target for every single target. And that's how you define custom intents in Xcode. Once you've defined your custom intent, it's really easy to donate it. All you have to do is instantiate your intent object and then populate its parameters and create and donate an INInteraction object. Make sure to do this every time the user performs the equivalent of your shortcut in your app. In this case, every time the user orders soup, I want to donate this intent, because that's going to help Siri learn when is the best time to predict it? So, all that's left, now that we've defined our custom intent, is to handle it. And, just like with NSUserActivity, you should implement the continueUserActivity method in your app delegate in order to handle your intent. So, when an intent is opened in your app, it's passed in as an NSUserActivity object, whose activity type is the name of the intent class that you generated earlier. In this case, it's OrderSoupIntent. But if you only implement continueUserActivity, your shortcut will open your app every time. It won't run in the background, or work inline in Siri, or support things like custom voice responses. So, for the best experience, you want to create an extension to handle your shortcut in the background. And to do that, create a new target in your Xcode project and choose the Intents Extension template. Make your intent handler conform to the intent handling protocol. In this case, it's OrderSoupIntent handling and then implement these methods, which are confirm and handle. Now, note that unlike traditional SiriKit, you don't need to implement any resolve methods, because your intent is ready to go without any customization needed or any follow-ups from the user. So, in confirm, you should check all of the values of the properties of your intent to make sure that they're valid. And, if they're not, you should return an error code if you don't think you'll be able to handle that intent. And then you should handle, actually perform your shortcut. So, in this case, that means actually placing an order for the soup. And then you return a response object that indicates the outcome, such as success. Again, you should implement an intents extension for every shortcut that can run in the background, because then they'll run right in line inside of Siri, on the lock screen, in search, in the Shortcuts app, and on the Siri Watch Face, without having to launch your app. The most valuable shortcuts are the ones that run in the background, but you can also build a lot of great other shortcuts. And keep in mind that, even if you implement an intents extension, you should always implement continueUserActivity, because the user will have the opportunity to open the shortcut in your app from Siri, for example, by tapping on the card in Siri that shows your apps' custom UI. So, there's one more thing that I want to tell you about, which is INRelevantShortcut. Now, INRelevantShortcut is the way to expose shortcuts to the Siri Watch Face, and you do it just by providing INRelevantShortcut objects that include your intents or user activities. You can, optionally, include relevance information in your relevant shortcuts, which is a hint to the Siri Watch Face as to when your shortcuts will be relevant and when's the best time to show your shortcut? Now, the really cool thing about INRelevantShortcut is that it works even if you don't have a Watch app. So, you can actually expose relevant shortcuts from your iOS apps, and if they run in the background, they'll appear right on the Siri Watch Face. Okay, now that we've explored all the ways that Shortcuts can be used and how to adopt the APIs, I'm going to turn it over to my colleague Willem who's going to give you a great demo of how to adopt shortcuts in Xcode. Willem? Thanks, Ari. I'm so excited to be the first one to demo adopting Shortcuts. Before I dive into Xcode, let me show you the app we've been working on. As Ari mentioned, we've been working on a soup-ordering app called Soup Chef. Let me show it to you. So, this is Soup Chef. When I launch the app, I'm presented with my order history. Since I haven't ordered anything yet, this is still empty. I can place a new order by tapping the plus button. This presents me with the soup menu, where you can see the available soups I can order. Let's order a tomato soup. Next, I can specify the quantity and options for my soup. I'll order a single tomato soup with red pepper, and when I'm ready to order, I tap the Place Order button. I'm brought back to the order history where you can now see the order that I just placed. I can tap the order to see more details about a specific order. This view has an associated user activity. I think it would be pretty cool to suggest this to our users, since our users like to reminisce about the good soups they've ordered in the past [laughter]. And it would be great if this could be suggested to them. So, let's go to Xcode and see how we can do that. Here, I'm looking at the viewController for the order detail view. I'm creating a user activity, and I'm setting the requiredUserInfoKeys. I'm also making sure that it's EligibleForSearch. For it to be suggested, I also need to make it eligible for prediction. And that's it. Let's try it out. First, I need to make sure that I donate the user activity, so I'll go back to the view. That should be enough. Now we want to make sure that the donation actually happened, and, to do that, we added two new developer settings that make it super easy to see a recent donations in search and on the lock screen. To enable it, I go to Settings, and I scroll down to the developer section. At the bottom, there are two new toggles, Display Recent Shortcuts and Display Donations on Lock Screen. Let's enable both. I can now go back to the home screen and pull down to go to search, and I see the donation that I just did. Great. I can tap it, and the app is launched with that user activity, and I'm back to the order that I was just donated. Great. So, this is pretty good, but I think we can do a lot more. Since the main capability of our app is ordering soup, it would be great if this could be suggested to our users. And I want users to be able to do that without having to launch the app. So, I'll implement it using an intent. There isn't a built-in soup order intent, but now in iOS 12, I can create a custom intent, so I'll do that. And to start by creating an intent definition file, I'll go to File, View. File and select the SiriKit and then definition file. Click Next. Now I can give it a name. I'll keep it to Intents, and I'll put it in the Resources group. All right, and I'm ready. I click Create. I'm presented with our intent definition file and our new Intent Editor. Before I add the intent, I'll make sure to include the intent definition file in the right targets. As Ari said, we need to add it to all the targets where the intents are used. So, I'll add it to our shared framework, and, since we're using a shared framework, I don't want it to generate go for app target. So, in the dropdown, next to the app target, I'll select No generated classes. Now that that's done, we're ready to add our intent. I'll click the plus button in the lower left corner and select New Intent. I'll give it a name OrderSoup. Next, I'll select the category. In this case, it's an order intent. Fill out the title, OrderSoup, and the description, Order a soup from Soup Chef. Since this involves a real-world transaction, I want to use it to confirm this order before placing it, so I'll select User confirmation required. Next, let's define the parameters. I'm going to define three parameters, one for the soup, one for the quantity, and one for the options that the user selects. I'll start with soup. I click the plus button in the parameter section and fill out the name, soup. Since soup is a custom object of our app, I'm going to select the custom type. Next, I'll add quantity. I click plus again, fill out the name, and this is going to be an integer. Next, options. Again, options is a custom object of our app, so I'll use the custom type. And since the user can select multiple options, I'll also check the array checkbox. Finally, we need to define the shortcut types. At this point, I'm just going to define a single shortcut type that contains all the parameters. I'll click the plus button in the shortcut type section and select the parameters I want to include in the shortcut type, so I'll select soup, quantity and options. When I'm ready, I click the Add Shortcut Type button. I fill out the title Order, and then I'm going to insert the quantity and the soup with options, and I'll leave the subtitle empty, because I've already conveyed all the information I need to in the title. I'll also leave Support background execution checked, because I want the user to run this intent in the background. So, we just defined our first intent. Let's start using it. I'll start by adding two helper methods to our order object to make it easier to convert between it and the intent. I'll go to the order class and, at the bottom, I'll add an order extension. It contains a computer variable that returns an intent. In it, I create the intent. I set the soup quantity and options, and I return the intent. Extension also defines a new initializer that dates an intent. In it, I extract the soup, quantity, and options, and then I initialize the order with those values. Great, this will be very helpful. Next, we need to make sure that we donate the intent every time the user places an order. So, I'll go to the soup order data manager, and, in the placeOrder method, I'll add our donation logic. I create an INInteraction that contains the intent of the order, and then I just donate that interaction, and that's it. Finally, we need to handle the intent. I'll start by adding support in the app itself, so I'll go to the AppDelegate, and, in the continueUserActivity, I'll add support for it. I check if the activityType of the userActivity is equal to the class name of the intent that I want to handle. If that's the case, I extract the intent from the userActivity, and I generate an order with it. Finally, I present the order view. So, since we're launching the app, I'm assuming that the user doesn't want to immediately place the order but wants to make some customizations before placing it, so that's why I'm presenting the order view instead. And, finally, let's add support with an intents extension so the user can run this intent in the background. I'll need to add an intents extension first, so I'll go to File, View, Target and select the Intents Extension target. I'll give it a name, SoupChefIntents, and I click Finish. Since I've added a new target where I'll use the intents, I need to make sure that the intent definition file is included in that target. So, I go back to the intent definition file and add it to that target. Again, I don't want to generate code in that target, so, in the dropdown, I'll select No generated classes. I also want the extension to have access to the same data as the app, so I'll add it to the same app group. In the Project Settings, I select the intents target and in the capability step, I'll add it to the same app group. Great. Now, we're ready to implement the intent handler that was created with this target. First, I'll import our shared framework, SoupKit [laughs]. Since we're going to handle the OrderSoupIntent, this intent handler needs to conform to the OrderSoupIntentHandling protocol. This was generated as part of our intent. This protocol has one required method, the handle method. Let's implement it. In the handle method, I get an intent. I create an order from that intent, and, if that succeeds, I place the order, and I call the completion with an intent response with a success code. If I'm unable to create an order from the intent, I call completion with an intent response with a failure code. And that's it. I've just added support for a new intent in my app. Let's try it out. First, I'll need to donate the intent. So, I'll place another order. Click the plus button and this time I'm going to order a clam chowder with croutons, and I'll place the order. Now, if we go back to the home screen and pull down, I see my donated intent. Great. I can tap it, and I'm presented with a shortcut view and the order button. If I tap the order button, the order will be placed in the background, and it's ordered. I can go back to the app and see if it was actually ordered, and, yeah, there is a new order in my app. Great, so easy. I can also, instead of tapping the order button, tap the shortcut view, itself, which will launch the app with the shortcut, and, as I implemented it, it shows the order view, where I can customize the order. So, I'll add cheese, too. Who knows? And I'll place the order. Great. Finally, let's add the shortcut to Siri. I'll go to Settings, and I scroll up to the Siri and search. Here I can select the shortcut that I want to add to Siri. In this case, I'll add the order one clam chowder with croutons. When I tap the record button, I can say the phrase I want to associate with the shortcut. Soup time. Now, they got it. Let's take a step back and look at what we just did. We started by making an NSUserActivity that was already implemented in our app eligible for prediction. This allowed it to be suggested and is a really easy way to expose the content of your app. Next, we defined a custom intent. This is really the best way to represent a piece of key functionality of your app. In our case, that was ordering a soup. After defining the intent, we made sure that we donate the intent every time the user placed an order inside of the app. And, finally, we handled the intent, both in an extension to support background execution and in the app itself so the user can launch the app with the shortcut. So, now that we've seen how you can adopt shortcuts, let's take a look at how those shortcuts are suggested and what you can do to make sure your users see the best possible suggestions. Every time a user performs an action in your app and your app donates this, the system will look at the time, location, and other signals. For a time, we consider, among other things, the time of day and the day of the week. Well, for the location, we look at the overall location of the user and see if it's a significant location for that user. Let's see how the system now uses these to make a suggestion. Here, we'll just consider the time. Monday, at lunch, I order a tomato soup with croutons. That evening, I don't feel like croutons, so I order a tomato with red pepper instead. Next day at lunch, I, again, order a tomato soup with croutons. I keep doing this throughout the week, and, on Friday, at lunch, Siri might try to find a suggestion for me. It will look at my past behavior and try to find a pattern in it. And since it's lunchtime, and I usually order a tomato soup with croutons at lunchtime, Siri might notice this and suggest this to me, which is great. It's exactly what I wanted. So, this is all pretty high level, so let's take a look at how it actually works, starting with NSUserActivity. Imagine a user activity for the place order screen in Soup Chef. The user info dictionary could contain three keys, soup, quantity, and scroll position. The last one is there, so, in Handoff, you can bring the user back to the exact position they were last in. Let's take a look at how this can be suggested to the user. We start by donating a user activity where the soup is tomato, the quantity is 1, and the scroll position is 79 points. Next, we donate a similar user activity, but now scroll position is changed to 110 points. We keep doing this, and, at some point, Siri will try to find a suggestion again. It will look at past behavior and try to find a pattern of equal user activities. But since the scroll position is so inconsistent over time, it might not be able to find a suggestion. So, how can we fix this? We can use the requiredUserInfoKeys. RequiredUserInfoKeys is an existing property of a user activity. It represents the minimal amount of information that is necessary to restore the app to the state that the user activity represents. And for suggestions it will be used to determine which keys of the user info dictionary will be used to compare when looking for patterns. Let's apply this to a previous example. Now we specify that requiredUserInfoKeys are soup and quantity. Again, we donate the user activity where the soup is tomato, the quantity is 1, and the scroll position is 79 points. But now the scroll position will be ignored. Next, we donate something similar, and, again, the scroll position is ignored. We keep doing this, and, at some point, Siri will try suggestion again. Now, I will look back and try to find a pattern of equal user activities. And since it's no longer considering the scroll position, it might be able to say, "An NSUserActivity with soup tomato and quantity 1 is a good suggestion for this user." So, as you just saw, it's really important to specify the right set of keys as being required. Otherwise, your users might not get any suggestions at all. So, that's how it worked for user activity. Intents work similarly but offer you a lot more flexibility. The main signal for intents are the shortcut types that you define. Each shortcut type defines a combination of parameters that is valid for suggestion. It's similar to the requiredUserInfoKeys, but instead of having just one set, you can define multiple. Let's apply this to our Soup Chef app. Earlier, I defined an OrderSoupIntent with three parameters, soup, quantity, and options. At the time, I only defined a single shortcut type that combined all of these parameters, but, ideally, you would define more shortcut types, since that gives more options to the system to find patterns in your user's behavior. So, now I'll define three. One shortcut type that combines soup and quantity, one that combines soup and options, and one that combines all three parameters. Let's apply this to another example. Again, I start on Monday, at lunch, by ordering a tomato soup with croutons. The Soup Chef app donates this to the system, and the system will split us up into all the possible combinations based on the shortcut types that I just defined. That evening, I order a tomato soup with red pepper. Again, it gets donated, and it will be split up in all the possible combinations. Next day, at lunch, I order a tomato soup with croutons. It gets donated and split up. I keep doing this throughout the week, and, on Friday, at lunch, Siri might try to find a suggestion. It might see that I often order a single tomato soup. But since it's lunchtime, it also can see that, at lunchtime, usually order a tomato soup with croutons. Since that is a more specific shortcut, it will end up preferring to suggest that. So, I might get a suggestion to order a tomato soup with croutons. Great. So, that's how suggestions are made. Let's now take a look at how to make sure those suggestions are good, and it all starts from making good donations. A good donation should be something that is likely to be repeated. For user activity, there could be some content they might look at often. Or, for an intent, there could be an action the user would take regularly. You should make sure that the payload of what you're donating is consistent across all your donations, since that is what will be compared when looking for patterns. A good donation should also not include a timestamp, since that might not longer be relevant by the time this donation would be suggested. For instance, a shortcut that shows appointments for a specific day is probably not that useful, since, if you see that the next day or later, you're probably no longer interested in the meetings of that specific day. A shortcut with a relative time, however, is a lot more useful. You should also make sure that you donate exactly once for each user action, even if that user action involves multiple actions inside your app. Finally, selecting the correct parameters of your intent is also important. So, let's take a look at two possible types, starting with enums. You can define enums in your intent definition file next to your intents, and use it as the types of your parameters. We recommend that you use enums whenever the values for a parameter are bounded. For instance, if you would add a size parameter to a order soup intent, it might make sense to make that an enum, since the possible sizes are probably just small, medium, and large. Using an enum will lead to better suggestions and clearer titles and subtitles for your users. To learn more about how enums are used to generate titles and subtitles, I recommend that you watch the localization session. Another type you can use is the custom type. Using a custom type will result in an INObject and your generated intent code. An INObject combines an identifier with a display string. The identifier can be used to refer to an object internal to your app, while the display string contains the user readable representation of that object. That way both your users and your app always understands what the value of this parameter is. Finally, using an INObject also prevents the possible dependency between parameters. Let me illustrate that. There are two ways to represent the combination of an identifier with a display string. You could add two parameters to your intent, one for the identifier, one for the display string, or you could add a single parameter using the INObject. With the first approach, you have a dependency. Since the display string depends on the object as referenced by the identifier, we discourage having these dependencies as they could cause issues when we suggest this intent later. A good shortcut should also have an understandable title, subtitle, and image. It should represent what will happen when the user taps the shortcut, since it will be the only thing the user will see before interacting with it. And, of course, you want to test your shortcuts to make sure that they look right and that they behave as expected. There are a couple of ways to do that. As I showed you earlier, we added two new developer settings that allow you to see your recent donations on search and on the lock screen, instead of your regular Siri suggestions. By enabling these, you can see what your users will see when those donations would be suggested, and you can try interacting with them to make sure that they behave as expected. Another way to test your shortcuts is by adding them to Siri. An easy way to test them then is by editing the Xcode scheme to automatically invoke Siri without constantly having to say the phrase. In the scheme editor of an intents extension there is the Siri Intent Query field that you can use to provide the utterance to invoke Siri with. And, finally, you can create a custom shortcut in the Shortcuts app that uses your shortcut. This allows you to test the behavior of your shortcut when it's chained together with other shortcuts or steps from the Shortcuts app. So, now that we've seen what a good shortcut donation is and how does donation get suggested to your users, let's take a look at a couple of privacy considerations and how you can make sure your user are never upset by what is suggested to them. Your users expect that when they delete something from within your app that it's deleted for good. It's important to honor this so you maintain your users' trust and your so users aren't presented with suggestions for things that are not relevant to them anymore. So, if you donate shortcuts that contain information that the user can delete, you should make sure to delete those donations at the appropriate time. Let's take a look at how to delete donations, starting with NSUserActivity. There are two ways to delete a donated user activity. If you use Spotlight indexing and you set the relatedUniqueIdentifier, then deleting the content from Spotlight will automatically delete the donated user activity. Just set the relatedUniqueIdentifier on the contentAttributeSet to the identifier of the searchable item that it matches with. Then, when that searchable item would be deleted, it would automatically delete the user activity. If you don't use Spotlight indexing, you can use the persistent identifier property on NSUserActivity. This is a new property that you can set so you can keep track of your user activities and delete them at the appropriate time. To use it, just set the persistentIdentifier property before donating user activity. Then, when you want to delete it, you call deleteSavedUserActivities with the identifiers that you want to delete. You can also delete all your donated user activities. For instance, when your user logs out, you can do that by calling deleteAllSavedUserActivities. Intents have an existing API, which is similar to this new user activity API. Since you donate intents through INInteraction the leading and donated intent also happens through INInteraction. An INInteraction has an identifier and a group identifier property that you can set and then later use to delete one or more donated interactions. Just set the identifier and or group identifier before donating the interaction. Then, when you want to delete it, you call delete with an array of the identifiers that you want to delete. You can also delete all donated intents with a shared group identifier by calling delete with the group identifier. And, finally, just like NSUserActivity, there is a way to delete all your donated intents by calling deleteAll on INInteraction. Please make sure to delete your donations at the appropriate time. This will give your users the best possible suggestions and never make them wonder why they're seeing suggestions for things that are no longer relevant to them. So, now that we've looked at different things to consider when creating and donating shortcuts, let's end by briefly looking at what you can use to create the best possible media shortcuts. We created a new intent that works great with Shortcuts. This intent is called INPlayMediaIntent and allows you to create and donate shortcuts to play audio or video content. When handling this intent in your extension, you can choose to have it handled by your app in the background [inaudible]. This allows you to start audio playback straight from your app. Besides being suggested in search and on the lock screen, shortcuts with this intent will also be suggested in the playback controls on the lock screen when the user connects their headphones, making it even easier for them to start listening to your content. And, finally, shortcuts with this intent work great on the HomePod. Just add a play media shortcut to Siri on your iPhone and invoke it on your HomePod. The audio will start playing from your phone through HomePod. We also created a new API that allows you tell the system about new content they might be interested in. This is great for periodical content, where the content that you would want to be suggested to your users isn't something they've listened to or watched before. So, those are a couple of things that we have added to make great media shortcuts. Now, let's summarize what we just talked about. Shortcuts enable powerful, new experiences with your apps. It provides new ways to engage your users by exposing your app throughout the system in a variety of places. So, just search the lock screen, the Siri Watch Face on your Apple Watch, and in Siri itself. You can also use it in the new shortcuts app. You can adopt shortcuts by using NSUserActivity, which is a really easy way to expose the content of your app. Or you can use intents which provides you with a deeper integration in the system and can unlock entirely new experiences for your users. For more information, our session is 211, and you can find our page on developer.apple.com. You can also find us in the labs throughout the week. Thank you so much for coming. And we can't wait to see what shortcuts you'll create. Enjoy the rest of your conference.  So, good morning. Welcome to our session on What's New in ARKit 2. My name is Arsalan, and I am an engineer from ARKit team. Last year, we were really excited to give ARKit in your hands as part of iOS 11 update. ARKit has been deployed to hundreds of millions of devices, making iOS the biggest and most advanced AR platform. ARKit gives you simple to use interface for powerful set of features. We have been truly amazed by what you have created with ARKit so far. So let's see some examples from App Store. Civilisations is an AR app that brings historical artifacts in front of you. You can view them from every angle. You can also enable x-ray modes to have mode interaction. You can bring them in your backyard, and you can even bring them to life exactly they look like hundreds of years ago. So this is a great tool to browse historical artifacts. Boulevard AR is an all right app that lets you browse the work of arts in a way that's never been possible before. You can put them on ground or wall, and you can really go close to them, and you can see all the details. It's just a great way to tell you story of arts. ARKit is a fun way to educate everyone. Free reverse is an app that places immersive landscape in front of you. You can follow a flow of a river going through landscape and see communities and wild life. You can see how human activity impacts those communities and wildlife by constructions. So it's a great way to educate everyone about keeping the environment green and through sustainable development. So those were some of the examples. Do check a lot more examples from App Store. So some of you are new to ARKit, so let me give you a quick overview of what ARKit is. Tracking is the core component of ARKit. It gives you position and orientation of your device in physical world. It can also track objects such as human faces. Scene understanding enhances tracking by learning more attributes about the environment. So we can detect horizontal planes such as ground planes or tabletops. We can also detect vertical planes. So this lets you place your virtual objects in the scene. Scene understanding also learns about lighting conditions in the environment. So you can use lighting to accurately reflect the real environment in your virtual scene. So your objects don't look too bright or too dark. Rendering is what actually a user sees on the device and interacts with the augmented reality scene. So ARKit makes it very easy for you to integrate any rendering engine of your choice. ARKit offers built-in views for SceneKit and SpriteKit. In Xcode, we also have a [inaudible] template for you to quickly get started with your own augmented reality experience. Note also that Unity and Unreal have integrated full feature set of ARKit into their popular gaming engines. So you have all these rendering technologies available to you to get started with ARKit. So let's see, what is new this year in ARKit 2. Okay. So we have saving and loading maps that enables powerful new features of persistence and multiuser experiences. We are also giving you environment texturing so you can realistically render your augmented reality scene. ARKit can now track 2D images in real-time. We are not limited to 2D. We can also detect 3D objects in a scene. And last, we have some fun enhancements for face tracking. So let's start with saving and loading maps. Saving and loading maps is part of world tracking. World tracking gives you position and orientation of your device as our six degrees of freedom pose in real world. This lets you place objects in the scene such as this table and chair you can see in this video. World tracking also gives you accurate physical scale so you can place your objects up to the correct scale. So your objects don't look too big or too small. This can also be used to implement accurate measurements, such as the Measure app we saw yesterday. World tracking also gives you 3D feature points so you can, you can, you know some physical structure of the environment, and this can be used to perform [inaudible] to place objects in the scene. In iOS 11.3, we introduced relocalization. This feature lets you restore your tracking state after AR session was interrupted. So this could happen, for example, your app is backgrounded or you're using picture in picture mode on iPad. So relocalization works with a map that is continuously built by world tracking. So the more we move around the environment, the more it is able to extend and learn about different features of the environment. So this map was only available as long as your AR session was alive, but not we are giving this map available to you. In ARKit API, this map is given to you as ARWorldMap object. ARWorldMap represents mapping of physical 3D space, similar to what we see in this, on the visual, on the right. We also know that anchors are important points in physical space. So these are the places where you want to place your virtual objects. So we have also included plain anchors by default in ARWorldMap. Moreover, you can also add your custom anchors to this list since it is mutable list. So you can create your custom anchors in the scene and add them to World Map. For your visualization and debugging, World Map also give you raw feature points and extend, so you know the real physical space you just scanned. More importantly, World Map is a serializable object, so it can be serialized to any data stream of your choice, such as file on local system or to a shared network place. So this ARWorldMap object enable two powerful set of new experiences in ARKit. The first is persistence. So just to show you an example how it works, we have a user starting world tracking, and he places an object in the scene through ARKit hit testing. And before leaves the scene, he will save World Map on the device. So some point, sometime later, the user comes back, and he is able to load the same World Map, and he will find the same augmented reality experience. So he can repeat this experience as many times he wants, and he will find these objects on the table every time he will start his experience. So this is persistence in world tracking [applause]. Thank you. [applause] ARWorldMap also enables multiuser experiences. Now your augmented reality experience is not limited to a single device or single user. It can be shared with many users. One user can create World Map and share with one or more users. Note that World Map represents a single coordinate system in real world. So what it means that every user will share the same working space. They are able to experience the same augmented reality experience from different point of view. So this is a great new feature. You can use World Map to enable multiuser games, such as the one we saw yesterday. We can also use ARWorldMap to create multiuser shared educational experiences. Note that we are giving ARWorldMap object in your hands, so you are free to choose any technology to share with every user. For example, for sharing you can use air drop or multipeer connectivity that relies on local Bluetooth or WiFi connection. So it means that you don't really need an internet connection for this feature to work. :15 So let's see how ARKit API makes it very easy for you to retrieve and load World Map. On your AR session object, you will need to call get current world map at any point in time. This method comes with a completion handler in which it will return you and ARWorldMap object. Note also that it can also return and other in case World Map is not available. So it's important to handle this error in your application code. So once you have ARWorldMap, you can simply set initial World Map property in world tracking configuration and run your session. Note that this can be dynamically changed as well, so you can also reconfigure AR session by running a new configuration. So once AR session is started with ARWorldMap, it will follow the exact same behavior of relocalization that we introduced in iOS 11.3. It is important for your experience that relocalization works reliably. So it is good to, it is important to acquire good World Maps. Note that you can call get a current world map at any point in time. So, it's important to scan your physical space from multiple point of views. So tracking system can really learn about physical structure of the environment. The environment should be static and well textured so we can learn, extract more features of it and learn more about the environment. And also, it's important to have dense feature points on the map, so it can reliably relocalize. But you don't have to worry about all those points. In ARKit, API really makes things easier for you by giving you WorldMappingStatus on ARFrame. WorldMappingStatus is updated in every ARFrame and can be retrieved by WorldMappingStatus property. So let's see how this works. So when we start world tracking, WorldMappingStatus will be not available. As soon as we start scanning the physical space, it will be limited. The more we move in the physical world, world tracking will continue to extend the map. And if we have scanned enough physical world from current point of view, WorldMappingStatus will be mapped. So note that if you point away from a map physical space, WorldMappingStatus may go back to limited. So it will start to learn more about the new environment that we are starting to see. So how you can use WorldMappingStatus in your application code. Let's say you have an app that lets you share your World Map with another user, and you have a shared map button on your user interface. It's a good practice to disable this button when WorldMappingStatus is not available or limited. And when WorldMappingStatus is extending, you may want to show an activity indicator on UI. So this encourages your end user to continue moving in the physical world and continue scanning it and extending the map, because you need that for relocalization. Once WorldMappingStatus is fully mapped, you may enable your share map button and hide your activity indicator. So this will let your user to share the map. So let's see a demo of saving and loading World Map. Okay. So can we switch to AR 1. Okay. So for this demo, I have two apps. In one app I will retrieve and save World Map to a local file, and in my second app, I will load the same World Map to restore the same augmented reality experience. So let's start. So as you can see, WorldMappingStatus on top right corner. It was not available. As soon as I start to move in the environment, it is now extending my World Map. So if I continue to map and move in this environment, the WorldMappingStatus will go mapped. So it means that it has seen enough features from this point of view for relocalization to work. So it is a good time to retrieve and serialize World Map object. But let's make this augmented reality scene more interesting by placing a custom anchor. So through hit testing, I have created a custom anchor, and I am overlaying this object, basically it's a old TV. I think most of you may have seen in the past. So of course I can still continue mapping the world, and let's save the World Map. So when I saved my World Map, I could also show raw feature points that belongs to this World Map. So those blue dots that you see, they are all part of my World Map. And also as a good practice, I saved a screen shot of my point of view where I saved World Map. So now we have serialized World Map to our file. We can now restore the same augmented reality experience in another app. So let's try that. I will start this app from a different position, and you can see this is my world origin. It is defined on this side of the table, and my world tracking is now in relocalizing state. So this is the same opening relocalization behavior that we introduced in iOS 11.3. So let me point my device to the physical place where I created World Map. So as soon as I point to that same space, it restored my world origin back to where it was, and at the same time it also restored my custom anchor. So I have the exact same AR experience. Thank you. So now note that I can start this app as many times I want, and it will show me the same experience every time I start. So this is persistence. And of course, this can be shared with another device. So back to slides. So this was saving and loading map. It's a powerful new feature in ARKit 2 that enables persistence and multiuser shared experiences. In ARKit 2, we have faster initialization and plane detection. World tracking is now more robust, and we can detect planes in more difficult environments. Both horizontal and vertical planes have more accurate extent and boundaries, so it means that you can accurately place your objects in the scene. In iOS 11.3, we introduced continuous autofocus for your augmented reality experiences. IOS 12 comes with even more optimizations specifically for augmented reality experiences. We are also introducing 4 by 3 video formats in ARKit. Four by three is a -angle video format that greatly enhances your visualization on iPad because iPad also have 4 by 3 display aspect ratios. Note that 4 by 3 video format will be the default video format in ARKit 2. So all of these enhancements, they will be applied to all existing apps in the App Store except 4 by 3 video format. For that, you will have to build your app with the new STK. So coming back to improving end-user experience, we are introducing environment texturing. So this greatly enhances your rendering for end-user experiences. So let's say your designer have worked really hard to create these virtual objects for you, for your augmented reality scene. This looks really great, but you need to do more for your augmented reality scene. You need to have position and orientation correct in your AR scene so that object really looks like it is placed in the real world. It is also important to get the scale right so your object is not too big or not too small. So ARKit helps you by giving you the correct transform in world tracking. For realistic rendering, it is important to also consider lighting in the environment. ARKit gives you ambient light estimator that you can use in your rendering to correct the brightness of your objects. So your objects don't look too bright or too dark. They just blend into the environment. If you are placing your objects on physical surfaces such as horizontal planes, it is also important to add a shadow for the object. So this greatly improves human visual perception. They can really perceive that objection is on the surface. And last, in case of reflective objects, humans wants to see reflection of the environment from the surface of the virtual objects. So this is what environment texturing enables. So let's see how this object looks like in an augmented reality scene. So I created this scene yesterday evening when I was preparing for this presentation. So while eating those fruits, I also wanted to place this virtual object. And you can see, it is correct to scale, and you can see more importantly you can see a reflection of the environment in the object. On your right side of this object, you can see this yellow and orange reflection of those fruits on the right, and on the left, you can notice the green texture from the leaves. And in the middle, you can also see reflection of the surface of the bench. So this is enabled by environment texturing in ARKit 2. Thank you. So environment texturing gathers scene texture information. Usually it is represented as a cube map, but there are other representations as well. Environment texture or this cube map can be used as a reflection probe in your rendering engines. This reflection probe can apply this as texture information onto virtual objects, such as the one we saw in the last slide. So it greatly improves visualization of reflective objects. So let's see how this works in this short video clip. So ARKit, while running world tracking and scene understanding, continues to learn more about the environment. Using computer vision, it can extract textured information and start to fill this cube map. And this cube map is accurately placed in the scene. Note that this cube map is partially filled, and to set up reflection probes, we need to have a fully completed cube map. To have a fully completed cube map, you will need to scan your full physical space, something like a 360-degree scan you do with your panoramas. But this is not practical for end-users. So ARKit makes it very easy for you by automatically completing this cube map using advanced machine learning algorithms. Note also that all of this processing happens locally on your device in real-time. So once we have a cube map, we can set up reflection probe and as soon as we place virtual objects in the scene, they start to reflect the real environment. So this was a quick overview of how this environment texturing process works. Let's see how ARKit API makes it very easy for you to enable this feature. So all you have to do in your world tracking configuration to set environment texturing property to automatic and run the session. So this is as simple as this. AR session will automatically run this environment texturing process in the background and will give you environment texture as an environment probe anchor. AREnvironmentProbeAnchor is an extension of AR anchor, so it means it has a six degrees of freedom position and orientation transform. Moreover, it has a cube map in the form of metal texture. ARKit also gives you physical extent of the cube map. So this is area of the influence of the reflection probe, and it can be used by rendering agents to correct for parallels. So such as in case your object is moving in the scene, it will automatically adapt to new position and new texture will be reflected in the environment. Note that this follows same lifecycle as every other anchor such as AR plane anchor or AR image anchor. Furthermore, it is fully integrated into ARSCNView. So in case you are using SceneKit as your rendering technology, you just need to enable this feature in world tracking configuration. The rest is done automatically by ARSCNView. Note that for advanced use cases, you may want to place environment probe anchors manually in the scene. So for this, you will need to set environment texturing mode to manual, and then you can create environment probe anchors at your desired position and orientation and add them to AR session object. Note that this only enables you to place the probe anchors in the scene. AR session will automatically update its texture as soon as it gets more information about the environment. So you may use this mode in case your augmented reality scene has a single object. You don't want to overload system with too many environment probe anchors. So let's see a quick demo of environment texturing and see how we can realistically render augmented reality scene. So we can switch to AR 1. Okay. So for this demo, I am running world tracking configuration without environment texturing feature enabled. So as you can see on the bottom switch controller, it's just using ambient light estimate. And let's place the same object that we have seen before. And you can see it is, it looks okay. I mean you can see this on the table. You can see the shadow of it, and it looks like a really good AR scene. But what we are missing is that it does not reflect wooden surface of the table. So moreover, if I place something in the scene such as real fruit, we don't see a reflection of it in the virtual object. So let's enable environment texturing and see how it can realistically represent this texture. So as you can see, as soon as I enable environment texturing, the object started to reflect the wooden surface of the table as well as the texture from this banana. Thank you. So this greatly enhances you augmented reality scene. So it looks as real as possible, as if it is really on the table. Okay. Back to slides. So this was environment texturing. It's a powerful new feature in ARKit 2 that lets you create your augmented reality scene as realistic as possible. Now, to continue with the rest of the great new features, I will invite Reinhard on stage. It's working? Oh, okay, great. Good morning. My name is Reinhard, and I'm an engineer on the ARKit team. So next let's talk about image tracking. In iOS 11.3, we introduced image detection as part of world tracking. Image detection searches for known 2D images in the scene. The term detection here implies that these images are static and are therefore not supposed to move. Great examples for such images could be movie posters or paintings in a museum. ARKit will estimate the position and orientation of such an image in six degrees of freedom once an image has been detected. This pose can be used to trigger content in your rendered scene. As I mentioned earlier, all this is fully integrated world tracking. So all you need to do is set once in your property to set it up. In order to load images to be used for image detection, you made load them from file or use Xcode's asset catalog, which also gives you the detection quality for an image. So image detection is already great, but now in iOS 12, we can do better. So let's talk about image tracking. Image tracking is an extension to image detection with a big advantage that images no longer need to be static and may move. ARKit will now estimate the position and orientation for every frame at 60 frames per second. This allows you to accurately augment 2D images, say magazines, board games, or pretty much anything that features a real image. And ARKit can also track multiple images simultaneously. By default it only selects 1, but in cases, for example, the cover of a magazine, you may want to keep this set to 1, or in case of a double-page magazine inside a magazine, you want to set this to 2. And in ARKit 2 on iOS 12, we have a brand-new configuration called the AR Image Tracking Configuration that lets you do stand-alone image tracking. So let's see how to set it up. We start by loading a set of reference images, either from file or from the asset catalog. Once I'm done loading such a set of reference images, I use this to set up my session that can be of type world tracking by specifying its detection images property or of type ARImageTrackingConfiguration by specifying the tracking images one. Once I'm done setting up my configuration, I use this to run my session. And just as usual, once the session is running, I'll get an ARFrame at every update. And such and ARFrame will continue an object of type ARImageAnchor, once an image has been detected. Such an ARImageAnchor is now a trackable object. I can see this by conforming to the AR trackable protocol. This means it [inaudible] is tracked, which informs you about the tracking state of the image. It's true if it's tracked and false otherwise. It also informs about which image has been detected and where it is by giving me it's position and orientation as a 4 by 4 matrix. So in order to get such image anchors, it all starts by loading images. So that's good. Let's have a look at what good images could be. This image here could be found in a book for children, and in fact, it works great for image tracking. It has a lot of distinct visual features. It's we'll textured and shows really good contrast. On the other hand, an image like this, which could also be found in a textbook for kids, is not recommended. It has a lot of repetitive structures, uniform color regions, and a pretty narrow histogram once converted to gray scale. But you don't have to identify these statistics yourself as Xcode is here to help. So if I import these two images to Xcode, I see the sea life one without any warning, which means it's recommended, and the one about the three kids reading showing a warning icon, meaning it's not recommended. If I click this icon, I get a precise description why this image is not recommended to use image tracking. I get information about the histogram, uniform color regions, as well as the histogram. So once I'm done loading images, I'm left with two choices of configurations. First one is ARWorldTrackingConfiguration. So let's talk about that. When we use image tracking with world tracking, image anchors are represented in a world coordinates system. This means that image anchors optionally plane anchors. The camera and the world origin itself all appear in the same coordinate system. This makes their interaction very easy and intuitive, and what's new in iOS 12 now images that could previously only be detected can now be tracked. And we have a new configuration, the ARImageTrackingConfiguration, which performs stand-alone image tracking. This means it's independent from world tracking and does not rely on the motion sensor to perform the tracking. This means this configuration is not to initialize before it starts to identify images and could also succeed in scenarios in which world tracking fails, such as moving platform like an elevator or a train. I think in this case ARKit would estimate the position and orientation for every frame at 60 frames per second. And implementing this can be done in four simple lines of code. So all you need to do, I create a configuration type ARImageTrackingConfiguration and specify a set of images I'd like to track. In this case, I specified a photo of a cat, a dog, and a bird. I tell the configuration how many images I'd like to track. In this case, I specified this to be 2. In my use case, I imagined only 2 images will interact but not 3 at the same time. Note that if I'm tracking 2 images and a third comes into its view, it won't be tracked, but it will still get a detection update. And then I use this configuration to run my session. And as I mentioned earlier, you can also do this using world tracking by simply switching out these two lines. The only difference between image detection and tracking is the maximum number of tracking images. So if you have an app that uses image detection, you could simply add this, recompile, and your app may use tracking. So in order to show you how easy this really is, let's do a demo in Xcode. Can we go to AR 2? Yes. So for this demo I'd like to build a AR photo frame, and for this, I brought a photo of my cat from home. So let's build this using Xcode. So I started by creating a iOS app template using Xcode. As you can see by now it's pretty empty. Next, I need to specify which image I'd like to attach. For this I imported the photo of my cat, Daisy. Let's open her up here. That's my cat. I need to specify a name. I give it the name Daisy, which is the name of my cat, and I specify here the physical width of the image found in the real world, which is my photo frame. I also loaded a movie of my cat. So let's bring this all together. First, I will create a configuration, which will be a configuration of type ARImageTrackingConfiguration. I load a set of tracking images from the asset catalog by using the group name photos. This will contain only one image, which is the photo of my cat, Daisy. Next, I set up the configuration image tracking by specifying the tracking images property here, and I specify the max number of tracked images that we want. At this point, the app will already start an AR session and provide you with image anchors once an image has been detected. But let's add some content. I will load the video [inaudible] a AV player from the video by loading it from the resource panel. Now let's add it on top of the real image. So for this, I'm checking whether the anchor is of type image anchor, and I create an SCN plane having the same physical dimension as the image found in the scene. I assign the video player as the texture to my plane, and I start playing my video player. I create an SCN note from geometry, and I counter rotate to match the anchor's coordinate system. So that's it. This will run. Let's see it live. So once I bring the frame of my cat into the camera's view, the video starts playing, and I can see my cat interact. Since ARKit estimates position in real-time, I can move my device freely, or I can move the object. So I can really see that there's an update at every frame. Oh, oh, she just left. I guess it's the end of the demo, let's go back to the slides. So as you can see, it's really easy to use image tracking in ARKit. In fact, it's much harder to make a video of your cat. So image tracking is great at interacting with 2D objects, but we're not limited to plainer 2D objects, so let's talk next about object detection. Object detection can be used to detect known 3D objects in the scene. Just like image detection here, the term detection means that this object needs to be static and can therefore, or should therefore not move. Great examples of such objects could be exhibits in a museum, certain toys, or household items. And like image detection, objects need to be scanned first using an iOS app running ARKit. For this we offered the full source code of a full-featured iOS app that allows you to scan your own 3D objects. Such objects have a few properties such that they need to be well textured, rigid, and nonreflective. And they need to have roughly the size of a tabletop. ARKit can be used to estimate the position and orientation of such objects in six degrees of freedom. And all of this is fully integrated into world tracking. So all you need to do is set one single property to get started with object detection. So let's have a look how it can be set up. I load a set of AR reference images from file or from Xcode asset catalog. I will talk about the reference objects in a second. Once I'm done loading these reference objects, I used them to set up my configuration of type ARWorldTrackingConfiguration by specifying the detection objects property. When I'm done setting up my configuration, again I run my session with it. And just as image detection, once the AR session is running, I get an ARFrame with every update, and in this case, once an object has been detected in the scene, I will find an AR object anchor as part of my AR frame. Such an AR object is a simple subclass of AR anchor. So it comes with a transform, which represents its position and orientation and six degrees of freedom as well as it tells me which objects has been detected by giving me a reference to the AR reference object. And implementing this can be done with three simple lines of code. I create a configuration of type ARWorldTrackingConfiguration and specify a set of objects I'd like to detect. In this case, I envision to build a simple AR museum app by detecting an ancient bust and a clay pot. And I use this to run my session. So in fact, at the office, we build a very simple AR museum app, so let's have a look. So once this bust gets into view of my iOS app, I get a six degrees of freedom pose and can use this to show the scene, very simple infographics floating about the statue. In this case, we have simply added date of birth, the name of this Egyptian queen, which was Nefertiti, but you could add any content that your rendering engine allows you to use. In order to build this app, I had to scan the object first. So let's talk about object scanning. Object scanning extracts accumulated scene information from the world. This is very much related to plane estimation in which we use accumulated scene information to estimate the position of a horizontal or vertical plane. In this case, we use this information to gather information about the 3D object. In order to specify which area to look for the object, I just specify a transform and extend in the center. This is essentially a bounding box around the object just to define where it is in the scene. Extracted objects are fully supported by Xcode's asset catalog, so it makes it really easy to [inaudible] a new app and reuse them as many times as you want. And for scanning, we added a new configuration, the ARObjectScanningConfiguration. But you do not need to go ahead and implement your own scanning app as the full sample code is available for full-featured scanning app called Scanning and Detecting 3D Objects. So let's have a look how this app works. I start by creating a bounding box around the object of interest, in this case, the statue of Nefertiti. Note that the bounding box does not need to be really strict around the object. All we care is that the most important feature points are within its bounds. When I'm satisfied with the bounding box, I can click press scan, and we start scanning the object. I can see the progress going up and [inaudible] representation indicating how much of the object has been scanned in a spatial manner. Note that you do not have to scan the object from all sides. For example, if you know that a statue will be facing a wall in a museum, and there is no way that you could detect it from one specific viewpoint, you do not need to scan it from that side. Once you're satisfied with the scan, you can adjust the center of the extent which corresponds to the origin of the object. The only requirement here is that the center stays within the object's extent. And lastly, the scanning app lets you perform detection tests. So in this case, detection was successful from various viewpoints, which means it's a good scan. And our recommendation here is also to move the object to different location to test whether detection works with different texture and under different lighting conditions. Once you're done scanning, you will obtain an object of type ARReferenceObject, which we have seen earlier in the diagram. This object can be serialized to usually and AR object file extension type. It has a name, which will also be visible in your asset catalog as well as the center and the extent used for scanning it. And you will also get all the raw feature points found within the area when you performed your scan. So this was object detection. Keep in mind, before detection object, you need to scan them, but there is the full source code available for you to download it right now. So let's talk next about face tracking. When we released the iPhone X last year, we added robust face detection and tracking to ARKit. Here, ARKit estimates the position and orientation of a face for every frame at 60 frames per second. Here we can use the, this pose can be used to augment a user's face by adding masks, hats, or replace the full texture of a face. ARKit also provides you with a fitted triangle mesh coming to form of the ARFaceGeometry. This type, the ARFaceGeometry, contains all the information needed to render this facial mesh, and it comes in the form of all vertices, triangles, as well as detection coordinates. The main anchor type of face tracking is ARFaceAnchor, which contains all information needed to perform face tracking. And in order to render such geometry realistically, we added a directional light estimate. Here, ARKit uses your light as a light probe and estimates this ARDirectionLightEstimate, which consists of the light intensity, direction, as well as the color temperature. This estimate will be sufficient to make most apps already look great, but if your app has more sophisticated needs, we also provide the second-degree spherical harmonics coefficients that gather lighting conditions throughout the entire scene for you to make your content even look better. And ARKit can also track expressions in real-time. These expressions are so-called blend shapes, and there's 50 or more of them. Such a blend shape assume a value between 0 and 1. One means there's full activation. Zero means there is none. For example, the [inaudible] open coefficient will assume a value close to 1 if I open my mouth and a value close to 0 if I close it. And this is great to animate your own virtual character. This example here, I've used the jaw open and eye blink left and eye blink right to animate this really simple box face character. But it can do better than that. In fact, when we built an emoji, we used a handful more of such blend shapes. So all the blue bars you see moving here were used to get over the head post to map my facial expressions on the panda bear. Note that ARKit offers everything needed for you to animate your own character just like we did with an emoji. Thank you. So let's see what's new for face tracking in ARKit 2. We added gaze tracking that will track the left and the right eye both in six degrees of freedom. You will find these properties as members of ARFaceAnchor as well as a look-at point, this corresponds to reintersection of the two gaze directions. You may use this information to animate again your own character or of any other form of input to your app. And there's more. We added support for tongue, which comes in the form of a new blend shape. This blend shape will assume a value of 1 if my tongue is out 0 if not. Again, you could use this to animate your own character or use this as a form of input to your app. Thank you. So seeing myself sticking my tongue out over and over is a good time for summary. So, what's new in ARKit 2. Let's have a look. We've seen saving and loading maps, which are powerful new features for persistence and multiuser collaboration. World tracking enhancements simply shows better and fasting plane detection as well as new video formats. And with environment texturing, we can make the content really look as if it was really in the scene by gathering texture of the scene and applying it as a textured object. And with image tracking, with image tracking, we are now able to track 2D objects in the form of images. But ARKit can also detect 3D objects. And for face tracking, we have gaze and tongue. All of this is made available for you in the form of the building blocks of ARKit. In iOS 12, ARKit features five different configurations with two new additions, the ARImageTrackingConfiguration for stand-alone image tracking and the ARObjectScanningConfiguration. And there's a series of supplementary types used to interact with the AR session. The ARFrame, the ARCamera, for example. And this got two new additions, the ARReferenceObject for object detection and the ARWorldMap for persistence and multiuser. And the AR anchors, which represent positions in the real world, the anchor types. Got two new additions, the ARObjectAnchor and the AREnvironmentProbeAnchor. I'm really excited to see what you guys will build with all these building blocks in the ARKit available in iOS 12 as of today. There's another real cool session about integrating ARQuickLook into your own application to make your content look simply great. With this, thanks a lot, and enjoy the rest of the conference.  Hello. Good morning, everyone. And welcome to this year's session on HTP Light Streaming. My name is Emil Andriescu. Today's talk is about measuring and optimizing HLS performance. First, let's reflect for a second on why we should care and why it is essential for application. Let's pretend it's Saturday night, you're in your favorite spot on the couch, you've skillfully browsed through all the reviews, title is set, popcorn is ready, and you eagerly tap play when this happens. Faced with this mysteriously and never-ending animation, you ask yourself, what could be worse. Let's face it, you know it, it's a playback error. But what do customers really expect from HTP Light Streaming? Well, they expect high-definition image, high fidelity sound, and instant media response when they tap play. Yet, streaming applications over the internet are always at the mercy of the network, so how do we reconcile? Well, HTP Light Streaming was designed to address this, that is to provide the best possible uninterrupted streaming experience in an unpredictable network environment. So why are we here? Well, there's more. Over time, HLS has evolved into a more dynamic ecosystem, supporting new offering features such as I-frame playback, new media formats, and of course new codecs. At the same time, we're constantly adding powerful iOS, tvOS, and macOS APIs such as you can tune and adjust playback to your target audience and provide a much richer user experience. Delivery patterns and transport protocols, they are also evolving, so it is important to look at your server side performance in connection to how content is being consumed, either on a mobile device or in the living room. Given all these options, how can you be sure that you are providing the best possible user experience to your audience? Well, the first step is to understand and quantify the user experience in conjunction to changes that you make to your content, application, or delivery. This is an area where we believe it is imperative to measure rather than to guess which configuration is optimal. What is this session about? First, we want to establish a common language for discussing streaming quality of service. Second, we want to discuss how to objectively measure your application streaming performance. Third, we want to help you identify and solve some of the problems that impair streaming quality of service. And finally, we want to get those master playlists right. This is because many of the problems and issues that we see with streaming quality are actually rooted in the authoring of the master playlist. Before going into detail, let's begin with a brief overview of an HLS playback session. As you'd expect, it begins with the download of a master playlist. Once the playlist is passed by AV Player, it knows what content it refers to. In this case, we have two bitrates, 1 megabit and 2 megabit. AV Player will pick one of these, will go ahead and download a media playlist together with additional artifacts such as keys, and then continue to download media segments until the buffer level is sufficient for playback. When that happens, the AV Player item will communicate a prediction of playability by setting the is playback likely to keep up property to true. If you've preset the AV Player Rate to 1, so you're using the Autoplay feature of a AV Player, the player will go ahead and start playback immediately. We call this Time Interval Startup Time. From this point on, the wall clock, also known as real time, and the player item time base will advance at the same speed, with one condition, which is that content must arrive at an equivalent or faster rate than that which AV Player is consuming. If that's not the case, AV Player will try and switch down to the 1 megabit here. If network still cannot keep up with real time at 1 megabit, well the buffer will eventually run dry, and AV Player has no choice here, it needs to stop playback, event which we call a stall. The player will remain in this state not only until data starts flowing again, but up until there's a sufficient level of buffer for the player item to trigger another positive playability prediction. After that, playback can continue normally. Now let's discuss about quantifying the user experience for such a session. We do that by defining a set of Key Performance Indicators or KPIs. We picked five of them that we believe are most representative for HTP Light Streaming. One question you may ask is how much time do my users spend waiting for playback to start. Is it one second? Is it five seconds or maybe 30 seconds? This is an essential point in terms of user experience. Further, playback stalls, like the one we just saw, they are disruptive to the user. We care both about how often they occur, but maybe more importantly, how long does it take to recover from a stall? Yet, the best strategy without knowledge of the future to not stall is to deliver content at the lowest available bitrate. But of course that's not what we want. We want to deliver the best audio and video quality while still not stalling. So there's a clear tradeoff between risk of stalling and media quality, right. For that, we need another measure of the overall media quality for a session. And finally, playback errors. We talked about that. They are more disruptive than stalls, right. What can we do to track playback errors? Okay. Let's begin with startup time. There are multiple APIs that you can use to obtain or compute startup time. First, don't use the AV Player status changing to ready to play. That doesn't tell you that playback is going to start. However, if you are using Autoplay, so you're setting the rate, the player's rate in advance, you can use the AV Player item status changing to ready to play or the AV Player item is playback likely to keep out changing to true. These are observable properties. When that happens, you know that playback is about to start, but there might be a few milliseconds before playback actually starts. So what we recommend is to either use the AV Player time control status changing to playing or to track the player item time base, and there's a notification that allows you to do that. AV Player relies on heuristics to avoid stalls, but we know it, sometimes they're unavoidable. You can monitor as stalls occur by registering to the AV Player item playback stall notification. The suggestion here is to count the occurrence of stalls. Of course, if you want to compare and aggregate stall behavior across sessions of different duration, then you need to normalize this. How do you do that? Well, we recommend that you use the total duration watched and compute the stall rate in terms of stalls per unit of time watched, such as stalls per hour. A stall of 30 seconds is much worse to the user than a stall of one second. This is why we also care about rebuffering time or stall duration. By measuring the time interval between playback stalled notification and when the player item time base changes back to 1, you can compute an accurate stall duration. Again, the total duration can be normalized using the duration watched of the session. Well, you might be wondering at this point, how do I compute the duration watched of a session? And the answer is, through the Access Log. Let's see how we do that. So this is a snippet of code. First, we get a reference to the Access Log from the player item. We iterate through the events in the Access Log, and we simply sum up each events duration watch. And there you have it. We computed a total duration watch for a session. And now you may be wondering, well what is this event? What is an event in the access log mean? Well, for that let's look at how AV Player Item Access Log works. So the AV Player Item Access Log provides a history of your session. It is initially null, but as playback occurs, you're going to receive an AV Player Item New Access Log Entry notification, and by that time, you'll have an Access Log. You'll see that events in the Access Log contain information on various areas such as the current variant URL, the current bitrate, duration watch, number of stalls, and so on. These values are initially by convention initialized to a negative value or null. As playback occurs, they are updated with actual measurement data and the actual variant URL that you're playing. There are two cases in which we will add new events to the Access Log, and that is variant switch, like in this case, or a playback seek. But before a new event is added, the old one becomes immutable, and then we add the new event. Now, keep in mind that while these values here are constantly updating as playback occurs, so the values in the last event, they are not observable properties. We also mentioned that we care about media quality. How do we compute that? A way to measure if the user is presented with the best possible media quality is of course to look at the video bitrate being served. Here we don't really care about the startup time or the stall duration, so let's remove those. So we're left with the playback state. In this example, we see that we played for a longer time on the 2 megabit variant and less time at 1 megabit. By time weighting each bitrate, we can obtain a single value of video quality that we can compare across sessions. We call this measure a Time-Weighted Indicated Bitrate, and computing it is just as simple as with the total duration. Once again, we get a reference to the Player Items Access Log. We iterate through the events in the log. We compute the time weight of each event with respect to the total duration watch we computed earlier, and finally, we sum up the weighted bitrate value. Now keep in mind that some of these properties may not be initialized, so do the appropriate checks in your code. Another event which you must absolutely track is of course playback failure. To do that, you observe the AV Player item status. If the value ever changes to false, it means AV Player encountered an unrecoverable error. A good way to transform this observation into a KPI? Well, one way to do it is to look at the percentage of failed sessions with respect to total sessions, but there might be other ways to do it. One thing I want to stress here is that not all errors in your stream may be fatal. Some may impact media quality while some might not even be perceivable by the user. But nonetheless, if there are errors, they convey that there is an issue with your stream. So how do I get more insights on the stream, right, what happened? And the answer is from the Player Item Error Log. The AV Player Item Error Log. The Error Log conveys failures with varying degrees of user impact. It works in a similar fashion as the Access Log except that events represent error rather than player access states. They cover various areas, such as delivery issues, network issues, content authoring errors, and so on. For instance, they can give you an insight on why a stall occurred, such as no response for a media file for about ten seconds. So we talked about startup time that you can track for every session. We encourage you to take a look at the distribution of startup times for your application. We also talked about stall occurrence and stall duration. We mentioned that Time-Weighted Indicated Bitrate is a good indication of experienced media quality across a session, and finally, you probably want to keep the percentage of failed sessions as low as possible. Keep in mind that not all KPIs are comparable across sessions. One example of that is that AV Player foundation uses the AV Player layer size on the screen to evaluate illegible variants for HLS. So for instance, if you've got 10 ATP content, it will probably not be displayed on a 200 pixel view, but it doesn't mean the user experienced poor image quality. What to do then? We recommend that you gather additional context information along with your streaming metrics. This will allow you to later partition your playback sessions in classes that make sense for your application. Sample code for this section is available on the Apple developer website as part of the HLS catalogue sample. Now, please let me welcome Zhenheng Li, who will talk to you about ways to improve HLS performance. Thank you. Thank you, Emil. Hello, everyone. My name is Zhenheng. We have discussed all the KPIs that our users care the most. In this part of talk, let's focus on ways to improve these APIs. We will look deeper in three areas. One, how to reduce the startup time. Two, how to investigate and avoid stalls. Three, how to investigate and avoid errors. Let's get started. So what can delay start of playback? Here is an example of the operations from the user clicks play until the video start to play back. The application create every asset and start inspection of the asset to find out durations and awardable media options of the asset. It takes a few round trip between the device and the content server to download the master playlist and [inaudible] playlist. After that, the application create AV Player and AV Player Item. Buffering starts. Oftentime, buffering is interrupted, content is encrypted. It takes a few round trips between the device and [inaudible] to fetch the decryption keys. Once the keys are fetched, buffering resumes. However, it may be interrupted again. Let's say the application offers a feature, resumes from the previously watched point. Application sets a sic time, a set [inaudible] time on the player on behalf of the user. Every player discard the existing buffer and start download from a new location. Segment 100. Again, it maybe interrupted. Users has a language preference setting in the application. She or he prefers Spanish audio. Thus, application sets media selection on the player item, existing audio buffer being discarded, player start downloading from a different language variant. In a few seconds later, player item notifies playback is like to keep up, application sets a rate. Playback starts, and it continues. All this time, user is waiting. So as we can see, it does take a few time-consuming operations to start up, run a trip between the device and the content server and the key servers. Round trip times between AV Player and applications, oftentimes these two sit at different processes. So how the application measures the time cost and startup time? It may measure the time spent between the API calls and the player and the Player Item status change notifications. Every player item also offers startup time in the Access Log. This time is measured by the AV Player item, represents the time for buffering only. It's measured from the start media downloading until the first playback is selected to keep up. So our user wants the video to start fast, in at most a few seconds. There are ways to achieve that. One option, we can move some operations to a different stage before the user clicks play. For example, AV Asset creation and inspection can be moved out. Key fetching can be moved out. Thus, when the users starts a video playback, there is less waiting time. So where do we move those operations to? While your user is viewing the video catalogue or video info, it's a good time to create an inspect AV Asset before the user decides to play. Now, last year we had introduced AV Content Key Session API. This new API decouples the media load from key fetching. It gives the application total control on key management. It offers ways to optimize key fetching, such as bundling up multiple key requests back to the key server. If you happen to adopt AV Content Key Session, spending a few hours of engineering hours, your user will notice a faster startup time. So we have moved the AV Asset creation and key fetching out of startup time. Now what's left is mainly the AV Player Item buffering time and the communication time from AV Player and your application. Oftentime, app may be able to avoid buffering, such as due to [inaudible] or due to media options. We can even try to reduce the round trip time between the player and the application. Thus the startup is further reduced. Let's take a look. When you create AV Player Item, if you know where your user is intending to start the playback, set the current time on player item. If you know what are the media options such as which language to download for playback, set that as well before you set the AV Player item onto the player. Same with the AV plyer. As soon as the user click play, set rate before the start downloading for the AV Player Item. Thus, the playback will start automatically as soon as Player Item has enough to play back. In summary, set up AV Player before buffering. Set AV Player rate before setting the player item onto the player. A lot of application offers a feature to allow the user choose multiple videos and play one after another, such as binge watching TV episodes. We have seen implementation such as one player and one player item per video. There's always a startup buffering time for each new video. You may reduce that buffering time for the new video by AV Queue Player. Create multiple player items, include them all on the play queue. While the player is playing the current item, when the media download finishes for the current item, player will start downloading for the next one while the current one is still playing. Thus, the next player item will start playback as soon as current event play to the end. So do use AV Queue Player to play multiple items and enqueue second AV Player Item well in advance. So what's left now? Buffering time. First, what determine network buffering time. Four factors. The choice of your variant, the content bitrate, your playlist target duration, and of course, last, the network bandwidth. Let's take a look a few examples of buffering time. First, it's a simple master playlist. It specifies an ATP video at about 5 mbps. Let's assume the network bandwidth is around 6 mbps. Our target duration is 10 seconds. In most of the cases, player item buffers one segment before it notifies playback it like to keep up. However, the same master playlist, almost the same network condition, the user may observe slower startup. The reason is, remember, the network bandwidths change, and the content bitrate also change. In this case, there are a few segments take longer to download. Thus, it takes longer to start. To solve this problem, offering a variant with lower bitrate may help. Player may decide to switch down and start up sooner. When all other information is absent, the first listed variant will be your startup variant. So in this example, same two variants. The lower bitrate is listed first with same network condition. Player will start faster, start up faster and also switch up pretty quickly given the network bandwidth is sufficient for playback. In summary, to reduce network buffering time, make a wise choice of initial variant. Lower content bitrate means shorter buffering time, but it is a tradeoff of video quality. If you are offering multiple media formats such as HDR and SDR videos or stereo audio and multiple-channel audios, make sure the initial variant for each media format are on similar level of bitrate so your user will have a similar experience regardless what kind of viewing setup they have. That's all about reduce startup time. Our video has started. Next, let's talk about stalls. To be really clear, stalls can happen, especially when the network bandwidth is really low. So in this part of talk, let's focus on how to investigate stalls and how to improve or avoid stalls. How the application investigate stalls. The application should be listening to the stall notification at all time. And the application should be also checking the AV Player status such as is playback likely to keep up. AV Player Item also offers Error Log and Access Logs. The application should be listening to an exam those logs when the stall happens. Next, let's take a look two stall examples. First, stall notification has been received by the application. The application should have received the Error Log as well. The error comments give you detailed information on what has happened. In this case, it says media file not received in 15 seconds. Application checks Access Log to find out what the AV Player was playing at the moment when the stall happened. It tells you detailed information such as the player was playing what content and such URI. The indicated bitrate is the content bitrate. In this case, 36 mbps, and that is a [inaudible] content. An observed bitrate is the current network bandwidth. In this case is 2.8 mbps. It's obviously due to the network bandwidth can't catch up with the content bitrate. So to deal with variable networks, remember to provide a full set of bitrate. Remember some of your users may have a slower network connection, or your user may be on the go, such as on cellular while viewing the video. If you're offering multiple video, multiple media formats, each codec combination needs it's own set of tiers. Not all stalls are due to network condition. Let's look at this one. Stall happened, Error Log tells you a different story this time. It says playlist file unchanged for two consecutive reads. If you check the Access Log at the time, player was playing live. They indicated the bitrate is rather low. The content is about 400K, and the network bandwidth is 3.7 mbps. This look like a content delivery issue. So to reduce or to avoid stalls due to content delivery, content server and CDN must deliver media files, segments, keys without any delay. Update live playlist at least every target duration. The CDN [inaudible] must be configured to deliver most recent playlist to avoid stale playlists. Synchronized discontinuity sequence number between playlist. Indicate server-side failure clearly using right HTTP status code. That's all about stall. What about error? How do we investigate errors? There are a few ways. We have Error Log and Access Log from AV Player Item. We also have error property from every player and player item that the application can observe. In addition, we have some media validation tools for you to detect the content issue. Let's look at them one by one. AV Player Item Error Log, they have talked a little bit about [inaudible] in this one. This type of Error Log is an indication that there is a problem with network or content format. However, they are not always fatal. When the error is indicated, playback may be perfectly fine at that moment. However, the application showed the check in the error comments to find out details, such as this one. We have seen it before, media file not received in 15 seconds. So it's an indication that your user may have observed or will observe stalls. Now next one is HTTP error, it says file not found. This an indication of a content delivery issue. The user may observe audio loss, video loss, or both. [inaudible] specified bandwidth for variant. Now this is an interesting one. It's an indication of a stall risk. However, the playback may be perfectly fine when the error is indicated. It means some of the segments bitrate is higher than what is specified in the master playlist. Last example, crypto format error, unsupported crypto format. This may be an indication of a failure, a playback failure. All this error message and a few more that are not talked about here are very helpful when we have AV Player and Player Item errors. Let's take a look. The application should be observing AV Player Item status and AV Player Item error property to find out this type of error. These errors are fatal errors. When the error indicated playback has been terminated already, so what should we do? How do we find out the cause? Here is example. The application is observing player item status when the status changed to failed. Application go off to check the AV Player error properties as well as the Error Log from the AV Player Item. Here is the error property from the player item. It provides some useful information. Error code from AV foundation error domain. It also provides some hint, go off and check the Error Log from AV Player Item. So corresponding AV Player Error Log gives you much more details. It tells you on this data and the time and what URI with what type of error. So in this case, it's unsupported crypto format. It also tells you what type of network interface the device was on when the error happens. Next type of error, HDCP. If you are offering content that requires HDCP protection, your application should be observing this long property name, property. It's output obscured due to insufficient external protection. The value of this property changes to two means three things. Current item requires external protection. Device does not meet the protection level. User will observe or is already observing video loss, like through [inaudible] for example. To avoid this issue, your master playlist should offer at least one variant that does not require HDCP for fallback. Remember, not all your users has the viewing setup that is HDCP capable. App user interface should reflect the property change to timely hint the user. A lot of playback issues are introduced by content authoring such as audio and video out of syncope or glitches while [inaudible] switching. In addition to the error investigation and handling that we have talked about, we would encourage you to use our media stream validator too, which is available on the developer website. That's all I want to talk about it today. Now let's welcome my colleague, Eryk Vershen, to talk about how to author the master playlist the correct way. Thank you. Thanks, Zhenheng. My name's Eryk Vershen. I'm an engineer working on HLS Tools. We've spoken about how to measure your performance and also how to address many of those concerns. However, one of the key elements to successful and error-free playback experience is to ensure that your master playlist is authored correctly. The master playlist is what allows the player to make intelligent decisions both before and during playback. So getting it right is critical. There we go. That's my advice. No, I'm just kidding. I think I need to give you a little more background to understand what Roger meant. We want you to put all of the encoding options you have into your master playlist and to describe them as completely as possible. Let's pretend you're asking me questions. This is the crucial question and the main thing you have to get right. Now, first you have to remember that just because a master playlist works doesn't mean it's right. I've actually seen master playlists that look remarkably like this. This is technically legal, and it's next to useless. I say, okay, well how about this one? It has a few more variants. Well, it's a little bit better, but it's still terrible. Can we even play this? What codec is it using? Is it HDR? Is it 60 fps? You need to tell us everything. We want you to tell us everything. For example, average bandwidth. Average bandwidth enables us to make better decisions about which variant to switch to. It's a better predictor or whether we'll be able to keep up with a stream. Codecs is what enables us to filter out things that we can't play, and resolution allows us to make good decisions about which variant to choose. Remember, we don't read the media playlists or the media segments until we have to. So you need to tell us things ahead of time in your master playlist. So here's a sample of a simple master playlist. This playlist allows the player to adapt to bandwidth changes and make good choices about which variant to use. Now, everything that we've done here is invisible to the user. It just makes the stream play better. Okay. Let's look at a common problem. Your stream plays, but you're not seeing any images in fast forward, or your not seeing a thumbnail in the scrubber bar. Here's the Apple TV scrubber bar. You can see how long your content is. You can see where you are in the content, where you want to go. Now, in order to get that thumbnail image, you need to give us an I-frame playlist, and the I-frame playlist is also what allows us to provide images in fast forward and reverse playback on your iPad or your iPhone. Now in order to talk about I-frame playlist, we first need to talk just for a moment about normal video. Now, here's a way of visualizing regular video segments in HLS. Each segment has content for a number of frames, so it has a duration in frames, and it has a particular average bitrate, and that bitrate varies from segment to segment. Now, because of compression techniques, most frames in a video can only be decoded relative to other frames. But I-frames, the I stands for intercoded frames, these are frames that are independently decodable, and they're the base frames that allow everything else to be decoded. Now, as I've shown you here, you might have more than one I-frame in a single segment, and the I-frames need not be in a, occur at regular intervals. An I-frame playlist is a playlist which just points to the I-frame content, that is only the I-frame data will be downloaded. And when we talk about the duration of an I-frame, we always mean the time from that I-frame till the next I-frame. Now, this particular set of I-frames, I've shown as extracted from my normal content, but you can also make what we call a high-density I-frame playlist. This isn't something just extracted from your normal content. Instead, you make it deliberately with more evenly spaced I-frames. This will allow us to work better. It allows us to give a much smoother result when you're fast forwarding. Now, here I'm showing you a master playlist without I-frame playlist added. Now, notice that the I-frame playlist has almost exactly the same tags as the normal playlist. The only difference is the I-frame playlist does not support the frame rate attribute because it doesn't make any sense in that context. Now, a good test for your I-frame playlist is to try and play it directly. That is, take the URI of your I-frame playlist and paste it into Safari. It should play at 1X speed, and you should see the I-frames displayed one after another in a slowly changing sequence. Now also I want to point out the difference in the bitrate. Notice that the I-frame bitrate is much lower than the normal bitrate. That should always be the case. Now, speaking of bitrates, we've defined how to compute the peak bitrate in the HLS specification. Make sure you do it that way. Otherwise, you may get that segment exceeds playlist, exceeds specified bandwidth error. Now, we're going to move away from video and talk about audio for a little bit. Now the most common question is how do I support multiple languages? Here's what the interface looks like. I've got a list of languages, and the user can select one. And here's a sample playlist. Now, notice what we did is we've added an audio tag, sorry, an audio attribute onto each of our video variants, and we've added a group, the media tags with group ID's. The group ID is simply a tag that allows you to associate the audio renditions with the video variants. So notice there are a number of differences between the two audio renditions. Just as with your variants, we want you to tell us as much as you can about your media. Now, there are two attributes that people tend to have trouble with on the media tags, and that's Default and Autoselect. Okay. So Autoselect says that the media selection code in the player is allowed to choose this rendition without any special input from the user. Most of the time, you want Autoselect set to yes. If you don't set this, the user's going to have to make an explicit choice to get that rendition. The default on the other hand is what to pick when the user has not given a preferred language. Generally this should be the original language of your video, and the default must be autoselectable because the system has to do the choosing. Now, this default has nothing to do with the default video variant. This is the default within the group of renditions. So, okay, great. I've got multiple language, but I'd really like to have some multichannel audio. I've got 5.1. Okay. Well the first thing to remember is not all devices can play multichannel audio. So you want to also provide the user with a stereo option. And you should think of this always as filling out a matrix. You need to have every format having every language. You may say, well, I don't have a multichannel original for my French. I don't have a 5.1 French. In that case what you should do is put stereo, your stereo content in that group instead. You need to have something in every slot of this matrix. So let's see a sample playlist again. This one is just like the previous example except I've changed the group ID, and remember that's perfectly fine because the group ID just serves to connect the audio renditions with the video variants. Now, here we had the multichannel group, and then I've set this up with French as stereo, so you can see how that's done. What you need to do is make sure that the codecs tag indicates all the different codecs that can occur within that rendition group. Now, notice that we had to duplicate our video variant. So now we've got two entries, one pointing to one audio group, and the other pointing to the other audio group. And you'll see this kind of duplication again in later slides. Well let's say rather than 5.1, I've got several audio bitrates. I've got some high bitrate audio, and I know I need to provide a low bitrate for some users. So in terms of the renditions, this is similar to what we had before. We still got a matrix. We want to fill it out with every language for every bitrate. And since these are both AAC, they're considered the same format. So if I also want to have another format, all I do is extend that matrix. Now, I want to mention that I've been saying language for convenience, but you should remember that it's the name attribute which is the unique attribute, not the language attribute. Now, in this playlist I'm not going to show you the media tags. I'm just going to show you the video variants with their audio group names. Now you want your low bitrate video associated with your low bitrate audio, and you want you high bitrate video associated with your high bitrate audio. And you'll always want to split this up like this. Don't do a situation where you have a complete set of video variants associated with your low bitrate audio and a complete set of variants associated with your high bitrate audio. Because if you do that, you can be at a high video bitrate and be bouncing between high and low audio bitrates. Now, here I've added in the AC3 content. Notice that again we had to duplicate our video variant entries, but they point to the same video playlist. Now, notice also that the bitrate on the video variants changes. Remember that's because the video, I'm sorry, the bitrate associated with the video variant is the bitrate of the video itself plus any associated renditions. Now, let's go back to the video for just a second because I want to have multiple video formats. I like to have HEVC, so I can have better quality at the same bitrate or I'd like to have Dolby Vision so I can have some HDR content. Again, we're kind of filling out a matrix. In this case, no matter which video format we choose, we want to end up with a reasonable set of variants. So the rows here are tiers based on quality, and we want to fill out the matrix with a variant in each tier in each format. Now, we don't have to necessarily fill out the higher portions of the tiers on our older formats. You can skimp a little bit there. But similar to audio, not every device supports things like Dolby Vision, so you want to provide an H.264 variant as a fallback. The main thing to remember is that in each column you want to have the bitrate form a nice progression. Now, this playlist has gotten a little too big to show on one slide, so I'm going to split it over three slides. This one shows you the H.264 variant. On this slide, we have the HEVC variant. Now, notice everything has a different video playlist that it's pointing to, and here's our Dolby Vision variant, and notice that everything here has had the same audio group. So, again, if we wanted to have multiple audio formats, we would need to replicate the video variants for each audio format. And again this wouldn't increase the number of video playlists we had to have. It would just increase the number of entries that we had in the playlist. Okay. We're almost done. Our last bit is about subtitles and closed captions. Now, you can probably guess how this works. Our variants need to point at the subtitle and closed caption groups that we're using. So we need to add an attribute to our video variant, and we need to describe the renditions. Now, notice that the closed caption rendition does not have a URI attribute. That tells the system that the closed caption data is found within the video content, not in a separate playlist. So, there you go, the right thing to do is to give us everything you've got. See, now you understand better what I meant. Okay. I'd like to quickly summarize the talk we've given today. Emil talked about key performance indicators, about how to get or compute the values and what they mean. And Zhenheng talked about ways to reduce startup time and how to go about resolving stalls and other errors. And I've talked about how to do master playlists. I'd like to briefly mention the HLS validation tools. They do identify many issues with master and media playlists, and it's worth your time to use them. As always, you can get more information from the WWDC app or the developer website. That's all we have today. Thanks very much for your attention and time.  Hello! So welcome to the second session of Core ML. My name is Aseem, and I'm an engineer in the Core ML team. As you all know, Core ML is Apple's machine learning framework for on-device inference. And the one thing I really like about Core ML is that it's optimized on all Apple hardware. Over the last year, we have seen lots of amazing apps across all Apple platforms. So that's really exciting. And we are even more excited with the new features that we have this year. Now you can reduce the size of your app by a lot. You can make your app much faster by using the new batch-predict API. And you can really easily include cutting-edge research right in your app using customization. So that was a recap of the first session. And in case you missed it, I would highly encourage you to go back and check the slides. In this session, we are going to see how to actually make use of these features. More specifically, we'll walk through a few examples and show you that how in a few simple steps using Core ML Tools. You can reduce the size of the model, and you can include a custom feature in your model. Here's the agenda of the session. We'll start by a really quick update on the Core ML Tools ecosystem. And then we'll dive into a demo of our quantization and custom conversion. So let me start with the ecosystem. So how do you get an ML model? Well, the best thing is that if you, if you can, if you find it online, you just download it, right? Very good place to download your ML models is the Apple Machine Learning landing page. We have a few models there. Now let's say you want to train a model on your data set. In that case, you can use Create ML. This is a new framework that we have just launched this year, and you do not have to be a machine learning expert to use it. It's really easy to use. It's right there in Xcode. So go and give it a try. Now some of you are already familiar with the amazing machine learning tools that we have outside in the community. And for that, last year we had released Core ML Tools, a Python package. And along with that, we had released a few converters. Now there has been a lot of activity in this area over the last year. And this is how the picture looks now. So as you can see, there are many more converters out there. And you really do have a lot of choice to choose your training framework now. And all of these converters are built on top of Core ML Tools. Now, I do want to highlight a couple of different converters here. Last year, we collaborated with Google and released the TensorFlow converter. So that was exciting. As you know, TensorFlow is quite popular with researchers who try out new layers so we recently added support for custom layers into the converter. And TensorFlow recently released support for quantization during training and that's Core ML 2 supports quantization. This feature will be added soon to the converter. Another exciting partnership we had was with Facebook and Prisma. And this resulted in the ONNX converter. The nice thing about ONNX is that now you have access to a bunch of different training libraries that can all be converted to Core ML using the new ONNX converter. So that was a quick wrap-up of Core ML Tools ecosystem. Now to talk about quantization, I would like to invite my friend Sohaib on stage. Good morning, everyone. My name is Sohaib. I'm an engineer in the Core ML team. And today we're going to be taking a look at new quantization utilities in Core ML Tools 2.0. Core ML Tools 2.0 has support for the latest Core ML model format specification. It also has utilities which make it really easy for you to add flexible shapes and quantize in your own network machine learning models. Using these great new features in Core ML, you can not only reduce the size of your models. But also reduce the number of models in your app, reducing the footprint of your app. Now let's start off by taking a look at quantization. Core ML Tools supports post-training quantization. We start off with a Core ML neural network model which has 32-bit float weight parameters. And we use Core ML Tools to quantize the weights for this model. The resulting model is smaller in size. Now size reduction of the model is directly dependent on the number of bits we quantize our model to. Now, many of us may be wondering what exactly is quantization? And how can it reduce the size of my models? Let's step back and take a peek under the hood. Neural networks are composed of layers. And these layers can be thought of as mathematical functions. And these mathematical functions have parameters called weights. And these weights are usually stored as 32-bit floats. Now in our previous session, we took a look at ResNet50. A popular machine-learning model which is used for image classification amongst other things. Now this particular model has over 25 million weight parameters. So you can imagine, if you could somehow represent these param -- these parameters using a fewer number of bits, we can drastically reduce the size of this model. In fact, this process is called quantization. In quantization, we take the weights for our layers which [inaudible] to minimum and to maximum value and we map them to unsigned integers. Now for APIC quantization, we map these values from a range of 0 to 55. For 7-bit quantization, we map them from 0 to 127, all the way down to 1 bit. Where we map these weights as either zeros or ones. Since we're using fewer bits to represent the same information, we reduce the size of our model. Great. Now many of you may have noticed that we're mapping floats to integers. And you may have come to the conclusion that maybe there's some accuracy loss in this mapping. That's true. The rule of thumb is the lower the number of bits you quantize your model to, the more of a hit our model takes in terms of accuracy. And we'll get back to that in a bit. So that's an overview of quantization. But the question remains. How do we obtain this mapping? Well, there are many popular algorithms and techniques out there which help you to do this. And Core ML supports two of the most popular ones: linear quantization and lookup table quantization. Let's have a brief overview. Linear quantization is an algorithm in which you map these full parameters equally. The quantization is parametrized by a scale and by values. And these values are calculated based on the parameters of the layers that we're quantizing. Now and a really intuitive way to see how this mapping works is if we take a step back. And see how we would go back from our quantized weights which are at the bottom back to our original float weights. In linear quantization, we would simply multiply our quantized weights with the scale parameter and add the bias. The second quantization technique that Core ML supports is lookup table quantization. And this technique is exactly what it sounds like. We construct a lookup table. Now again it's helpful if we imagine how we would go back from our quantized weights back to our original weights. And in this case, the quantized weights are simply indices back into our lookup table. Now, if you notice, unlike linear quantization, we have the ability to move our quantized weights around. They don't have to be spaced out in a linear fashion. So to recap, Core ML Tools supports linear quantization and lookup table quantization where we start off with a full precision neural network model. And quantize the weights for that model using the utilities. Now you may be wondering well great, I can reduce the size of my model. But how do I figure out the parameters for my quantization? If I'm doing linear quantization, how do I figure out my scale and bias? If I'm doing lookup table quantization, how do I construct my lookup table? I'm here to tell you that you don't have to worry about any of that. All you do is decide on the number of bits you want to quantize your model to. And decide on the algorithm you want to use, and let Core ML Tools do the rest. In fact -- In fact, it's so simple to take a Core ML neural network model. And quantize it. Then we can do it in a few lines of Python code. But why stand here and talk about it when we can show you a demo? So for the purposes of this demo, I'm going to need a neural network in the Core ML model format. Now, as my colleague Aseem mentioned, a great place to find these models is on the Core ML machine learning home page. And I've gone ahead and downloaded one of the models from that page. So this model's called SqueezeNet. And let's go ahead and open it up. As we can see, this model is 5 megabytes in size. It has a input which is an image of 227 by 227 pixels. And it has two outputs. One of the outputs is the class label which is a string, and this is the most likely label for the, for the input image. And the second output is a mapping of strings to probabilities given that if we pass an image, it's going to be a list of probabilities of what that image may be. Now let's start quantizing this model. So the first thing I want to do is I want to get into a Python environment. Now a Jupyter Notebook is one such environment that I'm comfortable with. So I'm going to go ahead and open that up. Let's open up a new notebook and zoom in on that. Alright. So let's start off by importing Core ML Tools. Let's run that. Now the second thing I want to do is I want to import all the new quantization utilities that we have in Core ML Tools. And we do that by running this. And now we need to load up the model which we want to quantize. And we just saw the SqueezeNet model a minute okay. We're going to go ahead and get an instance of that model. Send this to my desktop. Great. Now to quantize this model, we just need to make one simple API call. And let's try a linear, quantizing this model using linear quantization. And its API is simply called quantize weights. And the first parameter we pass in is the original model which you just loaded up. The number of bits we want to quantize our model to. In this case, it's 8 bits. And the quantization algorithm we want to use. Let's try linear quantization. Now what's happening is that the utility is iterating over all of the layers of the linear networks. And is quantizing all the weights in those layers. And we're finished. Now, if you recall a few moments ago I mentioned that quantizing our model had an associated loss in accuracy. So we want to know how our quantized model stacks up to the original model. And the easiest way of doing this is taking some data, passing and getting inference on that data using our original model. And doing the same inference on the same data using our quantized model and comparing the predictions from that model. And seeing how well they agree. Core ML Tools has utilities which help you to do that. And we can do that by making this call which is called compare models. We pass in our full precision model, and we pass in our model which we had just quantized. And because this model is a simple image classifier which it only has one image inputs. We, we have a convenience utility. So we can just pass in a folder containing sample data images. Now on my desktop here, I have a folder with a set of images which are relevant for my application. So I'm going to go ahead and pass a path to this folder as my [inaudible] parameter. Great. So now we see we're analyzing all the images in that folder. We're running inference on the, we're using full prediction or full precision model. And we're running inference on our quantized model. And we're comparing our two predictions. So we seem to have finished that. And you can see our Top 1 Agreement is 94.8%. Not bad. Now what does this Top 1 Agreement mean? This means that when I pass in my original model, that image of a dog for example, and it predicted that this image was a dog. My quantized model did the same. And that happened over 98, 94.8% of the data set. So I can go ahead and use this model in my app. But I want to see if other quantization techniques work better on this model. As I mentioned, Core ML supports two types of quantization techniques. Linear quantization and lookup table quantization. So let's go ahead and try and quantize this model using lookup table quantization. Again, we pass in an original model, the number of bits we want to quantize our model to. And our quantization techniques. Oops, made a typo there. Let's go ahead and run this. Now, k-means is a simple clustering algorithm which approximates the distribution of our weights. And using this distribution, we can construct the lookup table for our weights. And what we're doing over here is that we're iterating over all the layers in the neural network. And we're quantizing and we're figuring out the lookup table for that particular layer. Now, if you're an expert and you know that your model, you know your model architecture and you know that k-means is not the algorithm for you, you have the flexibility of passing in your own custom function instead of this algorithm and the utility will use your custom function to actually construct the lookup table. So we finished quantizing this model again using the lookup table approach. And now let's see how well this model compares with our original model. So once again we call our compare model's API. We pass in our original model and we pass in our lookup table model. And again we pass in our sample data folder. Again, we run inference over all the images using both the original model and the quantized model. And we see this time we're getting a much better, little bit better Top 1 Agreement. Now for this model, we see that lookup table was the right way to go. But again, this is model-dependent and for other models, linear may be the way. So now that we're happy with this and we see that this is good enough for at least my application, let's go ahead and save this model out. We do that by causing or calling save. I'm going to give it the creative name of Quantized SqueezeNet. And there we go. We have a quantized model. So this was an original model. And we saw that it was 5 megabytes in size. Let's open up our quantized model. And the first thing we notice right off the bat is that this model is only 1.3 megabytes in size. So if you notice, all the details about, about our quantized model are the same as the original model. It still takes in an image input, and it still has two outputs. Now, if I had an app using this model, what I could do as we saw in the previous demo. Is we could just drag this quantized model into our app and start using that instead. And just like that, we reduce the size of our app. So that was quantization using Core ML Tools. To recap, we saw how easy it was to use Core ML Tools to quantize our model. Using a simple API, we provided our original model, the number of bits we wanted to quantize our model to, and the quantization algorithm we wanted to use. We also saw that Core ML Tools has utilities which help us to compare our quantized model to see how it performs against our original model. Now as we saw in the demo, there is a loss of accuracy associated with quantizing our model. And this loss of accuracy is highly model and data dependent. Some models work well or perform better than others after quantization. As a general rule of thumb again, the lower the number of bits we quantize our model to the more of a precision hit we take. Now in the demo we saw that we were able to use Core ML Tools to compare our quantized model and the original model using our Top 1 Agreement metric. But you have to figure out what the relevant metric for your model and your use case is and validate that your quantize model is acceptable. Now in a previous session, we took a look at a style transfer demo. And this network took in an input image, and the output for this network was a stylized image. Let's take a look at how this model performs at different levels of quantization. So on the top, top left here, your left. We see that original model is 30 -- is 32 bits and it's 6.7 megabytes in size. And our 8-bit linearly quantized model is only 1.7 megabits in size. And we see that the performance by visual inspection it's good enough for my style transfer demo. Now we can see that even down to 4 bits, we don't lose out much in the way of performance. I would even argue that for my app at least, the 3 bit will work fine as well. And we see at 2 bit, we start to see a lot of artifacts and this may not be the right model for us. And that was quantization using Core ML Tools. Now I'm going to hand it back to Aseem who's going to talk about custom conversion. Thank you. Thank you, Sohaib. So I want to talk about a feature that is essential to keep pace with the machine learning research that's happening around us. As you all know, the field of machine learning is expanding very rapidly. So it's very critical for us at Core ML to provide you with the necessary software tools to help with that. Now let's take an example. Let's say you are experimenting with a new model that that is not supported on Core ML. Or let's say you have a neural network that runs on Core ML but maybe there's a layer or two that Core ML does not have yet. In that case, you should still be able to use the power of Core ML, right? And the answer to that question is yes. And the feature of customization will help you there. In the next few minutes, I want to really focus on the specific use case of having a new neural network layer. And show you how you would convert it to Core ML and then how you would implement it in your app. So let's take a look at model conversion. So if you have used one of our converters, or even if you have not, it's a really simple API. It's just a call to one function. This is how it looks for the Keras converter. And it's very similar for say the ONNX converter or the TensorFlow converter. Now when you call this function, mostly everything goes right. But sometimes you might get an error message like this. It might say, "Hey, unsupported operation of such-and-such kind." Now if that happens to you, you only need to do a little bit more to get past this error. More specifically, such an error message is an indication that you should be using a custom layer. And before I show you what is the little bit of extra effort that you need to do to convert, let's look at a few examples where you would need to use a custom layer. So let's say you have an image classifier. This is how it looks in Xcode. So it will be high-level description of the model. If you look inside, it's very likely that it's a neural network. And it's very likely that it's a convolutional neural network. So it has a lot of layers, convolution, activation. Now it might happen that there's a new activation layer that comes up that Core ML does not support. And it's like at every machine learning conference, researchers are coming up with new layers all the time. So this is a very common scenario. Now if this happens, you only need to use a custom implementation of this new layer. And then you are good to go. So this is how the model will look like. The only difference is this dependency section at the bottom. Which would say that this model contains a description of this custom layer. Let's take a look at another example. Let's say we have a very simple digit classifier. Now I came across this research paper recently. It's called Spatial Transformer Network. And what it does is this. So it inserts a neural network after the digit that tries to localize the digit. And then it feeds it through a grid sampler layer which renders the digit again, but this time it has already focused on the digit. And then you pass it through your old classify method. Now we don't need to worry about the details here. But the point to note is that the portion in green is what Core ML supports. And the portion in red, which is this new grid sampler layer, is this new experimental layer that Core ML does not support. So I want to take an example of this particular model and show you how you would convert it using Core ML Tools. So let's go to demo. I hope it works on the first try. Back, oh yes. Okay. So let me close off these windows. Let me get, clear this. Clear the ML. Okay, so I'm also going to use Jupyter Notebook to show the demo. So I just navigate to the folder where I have my pre-trained network. So what you see here is that I have this spatial transformer dot [inaudible] file. This is a pre-trained Keras model. And if you are wondering if I did something special to get this model. Basically what I did was I could easily find an open source implementation of spatial transformer. I just exhibited that script in Keras, and I got this model. And along with this model, I also got this grid sampler layer Python script. Now this grid sampler layer that I'm talking about, it's also not supported on Keras natively. So the implementation that I got online used that Keras custom layer to implement the layer. So as you can see, the concept of customization is not unique to Core ML. In fact, it's very common in most machine learning frameworks. This is how people experiment in new layers. Okay, so so far, I just have a Keras model. And now I want to focus on how can I get a Core ML model? So I'll open -- there, let me launch a new Python notebook. So I'll start by importing this Keras model into my Python environment. Okay? So I import Keras, I import the, the custom layer that we have in Keras. And now I will load the model in Keras. Okay? So this is how you load model, Keras models. You give the part to the model and if there's a custom layer, you give a part to that. Okay. So we have the model now. Now let's convert this to Core ML. So I'm going to import Core ML Tools. Execute that. And now as I, as I showed you before that this is just a call to one function to convert it. So let me do that. That's my call. And I get an error as expected. Python likes to throw these huge error messages. But really what we're focused on is this last line. Let me -- So as we can see in this last line it says that hey, the layer or sampler is not supported. So now let's see what we need to do to get rid of that. Maybe I clear this all so that you can see. Okay. So now I change my converter call just a little bit so I have my Core ML model. And now I'm going to pass one additional argument. It's called custom conversion functions. And this will be a dictionary from the name of the layer to a function that I will define in a minute. And that I'm calling a good sampler. So let me take a step back and explain what is happening here. So as we know the way converter works is that it goes through each and every Keras layer. It will, if you look at the first layer. Then [inaudible] its parameters to Core ML. If you go to the second layer, then translate its parameters and so on. Now when it hits this custom layer, it doesn't know what to do. So this function that I'm passing here that convert this sampler is going to help my converter in doing that. And let me show you what this function looks like. So this is a function. There are a few lines of code, but all that it's doing is three things. First, it's giving a name of a class. So as we might have noticed, the implementation of the layer is not here. The implementation will come later in the app and it will be encapsulated in a class. And this is the name of the class that we'll later implement. So during conversion, we just need to specify this class name. That's it. And then there's the description which is a, which you should provide so that if anybody is, if somebody is looking at your model, they know what it has. And the third thing is basically translating any parameters that the Keras layer had to Core ML. For this particular layer, it has two parameters. The output height, and output weight. And I'm just translating it to Core ML. If your custom layer that does not have any parameters, then you load, then you do not need to do, do this. If your layer has lots of parameters, they can all go here, and they will all be encapsulated inside the Core ML model. So as you might have noticed that all I did here was very similar to how you would define a class, right? You give a class name. Maybe a description, maybe some parameters. So now let me execute this. And now we see that the converter went, conversion went fine. So let me this is behaving very weirdly for some reason. If you don't mind, I'm just going to delete this all. So let me visualize this model, and you can do that very simply using function in Core ML Tools. That's called visualize spec. And here you can see a visualization of the model. So as we can see, we have the [inaudible] and some layers there. And this is our custom layer. And if I click on this, I see the parameters that it has. So this is the name of the class that I gave. And this, and these are the parameters that I set. It's always a good idea to visualize your Core ML model before you drag and drop just to see if everything looks fine. Okay. This is the wrong notebook. Okay. And now I'll save out this model. And now let's take a look at this model. So let me close this. Okay. Let me actually let me navigate to the directory that I have. And here's my model. So if I click on it and see it in Xcode just to see how it looks. We can see that it has the custom description here. Okay. Let me go back to slides. So what we just saw was with a few simple lines, we could exhibit a convert a function to Core ML. And the process is pretty much the same if you are using the TensorFlow converter or the ONNX converter. So we have our model here on the left-hand side. The custom layer model with the parameters. Now when you drag and drop this model into Xcode, you will need to provide the implementation of the class. In a file say, for example, [inaudible]. And this is how it would look like. So you have your class, so you'll have the initializer function. So this would be just initializing any parameters that we had in the model. And then the main function in this class would be evaluate. This is where the actual implementation of whatever mathematical function the layer is supposed to perform will go here, in here. And then there's one more function called output shape or input shapes. This just specifies the size of the output area that the layer produces. This helps Core ML in allocating the buffer size at load time so that your app is more efficient at runtime. So we just saw how you would tackle a new layer in a neural network. There's a very similar concept to a custom layer, and it's called custom model. It has the same idea, but it's sort of more generic. So with a custom model, you can deal with any sort of network. It need not be a neural -- it need not be a neural network. And basically gives you just more flexibility overall. So let me summarize the session. We saw how much more rich is this ecosystem around Core ML Tools and that's great for you guys. Because now you have lot of choice to get Core ML models from. We saw how easy it was to quantize this, quantize Core ML model. And we saw that with a few lines of code, we could easily integrate a new custom layer in the model. You can find more information at our documentation page. And come to the labs and talk to us. Okay, thank you.  Good morning, welcome to What's New in LLVM. I'm Jim Grosbach, your friendly neighborhood pointy hair boss. I'm here to tell you a little bit of background about the LLVM project before we dive into the deep technical details of all the exciting new things that we have for you today. Start off, LLVM is more than just the compiler, it is the background for the Clang compiler, for the C family of languages that we all use every day, but it also powers the Static Analyzer, the sanitizers, the LLDB debugger, and is the optimization code generation framework underneath the GPU shader compilers for all of Apple's mobile platforms. In addition to this, it also powers one additional little project that you may have heard of from time to time called Swift. And like Swift LLVM is an open source project. We all operate under the watchful eye of our LLVM wyvern here, he's normally a very friendly fellow, though I do have to caution you he gets a little bit cranky if you call him a dragon so don't do that. As an open source project LLVM is a partnership, we work with industry partners, academics, researchers and hobbyists from all over the world and in different parts of the industry and many more all over the place. This is really fantastic, we work together to build the greatest tools that we possibly can to move technology forward. And if you ever have a compiler itch that you would like to scratch we would like to invite you to participate with us and go to the LLVM website here at llvm.org or you can come talk to us later today later today in the LLVM labs and many of our compiler engineers from Apple will be there and I'm sure will be more than happy to talk your ear off about anything and everything compiler related you've ever wanted to know. So for today we have a great set of things that we want to share with you. We have updates on automated reference counting that makes it even easier for the compiler to help you with your memory management. We have new diagnostics in Xcode 10 and new checks in the Static Analyzer to help catch bugs in your project sooner at build time to improve the quality of your code. We have compiler features that improve security, both of Apple's platforms and of your apps. And new features to allow you to take advantage of all of the really great new things on the hardware architectures to get the performance that we all want out of our platforms and architectures. So with that I would like to invite my colleague Alex up to talk about ARC. Alex. Thank you, Jim. Automatic reference counting has greatly simplified Objective-C program since we introduced it a couple of years ago. A couple of restrictions made it harder to migrate from the old manual retain release mode over to ARC. I'm happy to say that we've now lifted one such restriction. Xcode 10 has support for ARC object pointer fields in C structures. Let's take a look at an example let's say we'd like to write a food ordering app and we'd like to create a data structure which represents a menu item. In Xcode 9 and earlier it would have been impossible for us to actually use a C structure with ARC object pointer fields, so we would have had to use a C, an Objective-C class here. Xcode 10 now allows us to actually create a C structure that has ARC object pointer fields. Let's keep going and keep writing our food ordering app. Let's create a function that orders free food for us. In the function let's create a variable item of type menu item with a price of zero. Then let's pass this item into another function that actually orders the food for us. When the item is created the compiler has to synthesize code which retains the ARC object pointer fields in the item. The code comments on the slide demonstrate the code that the compiler synthesizes. This code ensures the name and the price of the item are not released prematurely before the item is actually used. Now at the end of the function item goes out of scope and is deallocated from the stack so the compiler has to synthesize code which releases the ARC object pointer fields in the item. This ensures that the name and the price are not leaked when the item is released. Previously it was possible to use Objective-C object pointer fields when using manual retained release mode, but you had to write the retains and releases yourself. With ARC the compiler hides all of this complexity for you and synthesizes code that retains and releases the fields. So the compiler is really your friend here and it does the correct job of managing memory for variables on the stack and also for fields in other structures, and also instance variables inside Objective-C classes. But there is one place we have to put in a little bit of extra work to support structures with ARC object pointer fields and that place is heap. Let's go back to our structure, let's say you would like to allocate an array of menu items on the heap. Now if this was an Objective-C interface we could have used an NSArray here, but it's not so let's use malloc and free. Now this code actually has two issues. First issue, the memory is not zero initialized when it's allocated, which means that their pointers will be invalid which will cause undesired runtime behavior for your program at runtime. The second issue is that the ARC object pointer fields are not cleared before the memory is deallocated which will cause runtime memory leaks in your program. Now to fix the first issue you can replace the call to malloc with a call to calloc. This will ensure that your memory is zero initialized, which will remove all of those nasty unexpected runtime issues. To fix the second issue you can write a loop before it's allocated in your memory to clear out all of the ARC object pointer fields in your items. This will ensure that the name and the price in the items are not leaked when the items are freed. Now this is an exciting new feature and if any of you were put off from migrates over to ARC because of lack of features like that I hope that support from ARC object pointer fields in Xcode 10 will help you reconsider your choice. Now let's take a look at Objective-C pointers and structures in general and see where and how can the structures be used in different language modes in Xcode 10. So in Xcode 10 you can use structures that have Objective-C object pointer fields across different language modes. For example, you can use the same structure in C Objective-C or even Objective-C++. And it will work correctly even when you're compiling your code in ARC or in the manual retain release mode. In Xcode 10 we actually unified the Objective-C++ ABI between calls to functions that took in or returned structures that had ARC object pointer fields in Objective-C++. And this was done through an ABI change in Xcode 10 and ABI change affects functions in Objective C++ which return or take in a structure by value that has ARC object pointer fields and no special member functions like constructors or destructors. Now if you are not sure what this means for you or whether your code is affected by this ABI change please take a look at Xcode's release notes where we describe in more details the effects and the impact of this ABI change. Now there is one caveat when it comes to the ARC object pointer fields and C structures, they're not supported in Swift. So if you try to use a structure that has ARC object pointer fields from Swift you will just get a compilation error because the structure will not be found. In addition to new features like support for ARC object pointer fields Xcode 10 comes with a lot of new compiler diagnostics. We actually have over a hundred new warnings in Xcode 10 and today I'd like to talk about two of them. The first warning might be of interest to those of you who have mixed Swift and Objective-C code. So as you know Swift code can be imported into Objective-C and Xcode allows you to do that by generating a header file that describes the Swift interface using Objective-C declarations. And you can import this header file into your own Objective-C code to get access to the underlying Swift declarations. Now let's get more specific and let's talk about how Swift's closure parameters are important to Objective-C. So right now on the screen you see an example of a Swift protocol called Executor. This protocol defines a function member called performOperation which takes in a closure parameter called handler. Now in Swift closure parameters are non-escaping by default, which means that they should not be retained or called after the function returns. Now it can be easy for the program and to forget that this contract exists when conforming to the executive protocol in Objective-C. For example, as you see right now on the slide we have a dispatch Executor interface in Objective-C and conforms to the Executor protocol, so it provides the performOperation method which takes in the handler block parameter that corresponds to Swift's handler closure parameter. But just by looking at the Objective-C code we have no way of knowing whether the handler parameter can escape or not. Xcode 10 now provides a warning that helps us to remember that this parameter is actually non-escaping. To fix this this warning you can annotate your block parameter with the NS NOESCAPE annotation. You should also annotate the implementation of the method or the parameter in the implementation of the method with NS NOESCAPE annotation. Now the NS NOESCAPE annotation is simply a reminder for you the programmer to ensure that you don't store or call the handler block after they perform operation method returns. So it's there for you to help you remember that there is this contract that exists between your Swift and Objective-C code. Now the second warning might be of interest to those of you who work with more low-level code and who care about the way that structures are laid out in memory. Let's take a look at one structure. So in C structures have to follow strict layout and alignment rules. In this particular structure that you see right now on the slide the compiler has to insert a 2-byte pattern between the second and the third field of the structure. Sometimes you might want to relax these rules and the compiler provides a pragma pack directive that you can use to control the layout and the alignment of your structures. Now in this example we use the pragma pack push, 1 directive to remove this fixated layout and to ensure that our structure is tightly packed. This can be useful when serializing your structures or when transferring your structures over the network. Now pragma pack is typically used with a push and a pop directive, but it can be easy for the programmer to forget to insert the pop into the code. Xcode 10 will now warn about code that doesn't have a corresponding pragma pack pop directive and to point you to the location of the push. So to fix this warning you should take a look at the location of your push directive and insert the pop directive at the corresponding location in your code. So in our case we can insert the pop directly after the packed structure. Once we do that the new layout rules will apply only to the packed structure so they won't affect any other structures in our program. These two new warnings that I mentioned are enabled by default in Xcode 10 and they are there to help you write more correct and more robust code. And to talk more about more correct and more robust code I'd like to invite George up on stage who will talk about the new static analyzing improvements in Xcode 10. George. Thanks Alex, so I would like to tell you about some of the improvements we have done for Xcode 10 for the Clang Static Analyzer. So the Clang Static Analyzer is a great tool for finding HK hard-to-reproduce bugs in your program. And not only the Static Analyzer finds the bug for you it also displays the visualization in Xcode of the paths which [inaudible] the bug. So here nil is added to NSMutableArray which can cause a crash later on. And Static Analyzer shows you the path for this crash so you can see how the application can be fixed. And I would like to tell you about three of the new improvements we have done. Firstly, we have a new check for detecting Grand Central Dispatch anti-patterning, which can cause poor performance and hangs of your replication. Secondly, we have a new check for detecting a misuse of autoreleasing variables inside autorelease pools which can cause crashes with [inaudible]. And finally, we have improved performance and visualizations for the Clang Static Analyzer. So let's start with a new check for detecting Grand Central Dispatch anti-pattern. So many APIs on our platforms are asynchronous, but sometimes developers would like to use them in a synchronous way for one reason or another. Maybe because their code is already running on the background queue or maybe because the function cannot proceed at all until the required value is available. And the tempting solution there is to use a semaphore to ensure synchronization. So that's what's happening in this example, so here there is an SXPC object self.connection and we use its property remoteObjectProxy to call, to get the current task name asynchronously from a different process. And then we wait on a semaphore which is signal to inside the callback. And that helps to ensure that by the time the function returns the task name is available. So this approach works but has known performance implications. So the main problem here is when you wait using a semaphore on some asynchronous process you might be waiting on a queue with a much lower priority than yours costing prior inversion which [inaudible] performance and cause hangs. And moreover using a semaphore in such a way also spawns useless threads which further degrades the performance. And to help you address this issue now Static Analyzer warns on such cases helping to see where the issue occurs. Now let's see how the issue can be fixed. In the best-case scenario there is a synchronous API available which can be used in stat. So for an SXPC connection there is an [inaudible] API synchronousRemoteObjectProxy which when used in start eliminates the need for the semaphore and runs much foster. Alternatively, if no such synchronous API is available you could restructure your application to use continuations in stat and just calls the required function inside the callback. So this check is not enabled by default but we encourage you to enable it in build settings in order to make sure no such problem securing your application and it runs as fast as possible. Now let's talk about the second check for detecting the autoreleasing variables outliving the lifetime of the autorelease pool. So the autoreleasing qualifier specifies that the value has to be released once the control exits the autorelease pool. So here we have an example where we create an error variable inside the autorelease pool and once the control is outside of the autorelease pool the variable is released and subsequently destroyed. And autoreleasing pools are a useful feature of Objective-C to help contain the big memory footprint of your applications and to ensure that thumbprints are destroyed where necessary. However, it can cause unexpected crashes and they're even more unexpected because you don't even need to write the word autoreleasing in your application to have those crashes. So for instance, there is a validation function here and it takes in out parameter NSError. And out parameters are actually autoreleasing in Objective-C under ARC by default. So when we write to this out parameter inside the autorelease pool and then the function exits the error value is actually released. And then if the caller tries to read the value of this error variable they might crash with use-after-free. That pattern is already hard to detect, but it actually gets even worse when you don't even control the part of the application which has the autorelease pool. So here is a similar function which [inaudible] and out parameter error and then it calls an enumerateObjectsUsingBlock which is a popular foundation API which calls a block on every element of a collection. However enumerateObjectsUsingBlock actually calls [inaudible] given block inside the autorelease pool of return. So a similar problem occurs here that when we create an error value inside the block and write it to the out parameter it will actually get released by the time the control reaches out of enumerateObjectsUsingBlock. And then when the caller tries to read it they also can crash with the use-after-free. And previously we have introduced the compiler warning which warns when an implicitly autoreleasing out parameter is captured in the block. And the compiler warning suggested to make such parameters explicitly autoreleasing. But we have noticed that such issue kept occurring, so in Xcode 10 we introduced a more powerful Clang Static Analyzer warning which knows which APIs call the provided block inside the autorelease pool and warns about such cases. So now let's see how this issue can be fixed. And the simplest fix here is just to introduce a strong local variable and then when you're inside the block write a value into the strong variable in stat. And then only copy to the out parameter once the control is outside of the block and you know it's not inside the autorelease pool and it's safe to write into the autoreleasing variable. And finally, we also have improved performance and visualizations of the Clang Static Analyzer. So in Xcode 10 we have improved the analyzer to explore your program in a more efficient way so now it finds up to 15% more bugs during the same analysis time. And not only it finds more bugs the bug report it now generates tend to be smaller and more understandable. And what I mean by that is sometimes in Xcode 10 you would get examples which have a lot of steps and a lot of arrows and which would be somewhat hard to comprehend. And in many of those examples in your version of Xcode we give you a much smaller error path which is much easier to see and you can see the issue much faster. So in order to use Static Analyzer on your projects you can use Product, Analyze or you can even enable Analyze During Build to make sure no analyzer issue gets unnoticed. So I encourage you to use the Static Analyzer, it's a great tool to find your bugs before users do. And now my colleague Ahmed will talk about low-level improvements. Thank you George. So as Alex and George told you, we have lots of warnings and Static Analyzer checks in the compiler, but you also have the sanitizer and all of these tools help you find lots of bugs, including security bugs. So I'm sure you all have lots of tests and use all these great tools to find all the bugs in these tests. But for some of the most egregious security bugs we want to make sure that they don't happen in release builds if somehow they snuck past all the testing. So for those we have mitigations in the code generator that are always emitted even in release builds. So I'm Ahmed, I work on the code generator and today I'm going to tell you about a new mitigation in Xcode 10. So to see how that works we need to understand how the stack works. So here I have a simple C function called dlog and I use it to print a string that I'm passed into a dlog bug. So in this case it's called with a string hello. And the way this works is we need to allocate some memory to keep track of this call. So we allocate that into a region called the stack. So the stack grows down towards the null pointer or address zero. So when we do our dlog hello call this allocates what's called the stack frame and the stack frame contains things like the return address so that we know to go back to main. But it also contains other things like parameters and local variables. So for instance if I have a log file [inaudible] local variable that lives in the stack frame. So now if I try to make another function call to this dlog file function that in turn will allocate its own stack frame. And when it's done it's going to deallocate the stack frame and return back to the caller. So now let's look at this stack frame in more details. So let's say I change my function to have a local buffer, so it's a 4 bytes character array. And I'm trying to prepare my debug string by first doing a strcpy of the string that I'm passed into that buffer. So this does the obvious copy by [inaudible], so it does H-E-L-L. But then there's a problem at this point we already wrote 4 bytes and that we already exhausted all 4 bytes available in our buffer. So if we keep going which is what strcpy will do then we're going to override the return address and this is a big security problem. So if an attacker controls the string that I'm copying which is not that hard, then it can control the return address. If it can control the return address then they control basically what the program does next, so it's a big security problem. So if you had a test that caught this and you ran the address sanitizer, then you will have had an easy way to fix this. And really what I should have done here is strncpy that knows about the size or even better use a higher-level API like NSString or [inaudible] string. But still sometimes these bugs can survive into release builds and we avoid these by using what's called the Stack Protector. So the Stack Protector changes the layout of the stack frame to add a new field the canary so that when we do our write we have a little bit of code right before the return of the function that checks whether the canary is still valid. So if we keep writing in strcpy we're going to override the canary first and then we're going to check the canary first before returning and that's going to abort. So we turned ad potentially exploitable security vulnerability into a reliable crash and that's not good for an attacker. So this is what's called the Stack Protector. It defects certain kinds of stack buffer overflows, which is the attack that we just saw. And it's already enabled by default in many versions of Xcode. So next I'm going to talk about a trickier case where we introduced a new mitigation. So let's say I took my function, again my dlog function and I changed the buffer so that now it's a variable length array. And the length comes from a parameter called len. So let's say len in a specific call is something big like 15,000, so now the stack frame has to be at least 15,000 bytes long. But memory is not all immediately available, so memory is split into pages and the stack grows only when necessary. So for instance, when we try to access by 10,000 of the buffer that's in the next page of the stack that's not yet available so it's going to do a page fault in the CPU that talks to the opening system, the operating system sees that we have the right to grow the stack, and it grows it and we can continue writing. So this all happens under the hood. But say an attacker controls the length and it makes it huge, big enough that it spans many pages. So now there's a new problem, the memory is not infinite so if we keep allocating in this stack eventually we'll hit another region of memory that's already allocated and usually that's the heap. And when we do that then we're going to clash with the heap, with whatever is already used in there, so that's usually things like malloc and new. So if we try to see what would happen with our strcpy example then we will try to write the bytes one by one. So we do H-E-L, etcetera. And from the standpoint of the CPU, the code that's generated and the operating system this is all fine because we're just writing into a page that's already available and allocated. But it really isn't because this is part of the heap, this is not part of our local stack allocated array. So when we do our writes we're actually overriding some completely unrelated piece of information like I don't know a Boolean that checks whether we should check a password. So this is another important security flaw. So this is something that we mitigated with a new feature and the future works by emitting some new codes at the entry of the function that checks whether it's okay to have the stack frame. So it asks the operating system above the maximum size of the stack and if you try to make an allocation that's bigger than that then it actually aborts. And again, this turns a potentially exploitable security bug into a reliable crash and that's no good for an attacker. So this is Stack Checking, it detects something that you might have heard of called Stack Clash and it's enabled by default in Xcode 10. So next I want to talk about a new set of features we added in Xcode 10 and that's support for new extension, sect extensions. So as you all know we have lots of great Apple devices and one of the great things about Xcode is that with just a few build settings you can target your code for each of these devices. And so under the hood in macOS, iOS, watchOS, etcetera we tweak every OS so that it uses everything that's available on a specific piece of hardware. So it guarantees maximum performance no matter where we run. And so if you an app with extremely high-performance requirements that's something that you might want to do as well. So we have three features to talk about that are available in the iMac Pro and the iPhone 8 Plus and X. And let's start with the iMac Pro. So the iMac Pro has the Intel Xeon CPU which has a set of new features called AVX-512. So AVX-512 is a set of new instructions with vector registers. And these provide benefits over X86-64, so in X86-64 we can only assume that we have 128-bit vectors available, so that's guaranteed on any Mac ever that's Intel powered. Now it happens that any new Mac today has more than that, but the iMac Pro is the first that has 512-bit registers. And with the Auto-Vectorizer that's enabled in the Xcode Clang this is great because it means that we can have many more elements in the vector. So this can greatly improve throughputs. But there are other benefits with AVX-512, so for instance we not only have bigger vectors we also have more of them. So on X86-64 we only have 16 now we have 32, so this is a lot of data to process. And even if for some reason the auto-vectorizer is not able to make use of these vectors then we still have ore skill registers or even for code that just does float or double. There are lots of performance benefits in AVX-512. So let's look at how we can exploit it in my compute [inaudible] expensive function. So the first thing I'm going to do is to keep around my existing function because that's going to be the fallback that I have that runs on all Macs. Next, I can try to specialize my function. So one way to do that is using the target attributes. And that tells the compiler that it's okay to assume that this function has AVX-512, so it only runs on an iMac Pro. So if you use simd.h, for instance the simd float4 128-bit vector type then now we might have better performance than the AVX-512 version using the same code. And if you use the even larger vector types, so for instance simd float16, then now you have much better performance than the AVX-512 version where the 512-bit vector is actually native. And if you go all the way down to X86 intrinsics, then now you can start using the new AVX-512 variance, as well as the M512 types. So if you want to specialize larger units of codes, so not just individual functions but files, targets, libraries, then you can use the new AVX-512 value of the additional vector extensions build setting. So when you do that there are some things to keep in mind and if you're familiar with AVX-1 and AVX-2 these are very similar issues. So you can only pass large vectors, so 256 bits and up from and to AVX-512 functions. So the ABI is different from the generic and a specialized variance, so you cannot pass them between those. Additionally, these vectors are large and they're large enough that their natural alignment is too big for what's guaranteed by things like malloc. So you have to take that into account when allocating these anywhere other than the stack. And so in general all of these things are things that we are already go through lots of things in the opening system. So for instance, if you can at all use accelerate.framework and it's much easier to do so because we already specialized all the functions for every single microarchitecture. So this is AVX-512. Now we also have new features on the iPhone 8, 8 Plus and X. So one of the first feature is ARM v8.1 Atomics and that's thanks to one of the great things about the iPhone X and that's the A11 Bionic chip. So the A11 Bionic chip has one great new feature compared to the A10 which is its support for six CPUs, six cores running all at the same time and that's a first in iOS. And since you have more cores than you probably have more threads all at the same time and with more threads you might need more synchronization to make these threads cooperate. And that's implemented using atomics. So the A11 chip also introduces a new family of atomic instructions that are better optimized for the new extra cores. So let's look at how that works. So the way atomics work is through a small sequence of codes. So suppose I have a thread and it's trying to access main memory, so it has an atomic shared variable in there and it's just trying to increment it. So under the hood the code generator will emit a small sequence of codes that first takes exclusive excess of a cache line and that's a small region of memory that contains completely this atomic variable. Now that we have exclusive access we can load from the variable, then we can do our increment on the temporary loaded value and store the result back. And we know that this is safe because we have exclusive access, so no other thread could have changed the value while we're computing our temporary results. But now suppose another thread does access either the same variable or another variable in the same cache line. So both are going to try to have exclusive access over this variable and that is not possible, that's what it means to be exclusive. So both of them are going to fail their exclusive access and they're going to have to try again until one of them succeeds. And this is not ideal for performance. So in ARM v8.1 which is the architecture in the A10 CPU we have new instructions that do this all in a single step and in some cases, that can greatly improve performance. So again, this is something that you can specialize code for using the per function specialization or for entire targets. And this is something that's only really useful when you have your own C11 or C++ 11 atomics. So in general, it's much easier to use the higher-level libraries like GCD or PThread or os unfair lock, etcetera. So these are already tweaked for ARM v8.1, but they also cooperate to the operating system to have even better performance. So another feature in the A11 CPU is 16-bit floating points. So you are all familiar with the two standard floating point types, so we have double which is 64 bits and float which is 32 bits. So on A11 we also have the 16-bit float16, this has much less range and precision so it's not as useful for as many cases. But in some cases like machine learning or when you're trying to talk to GPU via Metal this is great because it's smaller and it's faster to compute. And that's even more true if you put them in vectors where you can put more of them in the same ARM vector. So this is also something that you can specialize code for and in general something to keep in mind with all of these features is that they're not available everywhere. So when you want to use them you have to always make sure that they're actually dynamically available on the device you're running and you can do that using sysctlbyname. And so in general we already do all this in system framework, so it's much easier to just rely on those. So these are three new instruction set extensions, we have on the iMac Pro AVX-512 and on iPhone X, 8, and 8 Plus we have Atomics and 16-bit floating points. So that's just part of all the new features in Xcode. So from ARC object pointers in C Structs to the improved static analyzer there are lots of great things in Xcode 10. And there are also some things that we didn't even talk about like for instance, over a hundred new warnings and support for C++ 17 standard library function. So if you want to learn more we have the video and the slides available on the website soon. And if you're here at the conference come join us at the lab this afternoon. Thank you.  Good afternoon, and welcome to WWDC. I know I'm competing now with the coffee and cookies, but I don't know if the coffee is gluten free and the cookies are actually caffeine free, so stick with me. My name is Frank Doepke and I'm going to talk about some interesting stuff that you can do with Computer Vision using Core ML and Division Framework. So, what are we going to talk about today? First, you might have heard that we have something special for you in terms of custom image classification. Then we're going to do some object recognition. And last but not least, I'm going to raise your level of Vision awareness by diving into some of the fundamentals. Now, Custom Image Classification, we've seen the advantages already in some of the presentations earlier, and we see like what we can do with flowers and fruits and I like flowers and fruits as much as everybody else. Sorry, Lizzie, but I thought we'd do something a bit more technical here. So, the idea is, what can we do if we create a shop and we are all geeks, so let's build a shop where we can build robots. So, there are some parts that we need to identify. And I thought it would be great if I have an app that actually can help customers in my store identify what these objects are. So, we've got to train a customer classifier for that. Then, once we have our classifier, we build an iOS app around that, that we can actually run on all devices. And when going through this, I'm going to go through some of the common pitfalls when you actually want to do any of this stuff and try to guide you through that. Let's start with our training. So, how do we train? We use of course, Create ML. The first step of course we have to do, is we have to take pictures. Then we put them in folders and we use the folder names as our classification labels. Now, the biggest question that everybody has, "How much data do I need?" The first thing is, well, we need a minimum of about 10 images per category, but that's on the low side. You definitely want to have more. The more, the better, actually your classifier will perform better. Another thing to look out for is highly imbalanced data sets. What do I mean by that? When a data set has like thousands of images in one category, and only ten in the other one, this model will not train really well. So, you want to have like an equal distribution between most of your categories. Another thing that we actually introduce, is augmentation. Augmentation will help you to make this model more robust, but it doesn't really replace the variety. So, you still want to have lots of images of your objects that you want to classify. But, with the augmentation, what we're going to do is, we take an image and we perturb it. So, we have noised it, we blur it, we rotate it, flip it, so it looks different to the classifier actually when we train it. Let's look a little bit under the hood of how our training actually works. You might have already heard it, the term, transfer learning. And this is what we're going to use in Create ML when we train our classifier. So, we start with a pretrained model, and that's where all the heavy lifting actually happens. These models train normally for weeks, and with millions of images, and that is the first starting point that you need to actually work with this. Out of this model, we can use this as a feature extractor. This gives us a feature vector which is pretty much a numerical description of what we have in our image. Now, you bring in your data and we train the set, what we call the last layer, which is the real classifier, on your label data and out comes your custom model. Now, I mentioned already this large, first pretrained model. And we have something new in Vision for this [inaudible]. This is what we call the Vision FeaturePrint for Scenes. It's available through Create ML and it allows you to train an image classifier. It has been trained on a very large data set, and it is capable of categorizing over a thousand categories. That's a pretty good distribution that you can actually use [inaudible]. And we've already used it. Over the last few years, through some of the user [inaudible] pictures that you've seen in photos, have been actually using this model underneath. We're also going to continuously improve on that model, but there's a small caveat that I would like to kind of highlight here. When we come out with a new version of that model, you will not necessarily automatically get the benefits unless you retrain [inaudible] new model. So, if you start developing with this, this year, and you want to you know, take advantage of whatever we come out over the next years, hold onto your data sets so you can actually retrain [inaudible]. A few more things about our feature extractor. It's already on the device, and that was kind of an important decision for us, because it makes the disc footprint for your models significantly smaller. So, let's compare a little bit. So, I chose some common, you know, available models that we use today. The first thing would be Resnet. So, if I train my classifier on top of Resnet, how big is my model? Ninety-eight megabytes. If I use Squeezenet, so Squeezenet is a much smaller model. It's not capable of differentiating as many categories, and that's 5 megabytes. So, it's about saving there, but it will not be as versatile. Now, how about Vision? It's less than a megabyte in most cases. The other thing of course why we believe that this is a good choice for use, it's already optimized. And we know a few things about our hardware or GPUs and CPUS and we really optimized a lot on that model so that it performs best on our devices. So, how do we train [inaudible]? We start with some labeled images, and bring them into Create ML, and Create ML knows how to extract [inaudible] Vision Feature Print. It trains our classifier and that classifier is all what [inaudible] will go into our Core ML model. That's why it is so small. Now, when it comes time that I actually want to analyze an image, all I have to do is use my image and model, and now in Vision or in Core ML, it knows another [inaudible] again how to train -- sorry. Not train. In this case, use our Vision Feature Print and we'll [inaudible] the classification. So, that was everything we needed to know about the training. But, I said there was some caveats that you want to kind of look at when we deal with the app. So, first thing, we only want to actually run our classifier when we really have to. Classifiers are deep convolutional networks that are pretty computational intensive. So, when we run those, it will definitely use up kind of you know, some electrons running on the CPU and GPU. So, you don't want to use this unless you really have to. In the example that I'm going to show in my demo laters, I really only want to classify if actually the person looks really at an item and not when just a camera moves around. So, I'm asking the question, "Am I holding still?" and then I'm going to run my classifier. How do I do this? By using Vision, I can use Registration. Registration means I can take two images and align them with each other. And you're going to tell me, "Okay, if you shift it by this amount of pixels, this is actually in how they would actually match." This is a pretty cheap and fast algorithm, and it will tell me if I hold the camera still or if anything is moving in front of the camera. I used VN Translational Image Registration Request. I know that is a mouthful. But that will give me all this information. So, to visualize this first, let's look at the little video. What I'm doing in this video is I show basically a little yellow line that shows me how basically my camera has moved or the Registration requests have moved over the last couple of frames. So, what out for that yellow line and see if it's long, then I have moved the camera around quite a bit, and when I'm holding it still, it should actually be a very small line. So, you see the camera is moving, and now I'm focusing on this, and it gets very short. So, that's just a good idea. It's like, "Okay, now I'm holding still. Now, I want to run my classifier." Next thing to keep in mind is, have a backup plan. It's always good to have a backup plan. Classifications can be wrong. And what that means, even if my classification actually has a high confidence, I need to kind of plan for sometimes that it doesn't work quite correctly. The one thing that I did in my example here, as you will see later on, I have something wherefore which I don't have a physical object. So, how do I solve that? In my example, I'm using our backward detector that we have in the Vision Framework to read some data backward label to identify this. Alright, enough of slides. Who wants to see the demo? Okay, what you see on the right-hand side of the screen is my device. I'm going to start now my little robot shop application. And when you see I'm moving around, there's nothing happening. When I hold still, and I point it at something, I should see a yellow line and then voila, yes step is a stepper motor. Okay? Let's see what else do we have here on this table? That is my micro controller. That's a stepper motor driver. We can also like you know, pick something up and hold it. Yes, this is a closed loop belt. What do we have here? Lead screw. And as I said, you can also look at the barcode here, if I get my cable [inaudible] long enough. And that is my training course. Hey, Frank? Of course, for that I didn't-- Frank? What's going on? Frank, yes, I'm going to need you to add another robot part to your demo. This is Brett. That's my manager. That'd be great. As usual, management, last minute requests. I'll make sure you get another copy of that memo. I'm not going to come in on Saturday for this. Alright, well what do we have here? We have a [inaudible] motor. Alright, let's see. It might just work. Let me try this. Do I get away with that? No, it can't really read this object. Perhaps so? It's -- no, it's not a stepper motor. So, that's a bug. I guess we need to fix that. Who wants to fix this? Who wants to fix this? Alright. So, what I have to do now is I have to take some pictures of the aforementioned, [inaudible] motor. So, I need to go to the studio and you know, set up the lights, or I use my favorite camera that I already have here. Now, let's see. We're going to take a bunch of pictures of our [inaudible] motor. And it's kind of important to just kind of vary it and don't have anything else really in the frame. So, I'm going for a [inaudible]. I need to have at least ten different images. Good choices. I always just like to put it on a different background. And make sure that we get a few captured here. Perhaps I'm going to hold it a little bit in my hand. Okay, so we have now a number of images. Now, I'm going to go over to my Mac and actually show you how to actually do then the training work for that. Okay, I'm bringing up my image capture application and let me for a moment just hide my [inaudible]. If I now look in my Finder, you can actually see I have my training set, which I used already earlier to train and model that I've been using in my application. And I need to create now a new folder, and let's call that Servo. And from Image Capture, I can now simply take all the pictures that I just captured and drag them into my Servo. Alright, so now we have that added. Now, I need to train my model again since my manager just ruined what I've done earlier. Okay. I used a simple scripting playground here. Not the UI, just because well, this might be something I want to later on incorporate as a build step into my application. So, I'm pointing it simply at the data set that we just added our folder to, and I'm just simply going to train my classifier, and in the end, write out my model. So, what's going to happen now as you can see, we're off to the races. It's going to go through all these images that we've already put into our folders and extracts the scene print from that. It does all the scaling down that has to happen and will then in the end, train and model based on that. So, it's a pretty complex task, but you see for you, it's really just one line of code, and in the end, you should get out of it, a model that you can actually use in your application. Let's see as it just finishes. We're almost there. And voila, we have our model. Now, that model, I've already referenced in my robot shop application. This is what we see here now. As you can see, this is my image classifier. It's 148 kilobytes. That's smaller than the little startup screen that I actually have. So, one thing I want to highlight here already, and we're going to go into that a little bit later. So, this image, that I need to pass into this has to be of a-- a color image and a 299 by 299 pixels. Strange layout but this is actually what a lot of these classifiers will do. Alright. So, now I have hopefully a model that will understand it. Now, I need to go into my sophisticated product database which is just a key list. And I'm going to add my Servo to that. So, I'm going to rename this one here. This is Servo. I'm giving it a label. This is actually what we're going to see. This is a servo motor and let's say this is a motor that goes swish, swish. Very technical. Alright. Let's see if this works. I'm going to run my application now. That's -- okay. Let's try it. There's our servo motor. Just to put this in perspective. This was a world first that you saw. A classifier being trained live on stage, from photos, all the way into the final application. I was a bit sweating about this demo [laughter]. Thank you. Now we've seen how it works. There's a few things I want to highlight actually when we actually go through the code. So, I promise, we're going to live code here a little bit. Okay, let me take everything away that we don't need to look at right now. And make this a bit bigger. Okay, so, how did I solve all this? So, we started, we actually created a Sequence Request Handler. This is the one that I'm going to use for my registration work, just as Sergei already explained in the earlier session, this is good for tracking objects. I will create my request, put them into one array, and now what do you see here for the registration? Just keeping like the last 15 registration results, and then I'm going to do some analysis on that and see if actually I'm holding still. I'm going to keep one buffer that I'm holding onto while I'm analyzing this. This is actually when I run my classification. And since this can be a longer running task, I'm actually going to run this on a separate queue. Alright, here's some code that I actually just used to open. This is actually like the little panel that we saw. But the important part is actually, "So, how do I setup my Vision task?" So, I'm going to do two tasks. I'm going to do a barcode request and I'm going to do my classification request. So, I set up my barcode request. And in my completion handler, I simple look at, "Do I get something back?" and also just since I'm only expecting one barcode, I only look at the very first one. Can I decode it? If I get a string out of it, I use that actually to look up -- that's actually how it worked with my barcodes to see then -- oh, yes, that is my training [inaudible]. Alright, so I add that as one of the requests that I want to run. Now, I'm setting up my classification. So, in this case, what I've done, I used my classifier and I'm loading simply this from my bundle and create my model from there. Now, I'm not using the code completion from Core ML in this case, because this is the only line of Core ML that I'm actually using my whole application, and it allows me to do my custom kind of error handling. But, you can choose also to use the code completion already from Core ML. Both are absolutely valid. Now, I create my Vision model from that. My Vision Core ML Model, and my request. And again, simply when the request returns, meaning I'm executing my completion handler. I simply look, "What kind of specifications did I get back?" And then I set this threshold here. Now, this is one that I empirically set against a confidence goal. I'm using 0.98. So, a 98% confidence that this is actually correct. Why am I doing that? That allows me to filter out actually when I'm looking at something, and maybe not quite sure what that is. Maybe we'll see that in a moment, actually what that means. So now, I have all my requests. When it comes to the time that I actually want to execute them, I created a little function here that actually mean, "analyze on the current image." So, when it's time to analyze it, I get my device orientation, which is important to know how am I holding my phone. I create an image request handler on that buffer that we currently want to process. And asynchronously, I let it perform its work. That's all I have to do basically for actually doing the processing with Core ML and barcode reading. Now, a few things, just okay. How do I do the scene stability part? So, I have a queue that I can reset. I can add my points into that. And then simply I had created a function that allows me to look basically through the queue of points that I've recorded. And then setting like, "Well, if they all sum together, only show a distance of like 20 pixels." Again, that's an empirical value that I chose. Then I know the scene is stable. So, I'm holding stable. Nothing is moving in front of my camera. And then comes our part of catch the output. So, this is the call that actually AV Foundation calls [inaudible] buffers from the camera. I'm making sure that I you know, hold onto the previous pixel buffer because that's what I'm going to compare against for my registration, and swap those out after I'm done with that. So, I create my Translational Image Registration Request with my current buffer. And then on the Sequence Request Handler, I simply perform my request. Now, I get my observations back. I can check if they are all okay. And add them into my array. And last but not least, I check if the scene's stable. Then I bring up my little, yellow box which is [inaudible] detection overlay. I know this is my currently analyzed buffer. And simply ask it to form its analysis on that. The one thing that you saw that I did at the end of the asynchronous call, in the currently analyzed buffer, I released that buffer. And you see here that I check if there is a buffer currently being used. Now, that allows me to make sure that I'm only really working on one buffer, and I'm not constantly queueing up more and more buffers while they're still running in the background, because that will starve the camera from frames. Alright, so when we run this, there's a few things I want to highlight actually. So, let me bring up Number 1, our console here a little bit on the bottom. And you can actually see, when I'm running this at first, and so, I'm going to run this right now here. Hopefully you should actually see something. You see that the confidence scores are pretty low because I'm actually [inaudible] over the [inaudible]. It's not really sure what I'm really looking at. The moment actually I point it at something that it should identify, boom, our confidence score goes really high and that's actually how I'm sure that this is really the object I want to show. Now, another thing I wanted to demo, let's look actually what happens in terms of the CPU. Alright, so right now, I'm not doing anything. I'm just showing my screen. So, when I'm just moving the camera around, scene is not stable, I'm using about 22% of the CPU. Now, if I hold it stable and actually the classifier runs, you see how the CPU goes up. And that's why I always recommend only run these tasks when you really need. Alright, that was a lot to take in. Let's go back to the slides and recap a little bit what we have just seen to now. So, go right to the slides. Okay, recap. First thing, how did we achieve our scene stability? We used a sequence request handler together with the VN Translational Image Registration Request, to compare against the previous frame. Out of that, we get our translation as of terms of an alignment transform which tells me the X and Y of like how the previous frame has shifted to [inaudible] the current one. Then we talked about that we want to only analyze the scene when it's stable. And for that, we created our VN Image Request Handler off the current buffer. And we passed in together both the barcode detection and the classification. So, that allows Vision to do its optimization underneath the covers and can actually perform much faster than if you would run them as separate requests. Next was the part about thinking about how many buffers do I hold in flight? And that's why I say manage your buffers. Some Vision requests, like these convolutional networks, can take a bit longer. And these longer running tasks are better to perform on a background queue, so that [inaudible] or whatever you do in the camera, can actually continuously running. But to do this particularly with the camera, you do not want to continuously queue up the buffers coming from the camera. So, you want to drop busy buffers. In this case, I said I only work with one. That's actually in my use case scenario works pretty well. So, I have a queue of 1, and that's why I simply held onto one buffer and check, as long as that one is running, I'm not queuing new buffers up. Once I'm done with it, I can reset and work on the next buffer. Now, you might ask, "Why am I using Vision when I can run this model in Core ML? It's a Core ML model." Well, there is one thing why this is important to actually use Vision. Let's go back and look what we saw when we looked at our model. It was the strange number of 299 by 299 pixels. Now, this is simply how this model is trained. This is what it wants to ingest. But our camera gives us something like 640 by 480 or larger resolutions if you want. Now, Vision is going to do all the work by taking these [inaudible] buffers as they come from the camera, converts it into RGB and scales it down and you don't have to write any code for that. That makes it much easier to drive these Core ML models for image requests through Vision. So, that was image classification. Next, we talk about object recognition. Now, a little warning. In this demo, actually a live croissant might actually get hurt on stage. So, for the squeamish ones, please look away. So, what we're using for our object recognition is a model that is based on this YOLO technique, You Only Look Once. That is a pretty fast-running model that allows us to get the bounding boxes off objects and a label for that. And it finds multiple of them in the screen. As you see in the screenshots. The advantage of those is that I get actually like where they are, but I won't get as many classifications as I can do with like our overall image classifier. The training is also a little bit more involved, and with that, I would like to actually refer you to the Turi Create session that was, I believe, yesterday, where they actually showed you how to train these kind of models. These models are also a little bit bigger. So, how does this look? Let's go over to our demo. Robot shop is closed. Time for breakfast. Alright. Let me bring up my quick template here and I have my new little application. It is my Breakfast Finder. And what do we see? Oh, we have a croissant, we have a bagel, and we can identify the banana. And see, they can all be kind of like in the frame. I'm going to detect them. So, some mention in these cooking shows, normally they show you how to do it, but then pull the prebaked stuff out of the oven. Well, this model, actually has been baked way before this croissant has been baked, and I can prove this. It's fresh. And still a croissant. Alright. It's fresh but still, got to chew. Let's look quickly how this looks in the code. So, what did I do differently? [Inaudible] actually in the terms of like setting up my request. All I have to do is use my Core ML model, just as I did in the previous example, create my Core ML request, and afterwards, I'm looking actually at simply, "How do I draw my results?" Now this is where we have something new to make this [inaudible] a little bit easier. And when we look at all of this, we get a new object that is the [inaudible] recognized -- Recognized Object Observation, and out of that, I get my bounding box, and my observation of like the labels. Now, that's one thing I would like to show you here. Let's run our application from here and I put a break point. Okay. Alright, we are now on our break point. So that I only look at the first label. So, what we are doing when we actually process these results, I'm going to take this, let's try object observation, labels. So, what you actually see is that I get more than one back. I get my bagel, my banana, coffee -- I didn't bring any coffee today. Sorry about that. And croissant, egg, and waffle. Now, they are sorted in the order of like the highest confidence on the top. Usually, this is the one that you're interested in. That's why I'm taking the shortcut here and just looking at the first one. But you always get all of the classifications back in terms of an array of the ones that we actually support in the model. Alright. That was our Breakfast Finder. Let's go back to the slides, and this time, I'm pushing the button. Good. So, we made this possible through a new API and that is our VN Recognized Object Observation. It comes automatically when we perform a Core ML model request, and if that model is actually using an object detector as a space. Like in this example, it is based on the YOLO based models. Now, you might say, "Well, I could have already run YOLO like last year. There were a bunch of articles that I saw on the web." But look at how much code they actually had to write to take the output of this model, to then put it into something that you can use. And here, we only had a few lines of code. So, it makes YOLO models really, really easy to use now. Let's go once more over this in the code. So, I create my model. From the model, I create my request. And in my completion handler, I can simply look at the area of objects, because we saw we can get multiple objects back. I get my labels from that, my bounding box, and I can show my Breakfast Finder. Now, there's one more thing I would like to highlight in this example. You saw how these boxes were a little bit jittering around because I was running the detector frame by frame, by frame, by frame. Tracking can often be a better choice here. Why? Tracking is much faster even in terms of like running, than actually these models would run. So, redetection takes more time than actually running a tracking request. I can use the tracker basically if I want to follow now an object on the screen because it's a lighter algorithm. It runs faster. And on top of it, we have temporal smoothing so that these boxes will not jitter around anymore and if you see some of our tracking examples, they actually move nicely and smoothly across the screen. If you want to learn more about tracking, the previous session from my colleague Sergei, actually talks about how to do all the implementation work of that. Alright, last but not least, let's enhance our Vision mastery and go into some of our fundamentals. Few things are important to know when dealing with the Vision framework. First and foremost, and this is a common source of problems, image orientation. Now, not all of our Vision algorithms are orientation agnostic. You might have heard early that we have a new face detector that is orientation agnostic. But the previous one was not. This means we need to know what is the upright position of the image? And it can be deceiving because if you look at it and preview on the final, my image looks upright, but that is not how it is stored on disk. There is something that tells us how the device is oriented, and this is called the EXIF Orientation. So, if an image is captured, that's normally in the sensor orientation, with the EXIF, we know what is actually upright and if you pass an URL into Vision as our input, Vision is actually going to do all that work basically for you and actually read this EXIF information from the file. But like when -- as we showed in the demos earlier, if I use my live capture feed, I need to actually pass this information in. So, I have to look at what does my orientation from my UI device current orientation and convert this [inaudible] to CG Image Property Orientation because we need it in the form of an EXIF orientation. Next, let's talk a little bit about our coordinate system. For Vision, the origin is always in the lower, left corner. And all processing is done in the up right -- if the image would be in an upright position, hence the orientation is important. All our processing is really done in a normalized coordinate space, except the registration where we actually need to know how many pixels [inaudible]. So, normalized coordinates means our coordinates go from zero, zero, to 1,1, in the upper right corner. Now what you see here is to that performed face and landmark detection request. And you will see that I get the bounding box for my face, and the landmarks are actually reported in relative coordinates to that bounding box. If you need to go back into the image coordinate space, we have utility functions and VNUtils like the VN Image [inaudible] from normalized way to convert back and forth between those coordinates. Next, let's talk about confidence scores. We touched a little bit on this already during our robot shot example. A lot of our algorithms can express how confident they are in their results. And that is kind of an important part to know when I want to analyze later on what I get out of these results. So, if I have a low confidence of zero, or do I have a high confidence of 1? Clicker. Here we go. Alright. Unfortunately, not all algorithms will have the same scale in terms of like how they report their confidence scores. For instance, if we look at our text detector, it pretty much always returns a confidence score of 1 because if it doesn't think there's text, it's not going to return the bounding box in the first place. But as we saw, the classifiers have a very large range actually of what this confidence score could be. Let's look at a few examples. In my first example, I used an image from our robot shop example and I ran my own model on it. And sure enough, it had a very high confidence, this is a stepper motor. Now, on this next examples, I'm going to use some of the models that we have in our model gallery. So, don't get me wrong. I don't want to compare the quality of the models. It's about like actually what did confidence they return and what actually want to do with this. So, what did this tell us basically when we want to classify this image? Well, it's not that bad, but it's really sure about it either. The confidence score of 0.395 is not particularly high, but yes, it has a sand part, there's some beach involved. So, that's usable basically as a result when I want to search for it, but might I label the image with that? It's probably questionable. Let's look at this next example. Girl on a scooter. What did the classifier do with this? Well, I'm not sure she's so happy to be called a sweet potato. Let's look at one more example. So, here's a screen shot of my code. What does the classifier do with that? It thinks it's a website. Computers are so dumb. So, some conclusions about our confidence scores. Does 1.0 always mean it's a 100% correct? Not necessarily. It will fill the criteria of the algorithm, but our perception, as we saw particularly with the sweet potato is quite different. So, when you create an application that wants to take advantage of that, please keep that in mind. Think of it. If you would write a medical application and saying, "Oh, you have cancer," that might be a very strong argument where you want to probably soften that a little bit depending on like how sure you can really be on the results. So, there are two techniques that you can use for this. As we saw in the robot shot example, I used a threshold on the confidence score because I really label the image and I you know, when it's actually filtering everything out that had a low confidence score. If on the other hand, I want to create a search application, I might actually use some of the images that I had and still show them, probably on the bottom of the search because there's still a valid choice, perhaps in the search results. As usual, we find some more information on our website. And we have our lab tomorrow at 3 p.m. Please stop by. Ask your questions. We're there to help you. And with that, I would first of all like to thank you all for these great application that you create with our technology. I'm looking forward to see what you can do with this. And thank you all for coming to WWDC. Have a great rest of the show. Thank you.  Good morning everyone. Happy Friday. My name is Skylar Peterson. I am an engineer on the iOS Accessibility team at Apple, and with me today I have Bhavya, an engineer on the Mac accessibility team at Apple. And together we're going to delve into what it means to create an exceptional experience in your app when it comes to accessibility. As a quick refresher, we tend to think of accessibility as meaning making technology usable by everyone. At Apple, we categorize the way in which we make that technology accessible into four areas of focus, cognitive, motor, vision, and hearing. Now we have a bunch of great features that we work on to address all of these areas, but at the end of the day, it's the work that you do in tandem with our assistive technologies that make our platforms stellar places for people with disabilities. Making sure that all the great content you work on is accessible to everyone creates a better, more inclusive community for all of us and fundamentally improves the lives of millions of people around the world. For today's talk, we're going to skip past a lot of the basics of making apps accessible. However, if this is your first exposure to accessibility or you've never checked out any of our accessibility APIs before, then I highly recommend that you check out some of the sessions from last year's WWDC. There's still a lot that you're going to be able to gain from our session. It may just feel like it's going a little fast for you because today we're going to focus on going beyond the basics, and I want to start by focusing in on this one word, usable. Usable is great, making sure that all of the content of your app is even visible to an assistive technology is a critical first step. But when it comes down to it, we don't want our apps to just be usable, right. No one's going to be out there bragging that their app is usable. They want it to be exceptional. We want people who use them to be delighted when they do. It may not be clear to you exactly what that means when it comes to accessibility, and that's what we're here to talk about today and hopefully provide you with some helpful guidance on how to do that. I'd like to focus on two main areas and the different considerations that you'll want to make for each when evaluating the accessibility of your own app. First, visual design, and second, the way that an assistive technology user experiences your app. So let's start with visual design. I'd like to begin by addressing transparency and blurring. We use blur and transparency across many parts of iOS. It creates a vibrant, unique look and feel for our operating system. But for some people, particularly those with low vision conditions, blur and transparency can have negative impacts on legibility and even cause a degree of eye strain. So we provide an accessibility setting to reduce blur and transparency, and it can have dramatic effects like this. Instead of blurring the backdrop, we fade it so that you still have the context of where you are in the system, but the visual noise is reduced, and the controls which before had color bleed-in from behind are more legible on their solid backdrop. Same with folders and finally spotlight. Now this is a particularly good example, because we've taken a sample of the user's wallpaper so that we can color the backdrop, and they could still feel personalized and contextual to the device, but we still get that increased contrast and legibility of having a solid background. As a developer, you can cater your own use of blurring and transparency by checking whether or not the reduced transparency setting is enabled and adapting your UI accordingly. On iOS, you would use is reduce transparency enabled on UI accessibility, and on macOS, accessibility display should reduce transparency on NS Workspace. Next, I'd like to talk about contrast. Contrast between content and it's background is hugely important when it comes to perceivability. As colors get closer to one another, they become more difficult to distinguish. As well, certain colors that are legible at bigger sizes don't work when content is scaled down. For example, with text, the letters bleed together more easily at smaller sizes. Now you can compute the contrast ratio between a particular color combination to see whether or not it's going to be legible, and we followed the recommendations laid out in the web content accessibility guidelines that states that the minimum contrast ratio you should aim for is 4.5 to 1. Obviously, the highest contrast that you're going to be able to get is between black and white, which comes out to be 21 to 1 and obviously works great at all text sizes. Now let's take a look at a shade of gray. This particular gray will work great for larger text, but it isn't so great for smaller text because your eyes can't distinguish the shapes of the letter forms. It's contrast ratio 4.5 to 1 is right on the line of passable. Let's like take a look at one last gray. It may not be apparent on this really large display, but this text is really hard to see on a small device, even at a large font size. It's contrast ratio of 2.9 to 1 is just too low. And you can use the built-in tool in our accessibility inspector in Xcode to show you what the contrast ratio between a particular color combination is and what text sizes that color combo passes for. And we follow the same guidelines that I mentioned before in this tool. However, even with a ratio that exceeds the recommended contrast ratio of 4.5 to 1, for people with low vision conditions, colors that meet the bar can still be problematic when it comes to legibility. So we provide a setting for increasing the contrast across the system. In previous versions of iOS, this setting was referred to as darken colors. So in your code you can use is darker system colors enabled to check. Though if you're using a standard UIKit control that you set a tint color on, you will actually get this work for free. On a macOS, you can use accessibility display should increase contrast. Next we have sizing. Changing the content size of your device can have dramatic effects in the way that content is displayed and perceived. And this is hugely beneficial to low vision users. So let's take for example a calendar. On the left, we have this view at the default text size, and on the right I've blown it up several sizes to one of the larger accessibility sizes. If we simulate what this looks like to someone with a low vision condition, then the benefits become immediately apparent. That text on the right is actually still legible. On iOS, you can check what content size a user has set their device to, and there are seven standard sizes with large being the default, but you can also enable even larger accessibility sizes, which increase that by five more. Now there's a lot more that I could say about dynamic type, but I'm going to leave it at that and instead encourage you to check out some of the fantastic resources that we have to figure out the best way to get dynamic type working in your own apps. Now for some, changing the size of text is overkill, but the default weight of fonts and glyphs can still make it difficult to read. So on iOS there's a setting for enabling bold text, which will also increase the stroke weight of glyphs. If you're using stand built-in UI controls and a system font, then it's likely that you don't need to do anything to get bold text working in your app, but if you're using your own text solution or a custom font, or you simply want to do something like making the dividing lines in your app thicker when bold text is on, then you can check whether or not the setting is enabled and adapt as necessary. Next, we have motion. Animation is fun, and it often makes content feel more alive. It can provide a direct correlation between user interaction and what effects their actions are having. However, certain conditions, particularly inner ear conditions that affect the balance center of the brain can make motion and animation problematic, which often results in things like dizziness or imbalance or even nausea. Now I'm going to go through some examples of animation types that can be problematic, so if any of you in the audience have a problem with motion, then you may want to look away momentarily as I play the default animations, which I am going to preface. The first is scaling and zooming. On the left, we have the default animation for opening an app, specifically the clock app, which I'm going to show now. We zoom into the location of the app's icon while it's UI scales into view. And on the right, we have the same interaction but with reduce motion enabled, which will replace that with a simple cross fade. Next, spinning and vortex effects. If we take a look at the full screen echo effect in messages, which I'm going to show now, we can see that content is spinning along the Z axis while also changing in scale, and these types of motions paired together can cause issues. In the case of message effects, if reduced motion is enabled, then we provide the user with a specific prompt to play the message effect instead of having it autoplay. So they can choose whether or not they want to see it. Next, we have plane-shifting animations. What I want to show here is the Safari tabs animation. So I'm going to tap on the show tabs button, and you see that the plane of the web cards is shifted to simulate a 3D space. And on the right, we have the reduced animation, which again is a simple cross fade, but we also blink the card so that you have the context of which one you're coming from that you got in the original animation. Multidirectional or multispeed motion is another trigger. I'm going to show the movement of the messages bubbles in a conversation. It's a little hard to see in this video, but it's a lot more obvious on device. As I scroll the messages, they have a springiness to them where they contract and expand away from one another. If we have reduce motion enabled, then we disable this effect and have them just scroll normally. Finally, we have peripheral movement. The weather app on iOS has subtle animations that play in the background to indicate the current weather condition. You cans see on the left that the clouds are sort of drifting through the air and the sun is sort of shimmering and shining. But if you're scanning the forecast below, then this animation is in your peripheral view, and this can be problematic in the same way that reading while riding in a car can make you feel sick. The horizontal motion above your area of focus triggers a reaction in your brain. With reduced motion on, we disable the background animation. Now typically we don't want to just remove all animation, so of course we have a setting that you can check to see if motion should be reduced and adapt accordingly. On iOS, you check is reduced motion enabled, and on macOS, check accessibility display should reduce motion. It's important to note that simply removing an animation is often not an adequate experience. You don't want to lessen the experience. You just want to add something else equally fun or engaging but different that works for the user. Finally, I'd like to finish talking about design by addressing UI complexity. Apps play key roles in our lives now, and it's critically important for all of us that our technology is simple and easy to use, that it enhances our lives instead of adding extra unnecessary burden. In the United States alone, one in six children have a developmental disability ranging from speech and language impairments to more serious developmental disabilities like autism. For people with cognitive disabilities or even a chronic illness, using an app can expend more energy than for someone who is neurotypical. So how do we make sure that our apps are simple and the least burdensome that they can be? Well they should be easy to navigate by having them similarly structured and through clear, logical cause and effect. We should be able to start using our apps and complete the most common tasks without encountering any barriers. And finally, our apps should behave in a consistent manner so that when I learn something one place, it applies to another. Using this standard UIKit views is great because people get familiar with way they work, but UIKit also functions as a design language for iOS. So if you're doing something custom, consistency with a parallel control in UIKit will help people intuit how to use your app. This really all boils down to the way in which people experience your apps, which is just as important for people who use assistive technologies like VoiceOver or Switch Control. So I'd like now to spend some time speaking to how you can improve the experience for those accessibility users. The experience for assistive tech like VoiceOver and Switch Control can be quite different from your typical user experience. VoiceOver uses many gestures like swipes and multifinger taps, while Switch Control scans groups and offers logical quick actions. For both, there are built-in equivalents for any standard gesture or interaction, but what makes experiencing an app with these technologies exceptional? We want to go from the bare minimum of it works to it works well. While despite the fact that assistive tech users experience your app in ways that are different, the same design principles that drive a good experience for a nonaccessibility user applies here. You want easy navigation, predictable behavior, prioritizing our action, and consistency. It's also important to note that if something is communicated contextually by separate elements, that same context is conveyed to our accessibility users. I'd like now to bring Bhavya up on stage to run through a quick audit of the accessibility experience of an app. Bhavya. Good morning, everyone. My name is Bhavya, and I'm a software engineer on the Accessibility team at Apple. Today, I'd like to talk to you about enhancing the accessibility experience of your app by sharing with you an app that Skyler and I have been working on. The name of this app is Exceptional Dogs, and the goal is to help make the dog adoption process easier by allowing users to browse a collection of dogs who are in need of a loving home. Let's take a look at this app. This is Exceptional Dogs. At the top, I have a carousel like UI, which is essentially a collection view of all the different dogs that I can browse. In the bottom left corner, I have a favorite button so that I can favorite particular dogs I like, and in the bottom right corner, I have a gallery button, which brings up a modal view with additional photos of the dog. However, not all dogs have a gallery button. Notice how this button fades away as swipe to Layla, who has no additional pictures. Beneath the carousel I have a stats view with information about the dog like its name and breed. Notice how this information updates as I swipe to the next dog in the carousel. Finally, at the bottom I have the name of the shelter with two buttons which can open the location of the shelter in maps or call the location. So there's quite a bit going on here in terms of visual complexity, but Skyler and I done our due diligence, and we've sprinkled in a little bit of accessibility, just enough to make sure that VoiceOver can reach every element on screen. So let's turn on VoiceOver now and see how a nonsighted or low vision user would digest all of this complexity. VoiceOver on. Exceptional Dogs. Pinky. So VoiceOver lands on the image and correctly announces the name of the dog. Great. I'm going to swipe from left to right now to scroll through the carousel. Layla, Bro, Pudding. Favorite off button, Pudding, Bro. So VoiceOver was able to correctly scroll through the carousel, but we weren't able to reach the favorite button until we hit the very end of the carousel. I could, of course, still get to the favorite button by placing my finger on it like this. Favorite off button. But it's important to remember that this is not how a typical VoiceOver user navigates. Rather, VoiceOver users swipe to reach to the next element, and currently, using swiping we aren't able to reach the favorite or gallery buttons until we hit the very end of the carousel. This isn't a great or exceptional experience because it really limits my ability to favorite or view the galleries for virtually all of the dogs in the carousel. I'll place my finger over the modal view now to activate the gallery button. Show gallery. Show gallery. Picture one of Bro. Image. Let's scroll through our gallery. Picture two of Bro. Picture three of Bro. Pinky. Layla. So, we were successfully able to scroll through the gallery with VoiceOver but focus went behind the modal view to the content in the background. This experience might seem workable because it gets the user to the content that they need to, but it's not great or exceptional because allowing focus to go behind the modal view is confusing and distracting for the user. I elected the modal view now by double tapping on the close button. Close, Layla. Let's swipe through our stats view. Name, breed, Layla, Labrador. When I hear a name, I expect to hear the actual name of the dog right after, but instead I hear breed. Visually, the layout of all this information makes sense, but to VoiceOver, its structure causes it to announce information out of order. This creates a lack of context for the user and places unnecessary cognitive load on them. I'll place my finger over the name of the shelter now and swipe to the buttons next to it. Skyler's animal shelter. Open address in maps. Button. Call. Button. Once again, these buttons are perfectly accessible like the labels from the stat view, yet they lack context. When I hear call, it isn't clear to me what this call action is referring to even though visually this is pretty evident since the call button is right next to the name of the shelter. VoiceOver off. So we found a couple of issues here. These issues don't completely render our app unusable, but they do place unnecessary cognitive load on the user. Remember, our goal here is to create an exceptional accessibility experience. I'd like to invite Skyler back to the stage now to talk about how we can elevate our experience for Exceptional Dogs to be exceptional. So we found some issues with our app. So let's take a look at some of our accessibility API that we can leverage to resolve them. From Bhavya's audit, we found some issues with our carousel. We could get to the favorite and gallery buttons but only after reaching the end of the list instead of for each individual dog. As well, we couldn't get to that display data below for the dog until reaching the end of the list. We need the actions and data for each dog to be quickly reachable. So what we can do is use UI accessibility element to create a new element in place of the collection view. UI accessibility element is a class used to create custom elements oftentimes for representing parts of your UI that are inaccessible, like something you've drawn in Core Graphics for instance. But we can also leverage it to create a custom experience, which is what we're going to do in this case. We'll need to return any custom elements from the accessibility elements property for the view that would contain them. This tells our assistive technology what the accessible sub elements of a given view are and the order that they should be navigable. So if we get our custom element in place, we can swipe to the content for each dog easily. Great. But then how do we change dogs? Well, we can take advantage of accessibility increment and accessibility decrement. By adding the adjustable trait to our custom element, we'll let the assistive technique know that our element responds to these callbacks so that in the case of VoiceOver when a user swipes up or down with their finger, then the collection view will scroll to the next or previous item. Now, next let's take a look at these information labels. When Bhavya was swiping through them, they were out of order, and the value labels lack the context of what information they were providing. Context is hugely important. If I just touch on the value label for whether or not the dog is fostered, all I'm going to hear from VoiceOver is yes. Well, yes what? What does that mean? What does that correspond to? As well, we have an issue with navigation. We're looking at having to swipe through 14 different labels just to find what we're looking for. But combining them would cut that in half and makes navigation much, much faster. It makes more sense to group these labels together as a single element so that I understand the relationship between label and value. We again can use UI accessibility element to accomplish this by creating elements for each pair of labels and returning them from the containing views accessibility elements property. We also saw that when Bhavya was swiping through elements in the app that the shelter info view was split up into three distinct elements, the name of the shelter, the location button, and a call button. The buttons have the same issue of lacking contexts as labels. If you reach them first without reaching the name of the shelter, then it's unclear what they correspond to. Having these three elements exist separately increases the amount of elements that a user is going to have to navigate through to find the content they're looking for. So it makes more sense to combine them into one and give us quicker navigation. As well, we want to think about what the priority is here. The most relevant piece of information is what shelter the dog is housed in. So we'll provide that first and add supplementary actions for locating and calling the shelter. To accomplish this, we can use our accessibility custom actions API. This allows us to add actions to an element that the user can cycle through until they find the one they're looking for and activate it. You can return an array of custom actions for a given view, and then for each action, you have a name, which is conveyed to the user as what's going to happen when they activate that action, and a target selector relationship, which will execute the action in your code. So we'll make that whole shelter view accessible and add custom actions to it for getting the location and calling. Finally, we have the gallery view. When Bhavya brought this up, we could swipe through all of the elements in the background, and though this view apps modal, that isn't the case, because instead of housing this view in a view controller and presenting it modally, we instead opted to just add it on top of a view hierarchy. And so since the content in the background is still visible, VoiceOver doesn't know that it should exclude it. Although everything in the gallery is accessible to VoiceOver, it gets confusing to the user whose lost the context of where they are and what effect their actions are having. To fix this, we want to override accessibility view is modal on the presented view, which will tell our assistive technology that only the elements of that view should be accessible right now. We'll also want to post a notification to let the assistive technology know that our on-screen elements have changed and that it should attempt to refocus. We can use the screen change notification to do so. I'm going to turn things back over to Bhavya now to show you how to use all the API we just covered to implement our solutions. Bhavya? Thanks Skyler. Let's elevate accessibility experience of our app from great to exceptional. I'll jump into Xcode. Let's start with enhancing the experience of the carousel at the top. Here, I have a class called Dog Carousel Container view which houses our collection view of the dogs, our favorite button, and our gallery button. As Skyler mentioned, we'd like to implement the increment and decrement functionality so that we can swipe up and down to scroll through our carousel. To do this, we can create a property on this class called carousel accessibility element, which will be a custom UI accessibility element that supports those gestures. Let's implement carousel accessibility element. This will be a subclass of UI accessibility element. It also needs to know about the current dog object it's representing, so let's create a property for that. We'll give it an initializer also, and let's give it some basic accessibility like the label and the value. Our label will be dog picker, and the value will be a combination of the name of the dog and it's favorited status. Okay. Let's get into the increment and decrement functionality. First, we'd like to override accessibility traits to return the adjustable trait. Let's first implement accessibility increment. We want our increment function to scroll the collection view forward. So let's create a function that does exactly that. Here, I have a function called accessibility scroll collection view. It takes in a bullion, which determines if the collection view will scroll forwards or backwards. Let's dive into this function. I get a reference to my container view, my current dog, and my list of all the dogs. If I'm moving forwards, I get the index of my current dog, and I validate that it doesn't exceed out of bounds, and if doesn't, I'll ask my collection view to scroll forward by one index. I'll do something similar if I'm going backwards except I'll make sure that my index won't become negative, and then I'll ask my collection view to scroll backwards by one index. Let's go ahead and make use of this function in our increment function. I'll pass in true to indicate that I want to scroll forwards. And similarly, I can implement accessibility decrement, the only difference being that I'll pass in false since I want to scroll backwards. And that's it for the carousel accessibility element. Let's scroll back up to our container view. Here, we need to expose the carousel accessibility element, and to do that, we can override accessibility elements. Within this function, I'll get a reference to my current dog and make sure it's valid before I go ahead and create my elements. Let's start by instantiating the carousel accessibility element. I'll first get a reference to the carousel accessibility element if one already exists, and if it doesn't, I'll instantiate one here. I'll set it's accessibility frame to be the frame of the dog collection view, and then I'll store a reference to it here. And at the end, I'll return this array of elements. Before I do that, I want to add some elements to my array. I'll check to see if my current dog has more than one image, and if so, I want my array to comprise of the carousel accessibility element, the favorite button, and the gallery button, but if the dog doesn't have more than one image, we should probably omit the gallery button. One thing I need to remember to do is to update the current dog property on the carousel accessibility element if that corresponding property gets updated on my container view. We're almost done. Recall that when I swipe to the carousel, the information, the content of the stats view is changing. However, when I'm swiping with VoiceOver, I need to let the app, I need to let VoiceOver know to update it's understanding of the accessibility elements on screen. To do that, I can post a layout change notification whenever a scroll occurs. I'll head over in my view controller file, and inside my scroll view did scroll view at the very end I'll post that layout change notification like this. And that's it for the carousel container view. A core part of this solution was using and creating accessibility elements, and I can use this same concept to help me solve a different problem in the stats view. Recall that in the stats view all the information was separated, and it lacked context. I can use this same concept to help me group the information for the stats view together and to add context. I'll head over to the doc stats view where all of these stats are laid out. I'll start by overriding the accessibility elements to create my elements. I want to cache the elements I create from this function so that I don't have to recompute them every single time I make them. To do that, I can create a private property called underscore accessibility elements, which will act as that cache and store these elements. Let's get into my function. First, I'll see if my cache is empty, and if so, I'll go ahead and populate my elements. I'll iterate over all my title labels, and I'll get a reference to the title label, which would be something like breed, and it's corresponding value, which would be something like Labrador. With the references to both the title and it's corresponding value, I can now create an accessibility element, which encompasses the information for both of these together. I'll start by creating my accessibility element here, and I'll set it's label and it's accessibility frame to be a combination of the corresponding values of both the title and the value. I'll add this element to my array of elements, and once I'm done looping over all the title and value pairs, at the end I'll set my cache equal to this array, and I'll return these elements. One thing we need to remember to do is to reset our cache every time the dog object on this gets updated so that we can compute the new elements for the new dog. To do that, I can simply set it to no over here. And that's it for the dog stats view. Next, let's work on adding some context to the call and open location buttons. I'll head over to the shelter info view where I have a reference to the name of my shelter, the map button, and the call button. We'd like the actions of both of these buttons to be custom actions on their containing view, and to achieve this, we can start by exposing that containing view as an accessibility element. Let's go ahead and give this a descriptive label, which would be the name of the shelter. Finally, let's override accessibility custom actions. Within this function, we'll create two custom actions, the map action and the call action. VoiceOver will describe these as open address in maps and call, but be sure to correctly localize these strings. Each of these actions will call a corresponding function, which activates that button. So, for example, map action will call activate map button, which activates the map button. I've conveniently defined this function right here. Be sure to return to [inaudible] to indicate that your method was successful. And that's it for the shelter view. Last but not least, let's work on improving the experience of the modal view. I've defined my modal view in dog modal view, and as Skyler suggested, to help VoiceOver recognize this as a modal view, I need to override accessibility view as modal. We're almost done. One thing to remember is that when I press the gallery button, we need to let the app know that the layout of the content on the screen has changed so VoiceOver can focus to the right content. To do that, I can post a screen change notification whenever the gallery button gets pressed. I'll head over to my view controller file, and within my gallery button press function, once I animate in the modal view, I'll post a screen change notification like this. And that's it. I already have the app prebuilt with all this new accessibility work, so let's take a look at what the new and improved experience is like. I'm going to turn on VoiceOver. VoiceOver on. Exceptional Dogs. Dog picker. Pinky. Adjustable. Swipe up or down with one finger to adjust the value. VoiceOver tells me I can swipe up or down on this. Let's go ahead and do that to scroll through our carousel. Layla. And I'll scroll down, sorry, swipe down. Pinky. I can also swipe from left to right to get to the favorite and gallery buttons like this. Favorite, off, button, show gallery, button. Now, not only is every dog in the carousel accessible to me but so are the favorite and gallery buttons from every dog in the carousel. I can swipe once more from left to right to skip the carousel and get to the stats view like this. Name, Pinky. So I don't have to navigate to the whole carousel. This feels like a much better experience now. Let's go ahead and keep navigating through our stats view to see what the experience is like. I'm going to swipe from left to right to navigate through it. Breed, mixed. Age, 7.0 years. Since this information is grouped together now, it provides me with way more context and navigation is faster since I don't have to navigate through each and every individual label. I'll place my finger over the name of the shelter now. Skyler's animal shelter. Actions available. VoiceOver informs me that there's actions available on this, so let's swipe up to hear them. Open address in maps. Call. Swiping up makes it clear to me that these actions are in reference to the name of the shelter and will be performed on them. It gives me way more context. Finally, I'll activate the modal view by double tapping on the gallery button. Show gallery, button. Picture one of Pinky. Image. I'll swipe from left-to-right to navigate through it. Picture two of, picture three, close, close, close, button. Focus no longer goes behind the modal view. This is a much better experience now. VoiceOver off. Well you just elevated the accessibility experience of our app from simply working to exceptional. We've achieved this by shaping the experience around the user and modelling the interaction around this experience, and in doing so, we've lifted a huge cognitive burden off the user. Your users will love this extra bit of effort you put towards the accessibility of the app so they can love it as much as anyone else. I'd like to hand it back to Skyler now. Thanks, Bhavya. Hopefully Bhavya's demo demonstrated for you how small tweaks can transform the accessibility experience for the better. It's important to note that creating custom interfaces for the accessibility of your app is not always the right answer. Sometimes the simple fix is the best fix, but what you really should be doing when making your apps accessible is thinking through beyond the surface level what it will really mean for someone who's using an assistive technology to use your app. To wrap this up, I'd like to return to where we started today. Usable is a great first step for making apps accessible, but we can do better. We can and we should provide our users with exceptional, delightful experiences that cater to their unique needs. There are a lot of ways to take an accessible app from being functional to being exceptional. Designing for accessibility up front can get you a lot of wins. Doing things like making sure your colors meet minimum recommended contrast ratios or that your layout can adequately adapt to changing size have benefits for all users. But you should also extend you default design to accommodate the settings that users have enabled. Finally, when crafting the accessibility experience for assistive technologies, do so purposefully. Prioritize your content. Make it easy and intuitive to navigate and provide context when necessary. For more information as well as access to a simplified finished version of the sample app you saw there, you can visit our session link on the developer portal. As well, right after this session we're going to be going over to tech lab 9 for an accessibility lab where you can bring any questions you have about making your apps accessible or about elevating the experience of your own app in terms on accessibility. So please come on by if you have any questions. Thank you and have a fantastic rest of your WWDC.  Hello everyone, I'm Julian, later we'll be joined by Hugo who designs sounds across our products. Before we begin I wanted to explain a little bit about why we're going to be talking about a button for a while here. I design and prototype user interfaces for future hardware. Because of the nature of my work I can't always use standard UIKit. I have built and rebuilt a lot of basic user interface elements like scrolling, zooming, paging, and of course buttons and I've learned a lot about the details involved in designing and building these controls. So I'm here to share a little bit about what I've learned with you. I want to show you how something so simple, a button, has a lot of consideration and complexity behind it. And I hope that through looking at this button you can learn a few things. I want you to use standard controls more thoughtfully. To consider when to build your own custom controls. And when you do build custom controls I want you to consider your interactions all the way through. So let's get started. Hugo and I are expanding our artisanal toast app empire. We are building a new simpler app that is connected to a toaster. If you imagine for a moment you're getting ready in the morning, you want to make sure you have breakfast ready by the time you're heading out the door. All you have to do is launch our app, press a button and expect a fresh piece of toast waiting for you in your kitchen. Now this works with our connected toaster. Our connected toaster goes through a few different states, it's ready and waiting for a new piece of toast, it's making toast, and finally the toast is done, the important part. But the way that our connected toaster ends up transitioning between these states is people using our app. When they use our app they can request a piece of toast to start the toaster going. When the toast is done they will of course eat it. Maybe for some reason they'd want to cancel it, I'm not really sure about this one. And of course we want to make sure our toaster is always ready to serve. I'm not going to disclose exactly how this works but trust me it does and it's great. We also have big plans for the future, we could have AR toast, queues of toast, multiple toasters per household, and eventually we'll connect this back up to our toast social network. My point is there's a lot going on in here and for today's session we're going to dive deep into the details of a single screen, in fact a single button on a single screen of our app. If we get to the details of our most important interaction right the rest will flow from it. So if we look at our system again I think the most important interaction here is actually when we request toast. Obviously, all the rest of this has to work, but requesting toast is the first impression and if people aren't able to start the toaster the rest of it doesn't really matter. So to do this today we're going to start by talking a little bit about what a button is, then I'm going to share with you a way of thinking about interactive controls. And finally, Hugo will join us at add some sound to our Button. So what is a button anyway? The buttons that we are talking about today or onscreen user interface controls. Buttons are indirect controllers of action. What I mean by this is when you tap a button the action happens somewhere else. This is as opposed to a direct interaction where here I'm grabbing the lever and controlling the lever directly. Direct interactions can reference the real-world experience and feel more fun. But the real power of indirect interactions like buttons is that they can be clearer and more powerful because of their separation. The interesting thing here being that even the physical buttons that these onscreen buttons are imitating are indirect. Even though the action, the result is happening somewhere else the button controls that action. And just like the action a result can be designed a button can be designed, we can consider both of these things separately and connected. To do this today we're going to be relying heavily on a single lens of design feedback. Feedback is how you let people know what your app is doing. I'm sure a lot of you have thought of a variety of different ways that you can provide feedback within your apps and are already doing so. But I think there are two major categories of feedback. The first is that you can tell people, you can literally put text on screen explaining what's going on or design icons that explain things. The second is that you can show people, you can allow people to experience the results of their action more like the real world through visual, audio or even haptic change over time. Now we're going to apply feedback to three phases of an interaction, not just its completion. We can think of these phases in terms of a physical button. Before I have pressed the button it's just hanging out there. During my interaction with that button it's being pressed in. And finally after I've pressed that button my finger is lifting off of it and it's returning to its original state. So what does it even mean to be providing feedback before I have begun interacting with a button, why does this even matter, the button's just there right. Well if we think about our iPhones for a moment, the iPhone screen is just a piece of glass. This piece of glass happens to have glowing pixels underneath it and what matters is what people think those pixels mean. Glass doesn't tell you that it's tappable, but something on screen might. You could see that you could page through something, drag something or of course for the purposes of this presentation tap on something. The academic term for what I'm talking about is perceived affordance. Affordance refers to the connection between an object and the person interacting with it. A person might know that they can grab a handle or press in on a physical button. But the perceived part of this is talking about what people know about those glowing pixels on screen. People only know what they can interact with on that iPhone screen because of their prior experience having used iOS before and their current context, the fact that they're on this phone in an app that they may or may not understand. So let's apply this to our toast app of course. How do I know what will happen when I tap this button, I just put a button up and call it okay right, no. What we do is provide a label, a label is feedback in which you are telling people literally what is going to happen. In this particular case I tried the label toast, but unfortunately toast refers to both the verb to toast and it also refers to the delicious piece of bread you're going to be eating for breakfast. So what should we call this label, toast toast? No. We could try to design an icon for this button but I think this isn't particularly clear, especially for something that people will be encountering for the first time in an unfamiliar context. So we thought about it a little bit and we came up with the phrase make toast. I know that in our diagram we refer to this as request toast but we spent some time thinking about what people using our app might understand and what feels most comfortable for our app. The next thing to think about is how do I even know that this is a button, just a rectangle floating on a black background might not be enough. But if you add some context like an iPhone screen you might start to understand it or you could add some prior experience. It could be the shape of the button, the manner in which is rendered or if it's used consistently enough just the color of text. In this case, I don't think people are going to be very familiar with our app they're going to be launching it once a day, more if we're lucky so we're going to focus on more standard iOS button shapes because we don't want to throw in something new. So we try these out on screen. Another thing that we can think about with respect to feedback before we begin using this button is how do I even know what this button relates to. It could be something about where in the app a button is located, these buttons have different meaning and feel. Or it could be something about how it's grouped with other controls or how close it is to other objects on screen. I'm starting to like this last one here so we're going to try it out on a real screen. I drew this user interface in Keynote and I put it on a phone and I'm holding this phone in my bedroom where I might expect to use it in the morning before I order toast. And I'm just trying to understand do I know what this button will do. The next thing we should think about is what happens during my interaction with this button. For a physical button I'd be pressing it in. Now the term feedback refers to telling people something about the result of what has happened and in this case, we haven't actually done anything yet, so we might use the term feedforward for this. Because feedforward refers to helping people understand what is happening while they're interacting with this button. Feedforward is a component of making interactions feel fluid, you can convey responsiveness for things like 3-D touch, pinch or slide movements. Thinking about how feedforward applies to our button we should be thinking about what is happening now, what is happening while my finger is making contact with the screen, with that button. In the case of our overall toast system we are in the middle of touching a button to request toast. So the part we're going to focus on is the toaster being ready and eventually transitioning to the toaster making toast. We should be thinking about what is happening and how I can hint toward what will happen. Let's say I've begun touching this button, how do I know that I've begun tapping this button, why does it even matter? Well, for one, the hit area of this button might not be what you expect, if you look closely at the circle maybe I'm not quite touching it. But perhaps the hit area is actually a little bit larger than the visual size, this is particularly important for very small controls if we made that lever interactive. So how do I know I've begun tapping this button? Well you might be tempted to animate the button, to have it fade in or grow very slowly, but this can make things feel unresponsive. This kind of animation might be appropriate for 3-D touch if it's responding continuously as I'm pressing in. But for a quick contact with the screen we might want to do something faster, if we are going to animate it do it very quickly. So what we do is provide a confirmation sound, a haptic or visual change. In this case, we're just going to highlight it instantaneously, make it feel a little bit faster. This also helps us know that a button is enabled and that it can work. Maybe we try connecting the feedforward on the button to the feedforward of the toaster itself, as I said we can start to hint toward what will happen. So here what I'm trying is as I touch this button maybe we show the piece of toast starting to appear. I feel like that might be a little bit too much for this button though, so I'm not going to go forward with that but I tried it. Another thing that we should think about with feedforward is what happens if I change my mind, let's say I'm kind of groggy in the morning, I pick up my phone I accidentally touch this button but I don't want to start my toaster yet. Well for most buttons I can actually drag my finger outside of the button to begin canceling it. This is something that's actually very important for all sorts of fluid interactions because it's what makes a fluid interaction undecided. If you're paging through different screens you don't know which screen it's going to end up on until you lift your finger and the same thing applies to a very simple button. I can bring my finger back inside the button and have the same feedforward show me that I am now activating this button again. Now that we've decided a little bit about the feedforward that we're going to have we should try this out on a device as well. So again, I know this seems really simple for a button, but this is very important for scroll, pinch, paging, a lot of touch interactions. Finally, we should think about how to apply feedback after I have lifted my finger up off of this button. For a physical button it's going to return to its original state. For our onscreen button we're going to have lifted our finger off the screen. This is of course the realm of classic feedback that most of you are familiar with. But the thing I want to point out here is that we can be thinking about feedback both for the button itself and the action, the result that is happening, and we can think about how to connect the two so people understand what has happened. Again, within our system we're only focusing here on getting people into making toast. So what happens when I lift my finger from this button? Does the unhighlight happen immediately or is there a slight delay so that even fast taps can be seen? Is there a timer, does this button support double tap, if so I may have to wait a little bit before showing the confirmation? If we think back to the two types of feedback we can just literally tell people what is happening. I can put some text on screen, I think it's very clear but it also requires reading and more text. So maybe we can show people, maybe the button itself provides some feedback here it's flashing or maybe it's something that represents the action that provides feedback. For example, the icon of the toaster itself could show you that it's starting to toast. Or what we can do is we can just do both, I can put text on screen and I can show the toaster. This is great because I might have missed that animation, but I can still see what state the toaster is in. But similar to interacting with the button with feedforward I should think about what happens if I change my mind. I've started toasting but maybe I want to cancel, maybe I want to stop the toaster. We could replace that button with a stop toasting button, but I think that's not particularly clear because this button looks an awful lot like the make toast button that was up there before. So maybe there's a separate stop button. I feel like that red is a little bit too much, so maybe we try that same button but separate from our make toast button. This is starting to feel a bit better, so what I should do is try it on device again. We should see how the feedback on the button itself connects to the feedback from the toaster communicating what has happened. To recap a little bit of what I went over. Feedback is how you let people know what it is that your app is doing. You can literally tell people, you can put text on screen it's fine or you can show people, you can allow them to experience the change over time. We can apply feedback to all three phases of an interaction, even a simple button. And something I was implicitly doing here is trying out options to understand what's too little and too much. The only way I can really know if an animation is appropriate or not is to try it. So what we've done here is we've designed a very important part of a much larger system, but I think it'll be a lot easier to figure out the flow of the rest of this once we get this one piece right. Now that our button is designed and pretty great I think we can start to think about sound, so I'd like to invite Hugo up to the stage. Thank you Julian. Hello everyone. Now we can make this quick and add a simple click sound to this button. Sounds pretty good or could it be better? Could Julian have done a better job and invited me a little earlier on in the process? I'd like to tell you a few things about how as I a sound designer could approach a project like this. I'll talk about where to look for inspiration, about designing the sound, and some key building blocks that we use while doing this. But before we dive in why do we talk about sound? Now some of you may think our users mute the sound on their phone quite often, is it important to add sound to my app? Well of course it's up to you but George Lucas has said that 50% of the experience of a movie is determined by what we hear, by its music and sound design. And sound is all around us, we use it to navigate through the world. And sound can help enrich the experience of your products. And it helps us remember the experiences of your app, as well as your brand, so it can have a big impact. So where do we look for inspiration when we want to design sound for a button? Well we can start by taking a little detour and looking at some real-world buttons. Some of them are made of cheap materials and sound like this, while others are made of high quality materials and sound like this. Well this was actually a recording of the volume button on an iPhone X, of course we amplified it quite a bit. These sounds though are essentially byproducts, they're not necessarily designed they're defined by the materials these buttons are made of. But because we're designed for software we have a lot of freedom, so we can choose to not play any sounds at all and often that is what we do. But if we do decide it's useful to add sounds to UI elements then we can take cues from real-world analogs to add meaning to these UI sounds and to help our users understand them. Now do you want to add sound to your app? Well it depends, think about category your app falls in, who will use the app and what will their expectations be, and what is the context, where will the app be used. For our app we know it will be used by people who like to eat toast and they're used to making toast in the kitchen, but from now on they can make it from anywhere in the house. They can still though use the audible cues they're used to getting from their toaster in the kitchen, so for that reason we think that this app is a perfect candidate for the use of sound. So let's look at the process of making toast and listen to sounds we encounter. Let's define if these sounds are useful and if they are if we want to use them in our design. So we'll use Julian's timeline for this and listen to the sounds that this toaster makes. First, we pull down the lever and then we press it in place. Now what we hear then is the toaster heating up with a subtle buzzing sound of the coil. [ Buzzing Sound ] Until our toast is done and the toaster ejects the toast. Now let's go through these steps one by one. First, when we pull down the lever we feel the resistance of the spring while we push it down, it's almost like if we compare it to what happens in our app it's like designing sound haptics and animation and synchronizing them all. But in our app, we decided to replace this with a simple button so we don't need to design a sound for it. And when we request the toast that is when we press our button, the next step, and that is what Julian asked me to design a sound for so let's get to work on that. We can record sounds, we can use electronic instruments, synthesizers to generate sounds and then we can use software to scope these sounds into the right shape. Now I'm not going into great detail about this today, but please watch last year's talk Designing Sound if you want to learn more about this process. In the end, we came up with three different options. This is option A, this is option B, and this is option C. Okay now option A is quite minimalist, it's so simple, it works pretty well with the simple UI of the app but it feels like there's something missing. And we found out that while we were designing these that it's much more satisfying if these sounds have two clicks, one when you press down and one when you lift off your finger. Now option B has this character, but and now it is a sound that's much more realistic, it doesn't really sound like a toaster but it does have this physical quality to it. For the same reason though it does sound a little harsh and a little metallic. And then there's option C, it's a bit of an odd one but it does have character and it's a sound that I would recognize if I hear it again if I use my app again. Another thing that's good about this sound is that it has this tonal confirmation built in, it's like throwing a checkmark telling you your action succeeded. I'll play it one more time see if you can hear this. And for all these reasons I think that option C is the one I would recommend going for, it's a simple sound, it has a friendly character, and it's something unique it's not something I'd recognize from any other app. Okay so it looks like we're done, at least we made our button click but we're not done making toast yet. So let's keep going, maybe there are some other opportunities for the use of sounds in this app. Now as good sound designers we want to tell the whole story. So after pressing our button the toaster heats up and we hear this subtle buzzing sound. Now we can -- is this helpful at all this sound? Maybe, it tells us something about the state of the device. So we can choose to play a sound that resembles this on our app as well, maybe a continuous sound, something like this. [ Notification Sound ] But making toast may take several minutes and having this sound play over and over again might not be the experience we're looking for right. So how about something a little more subtle like a timer sound that fades in and out every half a minute or so. That could work. But does this really need a sound at all because what we're really waiting for is what comes next, that's when our toast is done and this is what we want to notify our users about, that's the important moment. So we want to play a clean and simple notification sound that they won't miss, something like this. [ Notification Sound ] Okay so now let's listen to these sounds together. First, we press our button and then we decided for the next step when the process is active we don't really need a sound until we get our notification and the toast is done. And it's important to listen to these sounds together to make sure they family well together because this is how our app communicates to its users, this is the voice of our app. So now our app is in good shape. I'd like to tell you about some key building blocks of sound that we use when designing any kind of sound. The first one is timbre or tone color. And most of you will instantly recognize this one. It's a piano right, but how do we know it's a piano? How do we know it's not a vibraphone? It's determined by the material an instrument is made of, by its shape, and how it's being played using a mallet, a hammer, a bow or your hands. So these instruments can play exactly the same tone but sound very different. Now for our app we decided to go for a character, for a timbre that is friendly and it's not too harsh or metallic. The next one is frequency or the pitch of the sound. Some instruments play at very high frequencies and others play at very low frequencies. But also nonmusical sounds can be played at very high or low frequencies. And when we do this it gives an indication of the size of an object. So if we take our toaster and we record the toast eject sound, we play it at a very high frequency it sounds like a tiny little toaster. But if we take the same sound and play it at a low frequency it sounds like this big giant toaster. So next up is duration or the length of a sound. Sounds can be very short or ring out much longer. And if we design UI sound and we know our button is going to be pressed a lot of times in a row it's important for this sound to be very short and subtle. But if it's only pressed once in a while, which is actually the case for our toast app, it's okay if it's a little longer, if it's a little more elaborate. In fact, this sound only plays once every time we use our app. And then there's loudness or amplitude or volume. Now when we design ringtones or alarm sounds we don't want our users to miss them, we don't want them to miss their phone calls or sleep through their alarms. [ Notification Sound ] Our audio engineer Mitch is calling me from the back of the room and I hear it loud and clear. But if a UI sound would be loud like this it would become unpleasant. It only needs add this subtle [inaudible] layer to the interaction. Now let's quickly reply to Mitch. [ Keyboard Sound ] Now if I would miss this keyboard sound because I'm in a noisy environment it's not a real big problem, but I definitely don't want to miss my ringtones. So to recap the four building blocks of sound, our timbre, frequency, duration and loudness. And when you start thinking about designing sounds and adding sounds to your app it's really good to keep these in mind. And that concludes what I want to tell you about sound today, but before we end the session these are the most important things that Julian and I want to leave you with. Details are designed even if they seem obvious, even if they're as simple as a button, even if they're as simple as a click sound. Now you can take inspiration from real-world elements, but there's no need to copy them one-to-one. Also use the freedom and the flexibility of designing for software to create something that is completely unique and fresh. And then what we see, the animation, and what we feel, the haptics, and what we hear, the sound, they all combine into this one single experience. And lastly, but most importantly, learn by trying things out. You know we tried out a lot of things today and some of them didn't work that well while others worked great. But the only way for us to find out was to try them out first. Now I hope this all inspired you to spend some more time on the details you might normally take for granted. If you get these details right it can make your apps even more amazing. Thank you for listening, have a great day.  Good morning. My name is Michael LeHew, and I worked on the collections for the foundations team at Apple. And today, I want to talk about specific things that you should know about to ensure that your use of collections in Swift is as effective as possible. I'm going to cover a lot of territory today about details and all aspects of collections available for use in Swift. We're going to explore some common pitfalls and how to avoid them including performance issues, and I'm also going to offer very specific advice about when to choose to use specific collections. So let's begin. I want you to imagine a world, a world without collections. And in this world, we may not have arrays, but it still has bears, but every time we need another bear, we're going to need to define another variable, and we want to do things with these bears. We're going to need to repeat ourselves. Pretty scary, right? It gets worse though. This world also doesn't have dictionaries, but thankfully being clever developers, we can work around that with the function, where we painstakingly define each case that we care about, and using this function is very similar to using a dictionary, except none of those dynamic capabilities that we need. But who likes a mutable state, right? Fortunately for us though, we don't live in this world. Thankfully our world has bears and collections along with a rich first-class syntax for defining them. And APIs that help us not repeat ourselves when we iterate or retrieve the elements that they may store. Collections are so pervasive and share so many common features and algorithms that in Swift they all conform to a common protocol. Not surprisingly, it's called collection. And in Swift, collections are sequences whose elements can be traversed multiple times, nondestructively, and whose elements can be accessed via a subscript. Let's see, look at how they do that but considering an abstract representation of a collection. This could be an array defined contiguously in memory, a hash table, a red black tree, a link list or anything else that you can imagine. What matters is that it supports the concept of an initial index called start index. It can be used to access the initial element of the collection. It has an end index, which identifies the end of the collection, and collections support the ability to iterate from their start index to their end index. They can do this multiple times, and they also support a subscript to retrieve elements from the collection itself. Let's see what this looks like in code. Indeed, if we look at the definition of collection, it's declared as a sequence of elements. It also has an additional associated type called index, which must be comparable. It offers a subscript to retrieve elements from, or using that index, and we define a start and an end index identifying the bounds of our collection. And finally, we have a function called index after, which lets us get from one index to another. And this last function is so important because it allows the standard library to define many useful and powerful default behaviors with the incredible power of protocol extensions. Let's look at some of these. When you conform to collection, you gain access to an incredible range of functionality with properties that let you get the first and the last element or identify the count or check to see if a collection is empty. You also gain API that lets you iterate through a Collection using 4N syntax. And super useful functions like map, filter, and reduce. Now let's go ahead and make Collection even more powerful by adding our own extension. Collection already provides a way to iterate through every element, but I want a function that will let me iterate through every other element, skipping some of the values along the way and will do this by adding an extension to Collection. Let's start with the methods signature. We're going to call our function every other, and it's going to take a closure that will call on each element that we care about. We'll get the bounds of our iteration, and to find another variable, that starts at the start, which will mutate along as we go. We'll call the closure on the current element and advance our current index to the next one. And we may have invalidated our index at this point, we may have reached the end of the collection, so we need to make sure that we did that, and if we did, we can then advance our index one more time and thus skip every other element. And if we call this we see, if we call this on the close range from one to ten, we see that we skip the even elements. And so we see that Collections let us describe some really powerful behavior, but it turns out collections aren't alone. In fact, Collection is not the only protocol that we have. In Swift, we have access to a rich hierarchy of collection protocols, each greatly improving on the kinds of assumptions that we can make about our types. Let's go ahead and talk about a couple of these. We've already established that Collection lets you go forward from a given index, but there are also bidirectional collections, which let you go in the other direction. Now, of course, bidirectional collections are collections themselves, and so we can still iterate forward as well. The next most flexible form of collection is what's known as a random access collection, and with these, these add the requirement that it would be constant time to compute, or compute another index from another or to compute the distance between two indexes. The compiler cannot enforce this, and so when you conform to random acts as a collection, you're making a promise. But if you satisfy this promise, if you can deliver on this promise, the protocol gives you the power to access any index in the collection in constant time. And of course, random access collections remain collections, and so you can still iterate forward and backward. Now as Swift developers, we have access to many useful collections that conform to these protocols, collections such as array, set, and dictionary. But thanks to the general purpose utility of these protocols, many other types conform to these collection protocols as well, such as data, range, and string. Or the index collections, and they all gain access to all of this rich functionality by simply conforming to Collection in their own fashion. Indeed, once you know how any one of these types works, you can apply that knowledge to any of the others, and there are quite a few. So I'm going to talk about the details about how a type conforms to Collection, and it all begins with describing how it is indexed. Each collection has its own kind of index. And that index must be comparable. In some cases, the indices can look like integers, like arrays, but just because an index happens to look like an integer doesn't mean that you should use it like one. Now I want to ask a few questions that might have surprising answers. The first is how do we get the first element of an array, and instantly you probably all thought, well I'll just call array subzero. An array's index happens to be it. So you can sometimes say this and get exactly what you intend. But it isn't the best way to do this. I'm going to go ahead and ask the same question, but this time about a different collection. What's the first element of a set? Now this might seem like a really weird question, right? Sets are unordered. However, they are collections, and being collections, you can iterate through them, and when you iterate through a set, you're going to iterate through one element first, and so that's really the question we're asking here. So can I say set subzero? Happily, the compiler will say no, and that's because set's index type is an int, the type system wants us to use the correct index type. Lucky for us we already know how to do that. We know the collection protocol tells us that all collections have a start index, so let's go ahead and use that, and if we do this, this will actually work with all collections. Start index is always going to be the element of the first item that you would see when you iterate. But there's a nuance to watch out for when using indices to directly subscript into any collection. And that is, it can crash. I haven't actually asserted that these collections have contents in them, and so when you're, I'm just using start index, and these collections could be empty. It turns out though that accessing the first element in a collection is such a common task that there's an even better way. Simply call first. And if you call first, this is a lot safer, because the return time of this is optional, reflecting the fact that not all collections have a first element. So I have another question. It's the second element of a collection, and I want to emphasize collection here. It could be any collection, an array, a set, or something that doesn't even exist yet. Let's go ahead and add a new property to collection via protocol extension, and we'll call it second, and just like first, it's going to return optional, because not all collections have two elements. So, let's try writing this by subscripting 1. But here our zero-based indexing instincts are going to lead us astray, and then we'll be caught by the compiler again. We want this code to work with every collection, and not all collections use integers to index. So let's try a different approach. What I really want is one greater than the start index. But lucky here, the compiler will catch this as well. You can't add 1 to an arbitrary index type. Index types are supposed to be opaque or can be opaque. And what we really need to be doing here is we need to be using the API provided by the collection protocol to do this. So let's go ahead and do this. I commented out a sketch of the things that we're going to need to do to find the second element. The very first thing that we need to do is we need to check to see if the collection is empty. Collections are empty when their start index is exactly equivalent to their end index. So let's check for that and return nil, because such a collection doesn't have a second element. Oh, and so now we can assume that our collection has one element in it. We can use index after to get the second element or the second index, but we need to make sure that that index is valid, and let's see this visually. We advance after, but if the collection only had one element in it, we have now actually produced an invalid index, and if we tried to subscript with that index, we'd get that fatal error that we saw just a moment ago. So we check for it to be valid, and this is very similar to the empty [inaudible]. We just make sure that our index is not equal to the end index returning nil. Again, because two-element collections don't have a, or one-element collections don't have a second element. At this point, we have all the assumptions we need to know that our collection has at least two elements in it, and so we're safe to use the subscript operator with that index, retrieving the value that we desire. Now, it turns out, or that looks like a lot of code, but it's worth pointing out that this general purpose index math will work with any collection, which is pretty awesome, and it turns out Swift has a better way to do this though. There's something called slices, but before I show how to do it with slices, I want to talk about what slices are and how they work. Slices are a type that describe only part of a collection. And every slice has their own start and end index, and slices exist separately from the collections, from their originating collection. And what makes slices so efficient is they occupy no extra storage. They simply refer back to the original collection, and when slices are subscripted, they read out of the original buffer. And they can do this because they share the same indices as their underlying collection. And let's take a look at how that works. We can prove this to ourselves. We'll start with an array, and we'll ask that array to drop its first element, producing a slice that's one element shorter. And because we care about proving about the indices, we'll actually get the second index of the array by asking to advance one after the start index, and then we'll compare those. And indeed, they are the same. Now this dropfirst function looks exactly like we need to succinctly get the second element of our collection. So let's go back to our previous solution, and see how much more expressive we can be with slices. Remember all that fancy index [inaudible] code we had to write earlier? Well, by using dropfirst, we're going to let slices take care of all that fancy index bounds checking for us. And since first returns an optional, this will work as expected with empty and single element collections. Let's visualize what's happening here. We start with an array, and we form a slice by dropping the first element. We then use the first property to subscript into the slice, retrieving the element from the original collection. Now I don't know about you, but I'd much rather maintain this code. Now every type is free to describe its own slice type, and many do. For instance, arrays define array slices that are especially attuned to the most common use cases that arrays work with. Similarly, string defines a substring slice type, and substrings, again, are tuned to the special cases that are most common with strings. Some types, like set, will make use of the generalized slice type defined in the standard library. That's because sets are unordered. There's not very much else that they can do. They just basically need to maintain a start and an end index [inaudible] to the original collection. Data and range on the other hand are their own slice types, and so there's a lot of options that you have here. And there's one more thing about slices that I want to talk about before we move on. Let's suppose that we had a really large collection, like thousands and thousands and thousands of elements. And we add a couple of small slices to parts of that collection. It's important to remember that the slice keeps the entirety of the originating collection alive as long as the slice is around. And this can lead to surprising problems. Let see how this works in code. Let's suppose I have an extension on an array that allows me to return the first half, and I'm using the droplast function here to do so. And we have an array of eight numbers, and we call our extension, producing the slice, and then to try to get rid of that original storage of eight numbers, I go ahead and assign that array to an empty array. Our first clue that something interesting is happening occurs when we ask our slice for its first element. We're somehow able to return one, even though we threw away the storage for the original array. Either there was a copy or something magical is going on. And so if we wanted to eliminate that buffer though, and oh the magic that's actually going on is that we're holding onto the buffer. And so if we wanted to eliminate that, what we could do is we could form an actual copy of the array from the slice, and then if we set that slice to an empty array itself, that copy would still be valid. Let's visualize what just happened. We started with an array. We then formed a slice on the first half of that array. We then created a copy of that, setting the array to be empty and setting that slice to be empty. And only after we did that did the underlying storage go away. So in this matter, slices sort of work like lazy copies. You get to choose when you make a copy of the elements yourself, and it turns out that this concept of being lazy and doing something later is really useful in other contexts too. One such context is function calls. Now function calls in Swift are eager by default. That is, they consume their input and return their output as demanded. Consider this example. We start with a range from one to 4000, and ranges are a really succinct way of representing a lot of numbers. It's just a start and an end, and it knows how to produce them. We then map this though multiplying each value by two, and so we've now actually allocated an array of 4000 elements and performed our mapping function on each of them. We then filter that down to four elements. And so at this point, we've actually gone ahead and allocated 4004, you know, space for 4004 elements, but we only really needed the final four. And that's an awful lot of intermediate computation that maybe we don't always desire. It would be great if there was a way just to not do any of it, unless it was absolutely needed. And Swift's answer for that is called being lazy, just like in real life. We'll start as we did before with the range, and then we'll tell that range to be lazy. And when we do this, what happens is we wrap the original collection with a lazy collection, and when we perform operations on this lazy collection, what's going to happen is we're going to wrap it again. And so when we wrap the, when we call map on it, we actually aren't mapping. We're not doing anything with that closure other than storing it for later should we ever need to use it. Further, if I filter that lazy map collection, the filter simply wraps the map collection, noting that it's going to filter later on demand, but not right now. Now let's go ahead and ask our lazy filter collection for it's first element. When we do this, we'll start by asking the lazy filter collection for it's first element, but the lazy filter collection doesn't know. It wraps something that might know. And so it'll defer to the map collection. And the map collection also doesn't know it's first element, but it wraps a collection that might, and indeed, the range knows it's first element. The first element of the range is the value one, which it returns to the lazy map collection where now the lazy map collection can actually perform it's closure, computing the value 2, which it returns to the lazy filter collection as a candidatefirst element. Now lucky for us in this case, 2 happens to be less than 10, and so the lazy filter collection finds it first element on the first try, which it returns back to its caller. Now that's a lot of different computation. And I mentioned that lazy aims to only do calculation as needed on demand, but another thing that it avoids is creating intermediate storage. So I want to show an example of that. Let's suppose we had an array of different kind of bears. However, I want to point out that some of these bears are redundant. We already know that they're bears. They don't need to tell us again. So let's write some code to find those silly, redundant bears, and we'll do this using a lazy filter, as before. And in this case, producing a lazy filter is going to be a lazy filter collection that wraps an array of strings. In our closer here, were going to print out which bear we're currently iterating on before we do our predicate check. And we're doing this because I want to understand how filter works a little better, and then we'll call first, and when we do this, we'll defer to the lazy filter collection, and then the lazy filter collection will in turn defer to the original storage where we'll print out Grizzly, check the predicate, which in this case is false, Grizzly does not contain the word bear, and advance on to panda. When we get to panda, when we get to panda, we'll again print out panda, check to see if it contains the word bear, and advance on to spectacle. Spectacle gets printed, also doesn't contain the word bear, and we advance finally to gummy bears, which mercifully has the word bear in it, and which the lazy filter collection can now return to its caller. Now what would happen if we called first again? Well, it's the same story. We ask the lazy filter collection, which defers to the underlying collection, which repeats that calculation, returning it to its caller. Now this might not typically be what you want, and so if you find yourself needing to repeatedly ask the lazy collection to calculate its result, there's a way to make sure that that happens just once. We can ensure that the lazy collection is iterated exactly once by creating a new nonlazy collection from the lazy, and when you do this, it will still defer to the lazy collection, but now the iteration will proceed through the entirety of your underlying collection, producing essentially, you know, the nonlazy version of that lazy calculation. And so in this case, we get an array containing the string gummy bears. And if we print the first element of that ray, we don't need to consult the closure at all or the lazy collection at all. We basically stamped out the laziness and now have an eager array. So when should we be lazy? Well, lazy collections are a really great way to eliminate the overhead of chained maps and filters. They excel when you find yourself only needing part of the result of a collection calculation, or we should avoid using lazy if your closures have side effects, and your closures should rarely have side effects. And be sure to reify back, or I should say, be sure to consider reifying back into a regular collection when you cross API boundaries. Lazy should often be an implementation detail. Now up until now, we've been able to do a lot of cool things with just mutable collections, but of course Swift lets us mutate our collections as well. Let's talk about the two kinds of collections that we haven't talked about yet. The first of these is mutable collection. This adds a setter to the subscript so that you can change the contents of a collection but not its length. And you have to be able to do this in constant time. The next is called a Range Replaceable Collection, and this is the kind of collection that you get when you can remove elements from a collection or insert them. And now I want to talk about one of the questions I get asked all the time. Why does my perfectly reasonable collection code crash? And like all good question answers, I almost always follow up with a few questions of my own. Sometimes I start with the classic, well what are you trying to do? And I usually quickly follow up with, well how are you using your collections? Are you mutating them? Are you sure you aren't accessing your collections for multiple threads? And I ask these questions because their answers often lead to the root cause of the problem. Well, let's begin with the assumption that threads aren't involved. I'm not ready to think about threads yet. It's not even 9:30. Suppose we had an array, and we get the index of an element that we know is there, in this case the value e. And then we mutate the collection, say by removing its first element, and we go ahead and print the element associated with that index. Well when we do this, unfortunately this will produce a fatal error. The index is no longer valid. In fact, the index became invalid the moment we mutated our collection. A far safer approach would be to mutate our collection first and then calculate the index. It's worth pointing out that mutation always invalidates indices. This doesn't just apply to arrays. Let's take a look at how this problem could manifest with dictionaries. Let's suppose that we have a dictionary showing a few of a bear's favorite things. We'll grab the index of our bear's favorite food and print it out, confirming that, indeed, it's salmon. Next, we'll add a couple more favorite things that this bear has, and then we'll make sure that our favorite food is still salmon. And we'll see that, wait a minute, our favorite good isn't hibernation, it's salmon. And just like arrays, we invalidated our index the moment we mutated the dictionary. It's also worth pointing out that this code can crash. So how do we go about fixing this? Well, it turns out it's the same exact fix that we used with the array. We just simply recompute the index after we mutate. Well, one thing to keep in mind that when you're recomputing indices this is something that can sometimes be expensive. Some index search methods take linear time. And so you want to take care to only find the indices that you need. So here's my advice if you want to avoid finding yourself in these kinds of situations. Remember that mutation almost always invalidates your indices. You might get away with it sometimes, but it's really best to just treat this as a hard rule. You'll be much happier for it. Also remember that slices hold on to the underlying original state of the collection even after it was mutated, and because of that, really think twice about holding onto indices or slices when the underlying collection is mutable. And keep in mind that index computation can sometimes take a nontrivial amount of time. So take care to only calculate indices as needed. So a little bit later, let's bring threads into the discussion. I mentioned that one of the questions that I ask is are your threads accessible for multiple threads? And the reason why I ask this is because our collections assume that you will access them from a single thread. And this is a really good thing for reasons of performance. It makes it so that all single-threaded use cases don't have to pay the tax of locks or any of those other primitives that you could use to ensure mutual exclusion. And when threads are involved, only developers using the collections will have all the information needed to restrict access with the appropriate lock or a serial queue at a much higher level abstraction than us lowly framework developers could ever offer. So let's see what these kinds of problems could look like. Let's suppose we have an array that we aim to fill up with sleeping bears, and to simulate each bear being their own bear and in charge of themselves, we're going to get access to a concurrent dispatch queue that we'll use to tell each bear to go to sleep. And because this is a concurrent dispatch queue, it's some time helpful to like imagine the code running at the same time, which I'll simulate by putting them on the same line. And later in our application, let's go ahead and check in on those sleeping bears, and sometimes we'll see grandpa and cubs snoozing happily. Other times, cub will go to sleep first, and then it'll be grandpa. Sometimes, quite mysteriously, only grandpa is sleeping in. And other times, it'll be the cub, and sometimes our program just up and crashes, and nobody bear's getting any sleep. All of these possibilities suggest that there's a possible race condition, and indeed, it seems likely given all the potential threads involved in this example. And we can prove this to ourselves using the thread sanitizer or TSAN that's included within Xcode. And if we were to do so, we'd get output that kind of looks like this, and indeed, TSAN would catch the race. It would tell us there's a Swift access race. It would tell us which threads are involved and give us a summary at the end telling us which line to actually go start looking for our problem. And all that evidence is actually going to be really helpful to find the bug. So we've proven that there's a bug here. TSAN has never lied in my experience with them. So we can fix this by eliminating the bears' ability to go to sleep at the same time, and we'll do that with a serial dispatch queue. And now only one bear can go to sleep at a time. And so if we peek in on our sleeping bears again now, taking care to do so on the appropriate queue, we see that sure enough grandpa and cub are snoozing away peacefully like we expected. So my advice for working with collections for multiple threads is try to isolate your data so that it can only be seen from a single thread, and when you can't do that, make sure that you have an appropriate form in mutual exclusion, such as a serial dispatch queue or locks. And always use the thread sanitizer to double check your work. It's far better to catch bugs before they ship in your app than after. And I have a little bit more advice for working with mutable collections. The first of which is if you can avoid it, don't use mutable state at all. So far all the difficulties that I've described have come about because we've been working with mutable state. You can avoid all the potential for this complexity by avoiding mutable collections in the first place. Many times you can actually emulate the mutations that you want to perform by using a slice or using a lazy wrapper, and it's almost always easier to understand data that cannot change. And thanks to mutability being built into Swift, the compiler will help you if you're leaving a state mutable when you're not actually mutating it. Now I have one more piece of advice that actually concerns how best to use mutable state when you have to. And that's when you're forming new collections. You can help performance if you're lucky enough to know an exact count or a really good approximation of how many elements you're actually going to need. Most collection APIs have a way of being able to provide this hint, and when you do this, you get exactly the size you need with no overhead. If you don't, our collections are general purpose tools. They're meant to work on a wide variety of cases, and as you incrementally add elements, you may actually end up over allocating the amount of storage that you need, but taking care that you don't over estimate when providing such hints, because otherwise you'll find yourself in the same exactly situation where you're using up more storage than you actually need. And now, I want to move on to my final topic for today. And that's the wide range of collections that become available for you when you import Foundation and when you should consider using them. In addition to the standard library collections, when you import Foundation, you gain access to the great reference-type collections that objective C developers have been using for decades. And many of these also gain conformance in Swift and thus behave just the collections that we've been talking about. That said, there are a couple important things to keep in mind. First thing to keep in mind is that these NS [inaudible] collections are reference types. And this is best examined by considering an example. We're going to define value types and reference types and do the same things with them on two sides. So with our value type, we'll call it x. It will be an array of strings. We get an empty array called x. With a reference type, we get an empty array, but x is pointing to it. We then mutate that array with the value type. That array is mutated in line. With the reference type, that array is, the reference, the array that is being referenced is mutated in line. We add another variable. With the value type, something really special happens. We actually don't copy the storage at this point. Why is an array that knows that its storage is actually owned by x? And why is that actually going to perform that copy until either of those collections is mutated. The reference type is a little bit different. Y is just another pointer to the same underlying array. So let's go ahead and mutate y. We'll put another bear in that array. With the value type what happens is first we invoke that copy on write machinery. We're writing to a y, so we need to copy it, and then we can insert the next bear. With the reference, it's a little bit simpler. There's only one array. We simply put the panda in the array. There's a second thing that you need to keep in mind when working with the foundation collections in Swift. And that is, all objective-C APIs in Swift appear as Swift native value types. And this is actually really wonderful because it let's code in each language speak naturally with the types that they work with best. But how can this work? The two languages have completely different implementations for these collections. And the reason why it works is something known as bridging. Bridging is how we convert between the two different runtime representations, and this is something that's necessary because Swift and objective-C, I'm sure you've noticed, are very different languages with very different compile and runtime features. And while we've optimized bridging to be as fast as it can be, it's not free. There will always be a cost when bridging between the two languages. So what happens when we bridge? Well, when we bridge between the language, we have dispersed set up new storage, equivalent storage, so if you're taking n things in one language, you'll take up n space in the next one. Then we need to go element by element and convert potentially between them, and this per-element bridging can sometimes be recursive. For instance, if I have an array of strings, first we'll bridge the array, and then we'll bridge each individual string. And when this happens at the boundary between the two languages, we call it eager bridging. And collections will always be bridged eagerly when the elements of the collection need bridging as well. And this arises most often with dictionaries keyed by strings. When collection bridging is not eager, we call it lazy. And this happens when the element types of the collection aren't bridged themselves, such as NSViews. In this case, bridging will be deferred until the collection is actually first used. Let's make this concrete with some examples. We'll first consider an objective-C API describing an NSArray of NSDatas. Now NSArray is bridged to array, and NSData is bridged to value-type data. And so such a collection would be bridged eagerly. I mentioned a moment ago that NSViews are not bridged in Swift. They remain reference types in Swift. And so an NSArray of NSViews will be lazily bridged. The bridging won't happen until you first access or try to use that array. And finally, an NSDictionary with keys that are NS strings will be bridged eagerly because the strings need to come across in Swift as value-type strings. So now that we know what bridging is, how it works, and when it happens, we can move on to the most important question of all, which is when should you care about it. And the answer is really straightforward. When you measure it negatively impacting your app. Specifically, when you take a time profile or trace an instrument, pay special attention to where your code crosses between the languages, especially when this happens inside a loop. Some bridging is going to happen, and that's totally okay. What you're looking for though is a disproportionate amount of time or a surprising amount of time spent in code that you didn't write that has the word bridge in it. Let's look at a concrete example. Suppose I have a manuscript for a great children's story that I'm working on, but it's really long, so I'm only going to show a little bit here, but to really make it pop, I want to make every word or every instance of the word brown actually be the color brown, and in the interest of space, I'm only going to show highlighting the first word. To do this, I'm going to use an NS mutable attributed string. I'll pass my story in there. And then using the attributed strings string property, I'll ask for the range of the Swift string brown, which will produce a range of strings native index type. And as mutable string works with NS ranges, and so I'll use the convenience initializer that we introduced last year to convert to an NS range, and this, I'm calling again attributed strings string property to do the conversion. And then we'll color the first instance of the word brown. And when I go to run this code, I notice it's a little slow. So I profile it. And I see that, to my surprise, I thought all the time would be spent coloring the word brown, but indeed, it's actually computing the indices, and why is that? And the reason for that is that we're actually bridging our string multiple times across languages. Mutable attributed string is an objective-C reference type, and so when we're asking for the string property, we're actually having to convert from an NSString to a string. And we're doing it once here when we calculated the first range, and we're doing it a second time when we convert for the NSRange. You can imagine how expensive this would be if we did this in a loop looking for all the text to color. Now let's look into why this is happening. Every time I call text.string, we start in a Swift execution context. However, the implementation of NSMutableAttributedString is objective-C, and so in order to provide the result, we actually have to consult the original implementation. The original implementation returns an NSString, which is the reference type, and so when return to string, it needs to be bridged, graphing cluster by graphing cluster, character by character. And bridging happens whether it's a return type or a parameter. So now that we know those details, we can make, now we can actually make this a little bit better. Let's just bridge once. And let's remeasure our code and see that indeed we've improved our performance by half. But it turns out this year we can do a little better. Oh, and also, now we're not bridging where we do that. But this year we can do a little bit better. This year, if we actually [inaudible] to an NSString when we ask for the text.string's property, when we get the variable out, no bridging is actually going to occur. And further, by doing so, now that string is an NSString, when we call the range of property, we're actually going to get an NSRange out of it automatically. We won't need to do any of the range conversion between Swift native types and the NS ranges, which is pretty excellent. So let's measure this code and see how it is, and sure enough, this looks pretty good. Much, much faster than the, you know, almost 800 milliseconds that we were consuming previously. However, I do want to point out that we're still bridging here. And it's a teeny, tiny bridge, but we're still bridging. Brown here is a Swift value type string. And every time we call into the objective-C API range of [inaudible] NSString, we're actually going to bridge that teeny, tiny string back to an NSString. This is inexpensive in this case. I'm only doing it once, but you can imagine, if this was in a loop, that small amount would add up over time. And so you want to take care to avoid bridging the same small strings repeatedly. However, before you do such optimizations, always measure. And so now that we've seen the details of bridging, I want to offer a little bit of advice about when to use Foundation collections. You should consider using them explicitly when you need a collection with reference semantics. Don't need to write one of those for yourself, we already have many great ones. You should also use it when you're working with types that you know to be reference types. Things like NS proxies or core data managed objects. And the final time to consider using them is when you're round tripping with objective-C code, but I'd recommend strongly doing this only after you've measured and identified that bridging is indeed the culprit for whatever performance problems you may be seeing. And now, we've reached the end of today's dive into the incredible power of our world's collections in Swift. I want you to use this new-found understanding to review your existing use of collections. Look for places where you can improve your code through more effective use of indices and slices. Measure your code. And look for places where you could benefit by being lazy or by tuning how you bridge. Use the thread sanitizer to help audit your mutable state, and further hone your mastery of collections by apply all the concepts discussed today in Playgrounds and you own apps. Be sure to visit our last few labs today if you have any questions about collections. We're here to help. Thank you so much for you time. Now go out and be effective.  Alright, good morning. Thanks so much for coming out this morning to learn all about what's new in Cocoa Touch. Now normally, Eliza would join me up here a little bit later although this year actually you're just going to be hearing from me. So, she'll be back in later years. Don't worry about it. This morning we're going to talk about things in three main categories. We're going to start with some framework updates including things like performance and security. Then we're going to talk about some API enhancements across a number of different existing APIs in the SDK, including notifications and messages. And then we're going to end with Siri Shortcuts. So let's get started with our first topic: Performance Updates. Now we're going to talk about performance across three main areas: scrolling, memory, and auto layout. Now before we get in to it, it's important to keep in mind one little bit of background information about scrolling. Scrolling on iOS follows a pretty common pattern in most places. We load content to be displayed into the views and then we're just moving that content around. And while we're moving it around, most of those frames are really cheap to generate because we don't have to load anything new. But every now and then, a new view first becomes visible and the one frame when that first happens is quite a bit more expensive to generate than those other cheaper ones. Now, of course, once that one frame is loaded, we're just back to moving content around, so the amount of work we do on the CPU goes back to being pretty small for most of that other scrolling. So what's happening during that really expensive frame that causes that one to be more than all the others? Well, let's take a look from the perspective UI Table View but everything we look at here will really be the same for UI Collection View or really any of your own custom views that you may build that behave in similar ways. So the work in that expensive frame probably starts in your implementation of TableView, cell For Row At index Path delegate method. Now the first thing we're going to do in there is get the cell that we want to display. And so we're going to try and dequeue it ideally from the reuse queue although if one is not available already in the queue, we might actually have to do some memory allocation in order to get it ready. Once we have the cell, we're then going to populate it with your model data. Now how expensive that is will vary depending on your application but it can be including a fairly large amount of expensive operations like reading files, loading data out of databases, or other things of that nature. So you'll definitely want to look at the expense here in your own apps but it tends to be this is where a good portion of it will exist. Now, you may think looking here that that's the end of the expensive work but even once you return from this method, there's actually more work that has to happen in order to get the cell prepared to have it show up on screen. So, of course, next, we have to lay out all of the content in that cell. We need to size all the views and position them in the right spot. Now, this can actually be a pretty substantial amount of the total time that we're spending because it can include other expensive operations like measuring text. Once everything is properly sized and positioned, then it's time to generate any content that would need to be drawn using drawing calls and to do that we have to call draw Rect on all of the subviews within that cell. Again, this can be a pretty large amount of the time because we'll also be doing things like drawing text. So overall, there's a lot of work that has to happen across this whole piece of code and it has to happen in a really short period of time. On our 60-hertz devices, you have 16 milliseconds to complete all this work in order to make sure you don't drop any frames, and maintain smooth scrolling. On our 12-hertz iPads, on the iPad Pro, you have only 8 milliseconds to complete all of that work. So, it really needs to be done as quickly as possible. Now, to help with that, in iOS 10, we introduced a cell prefetching API and the idea with the prefetch API is to take some of this work, populating your cell with model data, and pull it out of this critical section that's happening on demand in that short window, do it earlier, and do it on a background thread so it can happen asynchronously with some of the other work, in particular those cheaper scrolling frames we talked about. Now adopting this is really easy. It's just a UI Table View Data Source Prefetching protocol and it only has two methods, only one of which is actually required. And the idea here is to move some of that expensive work of loading things from files or reading your database into here so that you don't have to do it on demand. You data is already prepared when the cell is needed for display. So this in most cases can be a really big win although while we were looking at some of our own apps in iOS 12, we actually noticed a case where this was causing an issue instead of helping us. So let's take a look at what that looked like. Now here's an example of a trace that we took while scrolling on an iPhone 6 Plus. The vertical bars across the top, those represent frames that we want to display. The alternating light and dark blue colors represent frames that we did swap to the display as they were changing. And that double wide light blue bar, that is a place where we drew the same frame for two full frame durations. So for a customer looking at the device while this was happening, that looked like a dropped frame or a scrolling hitch, which obviously is what we're trying to avoid. So what was happening in this case? Well, here you can see that red bar is representing the time that we're spending in the critical section we just talked about, all the self-[inaudible] index path, layout, and drawing. And here, it's actually taking longer than the 16 milliseconds we had to draw the frame. Now because the device can only swap new frames onto the screen at fixed positions in time, once we miss that deadline, we ended up displaying the same frame for two full durations, which was obviously not great. So why did that happen here? In this case, we're looking at an app where we actually had implemented the cell prefetching method so our data should've been ready. Ideally, this could've been done more quickly. Well, if we look at a little more of the trace, we can see what was going on. The cell prefetching API was being called at the same time that we were requesting the current cell. Now it wasn't being called for the data for the current cell. It was being called for data that we might need in a future cell, but it was getting run at the same time. And so there was now contention for the CPU as we tried to both load the current frame and also load data for a future frame that we don't actually need yet. So because of that contention, it actually caused both tasks to take a little bit longer. Now in iOS 12, we're much more intelligent about scheduling these background prefetch operations so that rather than happening concurrently and causing some CPU contention, they'll now happen serially, shortening the time that you need to take to load the current cell and helping avoid dropped frames in many cases. So once we had that fixed, we kept profiling our apps and we actually found another case where there was a bit of a surprising cause of some dropped frames. Now what we found was that when the device was not under load, there was no background activity, all we were doing was a little bit of scrolling in the foreground app. Counterintuitively, we could actually drop more frames than times when there was some small amount of light background work going on. That didn't really make a lot of sense. And to understand why it was happening, we had to drop down a level and take a look at the behavior of the CPU when it was scheduling our workloads. So let's take a look at another trace. Here, we've got the same situation. Double wide blue bar is causing us to drop a frame or rather is our dropped frame. Now here we can see a graph of our CPU's performance over time. Now during most of those cheap frames, our CPU performance is staying pretty low. There's no background work going on. All we're doing is scrolling things and that's exactly what you would want because when we're not doing expensive work for scrolling, you want to keep the CPU as low as possible to preserve battery life. So that was great. What wasn't great is that it took a little bit of time before it could ramp up. You heard about this yesterday in the keynote. Now when it did finally ramp up, it was already too late to have completed the work to load the cell that we needed to display and so we ended up missing that frame again. Now because we own the full software stack from top to bottom, in iOS 12 we took all the information we have in the high-level UIKit framework about what scrolling is happening and when these critical sections are occurring and pass that information all the way down to the low-level CPU performance controller so that it can now much more intelligently reason about the work that's happening and predict both when these bursts will occur and how much CPU performance will be required to meet the deadline for the historical demand that your app has had. So once that change happens, where the load starts right here, we end up seeing that we've ramped the CPU far more frequently or, far more quickly, to the exact amount of perform it needs to make sure that we hit that deadline and don't drop frames. This has caused a really great improvement across many different scrolling scenarios around iOS. So all of your applications will get all of this enhancements, both of them and a number of others, for free with no additional work on your part, but there are a couple of things that you can do to make sure that you get the most out of both of them. So first of all, if you haven't already adopted that tableView cell prefetching API or the Collection View one, definitely look into that because having your data ready is one of the best things you can do to make sure that loading cells is as quick as possible. Of course, it's also important that you profile your full critical sections of your cell loading and reduce that demand as much as you can. iOS 12 will now try to match the CPU performance to the needs of your application during this period but the best thing that you can do will always remain to reduce the amount of work that you have to do to make sure that you give your customers a really smooth scrolling experience. So that's scrolling in iOS 12. Next, let's turn our attention to our next performance topic: memory. Now, you might wonder why memory is showing up right here in the middle of a performance discussion, but I assure you that that actually makes a lot of sense and, in fact, the reason is because memory really is performance. The more memory that your app is going to use, the more that it will have an impact on the performance of your application. So to understand why, let's take a look at a really high-level overview of what the overall memory on the system might look like in a common situation. So, of course, you can see here, a lot of the system's memory is being used by other applications and the system itself. Your app is using some amount for the moment. And there's some that's being kept free to service new allocation requests as they come in. Now, most of that memory is probably not truly free. It's likely including things like caches or other things that can be quickly thrown away to make sure that the memory is available to satisfy demand right away but in general it's probably actually being used for something, but it is readily available. So let's look at what happens when your application requests some memory. Maybe you'll make a small request, something that can be satisfied by the amount that's currently available in that free pool. Well, if that's the case, it'll be return right away to your app and you can continue on with your work. Now, let's say, though, that your application makes a larger request, and maybe it doesn't need it for a long period of time, so you might not be real worried about it. Perhaps you're just going to load an image off disc and decompress it, perform some quick operation on some of the pixels and then throw it away. So it seems like a quick operation that you don't have to worry too much about. Well, we'll make that big request and now that is more memory that is currently easily readily available to the system. So it won't be able to satisfy it immediately and we'll have to go find that memory from somewhere else. And, in fact, the most likely place to come from is from other applications or things on the system. Now, that might not worry you too much because you're trying to get the memory for your app right now, so you're not so worried about what's happening elsewhere. But, of course, this will have an impact on something else later that your customers will be expecting will be working, but more importantly to you right now, it will have an impact on your app as well because the system has to do work to go get this memory for you. The fact that it's not readily available means that the kernel has to go find it and perform operations on the CPU in order to make it available. And that time it's spending getting that memory for you is time that could be better spent doing whatever your app does best. So finding ways to either chunk these larger requests or just reduce those overall peak demands can actually have both a big impact on the performance of your app while you're using it and also improve the customer experience across other apps later. So there's many ways that you can reduce the total memory usage of your apps and starting with a profile and instruments is a great way to look at that. But for this morning, we're just going to take a look at one new technique that iOS 12 brings along that helps reduce the memory usage of your apps and that's Automatic Backing Stores. So let's say we want to draw this lazy prairie dog in portrait mode on an iPhone X. Now, how big is that going to be? Portrait mode on iPhone X, 375 points wide and, to preserve our aspect ratio, it'll be 250 points tall. So how much memory will that use? Well, 375 by 250 at 3x, with 64 bits per pixel because this is a deep color device, is going to be 2.2 megabytes of memory to draw a prairie dog. That seems like a pretty decent amount, but in this case that's actually probably memory well spent. We're actually trying to represent this full-fidelity image and that's the amount of memory that's needed to draw it into this buffer. So that's probably expected. But now let's say that we were going to draw a lower-fidelity version of our prairie dog, maybe something in black and white that we were going to sketch out with core graphics, maybe it was drawn with an Apple pencil on an iPad. That might look something like this. So how much memory is our low-fidelity prairie dog going to use? Well, it'll actually use the exact same amount of memory. Now here, that clearly is not as good of a use of memory. It's all grayscale. All of that deep color information is not even being used in this image. So hopefully we could do much better and iOS 12 introduces automatic backing store support to help make that exactly be the case. So all of your views now that implement draw Rect will have their backing stores defined by the depth of the content being drawn into them. So in this case where we're using Core Graphics to sketch out only grayscale content will actually automatically use an 8-bit per pixel backing store instead of a 64-bit per pixel one. This reduces the memory demand for that drawn view by an order of magnitude down to 275 kilobytes from 2.2 megabytes. That's a really big improvement across many different applications, both yours and ours. So Automatic Backing Stores are enabled by default for all apps built with the iOS 12 SDK. All implementations of draw Rect on UIView will have this happen automatically as well as all content that you draw with UI Graphics Image Renderer into offscreen bitmaps. Now in the case of UI Graphics Image Renderer, we don't necessarily know what you're planning on doing with the image that you get out at the end. So in cases where you actually know that the automatic behavior is not what you want, from Image Renderer, you can specify the specific backing store style that you want, for example using the new Range API to specify specifically that you want an extended-range image. Now you can learn all about this and many more techniques around UI Image in the Images and Graphics Best Practices Session later this week. So that's our second top for performance. Next, let's move on to Auto Layout. Now Auto Layout in iOS 10 has some really great improvements. The team has been working incredibly hard to optimize Auto Layout for your apps as much as possible. So you'll find that Auto Layout is now faster by default in iOS 12. We'll talk about a number of ways that's the case. But while they were profiling and optimizing Auto Layout, they also were looking across the system at many of our own apps and how they were using Auto Layout and they found a number of cases where there were some common pitfalls that different apps were falling in to. So we'll show you some of the simple best practices that you can follow in order to optimize your app layout as well. So this morning, though, let's look at how Auto Layout is faster by default in iOS 12 by looking at the asymptotic behavior of Auto Layout as we add more views in some common user scenarios. Now, we're looking at the asymptotics because we really want to look at what happens as we dramatically increase the number of views. This can really make performance issues show up quickly when we go to a really like absurdly large number of views. That just helps us see it though. The optimizations really do apply and make things faster even for small numbers of views. So let's start with a really common pattern, the simplest case really, Independent Sibling Views inside of some container. Now, these views are positioned with constraints against the container or other views but not against each other. They're independent of one another. Now in iOS 11, the cost of these independent siblings, as you continue to add more, grew linearly. So that's actually pretty great. That's exactly what you'd expect for a situation like this. Basically, what we're saying is that there's a fixed constant cost for each additional new view that you added into your hierarchy that was positioned independent of those other views. Now because that was already growing with the exponentials that we expected, that remains the case in iOS 12; however, the team has worked really hard to reduce that constant cost to make additional views as cheap to add as possible. So let's look at a more interesting example next. And in this case, we're going to take a look at Dependent Siblings. So this is the case where you have multiple child views and they're now, they have constraints between each other. So their layout is dependent on one another. Now, unfortunately, in iOS 11, you'll find that the asymptotics here weren't quite as nice. In fact, it was growing exponentially. So the more views that you added, the surprisingly larger cost you would find as you added additional ones. Now, the team worked really hard to identify the source of this exponential growth and fixed the algorithm so that is no longer the case. So on iOS 12, these now grow linearly. And, of course, the team's also been working to reduce those constant costs for these cases as well. Now in addition to dependent siblings, there's another common type of layout that you'll find and that's Nested Views, when one view is inside of another and there's constraints out to those outer ones. This is obviously also a pretty common pattern in your apps and, unfortunately, here as well in iOS 11, we found some exponential growth. And again, great news. The team has also made this linear in iOS 12 as well. So there's a number of really great improvements across Auto Layout in iOS 12 and you'll see these improvements in your apps as soon as you start running them on iOS 12 yourselves. To learn more about all of that, get a better sense, develop a good sense of how to get some gut feel for the performance of different layouts and here are some of these topics I was talking about that are common pitfalls. Definitely check out the High Performance Auto Layout talk later this week. So for our final framework update topic, let's turn to Swiftification. As you heard, iOS 12 introduces Swift 4.2. And for Swift 4.2 we really wanted to make sure that UIKit had a really great feel when used in Swift across your app, especially where it interacted with other Swift Standard Library or places that Swift had common patterns. So, we audited all of UIKit and made sure that everything feels like it fits really naturally. Even better, we made sure that all of the changes that we made to UIKit are all automatically migratable so there's no additional work that you should have to do in order to get these updates. Now, these updates fall into really three categories that we'll talk about this morning, although there's actually a ton of improvements and consistency improvements that you'll find as you look at the SDK. But today we'll talk about nesting of types, constants, and functions. So let's first look at nesting types. Now in Swift 4, there were a number of types that were in the global name space, things like UI Application State. For types like this that have a really strong use along with another class, we've now nested them within this class. So we looked at all the enumerations and other global types of this sort and now have moved them to be child types of the relevant class. So this becomes UIApplication.State. This sends a much stronger message about the relationship between these two and makes them easier to find as well. Now in comes cases this can also help improve understandability and remove some confusion. So in this case, let's look at UI Tab Bar Item Positioning. Now do you think that's UITabBarItemPositioning or UITabBarItem Positioning? It could actually be either. Those are both classes. And in Swift 42, it is now perfectly clear that it is, in fact, UITabBar ItemPositioning. So in addition to nested types, we've also nested a bunch of constants. So if we look here at Swift 4, we had NS notifications were all in the global NSNotification.Name namespace and their associated user info keys were actually just global constants that were floating out there. So for consistency with AppKit and to make it easier to find and associate these types together, they've now all been nested under the class that they're used with. So something like did Change Status Bar Orientation is now under UI Application did Change Status Bar Orientation Notification and its user info key moved along with it so that they're all co-located. Now we've also audited all of the other global constants all throughout UIKit and nested all of them in appropriate places. So things like UI Float Range Zero and UI Float Range Infinite have not just become properties on UI Float Range so they're both easy to find and easier to use. In places to take a UI Float Range, you can now just type .zero or .infinite and, in fact, because they're now properties, Xcode can suggest them as auto-completions for you in places where they make sense. Now in addition to constants, we've also audited all of our global functions. So things like UI Edge Inserts and UI Image had some global functions for operating on different types. Now in Swift 4.2, these had become methods on the appropriate type. So it's now really easy to inset Rect or get png Data from an image in a really natural Swift feeling way. Now, here was one other big class of functions that I want to mention this morning and that was all of these string conversion functions for all of the many types in UIKit, CGPoint, CGRect, CGSize, CGVector, all of them, there's quite a lot, both to and from strings. Now, when we looked at these and tried to decide where they should go, we realized that they actually have two different use cases. One is for encoding and decoding. But the other is that they're commonly used to print things when you're just trying to get some debug descriptions. And those are two very different uses but, in fact, Swift has first-class support for both of those cases. And so we've made sure that all of these types will work really great with Swift's built-in support for both. So in Swift 4.2, all of these types will conform to Codable so that you can very easily do things such as encode and decode JSON for all of these different types. Of course, debug printing in Swift is actually even easier than in Objective-C because you don't have to do any additional conversion. The built-in introspection of the types can allow you to print them directly. So in Swift 4.2, you just actually pass these types directly to your print functions if you want to print them out for debug purposes or log them. And then finally, you may already have some existing code that was using the behavior of the old string conversion functions and need a compatible functionality going forward. And so for that we've actually just renamed all of these and moved them to be properties on NSCoder. This really helps to emphasize the fact that the intent of these methods was to be used for encoding and decoding, so it's a pretty natural fit for them to go over there. So these are just a few of the consistency improvements that you'll find across the iOS 12 SDK for Swift 4.2 but you'll find many more as well. Now speaking of encoding and decoding, NS Secure Coding, in iOS 12 there are now new secure by default encoding and decoding APIs. Adopting NS Secure Coding for all of your encoding needs on NS Keyed Archiver is really key to ensuring that you're protecting your customers from both malicious and corrupted data. You'll also find that the older insecure APIs are not deprecated. So you can learn all about that and get much more detail on it in the Data You Can Trust Session on Thursday at 9:00 a.m. And that's framework updates. Next, let's turn our attention to some enhancements to a number of existing APIs and we'll start with notifications. Notifications has a number of really great improvements in iOS 12 but we're going to focus on just three this morning. Interaction within custom notifications, grouping of notifications, and access to your notification settings within your own apps. So let's start with interaction. Now custom notifications have for a while now allowed you to define a predefined set of actions for those notifications. In iOS 12, this set of actions is now no longer static. It can be defined programmatically and you can change them at runtime. In addition to these actions, and even better than that, the notifications themselves can now be made interactive. So for example, here, you can see Messages is now allowing you to reply quickly to a message inline directly in that notification. Now in addition to interaction, iOS 12 now includes grouping of notifications by default and the default behavior will be to group all the notifications for a particular app into a single group. But, of course, your app may have custom needs to have more granular groupings so something like iMessage will group all of the messages from a particular conversation together and separate from all the rest of the notifications for that app. Now you can adopt this in your app as well by just tagging your notifications with a particular thread identifier and then all the notifications for that threat identifier will appear in a single group. Now the UI updates for notifications in iOS 12 also include some new ability for users to customize the delivery behavior of their notifications. But, of course, your apps may also include some existing, more granular controls for notification management within your apps as well. And iOS 12 introduces a new API that makes it easy for your customers to get directly deep linked into your notification settings UI exactly when they're looking for those more granular controls. So you can learn more about all of these notification things in What's New in User Notifications and Using Grouped Notifications later this week. That's notifications. Next, let's talk about messages. Now messages in iOS 12 includes some really new and exciting features in the camera. And you can bring all of your iMessage stickers directly into the camera as well. If you're using the Xcode sticker template, this will work automatically by default with no additional work on your behalf. But if you're building a more custom sticker experience using the MS Messages App View Controller, some small amount of adoption is required. Now there's a new MS Messages Supported Presentation Contexts API that you can add into your user info plist and then specify that you want to appear both in the messages context and the media context. Once you've done that, your apps will appear both in the App Strip and also within the camera. Now if at runtime you need to figure out which context you're in so that for example you want to customize the display of your stickers a little bit, there's a new API for that as well. By checking the presentation context, you can quickly see whether you're in messages or in the camera. Now in addition to these features, iOS 12 also brings a new access for interaction to your messages apps. In compact mode, previously swiping horizontally down in your messages app would switch between apps. In iOS 12, these horizontal swipes and interactions that move horizontally are now available for us by your apps directly so they'll interact with your apps rather than switching to a different app. And that's Messages. Next, let's talk about automatic passwords and security code autofill. Now iOS 11 introduced automatic passwords or password entry into your apps. And in iOS 12, we're taking these a whole step further. But let's go back to the beginning for a minute and talk about the entire experience. So for users that have passwords stored in iCloud Keychain, since iOS 11 it's now been possible to have those automatically get populated into your app in your login flows. Now in iOS 12, these passwords can also be stored into iCloud Keychain from your apps both from your login window flows and also from your password change request UIs. As soon as a user logs in, they'll be prompted to save the password to iCloud Keychain. Now even better, iOS 12 can automatically generate passwords in your new account flows and in your password change flows. Adopting this is really easy. You just make sure that you've tagged your password fields with either the password text content type, if it's a login field, or the new password text content type if it's either a new account or password change field. If your services have a specific requirement on passwords for example if they got required or disallowed characters or if they have other requirements such as maximum number of consecutive repeated characters, you can specify these requirements as well to make sure that the automatically generated passwords are fully compatible with all of your requirements. Now, the final bit of friction during some of your login experiences is when you have to take that two-factor authentication code, get it out of a text message and into your apps. IOS 12 makes this really easy by automatically identifying these notifications, noting the security code in them, and suggesting it right in the Quick Type candidate bar so that it's really easy to get it right into your app. Now the only thing you have to do to make sure this works in your app is to be sure that you're using standard iOS text interaction APIs in order to accept these passcodes. With all of these new features, iOS 12 is enabling a much more secure future with unique, strong passwords used for every service that you never have to memorize or type ever again. You can learn all about this in the Automatic Strong Passwords and Security Code Autofill Session later this week. Now, our final API enhancement topic is actually a bit of a review but now with a little bit more context. So in iOS 11, we introduced Safe Area insets. Safe Area insets are a really great way to make sure that your content is avoiding any overlap from other parts of the user interface such as bars on the top and bottom of the screen. This is really great on iPhones where bars are pretty straightforward, but it's really powerful as well. Safe Area insets give you that safe area coordinate in the local coordinate space of any view in your application, so it scales even the much more complex interfaces, things like iPad Split View which has different height bars on the master and detail side of the Split View. The Safe Area insets in any view underneath these bars will be appropriate for the amount that they're overlapped by the bar on their side of the split. So this is really great on devices with rectangular screens but it also is really powerful on devices that have non-rectangular screens like iPhone X. Now, you can see here we've got our larger bars at the top and bottom than we have on devices that have home buttons. And the Safe Area insets obviously have just grown to accommodate that larger size. Now, unique to iPhone X is that there are Safe Area insets even in cases where no bars are present and this extends the landscape mode too where it can really help you make sure that you've got a rectangular area at all times that's safe to display content and will never be clipped. So I want to thank you all for those of you who have adopted Safe Area insets and updated your apps for iPhone X. It's been a really great experience over the last year and I'm sure most of you have already done that. If you haven't, now is a really great time to do so. Your customers will always prefer apps that are being kept up to date and support for iPhone X is a really visible indicator of that being the case. So if you haven't, definitely go do it now. And to help make sure you have all the information necessary to do that, you can check out the UIKit Apps for Every Size and Shape Session later this week which will tell you both all about Safe Area insets and all of the other related inset APIs all throughout UIKit making it easy to make sure you have apps that scale to every shape and size. So that's our framework updates and our API enhancements. Next, let's talk about Siri Shortcuts. So Siri Shortcuts is an exciting new API in iOS 12. Siri Shortcuts makes it easy to get common actions out of your app and make them accessible via Siri. Now, Siri Shortcuts can be suggested proactively right on the coversheet making it easy to access actions that you would want to access at the exact time and place that you want to access them. Even better, they can also be suggested right on the Siri watch face on Apple Watch. Now not only are Siri actions suggested proactively but they can also be set up to be executed using a custom voice phrase. Now, adding Siri Action Support to your apps is really easy. You can use two APIs. There's NS User Activity, which you may already be using for support for Handoff and Spotlight integration, and there's also support for Siri Intents for more complex scenarios where you have more custom interactions. So let's look first at NS User Activity. Now as I mentioned, NS User Activity is a common API with Handoff and Spotlight and this could be a really great API to use if your Siri Shortcuts should get your customers back to a specific place in your app, for example loading a particular message or document, the same as you would do if you were trying to hand that off to another device. If you're already doing this, it's really easy to add support for Siri Shortcuts. You just set Eligible for Prediction to true. And if you're not, this may still be a great way if your shortcut fits into one of these categories. Now, if your app has other more custom needs or if you just want a lot more control, you can adopt the Siri Kit Intents API. Now Siri Kit Intents provides a number of predefined intents that you can easily adopt on your own. These are the same as the Siri Kit Intents in previous years. Now if your apps have more custom behaviors, though, you can now in iOS 12 define your own custom intents. Custom intents can be really flexible and they do anything that you would want. In this case here, I've created one to help me create my WWDC slides next year. Now the categories that you could put your intents in to are pretty broad already. So here I've used the Create category. But if your intents are even more generic than that, there are even more general ones available such as General Do, Run, and Go options. Now once you've created your intent, you also want to make it really easy for your customers to create these custom shortcuts to get to them. And so there's now an API from right within your app you can allow customers to create a custom voice shortcut. So here I've got a button that will just bring up a new panel enabling me to create a new shortcut right within my app as soon as I finished an operation. So if you're doing something like ordering a coffee in the morning and you notice it's something that might be done again, this is a great opportunity to offer to create a Siri Shortcut to do that next time. Now even better, you can also combine these shortcuts together using the new Shortcuts app available from the app store. So you can learn all about this and much more in the Introduction to Siri Shortcuts, Building for Voice Siri Shortcuts, and Siri Shortcuts on the Siri Watch Face Sessions later this week. So we've talked a lot this morning about what's new in iOS 12, but there are also a number of great sessions that are worth mentioning that aren't necessarily about what's new. So if you're new to creating apps for iOS, there's a really great session you should check out called I have This Idea For an App. So definitely check that out. And if you already have an app, and are just looking to add more polish, there's a couple other great sessions as well, A Tour of UI Collection View and Adding Delight to Your IOS App. So thanks so much for coming out this morning to hear what's new. Look forward to seeing you in the labs and I hope you have a great week. Thanks.  Hi everyone. My name is Jeremy and I'm an engineer on the tvOS team. Today I'm really excited to speak with you about what's new in TVMLKit that we've added in tvOS 12. For the uninitiated, TVMLKit is Apple's high level framework for quickly building content based applications on tvOS. Out of the box, it is compliant with our human interface guidelines. So your applications look beautiful and feel correct. It uses JavaScript to drive application logic and a special XML based markup language which you define that renders into user interfaces on the screen. In fact, some of the apps you're familiar with and use today are built using TVMLKit. Along with that, there are thousands more in the app store that you write. So let's get started with the enhancements we've made to TVMLKit, beginning with three things we want to talk about today. First, we have some enhancements to Web Inspector that allow you to further debug your applications and introspect them at a deeper level. Since last year, we've added a bunch of features and enhancements to the framework and I'm going to discuss three of them with you today. We're going to talk about new Web Inspector enhancements for better debugging, new data binding constructs that are added to make it more powerful, and finally, a new way to customize the playback experience on TVMLKit. Let's get started with Web Inspector. tvOS 11 introduced more support for Web Inspector that allow better introspection into your TVMLKit apps. Since then, we've enhanced this support even further. In tvOS 11.3, in addition to showing install event listeners on an element, you can now temporarily disable them. This vastly helps with debugging as you can now toggle the event handlers off and on at whim. In the network tab, joining at [inaudible] in document resources are image resources. This allows you to see the images that are being loaded as well as information about how long it took and where time was spent. If you are interested in seeing the actual image that comes off the wire, this is now also an option you have at your disposal. However, please note that this only works after you've connected the Web Inspector. It does not show what has already been loaded. Finally, my most favorite feature of all, the inspect button. Clicking this will show you the approximately element that represents the view that's currently in focus. If your elementary is collapsed, Web Inspector will expand it to the exact element and highlight it. To use Web Inspector, either download the latest version of macOS and install it, or use Safari technology preview. More information about how to use Web Inspector can be seen in our talk given last year using Web Inspector with tvOS apps. With that, let's talk about data binding. And before we jump into the new features of data binding itself, let me give you an overview of what it is. Data binding helps you easily transform your data to user interface elements through expressions in your template markup. This is really good because it allows you to decouple your data and your lot -- and your layout logic as well as your application logic itself. Because of this, data bound templates can reduce the amount of JavaScript code you actually have to write to convert your data to TVML documents, since the framework does this on your behalf. In fact, by authoring your documents on your behalf, it is able to do it in the most performing way so you don't have to worry about the right APIs to use. Let's look at a practical example to explain this. Say you want to generate a banner with a title and a description. And this is how you would typically do it without data binding itself. First, you will fetch the relevant data that's supposed to be shown to the user in this case, a title and a description. And upon fetching, you pass it onto a part of the JavaScript code you've already written that processes this data and generates the actual final document itself. Now, with data bindings, you can take out JavaScript processing step and provide binding specifications in your template itself, and TVMLKit will populate the final document on your behalf as directed. Effectively, your application just needs to worry about fetching and massaging data, and not worry about dom [phonetic] editing at all. In a nutshell, that's how data binding works and helps reduce the amount of code you write. Last year, we introduced the concept of data binding with some core concepts that include binding of attributes of an element, binding the text content of an element, and of course, binding items of a section in either a shelf, grid, or list. Let's quickly recap on these concepts with another example. The corresponding data bound template representation of this image element would contain a binding expression with the 'at' symbol, followed by the attribute name and the property you want to bind it to. Next, let's seek the example of generating text content of an element. Like in this case, a title element that has corresponding data of what it should actually be filled with. The data bound template representation for this title element will contain a text content binding and the property it maps to. Finally let's talk a little bit about items binding. It is a slightly different binding and involves a group of data pieces you want to show. It also only works with sections on a shelf, list, or grid. In this example, we have some data with the tree list items as an array. And the final representation should be a section of tree list item lockups. The corresponding data bound template for this section will contain two things; an item's binding and the property it maps to, and also a prototype to convert each data object contained in the array. In this case, it would be a data bound template representation of a list item lockup. So that's the tree binding constructs we've introduced in tvOS 11. For more information, please look at our talk, Advances in TVMLKit from last year's WWDC. This year, we've extended that vocabulary. To begin, we've added a more generic way of binding child elements of an element using the children binding. And to help better organize your DOM, we've added a couple of special elements, namely fragment and rules. We'll go into this in detail, starting with children binding. Children binding is a more generic form of items binding. Items binding is optimized for use cases where you have a section of a shelf, grid, or list in order to work efficiently with large data sets. They can be used outside these elements. For everything else, use children's binding. And it's because of a very simple reason. It works by generating children of a target element itself. And it behaves the same way as items binding does. You require the use of prototypes to define the elements that data should be converted into and this will be used as a template when generating the final DOM itself. Let's take an example to explain how this works. I am going back to the same example of tree items in an array. We have this very same piece of data but in this particular situation, we have three different menu items. And this would be used in a menu bar. This is the final representation we expect to see. Basically a menu bar tree, menu bar items. And this is a very simple way to specify the template for it. As you can see, it's similar to how you would do items binding. It has prototypes that's used to map data to elements and it has a binding expression. The only difference is that the element is expressed on [inaudible] section. Children binding works with any element. Now this works really well as long as you want to generate all the child elements itself. There might be cases where you want to only generate some of those children. Take, for example, an auto streaming app with a now playing menu item. The now playing menu item is a special menu item that should always be in the menu bar but would only appear if background audio is currently playing. However in this case, we still want the menu bar items to be data bound. In order for this work, we need to compartmentalize the data driven and non-data driven parts of the menu bar. And that's where fragments comes into play. So what are fragments? Fragments are invisible elements that the renderer doesn't see and it helps you organize your DOM. But what's special about fragments are that its children are visible. And because it is an element and children binding works with any element, the fragments itself works with children binding. So let's go back to our data in the final form we want and we have something like this. You have the menu bar items. Nice to encapsulate in a fragment and this is great because it allows us to do children binding like so. Now all we have done is moved the data around [inaudible] into fragment while keeping the menu bar intact with the now playing menu item. And because the renderer only sees the children of the fragment, this still renders as a properly formed menu bar. On the very subject of data itself, that you can use to map to user interface elements, this is another likely scenario where you have data that some do not change, and some change all the time. For example in this particular piece of data itself, it is for a video that has a poster image, a title, and a playback progress. In some situations, we want to show different user interfaces based on that information itself. In the case where playback progress hasn't started, it's naturally zero and we are interested in showing the poster image and the title for the video itself. But when we start watching the video, progress will naturally be greater than zero. For that, we want to show the same thing that never change, the poster image and its title. However to make things obvious this video's being played back, we want to show a progress bar that's filled through the playback progress' percentage itself. In the very same vein, we have now two use cases where data is different and we also want to show two different looks of what it is. In the first case, we -- all we have is an image and a title in the lock up. And in the second case, we have the addition of an overlay and a progress bar. Typically you would have application logic that outputs different x amount based on that data itself but with rules, you can have a single statically defined template that would give you either representations of the final document. So what are the rules? Well, rules used data states to redefine your final document which, in turns, refines your UI. It is an invisible element. So the renderer doesn't see it but it affects how the document is authored. Any operations within those rules happen on sibling elements that the rules are a part of. And the best way to show this to you is in another example of how this can be setup. So let's look at the rules that we would use to structure the very prototype that we want -- that we've shown as an example. We will start by defining the prototype as the lowest common denominator of what our user interface should look like. And in this case, we have an image and a title. However, you would notice that we also have a place holder for the progress bar. Placeholders are also special elements that are invisible to the renderer and in this case, would be used by the rules as a target for replacement when data states match. Now let's fill in our rules. A group of rules that act on sibling elements are encapsulated in the rules tag. Individual rules that match a data state are encapsulated as specialized elements, and specialized elements become active when it matches certain data states. And the way that that's matched is based on a query in the state attribute itself. When data state matches, the children of the special element -- specialized element are the list of operations that act on the sibling elements of rules. In this case, we want the placeholder to be replaced with an overlay element and its children. TVMLKit matches the elements to be replaced by looking at the tag attribute on any element. It effectively does replacement by first matching on that tag and then comparing the element name. If the element name is different, it would replace that element hole and in this case, placeholder becomes an overlay. However, if the element name matches, whatever that's new would be appended to what's already existing. And so we have a single template with rules that will generate two different output based on the state of the data that's provided. Effectively you can move your application logic to deal with how things are displayed into a statically specific template that exists within the context of elements that's transformed into user interfaces. Now let's switch gears and talk about playback in TVMLKit. TVMLKit has long provided extension points where you need more customization of your user interfaces, be it individual views or even whole templates. In tvOS 12, we are extending this to our playback pipeline, giving you control over playback as well as its associated user experience. This experience works with all the different playback styles we have, whether it is embedded or in a full screen. You do this by providing a TVPlayer object and its associated user interface as a UIViewController. These have close analogues to our JavaScript APIs. So that would be less confusion in talking to your JavaScript developers. And finally there's a limited JavaScript Bridge that's exposed. It allows communications between your native code and JavaScript itself. Let's talk about TVPlayer and that's the base where you can get your customized playback experience working. TVPlayer is a public AVPlayer adaptor to the Playback Pipeline. What this means is that TVPlayer effectively translates regular AVPlayer callbacks into what JavaScript expects. TVPlayer is also the object that you can use to dispatch custom events to JavaScript and by default, it already handles everything that AVPlayer has as playback events. So anything extra is on you to dispatch. Changes that JavaScript makes to the player are KVO observable. So you know when things are changed by your JavaScript developers. Finally the TVPlayer object plays media in a sequential fashion from the very first [inaudible] item all the way to the very last in its playlist. Whenever a player is needed, TVApplicationControllerDelegate will ask for a TVPlayer and you will have to return an instance in order to participate in the Playback Pipeline. The next step of the Playback Pipeline is to actually show Playback on the screen itself in the form of a user interface. This can happen anytime whether it's full screen playback or embedded playback. It is entirely up to you to create your own user interface. When TVMLKit needs a user interfact, the TVInterface [inaudible] will ask its delegate for a view controller. It will also pass it a reference to TVPlayer that is responsible for playing media in that view. With everything, there are several caveats to using TVPlayer and its associated user interface. The very first thing is you should handle any 'should' events yourself. These are usually tied to interstitials and since that's largely user interface, this should be handled by the native code that you write. If you use FairPlay encryption for your video playback, you need to use AVContentKeySession for secure key loading. For more information about AVContentKeySession, look at our talks from last year, Advances in HTTP Live Streaming, and a talk from this year about AVContentKeySession Best Practices. Finally, if your JavaScript developers use overlay and interactive overlays, this will not work out of the box. They are user interfaces and because you are building your own, you will have to handle it yourself. In summary, the changes we've made to TVMLKit and tvOS 12 are the following two: One is the data binding is now even more powerful, enabling you to build data driven templates in any form. We highly encourage you to check this out. And finally, if you've been long waiting to customize the playback experience itself, you can do it today by implementing your own native playback experience. For more information about this talk, please look at the following url. And thank you for attending WWDC 2018. Thank you. My name is David Duncan. I'll be joined on stage by my colleague Tyler Fox and Russell Ladd. And we're here to talk to you about building apps for every size and shape. So I don't know how many of you were here back when we released iOS2 on the original iPhone. But you had one screen size to work with then. But today, we've got iPhones. We've got iPads. Multiple sizes. We've got the iPhone 10, the brilliant new screen, and a new shape for you to build your apps to. For this talk we're going to be using our Bagel Times app as an example of a design that adapts beautifully to iPhone 10 and iPhone 8. And so these are the three things we're going to be talking to you about today. I'll be speaking about safe area and layout margins. How you can use them in your application to adapt to various screen sizes and shapes. Then Tyler will come up and talk to you about scroll views. How they interact with safe area, layout margins, and other UIKit technologies. And finally, Russell will get up and talk to you about using all UIKit tools to build adaptive applications. And so with that, let's get to talking about safe area and layout margins. So what is a safe area, and what does it look like on the devices that you've come to know and love? Well, on an iPhone 8 with a rectangular screen, the entire screen is a safe area. All of it's there, no overlaps on your content. On iPhone 10, there's a little extra space that's taken up on the top and bottom by the hardware that will be taken away from the safe area for your application. And in landscape, you've got something kind of similar where we've given you a symmetric layout with space on the bottom for the home indicator for you to lay your content in safely. But what other devices might have a safe area that's not the entire screen? Well, Apple TV actually might give you the situation because certain TVs will have a screen that actually extends past what's visible to the user. And those will represent their information based on something called overscan compensation. And that overscan will be represented as a safe area in any apps that you build for Apple TV. So now that we've seen a few examples of what the safe area looks like at the screen of a device, how does that get to your views? And how do you use that in your own applications to build adaptive applications? So let's just pick up an arbitrary view. Whole thing. There are four insets on the top, bottom, and left and right that represent areas that might have some kind of overlay or other thing that might occlude your content if you placed it there. You can access this on your UI views using the safe area insets property. A UI edge inset that has those four values. Now if you're doing layout with Auto Layout, you actually might want to just see the entire rec that's safe. And you can get that by looking at the safe area layout guide. Which is a UI layout guide that represents that information. It has layout anchors for you to do Auto Layout with. And a layout frame if you just want to see the actual rectangle. So now that we've seen how that's represented on our view, let's take a look at how it gets from one view to the next. So we'll just take away the text and add a subview. And this is going to cover most of the bottom edge of that view. Now, how does the safe area get calculated for this? Well, as we saw, that subview that we just added, it intrudes on the unsafe parts of the view on the left, right, and bottom of its superview. And so those are the values that you will see in the UI edge insets represented by the safe area insets property on that subview. And similarly, you'll see a safe area layout guide whose layout frame looks kind of like this. Now once you've seen this, and you've got a view, you might want to actually add additional insets for your UI. You might decide that you want to add controls through your view controllers that will then add to the safe area or subtract from the safe area for your subviews. So we'll go ahead and add another subview here. And along with that, it will have a view controller because a view controller is where we actually have the property that allows you to add additional insets. And that property is called additional safe area insets. We're going to go ahead and inherit the insets we got from our parent. Add those additional safe area insets to this view. And finally construct the final safe area that let-- , layout guide. What other things might you want to know about how safe area behaves in your application? Well, let's take a look at another example. Here we've got one view safely inside of the safe area of its parent. So that view safe area, of course, encapsulates the entire region of that view. And we'll move it closer to one edge and as you might expect, we're not going to gain a safe area because we haven't left the safe area of our parent yet. We'll move a little bit further out, and you'll see that the bottom inset on that safe area now grows a little bit. It takes up a part where it overlaps, where it extends outside of the safe area of its parent. And as we move closer to the edge there, it continues to extend. Now what might you expect to happen if we move this view further off the bottom edge of its parent? Well, how about that? The safe area did not grow any further as the view moved outside of its parent. No matter how far outside of the parent that view might go. And you might be asking yourself, "Why would that, why would we do that? What's the purpose here?" And the answer is animation. In this particular case, we're moving this subview out of its parent. We wouldn't want the content to stay inside of the safe area because then it wouldn't move with its own parent. And as an example, we can see this app. Where we're going to pull up a view on the bottom. If that view had been laid out against the safe area of the parent, or its parent, and that parent's safe area would extend the farther it grew off the bottom of the screen, then this area would have stayed on screen during the entire transition, which would have meant that you would have not seen the background come up with it. And so that's why the safe area never grows larger on any dimension than what its parent provides. So to summarize that safe area part, let's took a look at how you can interact with it. Again, we've already mentioned the safe area insets property and the safe area layout guide property. But if your view needs to react to when safe area insets change, then you can override the safe area insets did change method. Typical thing you might do is just call set needs layout. But if there's any additional logic you need to run, that's okay too. If your view controller needs to respond to its view safe area layout changing, then you can override view safe area insets did change on your view controller. And finally, if you're doing your interface in IB, you can use that safe area property that's shown in the view list to build your constraints against the safe area. And so a safe area is explained. Let's talk a little bit about layout margins. So layout margins are padding. They're a property that, in general, you have full control over and allows you to specify a space from the edges of the view. Just like with safe areas, they're represented by a UI edge insets property, this time called layout margins. Now last year, we also added directional layout margins. These differ from layout margins in that layout margins use a UI Edge Insets value while directional layout margins uses an NS directional edge insets. And the primary difference between those two structures is that directional insets use leading and trailing instead of left and right, which makes it really easy for you to create layout margins that adapt to RTL layouts. So you don't have to do the swapping between left and right when you use directional layout margins. Now, just as with safe area, we provide a margins guide as well called a layout margins guide that you can use with Auto Layout to layout content against that margin. And then we go ahead and put our content inside of that view. Now the next question you might be asking since we're talking about layout margins and safe areas together is how do the two interact? Well let's go ahead and bring those markers back. Oh yeah, I meant to talk about how you can change those [inaudible] layout. So with safe area, by default, we build the safe area, and then we build the layout margins with respect to the safe area. And we do this for the perfectly obvious answer that by default, you probably actually want this. You want your layout margins inside of the safe area because they represent additional padding against the layout that you want to do. But we thought to ourselves, "Hey, some people may not actually want this." And so we made it very easy for you to flip this default. And so if you just change the insets layout margins from safe area property from true to false, then we'll go ahead and move layout margins back to the bounds of the view. As a [inaudible] instead of being encapsulated inside. So what else can you do with layout margins? Well, we already saw that by default, safe area margins propagate down the hierarchy. But with layout margins, they go by default. Because by default, your layout probably wants to be fairly independent with margins as opposed to safe area, which represents a concept that really is for the entire view hierarchy. But if you want to have propagation, you can flip this on view by view by changing preserve super view layout margins from false to true. And we'll go ahead and line up those margins that are smaller than the parents in order to make sure that everything lines up naturally between a child and its parent view. Now in years past, you've probably also tried to change layout margins of your view controller views. And if you look at what we do by default. That this margin that you'll see against that view. Now we additionally added a property last year called system minimum layout margins and these margins are the minimum margins that we've combined against any margins you provide now to create the final margin. So if you wanted to add to the top and bottom of this, you can do that without disturbing the left and right margins that UIKit provides. But again, you might want a little bit more control. And so there's another property called view respects system minimum layout margins. If you want your margins to be completely underneath your control, flip this to false, and we'll use your margins as you describe them, no questions asked. And so to wrap up with layout margins, the properties you have on UI View are the layout margins property, edge insets, left, right, top, bottom. Directional layout margins. Great for your RTL layouts. Top, bottom, leading, and trailing. The Layout Margins Guide that you can use with Auto Layout to do all this there. And finally, if you have logic for when the layout margins change in your view, you can override layout margins did change and do whatever logic you need to do there. In Interface Builder to create a constraint that ties against the margin, just check that constraints to margins box. And so with that, I'll bring up Tyler Fox to talk to you about scroll views. Thanks, David. Good afternoon. And as David mentioned I'd like to talk a little bit about scroll views today. So scroll views are a really key part of the iOS experience. And they show up all across the system. You have them in table views, collection views, UI text view, and of course in all of the custom application in all of your apps. And so in our app, Bagel Times that we've been working really hard on as you can tell, this is our news article screen. And everything as you can see is sort of centered around a scroll view with the article. And this is we want to you know really showcase the high-quality content our writers are putting together. Like, you know, exploring the true inspiration for, you know, Apple Park. And we want to review some of the basics about how you can use scroll views in your apps to understand how they can help you adapt your content to devices of different shapes and sizes. So to do that, we're going to walk through an example here, and we'll start with a full-screen scroll view. And on the left I'm going to show you what things look like on more or less a real device. And on the right, we're going to look at kind of things behind the scenes. Understanding what's happening in a diagram. So everything that we talk about today is going to apply in-- we're going to talk about the y axis of everything vertically. But everything also applies equally to the x axis. And we're also going to use some simple values for illustration. So in this case we'll say that our scroll view has a height of 400 points. Now inside of your scroll view, you'll have some content. And you can think of the scroll view a lot like a metaphorical picture frame. The scroll view size represents kind of the fixed frame of the picture frame. And then inside of that, the picture is your content. And the content can kind of slide around as if it's on ball bearings. Now in this case, our content is vertically scrollable. And that happens because our content has a taller height via the content size than the scroll view's height. And here I've turned off clipping on this diagram on the right so we can kind of peek behind the scenes at what's going on. So scroll views use content offset as a way to represent the current scroll position of the scroll view. Right now, we're scrolled to the very top. So our content offset is at zero, meaning our top edge of the content is aligned with the top edge of the scroll view. Now, if we go ahead and scroll the scroll view down, that will slide our content up. So let's scroll to the bottom and watch that happens here. And as you can see, the content offset increases and goes all the way to 100 in our example when we reach the bottom. That's because the top edge of the sc-- the scroll view is now 100 points below the top edge of that content area. If we set a content offset of zero on the scroll view, that will put us all the way back up to the top. And we're back where we started. So that's pretty much just the basics of scrolling a simple scroll view around. Now let's talk about a very important concept about extending the scrollable area of the scroll view. And we do that through a property called content inset. So content inset is a mechanism for you to provide insets that you can specify from the edges of the scrollable area of the scroll view to the content inside. So adding content inset increases the scrollable area and makes the scroll view able to scroll to a larger area. So let's set a content inset on the top edge here and see what happens. So here we've set a content inset of 20. And as you can see, that extends that top edge of the scroll view so that the scrollable area is now larger. Now when our content offset is sitting at zero, our content is still aligned with the top of the scroll view. But the scroll view could actually scroll even farther towards the top now. So let's scroll around like we did before and watch how things have changed. We'll scroll to the bottom and just like before, we're still here at a content offset of 100. That's because we didn't change anything on the bottom edge. And we still have a bottom content inset of zero. But if we now go ahead and scroll back all the way to the very top, we end up with a content offset that actually goes negative. That's because we are now able to scroll beyond the top edge of our content. And so we can scroll as far as the negative content inset on the top edge. Now starting in iOS 7, this content inset technique became really important and that's because with iOS 7, translucent bars became really common across the system. And the idea was that you would display your content edge to edge. And it would underlap the bars so that you could get these nice colorful blurs with your content through the bars, right? Tool bars, navigation bars, and so forth. So because this was so common, we wanted a way to help automatically set the content inset on your scroll views to make this easier in your apps. And to do this, we had a property on UI View Controller. And this was called automatically adjust scroll view insets. And the intent, intent again was to automatically set the content inset on your scroll view whenever it had overlapping bars coming from a navigation bar or a tool bar. And that was whenever your scroll view was inside of a view controller. Which itself was contained inside of a navigation controller. Now this worked pretty well for some common cases. But if your app had a more custom or advanced usage of UI scroll view, sometimes having your code setting content inset and also UIKit setting that same content inset property could create some challenges. And so starting in iOS 11, we have a much more explicit and powerful way for you to get the same automatic behavior. And the mechanism for doing this is a new property on UI scroll view which we'll talk about now called adjusted content inset. So starting in iOS 11, we introduced a new property on UI scroll view. It's read only. It's adjust content inset. And this basically describes how the scroll view is actually going to behave. Well you're probably asking what's the difference between this adjusted content inset and the content inset that we just talked about? Great question. Here's how it breaks down. Adjusted content inset is equal to the content inset that your app provides plus any automatic insets coming from the system or UIKit. And so because we now separate these two, it's a lot easier to reason about what's happening. Of course you might be wondering where and when would I get a system inset on my scroll view. Well, one most common case will be safe area insets. Let's look at how this works. So we're back to our basic diagram here. But we're going to bring some safe area insets into the mix. So if we go ahead and start out right now we have no safe area insets on any edge. But we'll go ahead and add some safe area insets to the top edge of our scroll view. And what you'll see is that by default, the scroll view is automatically absorbing those safe area insets on its top edge into its adjusted content insets. And that's because our scroll view's vertically scrollable. And so what this does is this automatically increases the scrollable area and makes it so content can scroll out from underneath anything covering up the top edge like a bar or even the edge of the display. So take a look at an example here where we have the same scroll view on two different devices. iPhone 10 on the left, iPhone 8 on the right. And you can see this is a real example, the top safe area inset is larger on the iPhone 10 because of the larger status bar height and the sensor housing. And as a result, the scroll view has a larger top inset-- that's the gray region that's shaded at the top here. So this is one way that scroll view helps automatically adapt to any device that it's running on. Now let's go back to our diagram and talk about something else. When we have a scroll view like this where we have top safe area insets, let's go ahead and add a subview to our scroll view. And we'll put this right in the content area. And so right now this-- this subview is sitting fully inside of the safe area in the scroll view. But what if we start to scroll the scroll view down which will move that content up? Just like this so that part of that subview is now sitting outside of the scroll view safe area? Based on the safe area inset propagation that David just talked to you about, you might be thinking, okay, that means the subview is going to start seeing its own top edge safe area inset. That's actually not what happens. The reason why is because when a scroll view absorbs safe area insets into its adjusted content inset, it will no longer propagate those same insets down to its subviews on that same edge. This is really important. Scroll views use scrolling to move content around and move it out into the safe area. And if a scroll view were to also propagate safe area insets that it was using to extend its scrollable area, it would almost be like double accounting for those same insets in two different places. And so as a result, the subviews on the scrollable axis are kind of completely unaware that there are safe area insets on the edges that the scroll view is absorbing them into its adjusted content inset. So now that we understand how scroll views work together with safe areas, let's cover the options that we've exposed to let you control this behavior. The mechanism that you have to do this is a property on UI scroll view called content inset adjusted behavior. And it also is available on interface builder as you can see here. Now the default value for this is automatic. And most of the time, if not all the time, you're really going to want to leave this at its default value. But we want to walk through the options so that you understand what they do and know that they're available. And so you can make the right choice in your apps. So we'll start with the first one. The first one is the always behavior. This is pretty straightforward as you might expect. Scroll view is going to always incorporate any system inset like safe area insets into its adjusted content inset on any edge. And this works fine in our particular example here. We only have a top and bottom safe area inset on our scroll view. And so it will incorporate those and the content gets to move out from underneath the bars. We don't have any horizontal insets on the left or right. So no problems. But be careful with this one because if you have something like let's say a table view on the iPhone 10 in landscape, there are left and right safe area insets. Using this behavior the table view is going to incorporate those into its adjusted content inset, which increases the scrollable area, which is going to make a table view horizontally scrollable. You're not going to want that behavior. That's why we have the next behavior which is scrollable axes. For this one, the scroll view is going to independently consider things on the vertical axis and the horizontal axis. For each of those, if the content size exceeds the width or the height as appropriate, or if you set the always bounce horizontal or always bounce vertical properties to true, then the scroll view considers that axis scrollable. And it will go ahead and incorporate in the system inset into its adjusted content inset. So in this example right behind me we have a long article that's scrollable. And so we're getting those automatic insets incorporated. But what if we had a shorter article? Let's take a look. Okay. Here's a shorter article. What's going on here? Let's take a look under the nav bar and see what's going on. Ah. Looks like we've lost our system inset because it's not scrollable anymore. So our title's stuck all the way up underneath the status bar. So let's put back the nav bar and talk about how we fix this. Well, one way could be you could set always bounce vertical on this scroll view if that's the behavior you want. That will make the scroll view always vertically scrollable. Or we'll get to our next behavior, which is automatic. And so automatic works basically the same as scrollable axes, which we just talked about. But it has one additional behavior as part of it that is when the scroll view is inside of a navigation controller, the scroll view will go ahead and adjust its top and bottom content inset even if it's not vertically scrollable to account for top and bottom bars. So, even in this case where we have a very short article, it still means that we're getting the right insets. And this is generally the behavior you're going to want. And that's why we have it as the default. Just one quick heads up though. If you are setting the deprecated automatically adjust scroll view inset property to false, that will disable this behavior. And so it's going to behave basically like scrollable axes. Alright, that brings us to our last behavior. Never. Now with this one, as you expect, that means the scroll view is never going to adjust its content inset. However, that has some side effects. For one, that means that the scroll view will end up propagating safe area insets on all of its edges just like a regular view. And as we talked about before, that might end up giving you some behavior you don't really want. For example, if you recall your layout margins are relative to the safe area, which means your layout margins might end up increasing or changing as a result of this. This is also going to disable some very helpful automatic behaviors that scroll view provides like automatic scroll indicator insets. And so if you do a search online and you see a favorite, you know, question and answer website suggesting that you set your scroll view's content inset adjustment behavior to never, consider instead using additional safe area insets to increase the safe area insets. If your goal is to try to express to the system that you've added let's say a toolbar or some sort of other overlay. Or you could consider just modifying the content inset directly, the property that we talked about in the very beginning. That's for you to control. And you can use that to add or subtract from the effective adjusted content inset that the scroll view will see. So with that, I'd like to hand it over to Russell to tell you all about how to put this altogether. Thanks, Tyler. Now we've introduced many adaptive APIs to help your apps over the years adapt to different environments. And safe area is really just the newest of these. So I'm going to review some of these concepts and also talk about how they work with safe area. So let's go to the first screen of our application. And we've got a pretty standard setup here with a tab bar controller, continuing navigation controller, containing our content view controller. Now note that the views of all three of these view controllers are full screen. This is what enables the tab bar to extend underneath the home indicator, the navigation bar to extend underneath the status bar, and the content to extend and scroll underneath all of it. But we need to prevent these elements from overlapping. So let's see how safe area allows these components to do that. So the safe area insets start by flowing through the tab bar controller, which only receives insets on the home indicator and status bar because that's all that it sees. Since the navigation controller is inside of that, it also receives a safe area inside in the bottom that accounts for the tab bar. And the content view controller inside of both received safe area insets that account for both bars. Now what does the story look like in landscape? Similar. There are safe area insets in the top and bottom. But there are also insets on the left and right that account for the size of the screen. And those are propagated all the way down from the screen through the view controller hierarchy. Now I want to stop here and use this example to make a point about how you should think about using safe area when you implement your own views. So this custom view shouldn't know that it's running on an iPhone 10. It shouldn't even know that it's contained inside of container view controllers. This is the idea of encapsulation. If your views only read the safe area insets that are provided to them on all four sides, and are able to adapt to different, to arbitrary safe area insets, that will ensure that your views will be able to, will be modular, can be moved throughout your application and run in different, different environments and still not be occluded. Now let's jump to an article and talk about hiding the status bar. So hiding the status bar is a technique that would reclaim 20 points of vertical screen real estate on rectangular screen phones. And we're just doing this by overriding preferred status bar hidden in our content view controller and we're turning true. And this preference is then propagating up through our contained view controller hierarchy and respected by the root-- the root of the system. Now unfortunately on iPhone 10, preferring the status bar hidden does not also hide the sensor housing. So we can't slide content underneath it. [Laughs] So UIKit will protect you and will not allow you to create this UI. Instead, the behavior of navigation controller on iPhone 10 is that it will always display the status bar when the navigation bar is visible. So if you want to hide the status bar and reclaim vertical real estate, our recommendation is to hide the navigation bar and status bar together. And, in general, when you want to go-- when you want to create an immersive experience, go immersive and just hide all of your overlays and controls together. Not only does this look good and help your users focus on their content, it's also a design technique that will adapt well to all of our devices. Now, speaking of immersive experiences. Let's switch to the iPad and talk about rendering text in a very wide environment. So here you can see we have text, and it's not extending all the way to the edges of our view. The problem with doing that, if we had rendered the text all the way to the edge, is that it can become difficult to read, for your eye to track from one line of text to the next at a given font size. And so the solution is to always render text inside of a readable width, a recommended readable width provided by the system based on the user's currently selected dynamic type font size. Dynamic type being another adaptive element of our iOS. Now you can get the readable width with this API on UIView called the readable content guide. So this is another layout guide just like the layout guide for margins and safe area. And it works just the same. And I mentioned that this readable width depends on the user's currently selected dynamic type size, which means if the user changes their dynamic type size in Control Center or the setting app, the readable width will get smaller or larger to compensate. Now let's switch to portrait and bring in our sidebar with our article list to make the context for our article display much narrower. So here the maximum recommended readable-- readable width is much wider than the space we have to display the article. And the thing to note is that the readable content guide will still not report the maximum readable width necessarily. It'll be clamped to the layout margins, which means you can be confident in laying out your views against the readable content guide. And not worry that they're going to spill outside your margins. Now let's see how this works on a context for a safe area exists. The readable content guide functions just like-- just like layout margins where its insets add to those provided by the safe area insets. Now normally at the regular-- the default dynamic type font size of the system, the readable width is going to be wider than the width of a device of an iPhone in any orientation which means it won't come into play. However, even iPhones, if the user selects a smaller dynamic type font size than the default, it can come into play, so it's still nice for your application to adopt. Now let's jump back out to our article list table view and see how readable width can work here. Now the thing to know about table views is that they use their layout margins, I'm talking about the margins, to layout a lot of their UI elements. So that means the separators, the system accessories as well as the labels in the system cell styles. And any views you lay out inside of your own custom table view cells, if you laid them out against the margins, will play along as well. So that means if you adjust the margins of a table view, you can move all of these elements together. So if you have a table view with a lot of multiline text in it, it makes sense that you would want to adjust the margins of the table view to bring all these elements into alignment and still respect the readable width. So to do this, table view provides an API. Called cell layout margins follow readable width. Now when it's false, table view will use its normal system margins. And if it's true, all the content will inset. Now, something to note is that the default value of this property has changed in iOS 12. It is now false by default. It was previously true. This shouldn't affect the behavior of your appl-- of your applications too much, especially on phones. Our general recommendation is to leave the default alone and set it to true when you know that you have a table view that's going to contain a lot of multiline text where it would make sense. And this property's also adjustable from Interface Builder with the follow readable width checkbox. Now keeping with table views but moving on from readable width, there is something else to know how they work with safe area, which is that the content views of your table view cells will not extend beyond the safe area. However, by default, the background and selected background views do-- of UI table view cell do extend beyond the safe area. So if you have some content you want to place in custom table view cells that you want to overflow outside the safe area and bleed to the edges of the screen, you can either place it in the background or selected background views if the semantics of those views make sense. Or there's a property on UI table view called insets content views to safe area, which by default is true, but you can change to false to get your content views to also extend to the edges. And this property is also configurable from Interface Builder. Now let's rotate back to portrait. And in the main screen of our application, we mentioned this sheet that slides up from the bottom before containing a picker view. Now the safe area of the screen here means that we have to adjust the layout of the picker view to be inside this safe area. And most of the system controls, like UI picker view and other controls, and probably controls of your own many views don't make sense to have to know anything about the safe area. Because it's not clear how they would respond or like re-lay out themselves internally. So we have a recommended technique for handling the layout of these kinds of views, which is to place them inside of the container view. And the responsibility of this container view is simply to position its safe area unaware content inside the safe area by analyzing the safe area insets. And it can also provide the background, which extends beyond the safe area and makes your control feel connected to the edge of the screen in this case. The-- another technique we want to talk about related to safe area and positioning elements close to the edge of the screen is when you have a control or button that on one device you may want to place directly against the safe area and on another device where the safe area insets are zero you may want some padding. And the reason for doing this in design is that the safe area insets can sometimes incorporate some implicit padding. So let me bring-- I want to provide you a single solution that will work for both of these situations. So let's bring in a diagram, and I'm going to give you two auto layout constraints that will produce this result in these two different situations. So the first constraint we need represents the padding that we would other-- we would normally add. So this is just a constant constraint from the bottom of the super view to the bottom of our control. But in this case, we're going to make it not required so that we can break it when the safe, when the safe area insets are non-zero. The second constraint is an inequality constraint from the bottom of our control to the bottom of the safe area. This ensures that our control is always inside the safe area. So if I change the safe area insets to make them non-zero, you can see that the inequality constraint ensures our content is not clipped while still maintaining some minimum padding. Now we've talked about many different adaptive APIs in this talk and in previous talks. Layout margins help keep lots of your elements aligned. Safe area insets protect your views from being clipped or occluded. Readable width keeps columns of text comfortable to-- comfortable to read. Size classes inform when you should make large structur-- structural changes to your applications. And these APIs we use to implement higher-level components in our frameworks. So scroll views, tab view-- table views, container view controllers. The other kinds of things we talked about today reuse these things to ensure that our high-level components can adapt to all of our devices in the simplest way possible. So the takeaway is for you to design your applications in terms of these adaptive primitives as well. The advantage is that-- instead of coding for a specific device. Your code will be simple, flexible, and it'll guarantee that your applications are as future-proof as possible for all of our different environments. For more information, these slides and recording will be available online. Thank you all for coming, and I really hope you've enjoyed the conference.  Thank you. Good afternoon. And welcome. My name is Michele. And I work in the iOS User Notifications team. Just a few minutes ago, my coworkers Kritarth and Teja [phonetic names] introduced you to group notifications. And in this session, we're going to explore a little bit more detail in-- this feature in a little bit more detail. And we'll learn how to use it and how to improve the efficiency of your notification and how to make them better organized. We will start by a brief overview of how the feature works and how the different parts of the UI work. Then we'll look at the default grouping, app grouping. We will see how to create custom groups for your notifications so that you can adapt the groups to the content of your apps. And finally, we will learn how to create custom summaries for your new notification groups. Let's start with a brief overview of the UI of the feature. Notification groups collect sets of different notifications together and in a group, and group them in a small stack. This helps making Notification Center better organized and more efficient for the user so that he can see more notifications at the same time. Without any app taking over the entire screen and preventing them from seeing some of the content. Each notification group shows the most recent notification that was delivered to that group at the top. And we call these the leading notification. Right underneath each group shows a brief summary that gives a little bit more details about what are the notifications included in the group. Gives you a number for, so you have a measure of how many notifications are inside. And in some cases, also some details about the content of those notifications. Notification groups also help in triaging notification. Because for example you can clear them, all the notification in that group with just one slide. And clear them all. But it's just as easy to expand the group and see all the notifications that are inside. And read one of them at a time and maybe clear one. And then when you're done, you can clear all of them, all of the group using the group buttons at the top. This was a very brief overview of the feature just to get more accustomed to the different parts of the UI and the terminology. So now allow, now let's see how the groups work and how they are created. The default behavior is app grouping. Each app gets its own group. And all the notifications from that app go to that group. This is the behavior that you get if you don't do anything. And you just keep sending notifications without adopting any new APIs. In many situations, app groups are sufficient, and they work very well. For example, as you see here in the Podcasts app, all the notifications are in the same group because Podcasts sends notifications that are very similar. They're notifications for new episodes of [inaudible] the shows. So there's no specific notification that is more interesting or something that is more important to show differently. As I said, as we said, it's very easy to tap on the group, expand it, and see the details inside the group. So app grouping is good. It keeps notifications more organized and Notification Center more compact and easier to use. But many apps have more specific content. They may need different organization to make notifications more efficient. And so we can create custom grouping. So in this session we will see first the API to create custom groups for your notifications. And then we'll look at a few examples of apps in iOS 12 to explore some of the patterns that they applied, and how they tried to balance the amount of content that was visible to the user. And the amount of content that was grouped together to help with the organization of Notification Center. So this is how you create a group for your app. You just set a custom thread identifier in the notification content. After you've done this, all the notifications that you send with the same thread identifier will be grouped together in Notification Center. The thread identifier can be any string. Doesn't matter what you said there. But all of the notification will be grouped together. You just need to be unique. You just need a unique string that is identifier's group. And that is it. You created a groups for your notifications. But given that it is so simple, we need to pay attention at how we create these groups. And also, if you're familiar with notifications API, you notice that this is not a new API. This is an API that we introduced in previous iOS releases. So it's a really existing and current iOS. We introduced it to, for [inaudible] notifications and for private notifications. So if you already adopted this API to support these two other features in previous iOS releases. Your notifications are already grouping today in iOS 12. You may want to review how the groups are created since now the context is a little bit different. So it may be useful to adjust them a little bit. So in this example, you see how to create a thread, set a thread identifier on a local notification. And of course you can set it on a push payload for push notifications. Okay. No, now let's go over a few examples from iOS 12. The most important thing to keep in mind as we go through these examples is the goal on notification grouping. When grouping-- by grouping notifications we want to make notifications more efficient for the user. And improve the organization in Notification Center. I'm sure you're familiar with the situation where one app has sent many notifications. Maybe you're in a chatty, messages conversations and someone is sending a lot of messages. We want to improve that situation by organizing notifications a little bit better so that the user can use them more effectively. Our first example is Calendar. Depending on how you organize your life or where you work, Calendar may send you a lot of notifications for event changes, event invites. But not all of them have the same importance. There are some notifications that are sent by Calendar that are more important than others. And they are event alerts that you set up when you create an event. Or time to leave notifications. These are more important because are notifications that I need to act on right now. If I receive a notification that I have a meeting in 15 minutes, I need to start walking, because I have to go to the meeting. But many of the other notifications are not something that need to be responded to immediately. There may be updates to share Calendars. There may be time changes in other events that are later in the day or some other day. And so it's useful to separate them. So what Calendar is doing is using the default group, the app group, for the bulk of the notification that it sends. And it does that by not setting any thread identifier on a majority of its notifications. Mail is a default value. So if you don't set it, that's a default group. And then Calendar sets a specific thread identifier just for these notifications that are more important and that it wants to call out. So the result is that most of the notifications from Calendar that are information updates that are not something that I need to react to immediately. Or that I don't need to have in reference later. For example, to know where the event is. They're grouped together in one group. And the other notifications that are the more important ones, more urgent or the ones that I need to reference maybe later to find the meeting room, they're separate than this other one. So this is-- just expand and I see what else is in my group. And this is the pattern that we learned from Calendar. Separate important actional notification from informative updates. Our next example is Messages. Messages is maybe the most straightforward notification group's implementation. It's pretty obvious. Messages has conversations. And each conversation has its own group. But we still, we can still learn something important. We can still learn a good lesson from Messages. Why is it that Messages separate all these groups in different conv-- different conversation in different groups? Well notifications that Messages sends are generally from people. From your friends, from your family. They're notifications that we care about, that are important. And second they're notifications that usually are short-lived. Because usually you respond pretty quickly at a Message. And it will go away from Notifications Center. So what Messages does is it creates a thread for these one-to-one conversation. And it creates a separate notification group for these group conversation. Just generates any group identifier for that thread and they get grouped together. It can expand and see, still see all my messages. So what does-- what is the lesson that we learned from Messages? Create groups for meaningful, personal communications. These notifications from Messages are usually important. And they live very shortly in Notification Center. So we can create many separate groups for all of them. Our final example is Mail. And Mail shares some of the same characteristics that Messages has. So they're often direct communications with people. They have a same, a similar concept of threads. Conversations. But there is some major differences between Mail and Messages. Mail can have an even higher volume of notifications than messages. And on the other hand, they can also be more long-lived. Mail, email is usually used for slower communications. Something that I don't expect to respond to immediately. And so organizing Mail threads, organizing Mail notifications by thread will not be ideal. It will create a lot of threads in Notification Center. And the UI to see in those thread is not very efficient. Mail provides a specific dedicated UI to displaying mail threads. So how is Mail organizing its notifications? Well, first of all, Mail provides some features to already organize and prioritize the emails that arrive in your inbox. Mail provides separate accounts. You can have many different accounts set up in your device. Provides VIP. You can set any contact as VIP. You can create-- you can have favorite folders. And you can turn on notifications for specific threads inside your mail client. And so it seems like the user is already telling us which notifications are more important. Which email messages are more important and which are less important. And that is what Mail does. Mail organizes notifications first by starting with the account group. It creates one big account for all the emails that arrive in a specific account. If I had multiple accounts, each one would get its own group. But then if a contact that is in my VIP contact sends an email to that account. That email is separated out to a different group because I said that's a VIP person. That's a contract that I want to be informed about immediately when it sends an email. And if I turn on thread notifications for some specific threads inside of Mail, also, those also get separated out in their own group. So you see that while Mail shares some of the same characteristics as Messages, the approach of grouping notification is a little different. Because the use of the notification and of the app and the content is different. So what is Mail telling us here? Respect the user's priorities and organization. Mail provides feature to organize email and to prioritize the content. So we can use that to organize notifications that Mail sends. So now that we've seen how to create as many groups as we want, we'll see how to make these groups a little bit more clear. By providing some bits of information in the group. That describes the content that they contain. As we see in these example of my Notification Center from last week. Each group has a little short summary at the bottom. And it explains a little bit of what that group is about. For example, you see Mail says that I have a few emails from in my work account. Podcasts is telling me that I have a bunch of new episodes that were delivered to my Podcasts app to listen. And News tells me I have nine more notifications from National Geographic. Let's explore this one a little bit. If your nine notifications said default message that we set in groups if you don't customize your summary. But you can better describe your content. For example, if you have an app that sends messages. You may want to see, you may want to say there that you're sending nine more messages. So how you do that? The first thing you have to do is to pick a summary format. And this is a format string that just describes your content with a numerical placeholder. As we see here, number, more messages. And then you set this format string in the notification category that you're going to use to send a notification. If you notice, the summary format is set on the category. Whereas the thread was set on the notification content. This is because categories as you know are types, groups, of notifications that you send that are similar. For example, messages has different categories for one-to-one conversations. And group conversations. And this means that they can set different summaries for those two categories. That is why the summary format is set in the category here. And while you're here updating your notification category for notification groups, you can also set this hidden previous placeholder property which is very similar to the summary that we used for notification groups. The major difference between these two is the context in which they're used. The hidden previous placeholder is not a new feature. It's a feature that we introduced last year in iOS 11. And this customizes the text that is displayed instead of the notification when the user sets the notification to be private. If I set a notification to be private, I say nine more messages. But then when I authenticate and I unlock my device, I can see the expanding notification and the summary underneath that says eight more messages. And that is why the summary format has more messages. And the preview placeholder has only the number of messages. So that was the setup for a basic summary. But Messages for group conversation has a more interesting summary, where it says the number of messages. But also the people that sent those messages in that thread in that conversation. And we cannot do that with the formula that we just specified because it only has a numerical placeholder. So what we do is we create a different summary format that contains a numerical string-- a numerical placeholder and a string placeholder. As you see here, number, more messages from a string. And we will replace the second placeholder with a list of names that are in the notifications. Still set the summary format in the category. And then we have to collect these names. To send these names, you set them in the notification content again. Because each notification can be sent by a different person. And they can be different names. So we will collect all of those names, build them up in a string and replace them in the summary. Oh, this is the push payload of course also supports summary arguments. And this is how the summary looks after we collected all the names and formatted them in the summary argument. The names don't need to be unique. You can send many notifications with the same name. And in this, in the Mail case, for example, all the notifications have the same name because they're all in the same account. We will handle de-duplicating them and will display only one instance for each single name that is visible. Next example is Podcasts. Podcasts shows us another small detail about this API. What is special about this notification? In this notification, Podcasts is saying that there are two new episodes available in my leading notification Podcasts. And the summary says that there are seven more episodes from a number of shows. And what happens when I expand this group? What happens is that there are only three external notifications in the group. Not seven like the summary said. And in previous examples, the number in the summary matched the number of notifications that were contained in the group. So what is happening here? Well, since Podcasts does these bundling on notification and they try to limit the number of notifications they send. When a show releases multiple episodes at the same time. They have these notifications that say, "Two new episodes are available, three new episodes are available." And if you sum the number, the number of episodes in the bottom three notifications, there were the ones that accounted for the summary, you can see that the total is seven. So let's see how the API for this works. It's just another property on the notification content and it's called summary argument count. And this count expresses the number of items that summary argument account, accounts for. In the summary. We will collect again as before all the names. We will sum up all the counts. And we will create the summary. And again as we can see, these notification would have summary argument count three, one, three, total is seven. And that's the summary that we displayed. And as everything else, of course you can also set it in the push payload. The summary argument count is optional. Defaults to one so you can only need to set it if you're doing this type of bundling on notifications. You don't need to set it always. Now, we are playing with words and we are building sentences. And when you do that, you always need to account for different languages and you need to account for plurals in this case. Because we have numbers. Again, another Podcast notification. For example, in this case. It says, "Four more episodes." But if I only had one more episode, that string would need to be different. But the API only allows us to set one string. And now we need two. You need two in English. But if you want to start to localize your app or if your main language is not English, you're writing up for a different language. Some languages have different rules for plurals. They may not have two cases. They may have three. And they may have cases that apply with different rules. So iOS in the foundation frameworks provides support for localizing these kind of strings. So you don't need to learn all the rules. And you don't need to know all the languages that you're going to translate your app to. And it's very simple to adopt and translate each string to have the correct plural form. The first thing that we need to do to adopt this 'fix our problem with plurals' is to replace the literal string that we use for the summary format, with a localized string. And remember here that you need to use our special notification API for localized strings, because we need to store these localized string and set it aside. Because in the case that the language of the system changes later. If you send notifications, we send them with the correct localization after the system localization change. So after you set a localized string here, you need to localize the string. And you localize the string in a strings dict file. A strings dict file is a property list file that describes a performance string and a configuration dictionary. In the configuration dictionary here at the bottom, you see the two versions of the string for singular and plural in English. But as I said, all you need to do to support other languages and different pluralization rules is change this stings dict file. You create a new one. And you see here Hebrew has three different cases for plurals. And Russian has three different ones and they're different from Hebrew. And you don't need to know which one to use when. If you use the foundation API that we provide. This is for the simple summary format that we just saw. But of course we can also format summaries with arguments. You see we define the format string at the top. We need to match the configuration dictionary by key. And underneath we specify the two strings for the two different versions. With the numerical placeholder and the placeholder for the list that we're going to provide with names. So since we're exchanging these format strings between apps and the system. These are effectively part of the API. So there's only a limited number of formats that we can support. And we need to agree on those. And they are the ones that I just showed you in these examples. The first format that you can use is the one with one numerical placeholder. When you don't need the arguments and you need to specify an unsigned number. And the second format that you need, you can use is the one with the numerical placeholder and the string placeholder. We will automatically detect which string you are using and format them correctly. Now we're all done with the features of notification grouping. But before you leave, I want to give you a couple more tips about notification grouping and these API that will help you finish the last bit of polish in your app. The first step is about combining different summaries. Since we saw that you can set different summaries in different categories. And the thread group are defined in the notification content. That means that you can mix in the same group notifications that have different summaries. And what happens when you do that? So there's two main cases. If none of the summaries in that group have any arguments, we'll try to combine all the summaries in the list and display them like this. But if any of the notification summaries in that group have any arguments, we're going to have to fall back to the default message. The second tip is about enriched notifications and groups. So we introduced enriched notifications a while ago in iOS. And they continued working in with group notifications. And when the user presses on a group to see the enriched notification that loads your content extension for that group, what happens is that your notification will be loaded with the leading notification. The one that is displayed at the top, and you receive it in notification using the usual [inaudible] notification API. Once the extension is loaded, you can load additional notifications. For example, the same ones that are in the group or if you want to display different content, you can also load it with your own API. Then while your extension is open and is running, if additional notifications are delivered to the same group, so they have the same thread identifier, they will be delivered to your content extension using the same did received notification method that you receive at the beginning. And lastly, if you display, if you already displayed additional notifications to the user by loading them from the delivery notifications. Or by loading them from your own API, you should remove them from Notification Center. To continue keeping Notification Center organized and more efficient. I want to recap a little what we talked about. It was a long presentation, and there were a lot of details. But I only have two very important things that I want you to remember here. The first one is the goal of notification grouping. The goal of notification grouping is to better organize Notification Center and help your users be more efficient in when they're using your notifications. This will help them triage them, and will have them receiving important information more quickly and react better. And the second thing I want you to remember is to add custom summaries to improve clarity. As you see in my examples, for example in podcasts or in Mail, just that little bit of text underneath the notification group will help to see what else is new in that group. And I don't need to open the notification group just to see everything about the content. The summary already gives some information. We will have a lab shortly right after this session, downstairs. We will have another lab tomorrow morning, and we look forward to talking to you. And on Friday, there is another session about notifications that talks a little bit more about the design aspects. About how to think about your notifications globally across all the user devices and across different systems. Thank you very much, and I'll see you later.  Good morning everyone. My name is Karol Gasinski, and I am a member of GPU Software Architecture Team at Apple. We will start this session with a brief summary of what is new in [inaudible], in terms of VR adoption. Then, we will take a deep dive into a new Metal 2 features, designs specifically for the VR this year. And finally, we will end this session with advanced techniques for developing VR applications. Recently, we introduced new iMac and iMac Pro that have great GPUs on board; iMac is now equipped with [inaudible] based GPUs and have up to 80 gigabytes of video memory on board. While iMac Pro are equipped with even more advanced and even bigger based GPUs, with up to 16 gigabytes of video memory. That's a lot of power that is now in your hands. But we are not limiting our service to iMac on their own. With recent announcement of extended GPU support, you can now turn any Mac into powerful workstation that gives you more than 10 [inaudible] of processing power. And that's not all. Today we are introducing plug and play support for HTC Vive head mounted display. It has two panels with 1,440 by 1,600 and 650 pixels per inch. That's 78% increase in the resolution, and 57% increase in pixel density compared to Vive. And with support for better panels comes support for its new dual-camera front-facing system, so developers will now be able to use those cameras to experiment with pass-through video on Mac. And together with Vive Pro support comes improved tracking system. So, you might be wondering how you can start developing VR application on Mac OS. Both HTC Vive and Vive Pro work in conjunction with Valve SteamVR runtime, that provides a number of services including VR compositor. Valve is also making open VR framework that is available on Mac OS, so that you can do the map that works with SteamVR. We've been working very closely with both Valve and HTC to make sure that Vive Pro is supported in SteamVR runtime on Mac OS. So, now let's see how new Metal features that we're introducing can develop a [inaudible] Mac OS Mojave can be used to fiber-optimize your VR application. As a quick refresher, let's review current interaction between application and VR compositor. Application will start by rendering image for left and right eye into 30 multi-sample textures. Then it will resolve those images into iOS surface back textures that can be further passed to VR compositor. VR compositor will perform final processing step that will include [inaudible] distortion correction, chromatic aberration, and order operations. We can call it in short warp. Once the final image is produced, it can be sent to the headset for presentment. It is a lot of work here that is happening twice, so let's see if you can do something about that. See, now with VR application it wants to benefit from multi-sample [inaudible], it needed to use the dedicated textures per i, or single shared one, for both. But none of those layouts is perfect. The dedicated textures require separate draw calls and passes, as we just saw. While straight textures enable rendering of both eyes in single rendered and results pass, they are problematic when it comes to post-processing the effects. [Inaudible] textures have all the benefits of both dedicated and shared layouts, but currently they couldn't be used with MSAA. This was forcing app developers to use different rendering part-time layouts, based on the fact if they wanted to use MSAA or not. Or use different tricks to work around it. So let's see how we can optimize this rendering. Today, we introduce new texture types to the multi-sample already textured. This texture type has all the benefits of previously mentioned types without any of the drawbacks. Thanks to that it is now possible to separate from each other rendering space, which simplifies the post-processing effects, views count, so that application can fall back easily to monoscope rendering and control over anti-aliasing mode. As a result, application can now have single rendering files that can be easily adopted to any situation, and most important, that can be rendered with single draw and render pass in each case. So here we see code snippet for creation of mentioned 2D multi sample [inaudible] texture. We set up sample count to 4, as it's an optimal tradeoff between quality and performance, and at the same time, we set up our other length to 2 as we want to store each image for each I in separate slice. So let's see how our pipeline will change. We can now replace those 2D multi-sample textures with single 2D multi-sample [inaudible] one. So now application can render both I in single render pass and if it's using instancing, it can even do that in single draw code. So that already looks great, but we still need to resolve those 2D multi-sample array texture slices into separate iOS [inaudible] faces before we pass them to compositor. So let's focus on our way, application shares textures with compositor. So now, for sharing textures, we use IOSurfaces. They are sharing textures between different process spaces and different GPUs, that we've got [inaudible] comes a price. IOSurfaces can be only used to share simple 2D textures, so if you have a multi-sampled one, storing [inaudible] or having [inaudible], they couldn't be shared. That's why today we introduce shareable Metal textures that allow your applications to share any type of Metal texture between process spaces, as long as these textures stay in scope of single GPU. This file features [inaudible] advanced view of these cases. For example, sharing depth of your scene with VR compositor. But, of course, it's not limited just to that. Now, let's look how those textures can be created. Because shareable textures allows us now to pass complex textures between processes, we will create 2D array texture that we will pass to VR compositor. As you can see, to do that, we use new methods, new shared texture with this creator. And while doing that, you need to remember to use private storage mode, as this texture can be only accessed by the GPU on which it was created. Now, we see a code snippet showing us how our VR application would send IOSurface to VR compositor in the past. We will now go through this code snippet, and see what changes needs to be applied to switch from using IOSurfaces to shared Metal textures. So we don't need those two IOSurfaces anymore, and those two textures that were backed by them can now be replaced with single shareable Metal texture that is over 2D array type. We will then assign this texture to both texture descriptors from open VRSDK, and change its type from IOSurface to Metal. After doing these few changes, we can submit image for the left and right I to the compositor. Compositor will now know that we've passed shared Metal texture with advanced layout, instead of IOSurface, and if we check, if its type is 2D array or 2D multi-sampling array. If it is, then compositor will automatically assume that image for the left i is stored in slice 0, and image for right i is stored in slice 1. So your application doesn't need to do anything more about that. And of course, sharing Metal textures between application and compositor is not the only use case for shareable Metal textures. So here we have simple example of how you can pass Metal texture between any two processes. So we start exactly in the same way. We create our shareable Metal texture, but now from this texture, we create special shared texture handle that can be passed between process spaces using cross-process communication connection. Once this handle is passed to other process, it can be used to recreate texture object. But while doing that, you need to remember to recreate your texture object on exactly the same device as it was originally created in other process space, as this texture cannot leave scope of GPU. So now let's get back to our pipeline and see what will change. Application can now replace those separate IOSurfaces with one 2D array texture, storing the image for both i's. This allows further optimization as original 2D multi-sample array texture can be now resolved in one pass as well to just create it shareable through the array texture. But that's not everything. Let's look at the compositor. Once we have simplified rendering parts on application site, there is nothing preventing compositor from benefiting from those new features as well. So compositor can now use those incoming 2D array textures and perform work for both i's in single render pass as well. And as you can see, we've just simplified the whole pipeline. So let's do recap of what we've just learned. We've just described two new Metal features. Shareable Metal textures, and 2D multi-sample array texture type. And the way they can be used to further optimize your rendering pipeline. Both features will be soon supported in upcoming SteamVR runtime updates. So now, let's focus on techniques that will allow your application to maximize its CPU and GPU utilization. We will divide this section into two subsections-- Advanced frame pacing and a reducing free rate. We will start with frame pacing. And in this section, we will analyze application frame pacing and how it can be optimized for VR. So let's start with simple, single-threaded application that is executing everything in serial monitoring. Such application will start its frame by calling WaitGet pauses, to receive pauses, and synchronize its execution to the frame rate of the headset. Both Vive and Vive Pro has refresh rate of 90 frames per second, which means the application has only 11.1 milliseconds to process the whole frame. For comparison, blink of an eye takes about 300 milliseconds. So in this time, the application should render 50 frames. So once our application receives pauses from WaitGet pauses, it can start simulation of your trial [inaudible]. When this simulation is complete, and state of all objects is known, application can continue with encoding command buffer that will be then sent to GPU for execution. Once GPU is done, an image for both i's is rendered, it can be sent to VR compositor for final post-processing, as we talked about a few slides before. After that, frames scanned out from memory to [inaudible] in the headset. This transfer takes additional frame as all pixels need to be updated before image can be presented. Once all pixels are updated, [inaudible] and user can see a frame. So as you can see from the moment the application receives pauses, to the moment image is really projected, it takes about 25 milliseconds. That is why application receives pauses that are already predicted into the future, to the moment when photons will be emitted, so that the rendered image is matching the user pause. And this cascade of events overlapping with previous and next frame is creating our frame basing diagram. As you can see, in case of the single-threaded application, GPU is idle most of the time. So let's see if we can do anything about that. We are now switching to multi-threaded application, which separates simulation of its visual environment from encoding operations to the GPU. Encoding of those operations will now happen on separate rendering threads. Because we've separated simulation from encoding, simulation for our frame can happen in parallel to previous frame encoding of GPU operations. This means that encoding is now shifted area in time, and starts immediately after we receive predicted pauses. This means that your application will now have more time to encode the GPU [inaudible] and GPU will have more time to process it. So, as a result, your application can have better visualize. But there is one trick. Because simulation is now happening one frame in advance, it requires separate set of predicted pauses. This set is predicted 56 milliseconds into the future so that it will match the set predicted for rendering thread and both will match the moment when photons are emitted. This diagram already looks good from CPU side, as we can see application is nicely distributing its work [inaudible] CPU course, but let's focus on GPU. As you can see, now our example application is encoding all these GPU [inaudible] for the whole frame into a single common buffer, so unless this common buffer is complete, GPU is waiting idle. But it's important to notice that encoding of GPU operations on a CPU takes much less time than processing of these operations on the GPU. So we can benefit from this fact, and split our encoding operation into a few common buffers while a few common buffer will be encoded very fast, with just few operations, and submitted to GPU as fast as possible. This way, now our encoding is processing in parallel to GPU already processing our frame, and as you can see, we've just extended the time when GPU is doing its work, and as a result, further increase amount of work that you can submit in a frame. Now, let's get back to our diagram, and see how it all looks together. So as you can see, now both CPU and GPU are fully utilized. So [inaudible] application is already very good example of your application, but there are still few things we can do. If you will notice, rendering thread is still waiting with encoding of any type of GPU work before it will receive predicted pauses. But not all [inaudible] in the frame requires those pauses. So let's analyze in more detail to pick our frame workloads. Here, you can see a list of workloads that may be executed in each frame. Part of them happen in screen space or require general knowledge about pause for which frame is rendered. We call such workloads pause-dependent ones. At the same time, there are workloads that are generic and can be executed without knowledge about pauses immediately. We call those workloads pause independent ones. So currently, our application was waiting for pauses to encode any type of work to GPU. But if we split those workloads in half, we can encode pause independent workloads immediately and then wait for pauses to continue with encoding pause-dependent ones. In this slide, we've already separated pause independent workloads from pause dependent ones. Pause independent workloads is now encoded in [inaudible] common buffer, and is marked with a little bit darker shade than pause-dependent workload following it. Because pause-independent workload can be encoded immediately, we will do exactly that. We will encode it as soon as the previous frame workload is encoded. This gives CPU more time to encode the GPU work, and what is even more important, it ensures us that this GPU work is already waiting for being executed on GPU so there will be exactly no idle time on GPU. As soon as previous frame is finished, GPU can start with the next one. The last subsection is a multi-GPU workload distribution. We can scale our workload across multiple GPUs. Current Mac Book Pro has two GPU on board, and while they have different performance characteristics, there is nothing preventing us from using them. Similarly, if each GPU is connected, application can use it for rendering to the headset while using Mac's primary GPU to offload some work. So we've just separated pause-independent work and moved it to a secondary GPU. We could do that because it was already encoded much earlier in our frame, and now this pause-independent workload is executing in parallel to pause-dependent workload of previous frame. As a result, we further increased the amount of GPU time that you had for your frame. But, by splitting this work into multiple GPUs, we now get to the point where we need a way to synchronize those workloads with each other. So today we introduce new synchronization parameters to deal exactly with such situation. MTL Events can now be used to synchronize GPU work in scope of single GPU across different Metal cues and MTL Shared Events extends this functionality by allowing it to synchronize workloads across different GPUs and even across different processes. So here we will go through the simple code example. We have our Mac, with attached eGPU through Thunderbolt 3 connection. This eGPU will be our primary GPU driving the headset, so we can use GPU that is already in our Mac as secondary supporting GPU. And we will use shared event to synchronize workloads of both GPUs. Event initial value is zero, so it's important to start synchronization counter from 1. That's because when we would wait on just initialized event, its counter of zero will cause it to return immediately, so there would be no synchronization. So our rendering thread now starts encoding work for our supporting GPU immediately. It will encode pause-independent work that will happen on our supporting GPU course, and once this work is complete, its results will be stored in locker memory. That's why we follow with encoding brief operation that will transfer those results to system memory that is visible by both GPUs. And once this transfer is complete, our supporting GPU can safely signal our shared event. This signal will tell eGPU that now it's safe to take those results. So our rendering thread committed this [inaudible] common buffer, and supporting GPU is already processing its work. At the same time, we can start encoding command buffer for a primary GPU that is driving the headset. In this command buffer, we will start by waiting for our shared event to be sure that the data is in system memory, and once it's there, and the shared event is signaled, we can perform a brief operation that will transfer this data through Thunderbolt 3 connection, back to our [inaudible] GPU and once this transfer is complete, it's safe to perform pause-dependent work, so a second command buffer will signal lockout event to let pause-dependent work know that it can start executing. After encoding and submitting those two command buffers, rendering thread can continue as usual, with waiting for pauses, and later encoding pause-dependent work. So now we have a mechanism to synchronize different workloads between different GPUs. But as you can see, our secondary GPU is still a little bit idle. That's because in this example we decided to push through it, pause dependent workloads that have dependency with pause dependent ones. Excuse me. But of course there are types of workloads that have no dependencies, and they can happen at lower frequencies, the frequency of the headset. One example of such workloads can be, for example, simulation of physically based accurate [inaudible] or anything else that requires a lot of time to be updated. Such workload can happen in the background, completely asynchronously from rendering frames, and each time it's ready, its results will be sent to primary GPU. It's marked here with gray color to indicate that it's not related to any particular frame. So, of course there are different GPUs with different performance characteristics, and they will have different bandwidth connections. And your application will have different workloads in a single frame with different relations between them. So you will need to design a way to distribute this workload on your own, but saying all that, it's important to start thinking about this GPU workload distribution, as multi-GPU configuration are becoming common on Apple platforms. So let's summarize everything that we've learned in this section. We showed multi-thread application to take full benefit of all CPU codes. And split your command buffers, to ensure that GPU is not idle. When doing that, if possible, try to separate pause-independent from pause-dependent workloads, to be able to encode this work as soon as possible, and even further, splitting workloads by frequency of update so if your application will execute on multi-GPU configuration, you can easily distribute it across those GPUs. And while doing that, ensure that you drive each GPU with separate rendering threads to ensure that they all execute asynchronously. Now, you switch to reducing fill rate. Vive Pro introduces new challenges for VR application developers. To better understand scale of the problem, we will compare different medium fill rates. So, for example, application rendering in default scaling rate to Vive headset, produces 436 megapixels per second. And most advanced [inaudible] against that [inaudible] HD TVs have fill rate of 475 megapixels per second. Those numbers are already so big that game developers use different tweaks to reduce this fill rate. Now, let's see how Vive Pro compares to those numbers. Vive Pro has a normal fill rate of 775 megapixels per second, and if you add to that four times multi-sampling [inaudible] or bigger scaling rate, this number will grow even more. That is why reducing fill rate is so important. There are multiple techniques there and new techniques are made every day. So I encourage you to try them all, but today we will focus only on a few still as they are the simplest to implement and bring nice performance gains. So we will start with clipping invisible pixels. Here, you can see image rendered for left eye. But due to the nature of the lens work, about 20% of those pixels are lost after compositor performs its distortion correction. So on the right, you can see image that will be displayed on a panel in a headset before it goes through the lens. So, the simplest way to reduce our fill rate is to prevent our application from rendering those pixels that won't be visible anyway, and you can do that easily by using SteamVR Stencil Mask. So we've just saved 20% of our fill rate by applying this simple mask, and reduce our Vive Pro fill rate to 620 megapixels. Now, we will analyze implication of this lens distortion correction in more detail. We will divide our field of view into nine sections. Central section has field of view of 80 degrees horizontally by 80 degrees vertically, and we have surrounding sections on the edges and corners. We've color tinted them to better visualize the contribution to final image. So as you can see, corners are almost completely invisible and edges have matched less contribution to the image than in the original one. In fact, if you see this image in the headset, you wouldn't be able to look directly at the red sections. The only way to see them would be with your peripheral vision. So this gives us great hint. We can render those edge and corner sections and a reduced fill rate, as they are mostly invisible anyway. We render the central section as we did before. But then we will render vertical edges with half of the width and horizontal sections with half of the height. And finally, we will render corner edges at one-fourth of the resolution. Once our expensive rendering pass is complete, we will perform cheap upscaling pass that will stretch those regions back to the resolution at which they need to be submitted to compositor. So you are wondering how much we've gained by doing that. In case of 80 by 80 degree central region, we reduced our fill rate all the way down to 491 megapixels per second. But you remember that we just talked about clipping invisible pixels, so let's combine those two techniques together. By clipping pixels combined with multi-resolution shading, you can reduce your fill rate even further to 456 megapixels per second, and that is not a random number. In fact, that's a default fill rate of Vive headset, so by just using those two optimization techniques, your application can render to Vive Pro with much higher resolution using exactly the same GPU as it did when rendering to Vive headset. Of course, you can use those techniques when rendering to Vive as well, which will allow you to bring visualize of your application even further and make it prettier. There is one caveat here. Multi-resolution shading requires few render passes, so it will increase your workload on geometric [inaudible], but you can easily mitigate that by just reducing your central vision by a few degrees. Here, by just reducing our central vision by 10 degrees, we've reduced fill rate all the way to 382 megapixels per second. And if your geometry workload is really high, you can go further, and experiment with lower fill rate, lower regions, that will reduce fill rate even more. In case of 55 by 55 degrees central region, 80% of your [inaudible] eye movement will be still inside this region, but we've reduced our fill rate by more than half, to 360 megapixels per second. So of course there are different ways to implement multi-resolution shading. And you will get different performance gains from that. So I encourage you to experiment with this technique and try what will work for you best. So let's summarize everything that we've learned during this session. We've just announced plug and play support for Vive Pro Headsets, and introduced new Metal 2 features that allow you now to develop even more advanced VR applications. And I encourage you to take advantage of multi-GPU configurations, as they are becoming common on other platforms. You can learn more about this session from this link, and I would like to invite all of you to meet with me and my colleagues during Metal 4 VR Lab, that will take place today at 12:00 p.m. in Technology Lab 6. Thank you very much.  Good afternoon and welcome to Getting the Most out of Playgrounds in Xcode. My name is Tibet Rooney-Rabdau and with me today are my teammates Alex Brown and TJ Usiyan. I love that every time I have a new coding idea I can jump right into a playground to try it out. In today's session we will share with you our favorite work flows. This is a great session for those not too familiar with playgrounds in Xcode, as well as those diving deeper. We're going to start out todays talk with playground fundamentals, a brief overview of playgrounds, markup, and imbedded resources. Then we'll show you how to run step-by-step in a playground. We'll discuss tips to give you more control of your code execution. Lastly we'll finish with advanced techniques including the wonderful world of Custom Playground Display Convertible. We will use this protocol to customize your inline results. We will also discuss how to use workspaces to exercise code in your frameworks. Playgrounds provides an interactive place to explore Swift, create learning environments or others, and prototype parts of your app. Let's get started with a quick tour of Playgrounds in Xcode. When you first open a playground, Xcode shows you the standard editor. This is the basic editor mode you use in Xcode with a couple additions. What is unique about this editor when you're in a playground is the result sidebar on the right hands side displaying the result of each expression in your code. You also have the ability to add these results as inline results. This is done by selecting the rectangular button beside your result in the result sidebar. This will then show your result inline with your Swift Code. In this example you can see this demonstrated where "Hello, playground" is added below our string variable declaration. Let's take a look at the assistant editor mode. What is interesting about this mode in Playgrounds in Xcode is that you can visualize your results in more detail by using a live view. In this example I've created a live view using UIView that shows a welcome message. But you could also use UIViewController for iOS and [inaudible] playgrounds and for MacOS playgrounds NSView and NSViewController. Now let me show you a short snippet that demonstrates how to show Live View. First you need to import PlaygroundSupport. PlaygroundSupport is a framework provided by Xcode to allow playgrounds to interact with Xcode, including support for showing live views. You import this playground support framework so you can use its API. Once you've created a viewController using the standard UIKit or AppKit APIs, you need to hand it over to the PlaygroundSupport framework. You do this by setting the Live View property of the current playground page to your viewController. This signals to Xcode that it should show your viewController in the assistant editor. To give your playgrounds an extra bit of polish, you can include some markup text. It is especially great when you're making a playground to share with others. With markup you can include stylized text, images, and video in your playground. Now let's take a quick walkthrough what is possible with markup. Here I've included some markup comments with text for a poem I'm writing. A markup comment is like a regular comment but with a colon after the forward slashes. The rest of the comment is then treated as markup text. If you have multiple lines of comments next to each other they form one block of markup text. You can also use multiline comments by putting a colon after the first asterisks. Here's our markup in Xcode now. It is showing the raw markup of the poem I wrote. To show it in its rendered form, select the button in the top right corner of the window, which shows Xcode's Inspector. Then select Render Documentation under Playground Settings. The lines in my poem are now rendered. Other interesting things you can do with the markup are adding headings to a Playground peach. You can use headings to create structure in your playground. You can use number signs to supply up to three levels of heading. In this example the title of my poem "Roses are Red" as a first level heading, the subtitle, "An Ode to Markup" as a second level heading, and the byline as a third level heading. Remember to add at least one space between the number sign and the heading string, otherwise you'll have a number sign connected to your heading when rendered. This is what the headings look like rendered when included with the lines in my poem. You can see that the first level heading is largest, followed by the second level, and then the third level heading. You can also format the text in your markup content. You can wrap a string of characters on a single asterisk on each side to italicize the text between the asterisks. You can also use backticks to display text with Code Font. Finally, if you use two asterisks instead of one the text will be bold. Let's take a look at this rendered. You can see that red and blue are italicized. Markup is in Code Font and fun is in bold. Let's take a look at using lists in markup. If a markup comment starts with a number followed by a period, it will create an item in a numbered list. In this example, I've added the lines of my poem to a numbered list. Here you can see the lines of my poem rendered in a number list, each line representing an item in a numbered list. You can also create a bullet to list in markup. A bulleted list is like a number list except each line starts with an asterisks instead of a number. This is how my poem in rendered in a bulleted list with each line in my poem represented as a bulleted item in a bulletin list. Markup can also contain links. In this example I've created a link to roses and violets. To create a link you wrap the text in brackets and then place the destination of the link in parentheses after. Another way you can create a link is by using a reference. In this example I've created a reference named 1, but this can be any string. It doesn't have to be a number. When creating and using a link reference you wrap the name in brackets. When creating a reference you add a colon and then the destination to the link. Here are the links represented in rendered form, Roses, Violets, and Fun are shown in blue to represent a link. Up to this point we treated a playground as a single location, but playgrounds can contain multiple pages, each with their own markup and code. To create a new playground page, select the playground and then open the file menu. Select New, then Playground Page. You could also create a playground page by control clicking on the playground and then selecting New Playground Page. You can create links in your markup, which navigate between pages. To go to the previous page, you can create a link where the destination is @previous. To go to the next page you can create a link where the destination is @next. Finally, if you want to navigate to a specific page, you create a link where the destination is the file name for that page without the extension and by replacing any spaces or special characters with their percent in coded form. You can also imbed some additional content to make you playgrounds even more powerful. You can add additional Swift files to the sources folder, which is at the top level of the playground. Each page also has its own sources folder. Sources are compiled as separate modules where they are automatically imported where visible so you don't have to handle import statements. Since these are compiled as separate modules, you can use access control to control what is exported from your axillary sources. Anything you want to use as your main playground source should be marked as Public. A great example of things to put into the sources folder is helper code such as classes or extensions that are outside of the main playground. Playgrounds can also contain other resources. Resources are any other file that you'd like to use in your playground such as images, audio, video, storyboards, and ZIPs. Just like sources, there's a resources folder at the top level of the playground as well as for each page. You can use these resources that you've added to your playground both in your markup and code. Similar to how you create a link in your playground, you can use this highlighted syntax to imbed an image in your markup. You specify the image name, in this case MyPicture.jpg. You also specify alternate text describing the image along with hover title text that shows my pointer is over the image. Both alternate text and hover title text can also be used for accessibility to benefit voiceover users. And here we access the same imaging code using the standard UIImage or NSImage APIs. Similarly, to images you can also imbed videos in your markup. This is done by using a syntax similar for that images with additional support for specifying a poster image along with the width and height of the video. For other resources that you want to access in code, such as this video, you can use the standard Bundle APIs for finding resources. This example uses a URL for resource with extension API to ask the main bundle for URL for Mvideo.mp4. Resources in your playground are automatically treated as resources of the main bundle. For more information on the markup you can use in your playgrounds and the playground support framework we discussed earlier, please visit developer.apple.com. With that please welcome my teammate Alex Brown to the stage so he can tell you more about exciting new changes to playgrounds in Xcode 10. Thank you. Thank you, Tibet. I'm Alex Brown and I'm a Core OS Engineer. I want to ask you a question. Have you ever woken up with a great coding idea but then struggled to set it down before you reach your code editor? Perhaps it's your inbox that gets in the way or perhaps you haven't yet set up your project within Xcode. Whether you're a beginner just getting started with Apple APIs, a seasoned engineer with a deadline, or a data scientist building machine learning models, I want to tell you that Xcode playgrounds are the fastest way to get started coding against Apple's APIs. In Xcode 10, playgrounds are faster and more responsive than ever allowing you to execute your code in a step-by-step fashion. First I'd like to get you familiar with a new UI for doing this. If you're familiar with playgrounds, your eye should immediately jump to the blue line over the not line numbers. There's a new play button which tracks your pointer. A blue line over the line numbers mean these lines are ready to execute and when you click the Play button this means run all of the blue lines up to and including the one with the play button on it. Let's see what that looks like. There we go. You can see just the first three lines of the playground are executed. The results are visible on the right-hand side. You can also see that the play button has now gone gray. This indicates that these lines of code are no longer ready to execute. You just executed them. There's a second reason that they play button might be gray. This means you're hovering over some code, which is not a top-level line of code. This includes code inside function brackets and inside of fall loop. If you wish to execute a fall loop you need to move your pointer to the closing brace where the play button will go blue then you'll be able to execute it. There's also a great keyboard shortcut, Shift Return. This is just like pressing Return at the end of typing a line of code, but it also means execute this line, and it move the cursor to the next line ready to write some more code. Blue code also has a second interpretation. It means code that it's safe to edit without resetting your playground. Why is this important? If you edit code above the blue line this is modifying code that you've already executed. So you'll have to resent your playground to take account of your changes. When you edit code above the blue line, the playground resets automatically. Sometimes you'll need to reset your playground manually and you can do this using the stop button in the Debug bar at the bottom of the screen. So why run in a step-by-step fashion? Why have we added this feature? Firstly, executing just one more line is fast. It's way faster than restarting a playground and waiting for it to catch up with your thoughts. Secondly, it allows you to respond to live data. Write a line of code, execute it, see what the results are, and this should lead you naturally to the next line of code to write. And thirdly rerunning a playground every time can give you different values every time. This can happen, for instance, if you're accessing a network resource or you're generating random numbers. By executing step-by-step your data model stays stable and easier to understand. So let's take a look at a simple example that you can try yourselves. I like games. I like card games and board games and computer games. I even like writing games. Now I'm not too hot at it so I like to keep my games small. This is a really small game. It's the game of roshambo. It's the schoolyard game of rock, paper, scissors. I've written the rules as a simple function check, which tells us if we've won. I've also written a computer player, which simply makes one of the moves at random. By executing step-by-step we can execute line six where the computer player plays and see what the result is before making our move. In this case the computer player has played "rock" and we've played "paper". Paper beats rock and we win the game. This might sound a little bit like cheating, but it's an extremely powerful technique. It's an extremely powerful technique when you're trying to learn a new and unfamiliar API or exploring a data structure whose types and values you don't yet know. So that's a very simple example. Now we're going to look at something a little bit more magical with a demo. So we started off with one game, I'm going to move on to another. This is the game of Tic-tac-toe. I've already been working on this a little while. I've built the game engine and I've made a first version of the UI. I've moved all of that code aside into the auxiliary sources. This gets it out of our way and allows us to focus on the next steps -- actually playing the game and refining the UI a little. I'm also going to use a live view, but first let's look back at the blue control over the line numbers. You can see that it tracks the mouse pointer. So let's get started by loading the game board. Let's rotate it so we can see it. This is the Tic-tac-toe game board. You can see I've been able to execute the first half of the playground and I've still got some lines left in the playground to execute. These include my first move. Let's run that. There we go. So you can see we're going to be playing the game in code and viewing the results with the live view. Let's let the computer play and have a turn. Few, that's great. The computer player is completely random. So it's just possible it's going to beat me today. So I've shown you how we can execute some of the lines of code and then step-by-step execute lines of code which are already present in the playground. But we can do more than that. It's always safe to add new code at the end of a playground. So let's make another move. There we go. Now, I can click the Play button to execute this line of code and I think now we'll let the computer have another go. That's great. [ Laughing ] You should notice a couple of things here. Number one, in the beginning the board was flat. I had to rotate it so it was in a position where we could view it. As we execute more lines of code, it doesn't rotate back to its original position. This is pretty good because I could write lines of code to rotate it to the correct position, but this would involve something called [inaudible], and I don't think I'm quite ready to learn that yet. The second reason it's good is because if the computer player was allowed to play again, every time we executed new code it would effectively get a do-over, which would be a little unfair. Let's finish this up. I think I can take the game home from here. Is that the right move? That's great. You can see the game has highlighted my winning move by placing three red circles. That's not too fancy. I'm looking for something which really makes it feel like a celebration. So I've been working on a new version of the UI. In a traditional development environment, you would have to set up a complex harness to automatically play you deep into the game or whatever the API was you were using so that we could test out something that happened later on in the program. But in a playground, because I'm able to execute step-by-step, I can play it manually to the end of the game and then write new lines of code to update whatever's going on at that point. Let's do that now. I've prepared a new effect for the end of the game using a particle system. Let's use Shift Return to execute it. Boom. And that feels much more like a win. So let's review what we've just seen. By running step-by-step we can explore an idea a line at a time. This enables us to have a conversation between code and data. Every time we learn a new fact we can write the next line of code to explore it a little more. We can use Shift Return to keep our hands on the keyboard and our brains in the zone. And finally, by adding a dynamic live view to a playground, we can create a second view of our model and we can seamlessly shift between manipulating it in a graphical environment and using code. So whether you're a beginner, just learning about new Apple APIs or an experienced programmer trying to sketch out the bones of their next great app, the next time you have a coding inspiration we invite you to get started with a playground. Just in case you're still looking for ideas I've got three right here. Number one if you're creating your own API, one of the best ways to showcase it is to create a follow along tutorial for your API. Users can execute the code step-by-step and see in real time what it does. Number two you can download and explore some publicly available data using the playground step-by-step to drill down into your data. You could get this from maps, from local government, or perhaps a class project. Finally, like I did, you could create a game or an animation. Start off simple using SpriteKit or SyncKit and line by line enhance it until it's completely awesome. So I hope we've shown you today that playgrounds are not a toy. They're fun but they're serious fun. They allow you to explore code and data interactively. This is great whether you've downloaded an unknown snippet of JSON using a rest API from the Internet, or if you're dealing with hundreds of thousands or millions of lines of data for a machine learning application. And to find out more about that use case I encourage you to download and watch the Create ML sessions in your WWDC application. Secondly there's really no better way to learn how to use Apple APIs. Whether you're just getting started or your trying out a new API, which you've learned about at this WWDC. You don't just have to use Apple's APIs. You can also import your own frameworks into playgrounds. And one of the best ways to showcase this is to create custom representations of your data types allowing developers to view the most relevant information at a glance. And to talk more about these two advanced concepts, I'd like to invite TJ to the stage. Thank you, Alex. My name is TJ Usiyan and I'm an Xcode Engineer. By the time I finish my section, I'm sure that all of you will agree that every librarian framework that you ship will and can be improved with the addition of a playground. Playgrounds provide a richer experience when providing readme's, tutorials, and general API documentation. One of the ways that they can do this is by, as Alex said, providing your own representation in playgrounds. I'm going to cover all of these things starting with Custom Playground Display Convertible, which will allow you to provide custom representations of your types in playgrounds so that users can get a nice summary. I'll follow that up with how to import your frameworks into playgrounds. And finally, I'll cover a little bit of troubleshooting tips for when things don't go exactly as you'd expect. Let's start with Playground Display Convertible. So as you know, when a user types in a line of code values show up on the right hand side in the result sidebar. For types that are no optimized for playgrounds there are two ways that playgrounds will generate this description. For types that do not conform to Custom String Convertible, we will create a structured representation using the Swift type. For types that do conform to Custom String Convertible, we will use the result of calling descriptions. When this isn't enough sometimes your users will tap on the Quick Look button on the right-hand side. They will also tap or click on the Inline Results button. When they do this a custom representation will show up. Often a textual representation. And for many types this is perfectly enough but sometimes maybe you would like to return a number. This is still text though and there are times when text isn't going to be enough and you would like to return something graphical, maybe a picture. The way to control what you return is to implement Custom Playground Display Convertible, a new protocol introduced in Xcode 9.3 and Swift 4.1. It replaces Custom Playground Quick Lookable, which was deprecated in the same versions. Let's take a look at conformance right now. You can see here that conforming involves only returning one property, Playground Description. Playground Description is of type Any, which means you can return anything that you feel best describes your value. Now, there are certain types that have specialized representations already and those are provided by Apple. Here's a list of the types with specialized representations as of Xcode 9.3 and Swift 4.1. The types on the left have a particularly textual representation. The types on the right have a graphical representation. I invite you to try each of these and decide which best represents your value in a playground. Once you've done this you'll want to ship all of your types, and your values, and your API to your users. I'm going to show you how to import your own custom frameworks into playgrounds alongside Apple frameworks. Typically, when you build a single framework in a project, that framework will end up in the Built Products Directory. When you want to import it into a playground, that is where the playground will work. The simplest way to make sure that your playground will see your framework is to add the playground to your project. This is the strategy that I suggest for simple projects where you have access the project and are willing to edit it. This is what that would look like in the Project Navigator. Once you've added this project, or once you've added this playground to your project, I have a little bit of advice. Remember building is very much like walking in that you have to remember to build before you run. Now, sometimes you have more than one project, more than one framework, maybe you have two, three, four, and in this case that Built Products Directory situation that I just described is actually multiplied. There are multiple Build Product Directories if you have separate projects. This can be a problem when you want to import your code into a playground. The simplest way to address this is to add each project to a workspace, a single workspace, and then when you build each project, those frameworks will end up in that one Built Products Directory for your workspace. After that just add the playground to that workspace and everything should be fine. This is what that should look like in the project navigator. Now, let's say you've done this, you followed my instructions and things don't work out how you expect. You say, "TJ lied to me." You're going to want to check the Built Products Directory to make sure that everything ends up there as you expect. Here's how you can do that. You're going to go to File, Project Settings, and then we're going to click the Advanced button. On this screen in light gray we'll see that there is a Products destination. You'll click on that and that should bring you directly to the Built Products Director for the project that you have open. This is what that should look like in Finder. Now let's see all of this brought together in a demo. Now those of you who know me know that I have a pretty enduring interest in music. I'm actually so interested in music that I have found this new notation, well new to me notation, called Helmholtz notation. And I feel that many, many people should know about this and it should be much more widely used. So I've created a framework to share with other developers so that they can use it in their code. You can see here that I've included a playground and I've also, as the result of extensive research on Wikipedia written up a little playground that describes the notation. Follow it, followed by a few values so that we can see the results on the right hand side with the notation. If I render this documentation this will all end up in Pros with the links highlighted in blue just as Tibet spoke of. And before I build, or before I run I'm going to build. Then let's run our playground, yay. You can see here that on the right-hand side in the results I have this interesting notation. I really think that this notation is interesting because the lower pitches are represented by uppercase letters. The higher pitches are represented by lowercase letters and the exact octave is indicated by either commas or apostrophes, the count thereof. Now, hopefully you all get this looking at this notation, and hopefully I've explained it very well, but I've actually had some trouble with some of the people I'm tutoring. They're not taking to it as well as I'd like. So I actually have a separate framework that I've created with a keyboard visualization that I think will really help them ease into this notation. Since I want to use that, I'm actually going to create a separate playground in a workspace where I can import both frameworks without changing either of the frameworks. So to do that I'm going to quickly close this, I'm going to create a brand new workspace by going to File, New Work Space, and I'm going to name this Tutoring. Just put that on the desktop. Then I'm going to add each framework by going to File, Add Files. I'm going to start with Helmholtz, my framework that I just had open, then File, right there, Keyboard. Then I'm going to add a new playground by going to File, New, Playground. A blank playground is fine. My Playground is a fine name, a fine as name as any. I'm going to start by importing Helmholtz, and My Keyboard. After that I'm just going to quickly do a little test to make sure that pitches show up as I expect. I'm going to build each of my frameworks. I'm going to make sure to choose each one from the schemes. Later on I might want to make a separate scheme that builds both of these projects in one go, but I don't have time on stage right now. And then I'm going to run My Playground in it. Everything works as expected. But again this visualization is not helping. I have some younger students who have only seen a piano, so I have a little snippet here that quickly creates a view in playground description, configures it, and actually I have one more line that I wanted to write on stage. Custom text equals description. And after I configure my view I'm going to return it. Simple as that. Made much simpler by the fact that I happen to have a framework already on hand. Now, I want to run this line, line 4 again, but with my conformant. So I'm going to reset this playground by clicking on the stop button down here. And then I'm going to rerun this entire playground after I add my inline result. And you can see here -- You can see here that I have a nice visualization of a keyboard that my students can recognize along with the notation that I'm trying to teach. Now let's cover or let's go back over what we've learned today. Tibet covered the fundamentals of playgrounds, the layout and playground markup syntax. Alex explained the power of running step-by-step in playgrounds. And I covered Custom Playground Display Convertible and importing your own code. So hopefully, now that I've given this session and all of us have -- and we've explained playgrounds and the power that you can get, we hope to see that every project has a playground next year. If you have any questions please feel free to visit us in the labs and we'll see you next year.  Good afternoon. My name is Pete Hare and I'm an engineering manager on the App Store team here at Apple. We're here to discuss the best ways to build your app and server infrastructure to support the subscriptions. I'm going to go over a few topics here today. Firstly, we'll discuss the best ways to actually build your app and server architecture. We've got some tips and tricks around how to enhance the in-app experience for your user. My colleague Michael will get up and talk a bit about reducing subscriber churn. And then finally, we've got some new announcements around analytics and reporting tools available to you. But first up let's talk a bit about how to build your app and server infrastructure. Let's start off by asking a simple question, what is a subscription? Well a subscription gives a user access to your content or service by them repeatedly paying you some amount of money. When you look at this at the engineering layer a subscription is really just a set of repeating transactions each of which unlock some subscription period. To use subscriptions in your app there's a couple of things that you need to do as a developer to be able to handle these transactions as they come in. Let's go through each of these steps. Firstly, it starts off with your app receiving a transaction. Once your app has received a transaction you need to go ahead and verify that it's an authentic transaction, that money really has changed hands. Then it's up to you to update and maintain that user's subscription state for ongoing access to your service. So let's go into each of these in a little more detail. Firstly, let's talk a bit about receiving that transaction in your app. Now whether it's the initial purchase of a subscription or a renewal transaction for a prescription your app is set up to handle subscriptions and transactions using the StoreKit framework. Now when you are set up to handle transactions using StoreKit the App Store will make these charges on a user's credit card in the background. And anytime a transaction occurs it informs your app of these transactions via a thing called the SKPaymentTransactionsObserver. This transaction observer object is really the central piece of in-app purchases when it comes to your application. It's actually just a protocol in StoreKit, it looks like this and you can set it on any object. In this example we're just setting it on the AppDelegate itself, but the really important thing is that you add a Transaction Observer to the default payment queue as early on in the application lifecycle as possible. Once you've got a Transaction Observer registered on the default payment queue you're ready to start receiving transactions as they occur in the background. You receive transactions on a callback in the Transaction Observed called updatedTransactions and it's here that StoreKit will inform your app of a set of transactions for you to process. They can come in various different states that we're not going to go completely into in this talk, but keep an eye out for a transaction in the purchased state. That's a transaction that StoreKit is telling your app is ready for verification and unlocking. Once you've got a transaction the purchase state you're ready to go with that next step, which is to verify that it is an authentic transaction. So when it comes to checking for authenticity how can you know that money really has changed hands? You use a thing called the App Store receipt. The App Store receipt is just like a receipt you'd get in a department store, it's a proof of purchase that a user has bought something that they say they've bought. In this case it's a trusted record of the initial app download and any in-app purchases that have occurred for this app. This is a digital document, it's stored on the user's device, we provide an API for you to access it and it's put there by the App Store. We also sign this document using certificates so that you can check to make sure that it is an authentic document that's actually issued by Apple. And finally, the document is for your app on that device only. So if you've worked with subscriptions before you'll notice that maybe one user with multiple devices has receipts that look slightly different on each device. When it comes to actually verifying this transaction that your app's been told about the first step that you need to do is to actually verify that the document in question, the App Store receipt, is authentic. So how do you do that? You can do this in two ways. Firstly, you can use on-device validation and this is where directly on the user's device you can use a series of checks to check the certificates used to sign this app and verify that it's authentic. Or you can use the technique called server-to-server validation. This second technique is where you take that binary encoded receipt data, send it up to your own server, and then from your server over to the App Store for processing. And the App Store will actually do those checks for you. You can use either of these techniques, but whichever one you choose it's important not to use online validation directly from the user's device, this is not a secure way of actually validating this document is authentic. But let's compare each of these two approaches in a little more detail, especially around subscription management and order renewable subscriptions. Both of these techniques give you a way to validate that this is an authentic document. They also give you access to the contents of the receipt, any transactions that have occurred for this particular user. But when it comes to auto renewable subscriptions there's a few key advantages that server-to-server receipt validation actually gives you over on-device receipt validation. Firstly, we include some additional subscription information in the response to that validation check. You can use this, Michael will talk about a little later in this talk. Your server is always on to handle those renewal transactions in the background. This is really important if you've got a service with maybe multiple platforms. Your server is not susceptible to a device clock change. If you're using on-device receipt validation for subscription management on the user's device there's actually nothing stopping the user from winding their clock back and putting themselves into a valid subscription period, maybe a free trial that they've actually lapsed their subscription from. And finally, it's just a little simpler. The server-to-server validation you're dealing with JSON API, you don't have to build OpenSSL or use ASN.1 decoding. So with all these things in mind we're really encouraging more and more people to actually adopt server-to-server validation when it comes to maintaining an auto renewable subscription state. If you've got a simple utility app that doesn't need any kind of networking you can still use on-device validation for subscription management. And if you're interested in finding more about that I'd invite you to check out the video from last year's events StoreKit Talk where we went into more detail about on-device receipt validation. But for the purposes of this talk we're really going to focus more on the server to server techniques outlined here. So let's go back to our example and see how we can use server-to-server validation for this transaction that we're processing. Back here in our Transaction Observer you can access that binary receipt data using the App Store receipt URL API on the main bundle. Once you have this URL you can pull out the binary data located at that spot on the file system and you can pull out that receiptData and base64Encode and this will give you a nice string that you can send up to your own server for processing. You might provide some in-app networking API for the current user. When you're sending that data up to your server for processing firstly you'll do this securely obviously. You can send it up to maybe a process transaction endpoint on your server. In this endpoint you might have a user ID associated with the current user for an account on your own system. You can send this receipt data up to your server and then once you get that on your server you can establish a secure connection to the App Store's verify receipt endpoint. And you can send over that receipt data to the App Store. Here you can include a password field, this is just a shared secret between your app and the App Store. You can set this up in App Store Connect and store it on your server. And when you send this receipt data to the verify receipt endpoint the verify receipt endpoint will respond with JSON payload that looks a little like this. The first thing to check when you're verifying that this transaction is authentic is this status field. This is an indication that Apple has actually issued this document in the first place. Once you've verified that the status is zero like this you can check the contents of the receipt portion of this payload. This is the decoded version of that binary data that you just sent to verify receipt endpoint. So in here you can do things like verify that the bundle ID in this receipt matches your app's bundle ID. And then you can inspect the in-app array, this contains a list of transactions for this particular user for your app. And you can verify that the product ID associated with this receipt is the one associated with your app. So assuming that these all match you're determining that this receipt entitles this particular user to your subscription product you're ready to go ahead now with that third step, updating the user's subscription state. Now in the same way that each subscription period starts with a transaction it also ends with an expiry date. And the response from verify receipt tells us each of these expiry dates for each transaction. So looking back at this response from verify receipt you'll notice that there's this expires date field in the transaction and the response. Let's bring up the user table now that you might be saving this data on your server. You can take this expires date from this transaction and actually populate this into a field on your own server, maybe the latest expires date field for this particular user. And this field is going to become the new source of truth on your server for whether or not this user is a subscriber. While you're here you should also keep track of this other field, original transaction ID, and you can save that in the field called original transaction ID against this current user as well. Well come back to this one in just a moment as to why that's important. But once you've saved these two bits of information against this current user on your server you're ready to go ahead with the last step, which is to tell the device that this transaction has actually passed your verification checks. And then when your device gets this callback it can call Finish Transaction back down in your Transaction Observer. This is a really important step because finishing the transactional will actually clear it out of your payment queue. And if you don't call Finish Transaction it might reappear there the next time the app launches for processing. So make sure you finish every transaction that begins in StoreKit. Once you've finished the transaction you've got an updated subscription state in your server, the user is now free to enjoy that subscription period on your service. Now let's take another look at that user table I mentioned that you're storing on your server. Each user who purchases a subscription using this setup will be assigned a unique original transaction ID, that's that field that you saved from the transaction response. And this identifier essentially becomes that user's subscription ID. And it's important because it shows up in all subsequent renewal transactions. Let's take a look at how this works. So let's say that you're verifying a renewal transaction, this happens in just the same way you might use the same process transaction endpoint on your own server. When you do those techniques of verifying the transactions a valid one, and you get up to that stage of updating the user's subscription state you'll observe here that there's now multiple transactions since this is a renewal transaction. Now according to your existing server-side logic this latest expires date is now a date in the past, so the user is not currently a subscriber and you need to figure out are they still a subscriber based on the data in this receipt. So how can you use this receipt data and make that deduction? Well for discovering whether or not the user has an active subscription you can pull out the transactions associated with that original transaction ID. And then you can find the transaction that has the latest expires date. Now if you find a date in the past that's an indication that this user is no longer a subscriber anymore. But if you find a date in the future that's an indication that this user is in a valid subscription period. So let's look at how this works in the example that we're going through. Grab that original transaction ID associated with this user and pull out all the transactions associated with this subscription. Then sort those transactions by that expires date field and grab the one that has the latest expires date. Now you can take that expires date and update that latest expires date field associated with this user. And when you do that you're effectively extending that user's subscription by another length of time and your server-side logic now knows that the user is in a valid subscription window. Of course when you're dealing with renewal transactions like this that have come through StoreKit you still need to inform the device that it passed those validation checks. And have your device, your app call Finish Transaction back down in StoreKit again. So let's say you have this set up and working correctly. The App Store is charging the user's credit card in the background and you're using StoreKit to process these transactions as they come in through the app. And then your server is updating and maintaining this latest expires date field on your server. So you've got that server-side source of truth as to whether or not a user is subscribed. Let's now introduce a slightly more complex example though to the mix, which is maybe that you offer your service through a website as well. Now when the user accesses your subscription service through a website you know based on that latest expires date field that the user is a subscriber. But as much as we'd like to think that people use our apps all the time let's say that the user doesn't use your app for a number of days. And then during this time the App Store actually successfully renews a user's subscription in the background. When the user tries to access your servers through your website that latest expires date is now out of date because your server hasn't learned about that new transaction. So how can your server know about this transaction that's occurred on the App Store? You use a technique for this called status polling, this allows you to discover these transactions directly from your server. In order to get set up to be able to status poll from your server you just save a latest version of that encoded receipt data that you send up associated with each user. And what you can do is you can treat that encoded data just like a token. The reason why it can treat it like a token is every time you send up that encoded receipt data to the verify receipt endpoint the verify receipt endpoint doesn't just respond with a decoded version of that receipt data, it also includes any new transactions that have occurred for this particular user's subscription. It's located in a field called the latest receipt info field in that JSON response. And you can use that info to unlock those new subscription periods for this particular user without the app having to launch at all. Let's see how this works. So when you're verifying transactions just like we saw before you're sending up that receipt data. Now once you've determined that this transaction in question has passed those same checks that you did before you can take that receipt data and store that in a field called latest receipt data against the current user. And now that you have that latest receipt data stored, that's a base64Encode string with the user. When it comes time to answer that question, does my user have an active subscription you can just take that latest receipt data from your server directly and send it to the verify receipt endpoint. You can also include here an optional flag that's the exclude old transactions flag, this is just telling verify receipt that you don't even want to know about that decoded version of the receipt you just want to find out about any new transactions. Verify receipt will respond with that particular object, the latest receipt info object. And inside this object is those new transactions that have occurred before this receipt data was actually generated. And you can take the expires date directly from the response of the latest receipt info object and update it against the current user, again extending them access to that next subscription window. And so the user who is on your website trying to access your content can now access that next subscription period all without the app having to launch with that new transaction. If you are using this technique though status polling it's important to remember one thing and that's that when your app does come back online again transactions will still appear through StoreKit in the updated transactions callback. And you still should handle these transactions passing them up to your server for verification and finishing them back down on the user's device again, even if your server already knows about them through a status poll. We'd encourage you to use that as an opportunity just to send up that new latest receipt data for storage against the current user on the server. Now status polling works great when the user's credit card is able to be charged, but what if some subscription period the user's credit card has some kind of billing issue and the App Store isn't able to charge it for the next subscription period. Is this user destined to remain unsubscribed involuntarily? Not at all. When reacting to billing issues like this you can do three simple steps. Firstly, you can observe that there's been no renewal transaction for this particular user, their subscription has now lapsed. Secondly, you can direct that particular user to go and update their billing info. And then the third step, when a renewal transaction occurs unblock that user immediately after it happens. Now, steps one and two are pretty straightforward using the status polling techniques that we just went through, but step number three here uses a feature that we actually launched last year server-to-server notifications. Let's walk through this example, let's say that one subscription period the App Store has an error when trying to charge this user's credit card. And then you find out about the fact that there is no new renewal transaction for this user through a status poll. Your server correctly makes the calculation that this user is not a subscriber anymore and so when the user comes along to access your service through the website you give them some appropriate error message about the fact that their subscription wasn't able to be renewed. And you can direct that user to go and update their billing info in the App Store. Now when the user does update their billing info, maybe they just need to update an expiry date or something, two things happen. Firstly, the App Store will actually immediately charge that user's credit card and make a successful transaction. And when the App Store does do this the second part of what it does is it sends your server a direct HTTP post upon the successful renewal. And then the payload of this post includes the new transaction info for the transaction that's just occurred. You can use that original transaction ID field located in the payload to locate which user this notification is for. Once you find out which user you're talking about, well you can grab that latest expires date and update it for this new user giving them access again to that next subscription period. And then the user who's probably still sitting on your website trying to get access to your content will be able to immediately be unlocked since your server received that push straight from the App Store. It's really important to unlock users in a speedy manner when this sort of thing happens, especially when they go to all that effort for updating their credit card info manually and waiting for access to the servers. But there's one thing to note here and that's the notification is only sent if the subscription actually lapsed like we just saw. To discover successful renewal transactions you still need to rely on status polling techniques we just outlined before. But it's really easy to set up to use server-to-server notifications. All you do is enter a URL in App Store Connect. This is just an endpoint on your own server and if you enter it into App Store Connect the App Store will begin to send your server HTTPS posts for these status change events. And as we saw included in the post is that latest transaction info for the transaction that triggered it. You do make sure your server is ATS in order to receive these, but it's a really simple step that you can do to give a bunch of users a much better experience. So those are some tips and tricks around how to build your app and server architecture. Let's talk about three tips that you can use in your in-app experience to really enhance the user's experience. Firstly, we made the assumption earlier that the user was signed in to an account on your own service. In order to keep track of each subscription ID you need this particular user table located on your server. Now when it comes to actually creating accounts we think it's best to offer in-app purchase before an account creation step. Why is that? Well as the user it's a much better experience, you can just open the app for the first time and buy the subscription immediately getting access to the content you're after. For you it's better because you get higher conversion rates, users don't have to go through funnels of entering emails and passwords to be able to get access to giving you money. Now you can use the techniques that we just went through by using an anonymous account in these instances. And you can rely on the original transaction ID if you need to associate multiple devices. If you're using anonymous accounts like this, when it comes time down the road to actually give the user an account creation flow, you can just simply take them through a deanonymization step where you update those email fields and maybe other personally identifiable fields. So that's tip number one. Tip number two is around selling your in-app purchase. When it comes to selling your subscription, you can use a feature that we launched last year Introductory Pricing. Now there's an important step with Introductory Pricing which is that you need to know at runtime whether or not a user is actually eligible for an introductory price. And the reason you need to know this is you have to know which price to render to your user, whether to render the normal StoreKit price or the introductory offer that you want to offer a user to get them in the door. Now you can set yourself up to actually know this ahead of time by monitoring the transactions that are occurring in the background as they come in. Let's see how this works. When you're verifying transactions like we just saw earlier keep an eye out for these two fields. The is trial period field and the is in intro offer period field. If either of these fields are true that's an indication that an introductory offer or a free trial was used for this particular transaction. And if it was you should keep track of the product ID in question against this current user. You might store them in a field called consumedProductDiscounts. Now if you're keeping track of which products were used with introductory offers, when it comes time to render the price of some new subscription product that you want to show your user this is how you can do it. You can take those consumed product discounts saved against the current user and execute an SKProductsRequest with them. And the reason why is that the response from SKProductRequest now in iOS 12 includes the subscription group identifier so you know which subscription group this particular product is from. And now armed with this subscription group identifier you can keep track of that in a set of consumed group discounts for this particular user. So you know which subscription groups this user has used offers for. Now when you want to render the price string of say product A it becomes a simpler check. You can check to see if this user's list of consumed group discounts contains the group identifier for the one you want to sell them, in this case product A. And if it does, that's an indication that this user has actually used an introductory offer here before, so you can render the normal price string to this particular user. But if not, they're still eligible for that introductory offer so you can use that introductory price located on the SKProductObject. Now when rendering the price string nothing's really changed here it's the same techniques that you use for rendering any in-app purchase and I'm not going to go into too much more detail here, but I'd encourage you to check out the video of the session just before this one where they talked about rendering these price strings a little more dynamically. For more information about setting up introductory offers I'd also recommend you go to the What's New in App Store Connect session on Wednesday at 5 p.m. in hall three. So that's tip number two around introductory pricing. The third tip here is around subscription management. You can offer the user upgrades and downgrades between subscription tiers right in your app's UI. And to do this you can actually treat it just like selling an initial subscription. Now if the subscription you're selling the user is part of the same subscription group, so it's a different tier than the one the user has already subscribed to you can basically just create an SKPayment just like you would if you were selling them an initial subscription. And when you do this StoreKit actually handles the fact that it's an upgrade or downgrade for you. So you don't have to worry about that user being subscribed twice. Now if you don't want to provide your own UI for upgrades and downgrades in your app you can also just provide a link out to the App Store subscription management screen. We provide a link for you to be able to get to this screen directly from your app and here the user can upgrade, downgrade or even cancel their subscription. Now your app is often the first place a user will go for subscription management, to be able to upgrade, downgrade or cancel. So it's a really good idea to give some kind of link maybe in your app's settings for a user to be able to do this. To actually get to this screen there's a link available on our In-App Purchase Programming Guide and here it is if you enjoy writing down links. So these are some simple techniques you can implement in your app to give a user a pleasant experience using subscriptions. Next, I'm going to hand it over to my colleague Michael who's going to go over some great techniques for reducing your subscriber churn. Thanks. Good afternoon, my name is Michael Gargas and I'm a technical advocate on the App Store operations team. Today I want to talk about reducing subscriber churn inside your applications by using some of the tactics and methods that Pete just walked you through. Today we'll cover involuntary churn and voluntary churn, the two types of churn you'll see inside of your subscription applications, as well as some ways to win back those subscribers that you potentially may have lost or will lose. First, let's talk about involuntary churn. Involuntary churn is the loss of a subscriber due to a failed payment or billing issue on the platform. Now last year at Dub Dub DC we walked you through what we're doing in order to minimize involuntary churn inside of your applications. We announced our updated Billing Retry service where we expanded our retry duration from 24 hours to up to 60 days. We also implemented new retry strategies and we tuned them over time to recover more and more of your subscriptions. A date to remember is July 13th, 2017 because this is the day that Apple actively started recovering subscriptions for you. If we look at the performance of Billing Retry since launch we can see that our recovery rate has more than doubled. And when we look at involuntary churn we've cut this by over 2% platform-wide. Now if we look at how our tuning has impacted the recovery of subscriptions we can see that quarter over quarter we've been able to continue to recover more and more of your subscriptions. Now the net result of this has been 12 million of your subscriptions recovered since the launch of Billing Retry. So that's what Apple's doing to minimize involuntary churn for you. But there's also some tactics that you as a developer can do to minimize voluntary churn inside of your subscription applications. You can leverage some of the subscription specific receipt fields that Pete mentioned earlier. You can implement grace periods and during that time you can deploy some effective customer messaging. So let's take a look at an example subscription. Here we can see that our subscriber was set to renew on April 26th, however, they encountered a billing issue. So in order to let you know that Apple is actively attempting to collect funds from that user via the Billing Retry service we are going to surface a field in the JSON response aptly titled is in billing retry period. A value of one signifying that we're attempting to collect funds for that subscriber. If we go back to our example of subscription you can see that this has been added to JSON response. And when you see this in conjunction with the expires date this is your signal as a developer to implement what we call a grace period. You may ask yourself what is a grace period. A grace period is free subscription access while in a billing retry state, however it's before you have lost that subscriber, before they've churned out. The goal of the grace period is to improve recovery. So let's take a look at how we can do that leveraging some of the information in the receipt response. If we flip back to our example subscription you can see that our subscriber is in a billing retry state and was set to renew on April 26th. Here we want to add some server-side logic to use that expires date field and the is in billing retry period in order to add a period of time, in this example three days where that user will continue to have access to the service and technically stay subscribed. Now why would you do this? Well it's an opportune time to deploy some effective customer messaging to contextually communicate with your subscribers and let them know that there may be an issue with their subscription. You may want to do things like ask them to update their payment method or have them restate the value proposition of the subscription offering that they're subscribed to. And during this time you can offer limited service as well, such as a browse but not watch experience in an entertainment app. Here we can see Peak, a subscription developer on the App Store. Peak is leveraging the Billing Retry status fields in order to surface a contextual message to their subscribers letting them know that there's been an issue with their subscription. When engaged upon they're taken to an additional screen which clearly states what the issue is and how they can resolve it. But it would be really effective if from this screen you could drive that user or subscriber directly to our systems to update their payment details. So I'm excited today to announce that we have two new URLs coming shortly after Dub Dub DC this year, one to drive users directly to update their billing information and the others to take them to manage their subscriptions as Pete alluded to earlier for upgrades, downgrades and crossgrades. With that being said, a lot of developers will ask well when are we seeing our users be recovered. And on average we see the majority of users recovered within the first seven days of entering a Billing Retry state on the platform. This might be a time to offer a full access grace period because we see a lot of users self-recovering during this time. You may want to deploy that customer messaging towards the tail end of this to bring in some of those subscribers that might've taken longer to come back into your application. Let's flip back to our example subscription. What happens if this is effective and we're able to recover these customers? When a retry attempt is successful the date of the retry or recovery becomes the new subscription anniversary date moving forward and this will be reflected in JSON response when validating that successful transaction and finishing it. But we're not going to stop there we're also going to deploy our server-to-server notifications so that you can immediately unlock access on all platforms and close the loop with your customer letting them know hey you're all set. So that's involuntary churn where the customer didn't technically make a choice to unsubscribe from your application. But what is voluntary churn? Voluntary churn is the loss of a subscriber due to customer choice, either cancellation or refund requests. To be clear, this user actively made the choice to leave your subscription offering. So what can you do as a developer to minimize voluntary churn inside of your applications? Well Pete walked you through how to status poll earlier and you can implement that to get some key subscription details about your users and when you get that information you can use it to offer attractive alternative subscription offerings to potentially save that user. So let's talk a little bit more about status polling. With the release of server-to-server notifications there's really only two key reasons that you still need to status poll. The first is understanding will my subscriber churn in the subsequent subscription period and the second being has my subscriber renewed. We often get asked when should I status poll as a developer, when should I try to catch those users and see their subscription state changes. The most effective times that we see are doing that status poll at the beginning or end of a subscription period. By deploying this responsibly you're most likely going to catch most users that may want to voluntarily churn from your subscription offering. But when you status poll you'll also get access to some additional subscriber status fields. And you may want to take those fields and save the decoded JSON response from the verify receipt call in the user tables on your databases. Or alternatively, you can parse out specific fields such as the Billing Retry Status in order to segment your customers and maybe understand those that are in a retry state and those that are not. But the signal to understand if a customer will voluntarily churn is shown via a field called auto renew status. Auto renew status will let you know with a value of one that that subscriber will return in the subsequent subscription period and a value of zero letting you know that they will voluntarily churn at their next subscription anniversary date. Let's see what this would look like in our example subscription. Here we have a subscriber purchased on March 26th and they've made the choice to disable auto renew via the manage subscription setting screen. Now coincidentally, we status polled shortly after this happened and we were able to see via the receipt response that the auto renew status has changed to zero. It's at this point that you can update your user tables on your database or on your server and segment that customer as a potential voluntary churning customer. If we go back to the example subscription we've status polled, we've captured this customer might leave, so what should we do? As a developer this is your opportunity to present an attractive downgrade offer potentially in the same subscription group. Here we can see Peak trying to save that user by potentially having them take a lesser term or a lesser price subscription of a different duration or maybe a different offering. If that subscriber decides to engage with this the same way that Pete showed you by surfacing upgrades and downgrades within your applications we want to let you know what product they will renew on in the subsequent period. We do this via the auto renew product ID field in the JSON response. So this differs from product ID in that this is what the next offering will be after that subscriber renews. Here we can see in the example subscription our subscriber has elected to downgrade instead of churn out. We've changed the auto renew status to one and we've added auto renew product ID. It would also be beneficial to be notified of this change immediately, so for this we'll also send a server-to-server notification letting you know that your subscriber did change their renewal preference. This is useful if you want to maybe state the differing service levels between what they're currently subscribed to and what they will have in the subsequent period. Now it's impossible to run a 100% retention subscription business, so it's important to understand how you could possibly win back some of those subscribers that you might have lost. A win back is reengaging subscribers after they've churned by showing them resubscription offers or potentially surveying them to understand why they've left. If we look at our example subscription let's see what a voluntary cancellation would look like and how we can leverage that inside of our app. Here our user has elected to cancel via AppleCare. In order to let you know we're going to surface a cancellation date field in the JSON response. This is your signal to understand this customer has contacted AppleCare and either canceled or requested a refund. But as a developer you would want to know this information immediately. For this we'll deploy a server-to-server notification. This is important because you'll immediately want to shut off access to those users on all platforms and potentially prompting them to see alternative subscription offers. Now after this user has churned it would be important to easily segment between those that have voluntarily chosen to unsubscribe and those that have involuntarily been unsubscribed due to a payment issue or billing issue. So for that we have the field expiration intent in the JSON response. Now to be clear, this will only show up after the subscription has lapsed. And we really want to focus on two key values, the first being value one which signifies voluntary churn, the second signifying involuntary churn with a value of two. If we flip back to our example subscription where our customer canceled via AppleCare you can see that we've added the expiration intent field to the receipt response with a value of one. So what do you do as a developer when you see your subscribers in this state after they've already churned and you've segmented between those that have voluntarily and involuntarily churned? Well for voluntary you may want to survey those subscribers who have opted into an account on your system. You can ask them maybe why the service wasn't appropriate for them and what you can do to tailor it moving forward to provide a better experience for them or other users. Additionally, you can always just show alternative subscription products potentially within the same group because if they re-subscribe you want to continue accruing your time towards that 85/15 revenue share. Here we can see Peak when a user has voluntarily churned being shown a resubscription offer, in this case a 60% discount. For involuntary churn since the user did not actively make the choice to unsubscribe it's appropriate to just show the same or alternative subscription products. You may want to leave some persistent messaging if that user is logged in inside the application experience letting them know that they have lapsed, but that they can always come back. And you may also want to deploy a limited subscription experience, such as a browse but not watch experience in an entertainment app. Here we can see Tinder, when users interact with pro level or subscription level features they are continually prompted to subscribe. So in summary, if you take anything away from this section on reducing subscriber churn it's that you should be leveraging these subscription receipt fields effectively. You can then implement status polling to understand when your subscribers may voluntarily churn. You can then use that status polling to deploy some targeted and effective customer messaging. And then to close it out, presenting contextual subscription offers to these users to hopefully win them back or prevent them from churning in the first place. So with that I'd like to hand it back to my colleague Pete to discuss analytics and reporting. Thank you. Thanks Michael. If you haven't implemented handling of these JSON fields yet we highly recommend you try it and watch the great effect it has on your retention rates. It's not often as engineers we get the chance to make such simple architectural tweaks that can have such a big effect on a business' revenue, so take a look. Now we've got some great new updates today in the areas of analytics and reporting. Located in App Store Connect the sales and trends section contains a huge amount of useful information. Now you can get even more insight into your app's performance. This existing subscription summary dashboard now includes monitoring for subscriptions that are in those Billing Retry windows. This is great for gaining insight into your subscriber behavior and to determine the most effective amount of time to offer those grace periods for like Michael just talked about. This year we're also introducing a brand-new dashboard for subscription retention. This page offers a glance at how your introductory prices are going, as well as how many of the users are subscribed to the higher proceeds rate, that's that 85 to 15% split you get when a user has been subscribed for a year. The dashboard includes new graphs to help you quickly identify which subscription cohorts are the highest performing and you can monitor your subscription performance over time and compare your app's performance across different territories. Now all this new information is not just available inside the App Store Connect report, but it's now available through the new App Store Connect API. The report data here is available to you daily and you can script your own setups to import this maybe into your own data warehouses for further analysis. We're not going to go into any more information on the App Store Connect API here, but I really recommend you check out the automating App Store Connect session in the hall three on Thursday at 3 p.m., there's some really exciting enhancements in the area of automation. Now we've talked a bit about what you can get from receipts and what you can get from these App Store Connect reports. So as a bit of a summary, App Store receipts are useful for validating those StoreKit transactions and updating user subscription states, maintaining the state on your server. And you can also use them to understand individual subscriber behavior just like Michael showed you. For App Store Connect reports it's for a slightly different reason, they're better at that macrolevel analysis, maybe understanding subscription pathways of users of your app and maybe most importantly, understanding how much money is going to get deposited into your bank account for your subscriptions. Now we've covered a lot of topics here today, but as a bit of a summary remember that server-side state management offers you much more flexibility when it comes to managing subscriptions. If you haven't done it yet add that URL to receive notifications from the App Store. Consider offering an introductory price in your app, it's a great way to get users in the door to your own subscriptions. Add some simple messaging to reduce subscriber churn, using those fields that Michael walked us through. And for users that have actually lapsed offer some alternative subscription options maybe to win them back. Finally, remember to check out these new reporting tools available in App Store Connect. For more information on this session and for the video, this has been session 705. We're also going to be in the labs this week right after this and also on Thursday at 9 a.m. we'll have engineers from the StoreKit team and for App Store Connect ready to answer any questions that you might have about engineering subscriptions. Thanks a lot.  Ladies and gentlemen, please welcome Vice President of Software, Sebastian Marineau-Mes. Good afternoon, everyone. Welcome to the afternoon session of WWDC 2018. Now, we had a really, really great session this morning. I think you all enjoyed the keynote? Lots of great things were presented. And I think you saw that 2018 is a year with a strong focus on the fundamentals across our entire ecosystem where we pushed the boundaries in key technology areas. We're introducing numerous APIs and capabilities that enable new experiences covering a broad spectrum that ranges from machine learning, augmented reality, high performance graphics and of course, new development tools. Now, many of the improvements in the APIs apply to all of our operating systems so they all move forward together. And iCloud provides the fabric that enables a unified and consistent experience across all of our devices. In iOS 12, we've seen a huge number of incredible new features including these great new capabilities in AR, the camera effects in Messages, multi-way FaceTime, usage data with Screen Time, richer photos and of course, a great focus on performance. And with macOS, we're really excited to introduce Dark Mode, New Finder on desktop features, new apps like news and stocks, a redesigned Mac App Store, and a strong focus on security and privacy, watchOS 5 brings customizable interactive notifications, support for your app content and shortcuts on the Siri Watch face, background audio mode and improved workout API. And in tvOS, we're adding Dolby Atmos support so that video apps can deliver immersive audio experiences. We heard that this morning, really amazing. Secure password sharing from iOS devices so it makes it really easy to slide into through Apple TV apps, VPP support and enhancements to UIKit and TV ML Kit to make it even easier for you to build native apps that look and feel great. Now our great products are platforms and all of your apps truly impact the world. And when you think of the breadth and the scale of our ecosystem, it really makes us an essential part of our users' life. Be it helping them explore their creativity, connecting with the people they care most about or transforming the way that healthcare is delivered, we together focus on what's most important to our users and we deliver these great experiences. Now we think technology is most powerful when it empowers everyone. And so we work to make every Apple product accessible from the very start. We provide great capabilities that make our platforms and all of your apps accessible and we want to encourage you to take, keep taking advantage of these because it's really important to those users. Now, our users also entrust us with their most precious data. And so at Apple, we think deeply about privacy and security. And I'd like to invite Katie up on stage to tell you more about this. Katie? Thanks, Sebastian. When we think about privacy, we think about how to build privacy into all their products and services. And there could be a lot of details to think about. But it's important to think of the big picture, trust. Now it's up to all of us to ensure that users can protect, can trust us to protect their most sensitive data. From financial data to communications to location and photos, trust is crucial as technology becomes more and more integrated into our lives. So how can you build trust with your users? We focus on four key pillars and let me show you an example of each. Now, we don't require users to sign into Maps but instead we use rotating random identifiers that can't be tied to an Apple ID to enable relevant results. We use on-device intelligence to enable powerful features like search and memories in photos without analyzing photos in the cloud. We designed Face ID so all Face ID data is encrypted, protected by the Secure Enclave and doesn't ever leave your device. And when we collect users' data or allow a third party to collect data like photos, we make sure we do so with the user's consent. So let's dive a little bit deeper into transparency and control. You have all seen these alerts when you request access to location or photos. And this alert includes a purpose string. Now this is what you provide in order to explain why you're requesting data and how you will use that data. Now a good string includes a clear explanation of what features it will enable and what functionality it will improve. Now, the more specific you are with your users, the more likely they are to grant you access. We think it's critically important to understand and for users to understand how their data will be used. So app review is paying closer attention to these purpose strings. So if you have a purpose string like this, which you know, it's clearly not valid, you may get dinged by app review. Now this string technically explains how data will be used. But it lacks detail so it's really hard for your user to make a decision. Now some users may have concerns about granting your app microphone access but it may be key to your app's functionality. So that's why it's important to have a clear purpose string like this one that explains exactly how you are going to use the data. Now great features don't have to be at the expense of privacy. But instead can support them by making it clear to users how you're going to protect their data and how it's being used. Now, we care deeply about security. And in order to protect all the sensitive data that resides on the device in apps and in the cloud, we think about security holistically. And we provide technologies to make it easy for you to build secure apps. Here's a few examples of the technologies that we provide. On iOS, we automatically encrypt app data by default. Over the network, App Transport Security means you never have to patch client networking libraries again. Now in CloudKit in the cloud, CloudKit securely stores and syncs data across devices. Letting you focus on building a great experience for your users without having to worry about managing account state or your cloud credentials. And it enables you to take the best, advantage the best in-class security including built-in two-factor authentication. Since its launch three years ago, more than two-thirds of Apple ID accounts have adopted two-factor authentication. This is a huge success compared to the rest of the industry where we see less than 10% of accounts protected by two-factor authentication. But this is very important to us. And we work continuously to make our users' accounts more secure so you're the only person who can access your account even if someone else knows your password. And new in iOS 12, we're making using passwords more convenient and more secure for you and your users. We all know that a secure password is critically important to keeping your information and your identity secure. But they can be hard to remember and it's tempting to use weak or reuse passwords. And this creates problems for you as a developer as well. Now, users may abandon account sign up and you have to deal with password reset requests. But worst of all, is the potential for compromised accounts due to weak passwords. So we have a solution -- iOS 12 makes it easy for you and your users to always use a strong unique password by creating, storing and AutoFilling the password. But the really great thing is it will also work automatically in your iOS app too so they always get a strong password no matter where they create an account and it syncs to all of their devices. Now it couldn't be easier to offer automatic strong passwords. In fact, you may not need to make any changes within your app. So to ensure it just works, you need to associate your app with the domain. You may have already done this if you have adopted universal links. Then you need to label your user name and password fields. And if the passwords don't meet your app requirements, now you can even customize them. We've also made it easier for your users to get to their passwords. They can just ask Siri and once they've authenticated, they're taken right to their password list. And on top of that, to help clear up old password since, we're making it really easy to tell if any of your passwords have been reused across your existing accounts. Your iPhone would flag these passwords and take you right to the website where you'll be able to replace it with a strong password. We're also make it easier to deal with those one-time passcodes that are texted to you and your users much more convenient. They'll automatically appear right in the click tap bar and you can fill them in with just a tap. We're also creating a new extension point for third-party password managers to enable them to supply passwords for AutoFill and apps in Safari. Now these features work across iOS, the Mac and even Apple TV for a great experience across your Apple devices. We care deeply about privacy and security. And they're foundational to all of our products. So we provide the ability for you to build on this foundation to protect, secure and earn your users' trust. And now, handing it back to Sebastian. Thank you, Katie. Isn't -- aren't these new password features amazing? Really, really great. That was great. Thank you. Now, ultimately, we also promise our users great experiences. And we usually think about great experiences as being great innovative features. But equally important is not to compromise that delight with unpredictable and slow software. This is top of mind for the Apple Engineering Team. We develop tools and practices that help us with this. And then we work to bring these same tools to all of you so that you can apply them through applications. Available to you are a number of tools and techniques to help you make your code more reliable and robust. It's important for your app to be predictable. And of course, making your app run fast is critical. And for that, we have a number of performance tools at your disposal. Now we understand that optimizing performance across complex systems and applications is challenging. And this year, we worked a lot on this. We've developed a lot of new tools and techniques and want to bring you some of these powerful new capabilities. So in Xcode 10, we've extended instruments capabilities and enabled you to take it even further with your own custom tools and work flows. Now this all starts from a legacy API. Some of you may know this and have used it. I know I'm guilty of it -- printf, it's like the Swiss Army knife of APIs. We use it to debug and trace through our code but we all know that it's slow. And so two years ago, we brought you this new API called os log. It's an efficient and performant API that captures logs and tracepoints across all levels of the system. It's fast and lightweight and if you've not adopted it already, you really should. It's great. And our newest addition this year builds on top of os log and it's called os signpost. It's a powerful technique that provides rich, contextual data for your application in a format that instruments can interpret. So you could use signpost to trace through your code and you can also use it to bookend critical sections of your functions. And once you have the data, the real power comes in the built-in custom instruments visualization. Now, we have this new Custom Instruments support and the best way to convey the full power of this, I think, is through demo so Ken will show us what our tools can do. Ken? Thank you, Sebastian. So I'm working on my Solar System Exploration app here. And I've noticed I've got a little bit of a performance problem. So every time the app goes to update its data, you know, when it launches or when I press command R like that, you can see the UI, it gets really choppy. The planets, they kind of stutter as they move around their orbits. And then once the update completes, well, it's pretty smooth. So I want to figure out what's going on here. Now back over in my code, PlanetUpdateService.swift -- this is the file that handles that planetary update. So I want to add some logs, some signposts to help me understand what's going on in my code. So I'm going to start by adding a log handle. So I'm going to use the new pointsOfInterest category. Now this is a special new category. Anything that I log with it is automatically going to show up right inside instruments. Now, the first thing I want to see is when we kick off this update. And that happens in this method. So I'm going to add my first log statement here. I'm going to say requesting planet data so that we could see that. And then what I really want to know is how long is it taking to process and parse all the data that I'm doing here? So right here is where that happens. And to help me visualize this, I'm going to add a couple of signposts. So the first signpost is going to be a begin-type signpost here, just before I start doing the work. Then I'm going to add another signpost right here after I finish doing the work. That's an end-type signpost. So this is going to create a time interval for me, automatically calculate the delta and surface that right up through instruments. So let's profile this in Instruments and see what kind of data we get. So I go to Product, select Profile. Xcode's going to build my app, launch Instruments and then we'll start to see, well, we'll start seeing data stream in here. Now right here, you can see the pointsOfInterest track. So everything that I was logging with the pointsOfInterest category, that shows up here so this is my data. I want to zoom in. So I'm going to hold on Option and click and drag so we can get a closer look. And we can see this little flag right here that says requesting planet data. So that's a result of the first log I added in my code. Then these blue bars right here, this is where I'm processing and parsing data. So those are the results of the signpost I added. Now, as I look at this, I think I see what the problem might be right away. So every time I go to process data and parse it here, I can see a corresponding spike in the CPU use on the main thread. And to me, that is a bright red flag that I'm probably parsing and processing this on the main thread. Not a recipe for a smooth UI. So with just a log statement, a couple of signposts, you could see I can start to get some really great insight into the performance of my app. Now the new tools, they let you do way more than that. So with Xcode 10, there's a new template that lets you create a fully customized Instruments package. Now, one of my team mates, he's gone ahead and built one based on some network, some signposts that he added to our networking framework. And I've got the latest version he sent me here in my downloads. So let me open that up and when I do, Instruments offers to install it for me. So I'll say install. And now you'll see, I've got a new template. Here are my templates. These are called Solar Systems. I'm going to double click that. And then we'll start recording data again. Now, just like before, I have the pointsOfInterest tracked so that is on the data that I wanted to see. But now, I've got much more detailed information about the networking request that I'm making here. So again, let me zoom in so we can get a closer look. Now, this custom Instruments package here is giving me a great visualization into how I'm using this framework. So it's showing me things like for example here, how many network requests am I making on average every 10th of a second. Then down here, this track is showing me detailed information about each and every network request. How long did it take? It's even highlighting duplicate requests in red. So these are places where I'm asking for the exact same data more than once. It looks like I'm doing that maybe even more than 50% of the time. So I'm just leaving a ton of performance on the table and it's exactly these kinds of insights that I need to help me use this framework more effectively. So signposts, Custom Instruments packages, two really great new ways for you to visualize your data right in Instruments. And that's a look at the new performance tools. Sebastian? All right. Thank you, Ken. That's a really, really amazing demo. Really great tools that all of you can use to make your apps run even faster. Now, to recap, we just reviewed a lot of great tools and best practices that we can use to ensure that we delight our users and keep their trust. Now, I'd like to turn our attention to the Mac. OS X was launched 17 years ago and we've constantly pushed the platform forward. 64-bit support in Leopard, MacOS Mountain Lion introduced Gatekeeper, a key step forward in Mac security. And one of our key missions is to always push the Mac forward by extending its capabilities to take advantage of the latest technologies. But as we push the platform forward, we sometimes have to deprecate legacy functionality to ensure that we're not holding it back. Last year, we announced that High Sierra was the last MacOS release to fully support 32-bit apps without compromise. And this year, we're announcing that MacOS Mojave is the last release to support 32-bit at all. So as we remove 32-bit support next year, these 32-bit only frameworks will also be removed such as the QuickTime framework and the Apple Java framework. Next, let's look at security on the Mac. Gatekeeper has done a great job at avoiding large-scale malware attacks and this year, we want to push it even further. We're extending user consent, enhancing run time security and we're launching a new Notary Service. So let's look at these in more detail. As you heard this morning, we're extending the protections afforded to sensitive system resources. We've added camera and microphone and we now require user consent for API and direct access to all these resources. What does it mean in practice? Well, it means that your application has to gracefully handle those calls potentially blocking or failing as the user provides consent. It's also a really great idea, as Katie has pointed out, to provide meaningful purpose strings so when the user is faced with one of these dialogues, they understand why your app needs access. We're also going further in protecting sensitive user data. And only specialized apps like backup tools require access to this kind of data. And so we'll protect these locations by requiring user consent directly in the security and privacy preference pane. Next, we're introducing enhancements to run time protections. Now, a number of you have requested a way to extend the zip protections to your own apps. And our new enhanced run times, there's a new security baseline that requires risky capabilities to be opted in. So beyond strong code validation, if for example also protects your apps from code injection. The enhanced run time is fully backwards compatible. It's opt in through a simple switch in Xcode. And finally, we're introducing the concept of notarized apps. This is an extension to the Developer ID program for apps that are distributed outside of the Mac App Store. And it has two main goals. The first is to detect malware even faster than today before it gets distributed to our users. And second, provide a finer-grained revocation capability so that we can revoke a specific version of a compromised app as opposed to revoking the entire signing certificate. Now here's how it works. You develop the bug and build your app as before. And you sign it with your Developer ID Certificate. But before distributing it to your users, you submit to the Developer ID Notary Service. Once notarized, you distribute the app through your existing channel. Once your user runs the app on their system, MacOS Mojave will check with the Notary Service to make sure the app is properly notarized and is not known to be malicious. Now, the service is not app review. There are no new guidelines being imposed on Developer ID apps as a result of the Notary Service. It is used exclusively to analyze apps for security purposes. A future version of MacOS will require all Developer ID apps to be notarized by the service before they can be installed so we want you to get ready. It's available in beta today. We encourage you to try it out and give us feedback. And those are the enhancements to Gatekeeper in MacOS Mojave. Let's now switch gears and talk about the MacOS user experience. And to do that, I'd like to invite Kristen up on stage. Kristen? Thank you, Sebastian. I'm excited to be here. We have a lot of great features in MacOS Mojave including improvements to Finder, SnapShots and desktops docs. I'd like to focus on one in particular that you, as developers, can take advantage of. And that's Quick Actions. With Finder Quick Actions, we've embedded the tools you need right where you need them in the Finder preview pane. You can perform common actions on your files without ever leaving Finder. And there's different actions for different file types. As you can see here with video and here, with a PDF. And it's not just built-in actions. We know pro users especially like to create their own. And those actions are shown here in Finder as well. New developers will be able to provide custom actions from your applications using app extensions. And as an end-user, you can also combine shell scripts, AppleScripts and Automator Actions in Automator to create an action bundle. And these action bundles will be shown here in Finder as well based on file type. These Custom Actions get some prime real estate in Finder and even more so in Touch Bar. Touch Bar is great when customized. And you can customize Touch Bar to show these actions all the time or on the tap of a button. Moving on, in the keynote this morning, you got a sneak peek at another technology we are really excited about. An easy way to bring iOS apps to the Mac. We are in the midst of developing this technology in the context of these four apps, News, Stocks, Voice Memos and Home. These apps utilize UIKit and this is a new way to delivery great Mac apps. Of course, AppKit is our primary native framework and it takes full advantage of all the Mac has to offer. And in no way are we de-emphasizing that. However, we note that a lot of you have iOS apps and you don't have a native Mac experience. And for these cases, we want you to -- we want to give you an easy way to bring your apps to the Mac as well. So how are we doing this? These UIKit apps are running in a native environment on top of a native stack. And if you look closely, you'll see that the stack below the UIKit app has a lot in common with the stack below the AppKit app. In fact, these environments were built on a common foundation which in some cases has drifted apart over time. So we're taking this opportunity to rationalize the substrate which is great news for you developers independent of this technology because it makes it easier for you to write portable code. These apps get all the typical Mac features and I'd like to show that to you now. You've seen the new Stocks app for iPad. I'm running a Mac version of this app built from the same sources. Mouse events are mapped to UI events so I can click on a ticker symbol in the watchlist to see more information. I can move my mouse over the interactive chart to see the price at a point in time and I can click and drag to see the price change over a period of time. I'm going to click on an article to open it right here in app. Now since this is a Mac window, I can resize it as I would like and I can also take it full screen. I can navigate using two-finger scroll which is another example of event mapping. And if I want to copy some text, I can select it, pick it up and drag it and drop it in my Notes app. Now in this Note, I have a link to a news article so I'm going to click that to open it directly in News. And we've populated the menu with items for this application. So for example, I can go to the file menu and I can follow this channel. And notice how ESPN appears directly in my transition sidebar. Another Mac touch can be seen in the toolbar here where there's a red color contribution coming from the content underneath it. Now we have controls for the window in the toolbar including the share button so I can click on the share button to show this article with a friend. So that's a quick look at UIKit apps on the Mac. Now, thank you. We are continuing to develop this technology and we are working to fully vet it before making it available to you and your applications which we are planning to do next year. Next, Dark Mode -- you've seen that Dark Mode is a big thing for MacOS Mojave and we think it looks stunning. Let's take a quick tour. The window background is dark making the content pop. The sidebar is translucent and the content is blended vibrantly which preserves contrast with whatever may be underneath the window. And in a few cases, we found it valuable to change the icons slightly so you can see a slight darkening of this photo icon and a new dark trash can. But there's something very subtle here. The window background is actually picking up a slight hint of color from the desktop. To show you what I mean, here is a window on top of two very different desktop pictures. On the left side, there's a slight blue tint in the window from that slightly blue desktop picture. And on the right side, there's a slight orange tint from the predominantly orange desktop picture. This is not translucency. We're actually picking up our average color from the desktop and blending it into an opaque background. And we do this so that your window looks harmonious with a variety of desktop pictures. Let's look at what you need to do in your apps to support Dark Mode. Because we want to make sure to preserve compatibility with your applications, we are not automatically opting you in. You need to build against the MacOS Mojave STKit. For example, this is how Keynote looked when we first ran it after building on Mojave. It got a dark toolbar but it didn't otherwise adopt to Dark Mode the way we wished. The [inaudible] part is drawing too light of a background. The toolbar controls are faint and hard to read. The sidebar is the wrong material so it's too translucent. And in the selected segment in control, we have a white glyph on a white background. The good news is these issues were all easy to fix. We have simple API that support all the needs of Dark Mode. And in fact, most of these have existed for years and we just had to augment them a tiny bit. There's NSColor. There's Container Views with background color properties. There's Visual Effect View in materials. There's Template Images and a new way to colorize your content. So we updated Keynote with these APIs and this is the result. It looks great. These were pretty simple changes. We invite you to try this today. If you're already following the best practices of using asset catalogs in system colors, you could be pleasantly surprised at how close you already are. And since these techniques are available on previous releases, you can adopt and easily back-deploy. It, of course, depends on how many custom controls you have in your applications but for a few of our apps, it was as little as a day of work. We give you some useful tools for it as well. Well, I'd like to welcome Matthew to the stage to show you how Xcode 10 supports adoption of Dark Mode and much more. Thank you, Kristen. Our Xcode release this year is focused on the productivity. Work flow improvements, performance improvements and new feature support in all of our STKits. And of course, when running on MacOS Mojave, Xcode has a whole new look and feel. So let's start by taking a sneak peek at how Xcode can make your Mac apps look great in Dark Mode too. So here we are back in our solar system application. We've been converting it over to Dark Mode and we've made great progress so far. There's a couple of items left I need to finish here. There's a darker version of this globe my designers have provided to us. And there's these two hard-coded boxes that have colors that I need to change. Xcode's asset catalogs makes this easy. Let's start with this image. I'll change over to the tab with my assets and we can see, I've already defined dark variance for all of my colors. I'll select the group with all of my images and here's the planet image I'd like to add a dark variant for. That's easy. I'll select it. Go in the Inspector and add a dark variant. And my designers have sent me the assets here so I can just pull them out of my downloads folder and put them into the catalog. That's it. You'll see when I go back to my interface now, the globe is updated to match the appearance of the interface builder canvas. Now, I've already specified all the color variants that I need. So to update these boxes, I'll just select both of them, go to the Inspector and change the fill color to one of my catalog colors. We'll take the badge background color. Great, so now my interface is looking pretty good. Now when designing interfaces, I often like to check the other appearance as I'm going along to evaluate my progress. Interface Builder makes this easy. Down the bottom here, there's a new appearance bar that allows me to toggle between appearances. I'll just select the appearance on the left and now I'm seeing my application in the light appearance as well. So I can easily evaluate my progress. Let's run our application and see how we've done. We'll update our assets and we'll launch our application. And we'll see here that the application launches. And great, it's looking pretty good. Now the application launched in the dark mode to match my system. But while I'm developing, I can change the appearance. Down here in the debug bar is a new appearance toggle that's also in Touch Bar, and it gives me access to all the appearances. I can select the light mode, the dark mode, even high-contrast modes to evaluate accessibility. So I'll select the light mode. We'll load those assets, and there's my application in light mode as well. So very simply, with asset catalogs, interface builder, and our debugging tools, it's really easy to make your apps look great in Dark Mode, too. Now I know many of you have wanted a dark mode appearance in Xcode for a long time. It's been one of our most popular requests. In fact, just a couple weeks ago, there was a posting in the App Store about this feature. It was from a user name Ronnie Bo Bonnie. This is true -- I'm not making this up. But I just wanted to take a moment and say Ronnie, if you are out there, no charge. Now we also have some other improvements to our design tools to share with you today. Form-based UIs like preferences and inspectors are common in Mac apps. And Cocoa's NSGridView is the perfect system for laying them out. So we're bringing the power of NSGridView right into Interface Builder where you can now design your column- and row-based UIs just like working with tables in a spreadsheet. Drag-and-drop content [applause] -- yes. Yeah, you can clap for that. Spreadsheets can be cool. You can drag-and-drop content, use contextual actions, and you get system access to things like right-to-left layout. Now when designing your interfaces, the library is an important tool, and we have a whole new workflow for you because the library is now separate from the inspectors. You can now take the library and reposition it wherever you want. you can adjust the size to match your layout. And you can keep the library up while working or have it automatically dismiss when you are done. And the library works great with all of our content types, including media and code snippets. And finally, with our design tools, you'll notice they're just snappier, with faster document loading and more responsive canvas interactions. Now we've also spent time focusing on our source editing tools, keeping them fast, fluid, and informative. We started with performance, where you'll now see the editor loads large documents much faster, all while keeping smooth scrolling at 60 frames a second. Next, we double down on stability in SourceKit and enhance the robustness of our language integration. So now more of your colorful comments will stay inside of the editor rather than being about it. Co-completion and navigation are two essential workflows, and we've improved on both. Co-completion now provides more targeted results and limits completions to high-confidence matches. And when navigating, with jump to definition, the destination list will now provide contextual details like file and line information to help you easily get to where you need to go. And you'll see the same contextual information in the new callers option in the action menu, which is a seamless way to move throughout your projects. Now last year, we introduced refactoring for all languages, including Swift. And you, the Swift community, embraced the opportunity and added a number of new actions. These actions all streamline common programming scenarios and are built right into Xcode now, just a click away. Now refactoring is just one of many ways you can modify the source in your project. And to make it easier to keep track of your changes, we're introducing a source control change bar. The change bar is on the left side of the editor and highlights lines of code which have changed since your last checkout. The style and color of the indicator reveal the type of change, making it easy for you to see at a glance changes you have made, your team members have made, and those which might be in conflict. Now this feature is -- yes. I agree. I think this feature is pretty awesome. And I'd actually like to show it to you in a demo now. So we're going to go back to our solar system application, and I have some changes I'd like to make in one of our source files. It's our scene view controller here. So I'll scroll down in the editor to the place I'd like to make changes. And here we can see on the left -- just to the left of the line numbers -- the source control change bar is indicating there's some upstream changes a team member has made. In fact, if I had already made the changes to this line, you'd see the indicator turns red, highlighting a conflict. If I put my cursor over the indicator, you'll see it highlights the ranges of characters which have changed and are in conflict. And if I click on the indicator, it brings up an action menu with both a description of the changes and some actions I can take I see my team member has added more descriptive comments here. I think we'll take his change, so I'll use the action menu to discard my change, and go up under the source control menu to pull his changes in. So here's his changes -- very descriptive comments. I can scroll to the bottom of the editor, see if there's anything else I'd like to look at here. Here's another new feature of Xcode 10 -- our editor supports overscroll now. So going back to the lines of code I'd like to change, I'd like to convert these hard-coded functions into properties that pull the colors from the asset catalog. Now there are three of them that I'd like to change, and they're a bit spread out now because of all these comments. Well, no matter, with Xcode 10, we've improved code folding. Basically, you can now code fold anything you want. And we've brought back the code folding ribbon. So just to the right of the line numbers, I can click -- -- to collapse the code away. And we have this nice, svelte presentation of the collapsing now. Now this is the first function I'd like to change, and I see that all of these functions are very similar, and it would be great if I could make all of these changes all at the same time. Well, I can do that now, too, with multi-cursor editing. The key to multi-cursor editing is two fingers, control and shift. So I'll hold down those two keys and just click at the beginning of each of the other functions. We'll use range selection, and we'll just change that to VAR. We'll change those into colons. And we're pretty good so far. Now I happen to know that I've named my colors in the catalog the same name as my properties here. So we'll just select those names and copy them. And now let's go to the implementation and change that. So we'll drop three more cursors, and we'll just select all of this, type in named, paste in those colors, and we've made all those changes. It's like three times faster. Now multi-cursor editing also works great with column selection. So here I have all of my IBOutlets defined with week. If I hold down the option key and I select all of these in here [cheering] -- oh, yeah. Oh, yeah, so let's just convert those into [applause] unowned. And just like that, I can make my changes, and then use the source control bar to make sure that I got the changes I want. So those are some of the great new editing features you'll find in Xcode 10. So additions like the source control change bar and multi-cursor editing alongside performance and stability improvements. Xcode 10 continues to raise the bar on our source editing experience. Now in addition to the source control change bar, we are also extending our source control integration. We started first by unifying our conflict resolution system with Git, making the results more accurate, more predictable, and significantly faster. Next, we've enhanced the pull action to support rebase. So you can replay changes between branches [applause] -- yes, it's okay to clap for that. You can replay changes easily between branches without the unnecessary merge commits. And to keep your connections secure, Xcode will help you create SSH keys and upload them directly to your service accounts. And this is the perfect accompaniment for our service integrations, because in addition to GitHub, we're adding two new services this year, support for Atlassian's Bitbucket cloud and Bitbucket server -- -- and support for GitLab.com and self-hosting. There's a lot of source control love here. And both of these work great because their web interfaces will check out directly into Xcode. Now as Sebastian mentioned earlier, we are passionate about giving you great tools to debug and optimize your apps. And this year, we focused on the usability and performance of our tools. We started with LLDB, our lower-level debugger. Which now has faster startup and more precise access to your variables in the console and Xcode's variables view. Next, we've made downloading debug symbols five-times faster. So now it's more like seconds rather than minutes. We've enhanced our memory debugging tools to have faster loading, and saving of documents, and a new compact layout to help you visualize even more of your application at once. And earlier this spring, we introduced energy diagnostic reports. They're like crash logs, but for energy usage. These reports are automatically collected on iOS for test flight in App Store apps and surface details for foreground and background usage. These reports show up in the organizer and include stack frames to illustrate the issue. And just like with crash logs, you can open these reports in your project to navigate your code and find and fix the issues. Oh, and to go alongside these, we also have some improvements in testing. Earlier this spring, we enhanced code coverage, adding a command line tool to access coverage data, and giving you the ability to select individual targets to collect coverage for. This means your coverage reports can now be actively focused on the areas you are coding and testing. In addition to these, we're adding two new testing workflows this year -- actually, three. The first is that you can now automatically include or exclude new tests in your test bundles. Next, you can randomize the order that your tests are executed in to minimize accidental dependencies. And our biggest change for this year is you can now execute your tests in parallel inside of Xcode. Now last year, you could use Xcodebuild to test on many devices in parallel, sending all the same tests to each device. Now this is perfect for use with continuous integration where you want the broadest scale of testing. When you're working in Xcode, you're more often focused on a single configuration. And once you're testing, to finish as quickly as possible. This is the configuration Xcode 10 dramatically improves with parallel testing. Behind the scenes, Xcode will create copies of your Mac app or clones of your iOS simulator and then fan your tests suites out to them. This means you continue to test a single configuration, but your tests finish in a fraction of the time. And parallel testing automatically scales to the capacity of your machine, which means on an iMac Pro, it can be pretty awesome. How awesome, you might ask? Well, let's see in another demo. So we're going to go back to our solar system project one more time. And here we see the testing log for our Mac tests that we ran before. Took about 14 seconds. Let's run it with parallel testing now. I'll click and hold on the toolbar and select the test action. And we'll bring up the scheme sheet. In the options, I'll just click execute in parallel and click test, and we're going to build our tests for parallelization. And if you watch the doc in the lower right, you'll see that we launch the tests, we now launch many different processes -- this is one for each of our test suites -- and collect the results. And if we look at our testing log, it finished almost four times faster. So where parallel testing works great for unit tests, it works awesomely for UI tests. So I will select the iOS version of our application, and we'll kick off testing. So behind the scenes, we're going to go and clone the active simulator, and then set up a number of debug sessions for each one of these, and then switch over to a space with all of those simulators running. So you'll see we'll install different test suites on each of these simulators and kick off a different set of tests on each. So I can run all of my same tests faster on all these devices, which gives me the ability to add more tests and make a much better app. This is ludicrously awesome parallel testing in Xcode 10. Last year, we introduced a preview of our new build system written in Swift. Many of you tried it out with your projects and provided great feedback. And so I'm happy to say our modern build system is now on for all projects. In addition to greater reliability and stability, we also focused on overall build performance. You'll find the build system now has faster rebuilds, better task parallelization, and uses less memory. And the build system now includes new richer diagnostics to help you tune your project configuration to achieve the best build performance. Now staying on build performance for a second, I'd like to talk up another core component of our release, Swift 4.2. Over the last year, we have made steady improvements to compile times with Swift projects. We've sampled a number of open source iOS applications, and compared to our previous release, debug build performance with Xcode 10 is often twice as fast. And for release builds, code size is up to 30% smaller using the new size optimization, which is a great win for cellular downloads. Now in addition to these, Swift also adds a number of additions and runtime language improvements. Some of these are tongue-twisting APIs like synthesized hashtable conformance. A perfect place to try out these APIs is in Xcode Playgrounds because Xcode Playgrounds now include new [inaudible]-like interaction that allows you to evaluate new lines of code without restarting the Playground session. Here's a Playground of our solar system view. And the new lines of code added to move to the next planet are evaluated and return results all while the Playground continues to run. So all of these additions to the runtime language and tools continue Swift's great momentum as part of Xcode 10. And we also have another release coming up for you in the language, Swift 5. The focus of Swift 5 is greater adoption by delivering Swift as part of the OS. Apps will no longer need to include the Swift runtime when delivering on our newer OS releases, resulting in smaller downloads [applause] and faster launches. We're very excited about this, too, and we have made great progress toward this goal. And you'll see it in a release coming early next year. So Xcode 10 includes a number of great productivity improvements, alongside deep investments in performance, robustness, and stability throughout our tools. And all of this to help you do your best work now faster than ever. And that is Xcode 10. Next, I'd like to invite up John to tell you what's new in Machine Learning. John? Thank you, Matthew. Machine Learning is at the foundation of our operating systems and many of our applications. But our goal has been to provide simple and easy-to-use API to make Machine Learning accessible to everyone. And you've all done a fantastic job brining so many innovative features and intelligence to your applications. Last year we introduced Core ML with its base performance frameworks as well as Vision and Natural Language at a high level. And I'd like to start by showing you some improvements we're making with Vision and Natural Language. If we take Vision and, of course, a photo that we want to have depth, we now have APIs that support object detection and bounding boxes like this sign being held in the picture. We can do face detect, facial landmark detection. And also, barcode like this QR code can be detected in your image. Now in addition to the APIs we previously provided for depth, we now support people segmentation, so you can remove a person from a photo and separate them from the background or substitute in the background for something a little different. For Natural Language, we have a brand-new, easy-to-use, Swift-focused API. So you can take simple sentences like this one and automatically identify it as the English language. You can tokenize the sentence and convert it into its speech parts all with simple API. And as one other option, you can do named-entity recognition. Here, determining that the sentence is talking about Apple as the organization and a location in San Jose. Now you might think this is easy in languages like English, but we support many more, including French, German, Japanese, and this Simplified Chinese example. now let's look at Core ML. This is our foundation of our Machine Learning technologies. And just one year ago, we introduced Core ML here. And since then, we've got adoption of every major Machine Learning training framework and format. This is just incredible to have achieved in only one year. But we didn't want to stop there. We're introducing Core ML 2. And we focused on making the models execute faster, those models smaller, and making it far more customizable. And we know these are the features that were most recommended -- requested. To look at performance improvements, we've added a new batch API. Where previously you needed to do inference on each image, passing them between the CPU and GPU, you can now bundle those inference requests together and exploit the full performance of the CPU and GPU. Through this technique and enhancements to the Metal Performance Shaders underneath, we now have up to 30% performance improvement on large networks like Resnet. But if you're using a smaller network like the kind you're going to be using on iOS, we see up to four times improvement when running with MobileNet. Now we didn't stop there. We wanted to look at making the model smaller, so we now support quantization. So we can take a model that previously had shipped in 4.3 -- such as this example again from MobileNet -- and reduce it down to Int 8, and take its size from 17 megabytes to less than 5. This is a huge saving for the models that you bundle with your applications. Now you can do further reduction through features like table lookup quantization. And we support many other features, including support for custom models now and, a very popular feature, flexible shapes. So you no longer need to ship a model for each shape that you want to do inference on. You ship one model, and our simple API takes care of everything for you. now let's talk about Create ML, our brand-new, easy-to-use machine learning training framework. It brings together the power of Machine Learning, Swift, and Xcode, and Xcode Playgrounds. No more downloading packages from the Internet and going through long, complicated tutorials to portray a model. We support feature-level training such as image classification and natural language. And if you do want to go deeper in machine learning, we support traditional types of algorithms such as linear regression and boosted trees as well as traditional data processing. But we think people will want to use these feature type of training far more, so let's look at those examples. For Natural Language, you can now have your own custom Natural Language model that does text classification, word tagging, and of course, we support multiple languages. So you could train a model with very small datasets to do sentiment analysis such as these reviews for a movie where you just train with positive and negative in strings, and you build your own custom image -- custom text classifier. And then you could do the same for domain analysis, being able to train a model to understand whether you're talking about a hotel or a restaurant in a given sentence. Now we think, by far, image classification will be the most popular kind of training that people want to do, and so we've put a real focus on this. Traditionally, if you were training a very large model with what might only be a small dataset because as a developer that's all you have access to, your model wouldn't train well, and it over-fed, and you get poor predictions. Now Apple has extensive experience in training very large models with datasets of photos say in the -- with many millions. And we want to bring all that experience to all of you. And through a technique called transfer learning, you can train your own custom image classifier. So we'll bundle our model into our OS, so there's no need for you to ship that. You take your data, and use transfer learning with Create ML, and augment our model. That means you only need to ship the part of the model that you've augmented, bringing a huge saving to your applications. So we've worked with a number of developers who already have models in the around 100-megabyte range, just to add one intelligent feature to their application. And now, through transfer learning, they can take that model size down to three megabytes. Now this is far cooler to see if you see how it's all done inside Xcode and Xcode Playgrounds, so I'd like to invite Lizzie up to give you a demo of that now. Lizzie? Thank you, John. Let's take a look at how to create an app to classify different types of flowers. Now I've started by using a state-of-the-art image classifier model called Inception B3, but there are two problems with this approach. One, this model is quite large. It's taking up 100 megabytes in our app. And the second is even though this model has support for 1000 classifications, it can't correctly classify a rose. Now normally what I'd have to do is switch to a new development environment, download an open source machine learning library and spend hours training a new model. But now with the power of Create ML, you can do this in minutes and in Xcode. Now I'll switch to a new Playground and import Create ML UI. The next step is to define a builder that can build image classifier models. Then to enable drag-and-drop interaction with this model, we can show the builder in the live view. And see, on the side we get a prompt to drag in images to begin training. Now over on my desktop, I happen to have a bunch of different images of flowers organized into folders with the name of the particular one that they are. So we have some daisies, hibiscuses, and of course, some roses. Now what I'll do is I'll take this folder and drag it into the UI. And instantly, an image classifier model begins training on the Mac, accelerated by the GPU. And right away, I can see what the accuracy was on this training dataset. But what I'd really like to know is how it performs on new types of flowers that it hasn't seen. And I've set some of those aside here, and I can just drag them in to let the model begin evaluating on these new ones. And if I scroll, you can see what the actual label was for each type of flower and what the model predicted. Now 95% is pretty decent on this dataset. So what I'd like to do is add it into my app. And you can do so just by dragging and dropping it. I'll then add it. And if we take a look at this new model, we can see it's only 50 kilobytes. That's a huge savings. So I'll go ahead and delete -- -- I'll delete the 100-megabyte model and initialize the new image classifier. Now if I rerun the app, it's bundling this new model into the application. We can go ahead and test it to see if it can correctly predict on the images that we've trained it on, or new images of the same types of flowers. And indeed, it can correctly classify a rose. Let's try it on a hibiscus. And it can correctly predict on those, too, since we've trained it and incorporated it into our app. So as you've seen, we've been able to train our own classifier models using Create ML in a fraction of the amount of time to produce models a fraction of the size, all using Swift and Xcode. Back over to you, John. Thanks, Lizzie. Isn't that cool, a custom image classifier trained with three lines of Swift, in seconds, right there on a Mac? So we've looked at new Vision and Natural Language APIs and the enhancements we've made there; our improvements with Core ML 2 with smaller, faster models and even more customization; and Create ML, our brand-new machine learning training framework for the Mac. Now I'd like to talk about another area of intelligence that we've built into the OS, and that's shortcuts, a powerful new way for you to expose key capabilities of your applications through Siri. And you can even expose these key capabilities using voice commands. Previously, sections of the OS that had suggested features and actions for Apple's software are now accessible to you through shortcuts. We do all this prediction on device using machine learning that preserves your users' privacy. So you're probably asking how do you adopt shortcuts? Well, many of you have already adopted NSUserActivity for features such as Spotlight search and Handoff. And if you have -- and it's as simple as adding this one line of code making them eligible to prediction for the system. Yeah, one line of code. But if you want the full, rich experience of shortcuts, then you want to adopt the new Siri kit Intense API. That allows rich, inline capabilities of your application to be exposed in Siri, custom voice triggers and responses, and more importantly, more targeted predictions of when those shortcuts will be interesting to your users in the future. Now a great shortcut is one that accelerates engagement with your application, and increases engagement, too. It's one that's likely to be repeated more often. So in the TeamSnap example you want to be able to check your kid's soccer game schedule every Saturday morning. And ideally, it's one that can be engaged right there in the Siri UI and handled without the need to punch out to your app. But you do have the option if that's something that you want to do. Now when creating a shortcut, you need to do three simple things. You obviously need to define the shortcut and do it for those actions that really are interesting to the users. You need to donate when those shortcuts occur, even if that's in your application, because we need that signal to be able to predict those shortcuts in the future. And of course, you want to handle those shortcuts when they occur. Now if you've done all this, you get something pretty cool in that you can interact with your shortcut directly from home pod. So now without picking up your phone, you can just ask Siri from your home pod for your kid's soccer roster, and it will respond using the app. Now if you also want your shortcuts to be exposed through the Siri Watch Face, you can just adopt this new Relevant API. So that's shortcuts, a powerful new way to expose key capabilities of your application and increase engagement through Siri. Now I'd like to hand it over to Jeremy to talk to you about what's new in Metal. Jeremy? Thanks, John. So Metal is Apple's modern, high-performance, high-efficiency programming interface to the awesome power of the GPU at the heart of each of Apple's platforms. It accelerates both advanced 3D graphics and general purpose data parallel computations. And since we introduced Metal in 2014, we've seen it used for everything from smooth, high-performance UI to modern 3D games, advanced computational photography, and the latest in AR and VR experiences. And when we introduced our latest iPhones last fall, we were incredibly excited to reveal the next chapter in the Metal story with the A11 Bionic chip where Apple harnessed many years of deep expertise in hardware and software design to bring the Apple-designed GPU, optimized for Metal 2 with such innovative new features as tile shading and image blocks, and advancing the state of the art of GPU programming with both faster performance and lower power. Now your applications can use Metal directly for 3D graphics and GPU Compute. And Metal powers many of Apple's system frameworks for graphics, media, and data processing. Let me give you just one example. Our iOS camera framework uses Metal to calculate depth information, identify people in photos, and generate this depth-of-field effect in this gorgeous portrait image. And developers like Epic Games are using Metal's broad support across all of our platforms to bring their smash-hit game Fortnight to iPhone, iPad, and Mac. AMB's metal-accelerated radion [assumed spelling] pro-ender plugins, are now driving high performance, 3D contact creation and professional editing in Maxon Cinema4D and Autodesk Maya. And apps like Gravity Sketch are using Metal to power the next generations of artists in immersive professional VR editing. Metal's machine learning acceleration empowers iOS apps like BeCasso to transform your photos into beautiful paintings. And drives automatic, intelligent image editing in Pixelmator Pro for macOS. And those are just a few examples as the developer adoption of metal has been truly astounding, with more than 400,000 apps now using the Metal API. And all systems running an iOS 12 and macOS Mojave support Metal, which includes all iOS devices and all Macs released in at least the last five years, which means there are now well over 1 billion Metal systems for your applications and games. So with Metal's deep and broad support across all of Apple's desktop and mobile platforms, we are now deprecating the legacy OpenGL and OpenCL GPU framework, starting in macOS Mojave, iOS 12, and tvOS 12. Now apps using these legacy APIs will still continue to work in these releases, but deprecation is a first step as we wind down legacy technologies. So if you've not already done so, you should begin transitioning your apps to Metal. And we'll communicate more details about this transition in the near future. Now as you bring your apps to metal, we are here to help. The Metal API is dramatically easier to use and much more approachable than these other GPU programming APIs. It contains a familiar, yet powerful, C++ GPU shading language. And we provide a full suite of advanced debugging and performance profiling tools for using Metal, all built right into Xcode. We have GPU performance counters with advanced profiling to identify your most expensive lines of shader code and a visual API debugger for navigating your Metal function calls, a Metal System Trace to put your Metal commands in the context of everything else happening on the system. And we're really excited to announce two new powerful tools this year, a new Metal Dependency Viewer where you can investigate your complex, multipass rendering and command encoders, and an all-new, interactive GPU source code shader debugger where you can actually explore your Metal code right down to the pixel level. Now you have to see these new tools in action, so I'd like to invite Seth to give you a demonstration. Seth? Thank you, John. Xcode's GP debugger is the tool for developing your Metal applications. In the Debug Navigator on the left, you can see all the Metal API codes and [inaudible] codes used in your frame. And on the right, you can see the results of the selected [inaudible]. The main editor shows you the -- all the buffers, textures, and other resources that were used for that [inaudible]. Well, new in Xcode 10, we're introducing the Dependency Viewer, a powerful way to understand how complex render passes combine to form your scene. This gives you a blueprint of your frame in explaining and understanding how the complex render graphs at one application such as Unity's breathtaking "Book of the Dead" demo shown here. I can zoom out to see more detail. Earlier render passes are shown at the top, with the later render passes shown at the bottom. The lines indicate the dependencies between passes, with those for the selected pass highlighted in blue. As you can see, with more than 100 render passes, there's clearly a lot going on in this scene. Now as good as this scene looks, there's always room for more flair. So I did an additional render pass, the lens flare. But as you can see, something didn't [inaudible] quite right -- far, far to green. Well, let's zoom in, select a pixel, and launch the new Shader Debugger, a powerful interactive tool to let you visually debug shaders [inaudible]. In the main editor, I can see my source code. And in the sidebar to its right, I can see variables touched by each line of code. Additionally, I can expand any of these to see more details in line. These two views visualize the area around the selected pixel, corresponding to the highlighted region in the frame attachment. The view on the left visualizes the variable value. And the one on the right, the pixel -- the execution mask. This indicates which pixels executed this line of code. This is an incredibly powerful way to debug the massively [inaudible] execution of shaders on the GPU. Now you can see here that the shape of the execution mask matches that of the visual aberration, telling me that the issue exists on this line of code. Well, now that I know where the issue is, I can see what I've done wrong, using the vector length of the lens flare rather than the color of the lens flare. That will be easy to fix. I can now hit the update shaders button to quickly apply the fix, recompiling the shader and deploying it to the GPU. And here we can see that my lens flare is fixed, and the scene looks cool. So that's the new Dependency Viewer and GP Shader Debugger in Xcode 10, giving you powerful new tools to build your Metal applications. Jeremy? All right, [applause] thank you, Seth. So in addition to these amazing new tools, we're continuing to advance Metal with a fantastic set of new features in iOS 12 and macOS Mojave. Now I'm going to highlight just three of them today -- GPU-driven command encoding, machine learning training acceleration, and ray tracing. So first, GPU-driven command encoding. Now historically, your app would encode its GPU commands using the CPU and then subsequently execute those commands on the GPU. And while Metal enabled this encoding to be very fast, it could still become bottlenecked by the synchronization between the CPU and the GPU. Well, now in iOS 12 and macOS Mojave, you can actually encode those commands right on the GPU itself, freeing up precious CPU time for other use by your games and apps. And because you issue these commands right on the GPU using a compute shader, you can actually officially construct massive numbers of commands in parallel as well, unlocking completely new levels of rendering performance and sophistication. Next, I'd like to share the latest advances in Metal's support for machine learning. In iOS 12 and macOS Mojave, we have augmented our existing library of Metal performance shaders with an enormous array of all-new compute kernels, optimized to support machine learning training right on the local GPU on your iOS and Mac devices. And the performance improvements we are seeing from these new Metal performance shaders on training are truly stunning, with an order of magnitude faster training times. We're also really excited to announce we've been working with Google to bring Metal acceleration to TensorFlow later this year, and the early performance results are showing an astonishing improvement of 20 times the previous implementation. Yeah, it's awesome. And last, ray tracing. Now this is a time-honored technique to achieve incredibly realistic scenes, often used for high-end rendering and 3D product design. However, it traditionally had to be done offline because it was so computationally expensive. Now let me describe why very quickly. First, you would need to mathematically model the rays from a light source as they bounce off of objects through the scene, toward the screen, and into your eye. And to achieve higher and higher resolutions, you would need to add more, and more, and more rays until you could reach the desired resolution. And this simple 1k-by-1k image would take nearly 6 million rays to generate. Now each of those rays also must be processed with at least two sets of expensive mathematical calculations. First, you had to determine if a given ray intersects a particular triangle in your scene. And second, you must apply a material-specific shader necessary to generate the pixel. Now originally, both of these operations would have been performed by the CPU. But while the GPU can easily handle the pixel shading, the ray-triangle intersection itself could remain an expensive CPU bottleneck, and it would be incredibly difficult to move this to the GPU efficiently. But the new Metal Ray-Triangle Intersector solves this problem for you. And with this new API, you get a dramatic increase in performance of up to 10x in a very simple-to-use package, all pre-optimized for use with our iOS and macOS GPUs. And it really is that simple, just a few lines of code. And the ray tracing, like many GPU compute operations, is exactly the kind of operation that can efficiently scale with the available GPU horsepower. So we can actually get even more performance by using Metal 2 support for external GPUs. Now you really have to see this in action. And I'd like to invite Rav to give a quick demonstration. Rav? Thank you, Jeremy. All right, let's bring up this ray trace rendering of the Amazon Lumberyard Bistro scene using the CPU to perform the intersection calculations. And this implementation is optimized to run on all 10 cores in our iMac Pro. We've also added a little benchmark mode that times how long it takes to do 80 iterations of our ray-tracing algorithm. And for context, that requires performing over 6 billion intersection tests. And as you can see, we need about 12 seconds to do that on the CPU. So let's compare that to using the new ray -- the new Metal Ray-Triangle Intersector on the built-in GPU in the iMac Pro. And you can immediately see that it's much faster, and we only need about 1.3 seconds to do the same amount of work. It's so good, I'm going to do it again. Here we go. And it's done. So getting an almost 10-times performance increase is fantastic. But of course, we didn't just stop there. As Jeremy noted, ray tracing is well-suited for parallelization across multiple GPUs, so I can enable an external GPU that I previously attached to this iMac Pro and get the render time cut in half. So you'll note the green line that we've added to help visualize how we're splitting this workload across the two GPUs, with each GPU rendering half the frame in this case. So this is a great improvement, but as Jeremy says, you can never have too many GPUs. So let's add another two for a total of four GPUs now rendering the scene. So that's over 40 teraflops of compute capability with our iMac Pro, and we're rendering the scene 30 times faster than the CPU. We think that's pretty amazing, yep. And since ray tracing is so great for rendering shadows, I'm just going to turn off a couple lights here to get them to pop. And you can really appreciate how much faster the image converges on the GPUs. So the new Metal Ray-Triangle Intersector and external GPU support on macOS we believe is going to enable some great new workflows on apps that are taking advantage of ray tracing techniques. Thank you. Back to you, Jeremy [applause]. All right, that is really stunning. Thanks, Rav. So that's Metal 2 in iOS 12 and macOS Mojave, an easy-to-use, unified 3D graphics and GPU compute API with broad support across all of Apple's products, including the A11 Bionic and the Apple-designed GPU. GPU developer tools integrated right into Xcode and all-new features to support the latest advancements in machine learning training and ray tracing. There's never been a better time to move your app to Metal, and we can't wait to see what you'll create next. Thank you. And now, I would like to hand it over to Mike Rockwell to talk about what's the latest news in AR? Thanks. Thanks, Jeremy. So last year has been an amazing year for AR at Apple. With the debut of ARKit at last WWDC, iOS became the world's largest AR platform by a lot. There are hundreds of millions of AR-enabled iOS devices, and that number is growing rapidly. As Craig showed you this morning, with iOS 12, we're taking things further by making AR ubiquitous across the operating system. We can now experience AR content via the new QuickLook Viewer in Messages, News, Safari, and more. To do that, we had to work on and create a file format that we optimized for AR. And we worked with Pixar and Adobe to create a new mobile AR format called USDZ. It's based on the universal scene description format that's used across the industry for professional content creation. It's optimized for mobile devices, and it supports Rich 3D assets and animation. It's incredibly easy to use USDZ. On the web, it just takes a couple of lines of HTML, and it's also natively supported in SceneKit using Model I/O, so you can easily use it in your applications. We've also been working closely with industry leaders in content creation tools to provide native support for USDZ. And as you heard this morning, Abhay said this morning that he had a sneak peek for you about what they're doing at Adobe. So I'd like to invite him to the stage to give that to you right now. Abhay? Thanks, Mike. It's great to be back onstage. So as you heard in this morning's keynote, Adobe's Creative Cloud and ARKit will be able to reimagine and blend the digital and the physical worlds. Now this will require a complete reimagination of new design interaction models. So earlier today, we announced a new system for creating AR experiences called Project Aero that infused ARKit with the power of familiar Creative Cloud applications like Photoshop and Dimension. So in fact, for the first time, with Creative Cloud and iOS, you will now have a what-you-see-is-what-you-get editing in AR. So as you think about this -- and we looked at it -- ARKit is absolutely the leading platform for AR. And so we're really excited to partner closely with Apple as we go jointly explore and push the boundaries of immersive design. But to fully realize the potential of AR, you really have to work across the entire ecosystem. And so today, we are also announcing that Adobe will natively support USDZ format, along with Apple and Pixar [applause]. Now AR is a unique medium in that it allows interactive content to go extend well beyond the screen, where physical spaces around us literally become a creative canvas. So let's take a look. That's pretty cool. So at its core, Project Aero is part of Adobe's vision and mission to truly democratize creation of immersive content. As you hopefully saw in that video, creators and developers will be able to collaborate seamlessly to deliver a wide range of AR experiences using these tools. Stay tuned for more updates on Project Aero at our upcoming conference, AdobeMax. Personally, I couldn't be more excited about our partnership with Apple, as we go together jointly explore the limits of this emerging and powerful new storytelling medium. Thank you. Back to you, Mike. Thanks, Abhay. Isn't that awesome? Amazing stuff. Of course, the foundation of AR at Apple is ARKit. With robust device position localization, accurate lighting and size estimation, ARKit has made it easy to create AR applications. The iPhone X has provided groundbreaking face tracking that used to require custom hardware. After the initial release, we quickly followed up with ARKit 1.5, adding 2D image triggers, a high-resolution background camera, and the ability to suspend and resume tracking so that you don't have to restart an AR session if you get a phone call. Well, I'm incredibly excited to tell you about our next big jump forward, ARKit 2. ARKit 2 delivers a big set of advances, including improved face tracking, with a new ability to track your gaze and tongue. These highly-requested features allow you to take facial animation to a new level of realism. Turns out that the first thing kids do when they play with animojis is stick their tongue out. And I think a lot of you do, too. That's why we had to put that in there. To more accurately integrate objects into a scene, we've added environment texturing. ARKit creates textures based on what the camera sees in the real world -- notice that the globe is reflecting the real picture on the table below. But what about what the camera can't see? While using machine learning, we trained a neural network on thousands of typical environments. And this enables ARKit to hallucinate the rest of the scene. This means that you'll get plausible reflections of things like overhead lighting -- you can see that in the globe -- even though it's never seen the lighting in the environment at all. We've extended the 2D image detection to provide support for those -- tracking those images in three dimensions. So you can now have 3D objects that stick to images in the real world when they're moved around -- and not only in 2D, but also in 3D. ARKit can now detect 3D objects. You can scan objects via an API, or a simple developer tool we provide, and then later, these maps can be used to recognize those objects and their locations and trigger a contextually-relevant AR experience. An incredibly important feature of ARKit 2 is support for persistent experiences. You can see here in the video that we've mapped an environment and then placed a 3D object. This map can be saved and then later used to recognize the space and relocalize to that same coordinate system -- and not only on that device. You can share these maps to other devices to allow them to have the exact same experience. This makes it possible to create apps that provide persistent experiences you can go back to again and again. You could, for example, have an augmented reality pinboard in your home with pictures and artwork. And you can share these maps without having to go to the cloud. These can be done peer-to-peer locally on your devices. One other thing that we've done is we've allowed you to have the ability to share these maps in real time. And this lets you create multiplayer AR games. So to experiment with this, we created a new game called SwiftShot. And I'll show you the video that -- of it that we did. So SwiftShot is a blast to play, and we actually have it here at the show. If you haven't had a chance to go by, we have an AR game area. We wanted to share it with you, so we've actually made the full source code available for you to download under an open license. You can play with it and modify it as you like. We can't wait to see the creative things you'll do with SwiftShot. So that is ARKit 2, improved face tracking, environment texturing, image detection and tracking, 3D object detection, and persistent experiences as well as multi-user experiences. That combined with USDZ across the operating system makes iOS 12 by far the most powerful platform for AR. And we're really excited to be giving that to you today, and can't wait to see what you'll do with it. So with that, I'll hand it back to Sebastian. Thank you. Thank you, Mike. Wow, I think we've seen a ton of exciting new technologies today, and I hope you're really, really, really excited about this. We make it easy to leverage machine learning, build great new experiences with ARKit, high-performance graphics with Metal, a huge step forward on the Mac with Dark Mode. I know you all love this. And it's all backed by great advances in our development tools that are really critical to make the most of these super-powerful technologies. And we've also covered how we, together, can focus on what's most important to our users. All these great technologies and tools are available today as a developer preview from the WWDC attendee portal. Who here has started downloading them? Few people? Okay, you've got to hurry up. Distribution is limited. Please make sure to download it right away. And also, make the most of your week. There are more than 100 sessions here at the conference that go deep in all of these topics. Really, really great sessions. We also recommend that you make good use of all of the labs that we have, because you can get help from the many Apple engineers that are here onsite to answer all of your questions. So with that, I hope you have a great conference, and I'm looking forward to seeing you around this week. Thank you.  Hi everyone. Welcome to the Introduction to Dark Mode session. My name is Raymond Sepulveda, and I'm a macOS designer on the Human Interface Design team. For the first half of today's session, I'm going to be going over some of the design principles that went into the design of Dark Mode, as well as some of the design considerations that you should take into account when updating your own applications to take advantage of Dark Mode. Then, for the second half of the session, my engineering colleagues Rachel Goldeen and Taylor Kelly from the Cocoa Frameworks team are going to be going over how to adopt and support Dark Mode using an example application that they built together. The three of us are super excited to share all of these details with all of you, so let's jump right in. As we announced in yesterday's keynote, macOS 10.14 is going to be getting a new interface appearance, and we call it Dark Mode. The new design is very attractive and engaging, while at the same time being very calm and understated. These are great qualities for an interface, and they make it great for creative professionals who are dealing with heavily detailed images and colorful assets such as images and video assets. But, in actuality, it's actually great for just about any type of user who is looking to concentrate or focus on any given task at hand. So, whether you're trying to concentrate on writing the next great novel or you're trying to read one without disturbing your bedside partner, Dark Mode is really great for either type of situation. The interface shares a lot of family resemblances between the light appearance and the dark appearance, and that is because we wanted to encourage users to switch between both appearances easily and find the one that really best suits their needs. And so it was very important that things like control metrics, window layout, and the distribution of translucent materials be the same as much as possible between both appearances. As such it's also equally important that all of your applications take full advantage of both appearances so that they update correctly between both appearances no matter what the system is running under. You really don't want to be that one light appearance that's stuck in dark appearance. All in all, we're super happy with how the design turned out, and we are really anxious to see what you guys are going to turn your apps into as soon as you get hands on the developer beta and start adopting the Dark Mode. Aside from Dark Mode, another new interface feature that we added into macOS 10.14 are accent colors. Previous releases of macOS offered two accent colors, blue and graphite, but for macOS 10.14, we've expanded the color palette to include eight fun colors to really help you personalize your Mac. So, if you're like me and you love the color red, you're going to immediately switch over to the color red [laughs]. If orange or yellow are more of your preference, you're going to switch over to those, and we make those available as well. I should note that the accent colors are not just a sub-feature of Dark Mode, but they're also available in Light Mode. So if you prefer Light Mode but want green accent color, you can do that as well. So, now that we've covered a little bit of the basics of what Dark Mode is and what accent colors are also, let's dive a little bit deeper into some of the design principles that went into Dark Mode. At Apple, when we begin a new design project, we like to establish a set of design principles to help guide us throughout the entire design process. And, with Dark Mode, there were three in particular that we referred to quite often throughout the entire process. The first one is the most straightforward, so let's just jump right into that one, and that is that dark interfaces are cool, period, right? They are almost effortlessly cool in a way that light interfaces sometimes kind of struggle to be. But why is that, really? Part of it is because we all have these notions of what a dark interface is and what a dark interface looks like. And a lot of that is fed into us by what we've seen in pop cultures in things such as sci-fi movies and action movies. So much so that if I were to go around the audience and ask everyone to give me one-word association of what a dark interface is to them, I'd get responses such as cool, of course; slick; professional; futuristic; and even beautiful. So going into the design for Dark Mode, we really wanted to acknowledge all of these aesthetic associations that we all have with dark interfaces, while at the same time embracing a lot of the real benefits that a dark design offers you, such as a higher contrast for typography. We realize that Dark Mode is really, really cool, and that a lot of you are going to want to adopt Dark Mode on your apps permanently. But, in actuality, only a small subset of apps really should be dark all the time, and those are things that are media-centric or content-creation apps. Those are the ones that are best suited to staying dark all the time, so if your app doesn't fall into one of those categories, it's better to let your app follow what the system appearance is, rather than to make it dark permanently. The second principle was that dark interfaces are not just inverted. And, really, what's at the core here is that when you look at a dark interface, it can sometimes be easy to think of just taking the light interface and inverting it to end up with your final result. But, in actuality, that can be very creatively limiting, and so it's better to take a look at your elements and your application on a case-by-case basis and try to determine what is the visual cue that they're trying to communicate and then figure out whether an opposite visual cue is necessary. So, if you take something, for example, like, a standard Light push button in the light appearance, the buttons look like this in their active state. And, then, when you click on them, the buttons darken. And this is generally true of almost all of the controls in the light appearance in that their click state is a darker version of what their active state was. In Dark Mode, however, the general active state looks like this, and when you click on them, they brighten, and that, again, is generally true of all of the controls. They will all generally brighten up. Another example are group boxes. In light appearance, the group boxes have a dark fill behind them and are shaded such along the edges to give them a recessed quality such that they look like they're carved into the window background that they're inside of. In Dark Mode, however, the group boxes are treated almost as if they are self-illuminated. But, at the end of the day, the general purpose of the group box is the same between the two modes in that what they're really trying to communicate to the user is that the contents contained therein are all related somehow. Why I said that it was important to go on a case-by-case basis and try and determine what the visual cue that your elements are trying to communicate-- It's important, is because, in some cases, the visual cue that's being communicated is already effective enough in the Light Mode and should be left as is in Dark Mode. An example of this are the window shadows. As we all know, macOS is a layered window environment, and the window shadows really help to communicate the sense of depth, the Z-depth of the windows. So, if we were to have inverted the window shadows, for example, and started using a white shadow, for example, the entire UI would have become flat. You would have lost that sense of depth. Another thing to note is that the window shadows in macOS are constructed of two layers. We've got the diffused shadow and the rim shadow. So, if we look at this little portion of the screen and look at how the shadows were constructed and treated in Dark Mode, the first thing to note is that, in taking into consideration that users are going to be able to go back and forth between light and dark appearance quite often and that some apps are going to stay dark all the time, it was important for us to keep the diffused shadow exactly the same. So, we used the exact same shadow blur parameters for the shadows in Dark Mode. Next, we took a look at the rim shadows and saw what changes we needed to do for those, and the changes that we did for Dark Mode where we adjusted the opacity to make them a little bit higher so that the contrast edge was more defined and that we made them a little bit crisper. To further the focus and the edge detail on the windows, we added an inner rim stroke to the windows to really help define the windows and pop them off. The end result is that the visual cue still looks very similar to the light appearance, but the end result has been finely tuned for dark appearance so that it works much better. The second principle was that Dark Mode is content focused, and this was the one that we referred back to the most often throughout the entire process, actually. When you look at a light interface screen, it can sometimes be a little difficult to tell what the content is, where to focus your attention to, especially when there are lots of windows on the screen. And part of the reason for that is because a lot of the content that we all create is just bright in general. However, in a dark UI, the content stands out because of the background being pretty dark. All of the window chrome in the background generally just melts away, and the content becomes the primary focus of your attention. Generally, we found that there were three methods for how to treat your content regions inside of your windows to optimize the content focus. The first method was to just generally go completely dark. An app like Finder is a great example of where this is possible because of the way that the content is displayed. Finder's content are displayed as colorful file icons and so the dark background really helps the icons stand out as much as possible. The second method was to leave the content as is. Pages is a great example of what you see is what you get app, where the contents should be rendered and presented exactly as the user authored and created it. Dark Mode is a great UI for what you see is what you get apps, because the content really becomes the primary focus, and the UI really becomes the secondary focus and almost melts away. The third method was to make the content region appearance a user option. Mail is a good example of an app where the appearance of the content might be ambiguous as to whether the user would prefer it to be light or dark, and so the best thing to do is to make an application setting so that the user can determine whether they want it to be light or dark on their own, while their application's chrome area still respects the overall system appearance. One thing that all three of these windows had in common was that the chrome remained dark and became secondary to the content area. And Dark Mode kind of achieves this using a feature that we call desktop tinting, which causes the window's background colors to camouflage themselves with the desktop picture. We found that completely desaturated gray colors have a tendency to have a color temperature that was sometimes in conflict with what your desktop picture was, and this effect was magnified when you have a translucent sidebar area, for example, against a solid opaque area of your window, and so desktop tinting helps mitigate that. Once applied, the desktop tinting gives the whole window a harmonious color scheme, which helps it to blend in with the surrounding areas and really reduces that color temperature disparity. So, how do we do the desktop tinting effect? The first thing that we do is, actually, we take a sample of the desktop portion that is behind the window, and then we derive a average color for that area, and then we blend that average color with a gray, which then becomes the window background color. This background color is dynamic, so as the user moves the window around the screen, the color will update itself so that the window always stays in sync with the wallpaper portion that was behind it. So, if we look at the effect in action across a couple of different wallpapers, you can see that the window is adjusting itself on the right side to match the desktop picture that is behind it. Not only does the window background color adjust itself to take advantage of the desktop color, but the controls also get a little bit of that going through them also due to the control's opacity. For users that work in color-sensitive applications or have very color-sensitive content or just generally would prefer not to have the desktop tinting apply to their UI, we offer the Graphite Mode so that they can disable the color tinting altogether. If we take a look at the color swatches for the window background colors, along the top there, we can see the window background color in the Light Mode, which is the standard light gray that we're all accustomed to. And, in the middle there, we've got the under page background color, which is a mid-tone gray, and then we've got the content background, which is the white background color that we're all familiar with behind all of our content areas. In the middle row, you can see how the desktop tinting affects these colors, and, then, along the bottom, you can see the effects of the graphite accent color disabling the desktop tinting effect. So, now that we've gone over some of the design principles, let's go into a little bit of the design considerations that you should take into account when updating your apps. First, let's talk about colors. For macOS 10.14, we added a lot of colors. We also removed a lot of colors, updated some, and, as you can see behind me, these are some of our system colors. Along the bottom, we've got the dark variance of the colors, and, as you can see, the dark versions are generally a little bit brighter and more saturated. Rachel and Taylor are going to a little bit deeper into the color usage during their portion of today's session, but, for now, I just generally want to try and cover three colors in particular, which are the blues. Going from left to right, we've got controlAccentColor, systemBlueColor, and linkColor. Linkcolor, as the name implies, is meant to be applied to links, text links in particular. So, things that are hyperlinks, taking of the user to a website, or a link within your application that navigates the user to another area of your application-- those should be treated with linkColor. ControlAccentColor and systemBlueColor are really meant to be used on controls that have visible borders or button bezel backgrounds or for tinted glyphs. ControlAccentColor, as the name implies, is going to respond to whatever the system accent color is set to, in this case, red. So, if you have a custom control that is trying to mimic the appearance of a standard system control, or you just have a control that you would like to follow suit with whatever the system appearance is set to, you should use controlAccentColor. If you have a control where it's important that the control stays blue because of informational purposes or for branding purposes, then systemBlueColor is a better choice for you to use. Next, let's take a look at designing with vibrancy. When we talk about vibrancy, what we're generally referring to is the package of a translucent material working in conjunction with the content that is displayed above it. So, in this case, the vibrancy is the vibrant Finder sidebar. Generally, we have non-colored artwork that is meant to be treated vibrantly. An example of that is the glyphs and the text in the Finder sidebar here. The end result of the vibrancy is is that the colors become higher saturation, and the contrast is increased for things like the text. Colored artwork, such as the finder tags in this example, should be treated as non-vibrant. Same thing with colored iconography. The vibrancy will destroy the original colors that you have in your image, and so you want to treat those as non-vibrant. So, taking a look at how you can construct vibrant content, it might seem almost obvious to just use opacities, in this case, opacities of white. But while it's true that some color contribution from the background is occurring through the materials here-- and that's happening because of the opacity-- the end result is actually not as highly saturated as we would desire. And, as you can see along the bottom with the Quaternary Label Color, the legibility is a little bit compromised. Instead, what you want to do is construct these using opaque grayscale colors. One thing to note here is that the text color for the RGB label colors here are full black. What's going to happen once the Blend Mode is applied is that the text that is black is going to become completely transparent. So, if you have areas of your content that need to be treated as a knockout, use full opaque black, and if you have areas that need to be gradiate towards an opacity, use a gray that is ramping itself towards a full black. This is true in Dark Mode, and the opposite is true in Light Mode, in light, vibrant areas, I should say, where white full, opaque white is full transparent and gradiating towards that is going to give you a transparency. So, once we apply the vibrancy, we can see that the gray colors become a lot more saturated, and the legibility of the Quaternary Color Label is a lot more improved. Putting them side by side, we can see just how much of a difference there is between the opacity construction, versus the grayscale construction. In some cases, over certain colors, the opacity construction will fly, but it's not exactly the best end result, aesthetically or legibility wise. Next, let's take a look at artistically inverting glyphs, and, by artistically inverting, what I'm referring to is the process of creatively looking at your glyphs and determining what areas need to be adjusted to take advantage of the Dark Mode, for example. So, here we've got a set of our standard glyphs that we have in the light appearance. One thing to note is that the light appearances glyphs generally use a lot of edge strokes with hollow centers. And the reason for that is that we take advantage of the white backgrounds that these are generally placed on top of in order to denote form and shape. But, if we were to take these exact same vector forms and place them over a dark background, all of that sense of form and volume is kind of lost, and so we had to artistically invert them to bring back some of that sense of form and volume. Looking at how some of these are constructed, we've got a couple of simple examples, like this finder tag. On the left side, it's denoting a white tag with a dark eyelet cutout. On the right side, however, in Dark Mode, that sense of it being a white sheet of paper is lost. So, once we invert it, we get back that white sheet of paper with an eyelet knockout hole. Looking at how this is constructed, generally, what you would do is expand the clipping paths, I should say, and then select your eyelet shape and then reverse the knockout on that. And then it's just a game of selecting any redundant vector paths and removing those. Taking a look at a slightly more complex shape, for example, the house. We've got a white house on the left side, and on the right side, the house isn't white anymore. So, inverting it returns us back to a white painted house. Generally, the construction for this is about the same as the other process, but one important aspect of this glyph, in particular, was that we have to add a new shape into the vector form. In this case, we're mimicking the roof line, creating a shadow underneath the roof that we're going to knock out. And this is important, because if we were to have left the construction of the house without that shadow line, the roof would have no definition, and it would be really hard to see that at small size. Moving on to a more complicated glyph, like this mail envelope, for example. On the left side, we can see a sense of volume and shape. We've got two layers, generally. We've got the back flap of the envelope, and we've got the front flap of the envelope with a letter contained in between. On the right side, a lot of that sense of volume is just missing because of the lack of fill colors. And so, once we invert that, we return back to having a sense of volume. Again, generally, the process of construction for this one is about the same as some of the other ones, but it's a little bit more detailed and time consuming just due to the number of paths that there are. But, generally, what you want to do is break it apart into two pieces, again, so that you have the front and the rear portion, and then you have your letter portion in between. One important detail that this glyph exemplifies is the dual opacities that we use in some of our glyphs. In this case, the letter is set to 80% opacity in order to make it secondary to the envelope, itself, being opened, which is what we're trying to communicate. And the opposite is true in Light Mode also, where we do use 80% opacities, in some cases, also. So, quickly wrapping up what we've covered so far. Dark interfaces are cool, so cool that you're probably going to want to make your app potentially dark all the time, but unless your app is a creative or media-centric app, you should reconsider that, and, instead make your app follow what the system appearance is set to, instead. Don't limit your app design to just inverting your existing light appearance, and, instead, take a look at what elements are in your app on a case-by-case basis, and try and determine what are the visual cues that are trying to be communicated by the controls, and then determine whether an opposite visual cue is necessary or not. Then take a look at what type of content your app produces or displays, and then use that as a guidance to determine what type of method to use for how to treat your content area-- whether to go fully dark, whether to make it stay as is, or whether to make it a user option via the app settings. Then, taking a look at colors, generally, you want to make sure that you're using the three colors that we went over, appropriately, if you have controls that are text controls, try using text link if it's applicable. If you have controls that you want to follow the system accent color appearance, set those to control accent color, and if you have a control that has to stay blue all the time, adopt system blue color. And then Taylor and Rachel are going to go into a lot more detail about color uses, so definitely pay attention to their section. When constructing your vibrancy areas, make sure that you're constructing them in a way that takes full advantage of the vibrant effect blending, so make sure that you're using grayscale colors that are opaque. And if you have areas that need to be knockouts or have opacities, ramp towards black in Dark Mode or ramp towards white in Light Mode. And then, lastly, make sure that the glyphs that you're using are artistically inverted in order to showcase the intent that the glyphs are trying to communicate to the user as effectively as possible. Finally, make sure to go to the Human Interface Guidelines to see more design guidance. And then as well as checking the design resources section, where, by the end of the month, we're going to be making available updated Photoshop, and sketch templates, and kits for you to use. And with that, I'd like to hand it off to Rachel to go over some details on how to adopt and support Dark Mode. Thanks Thank you, Raymond. Hello everybody. Let's take a moment just to thank Raymond and the rest of the design team for designing Dark Mode. Isn't it awesome? Now Taylor and I will take you through the engineering basics of adopting Dark Mode in macOS Mojave. A while back, Taylor and I were chatting, and we discovered that we were both huge fans of chameleons. And while we don't yet own our own pet chameleons, we thought we'd be prepared by writing an app called Chameleon Wrangler. We did this back in High Sierra, and so we thought, "Hey, let's update it for Mojave." The first thing we did was put our system in Dark Mode and launch our app from the dock. And it didn't really look too dark. In fact, it's exactly the same between light and dark. And you'll note throughout this portion of the talk that on the left, when we want to compare, we'll have the light on the left and the dark on the right, so you can compare them side by side. Can't actually run the system this way. It's just for show. So, what we needed to do was link against the 10.14 SDK and Xcode 10. We built and ran our app, and voila. It's a lot darker. Comparing with light, you can see a lot of things are different and darker, automatically, but there's still some things we'd like to fix up. So, we'll go into detail about some of the colors. There's a lot of colors. Let's take a look at this header area that's light green. We implemented this using a hardcoded color, where we specified our custom RGBA values. The problem with this approach is that it doesn't change between the different appearances. You're stuck with the same color no matter what the system preference setting is. Instead of doing it this way, we're going to take advantage of colors in asset catalogs. You've been able to do this since 10.13-- have colors in asset catalogs, but now you can specify colors for different appearances. So, we've added a dark version of our header color. Asset catalog colors have several advantages. You can give them meaningful names that describe where the color is used or how it is used, and those are often called semantic names. You can have multiple definitions of the color, as I showed you, with the light and dark versions and also for different color spaces-- sRGB versus P3. And it gives you a central location for defining all of your colors. That makes it easy to go in and tweak your colors without changing any code if needed. So, here's our color now in a asset catalog, and it's called headerColor, because it's the header color. And let's see how it looks now, using headerColor. Looks a lot better and dark with that dark green. But something becomes obvious, and that's the text. Chloe's name and age are hard to read now against the darker green. Things looked okay in light, but they don't look good in dark. Well, there, we were using black and dark gray, and those are static colors, also. Not exactly the same as hardcoded, because we're using available colors, but they don't change. So, instead of using asset catalogs this time, we're going to take advantage of dynamic system colors. NSColor has many dynamic colors available for you to use with definitions across the different appearances. And a special thing about NSColor is that the colors are resolved at draw time, which means that the actual RGBA values that are used depend on the circumstances at the time that the color is drawn. I'm going to go through some examples of dynamic system colors now. Starting with the label colors that Raymond talked about earlier, and I'm showing the dynamic colors on the top half and some corresponding static colors just using black in varying levels of opacity on the bottom half. And, as you can see, in the light appearance, everything looks fine. But over in dark, labelColor looks great still, but the black colors disappear against the dark background. Here's an example in the system in Mail of where labelColor and secondaryLabelColor are used. The more important text is given the labelColor, and the less important text is secondaryLabelColor. To find out which colors are dynamic, in the Interface Builder popup menu for colors, it'll show you the list, and they've done a really nice job of making those be the dynamic colors. So, now switching, once we know that, black color and dark gray to labelColor and secondaryLabelColor, this fixes our text, and it looks great in both light and dark. A little bit more about dynamic system colors. As Raymond showed you, there are a bunch of colorful system colors-- systemRed, systemYellow, systemGreen are just three examples. And they subtly change for the different appearances so that they look great in both places, as opposed to the static, plain red, yellow, green colors. Another special thing about using systemRed, etc., is that they will match other uses of systemRed throughout the OS as well as matching on iOS. So, it's great to stick to these kinds of colors when you have an informational content in your color. It needs to be red or yellow for a warning, for example. Raymond also talked about linkColor, systemBlue, and controlAccentColor, so I wanted to talk about that again. LinkColor and systemBlue are very similar but slightly different and use linkColor for actual links to webpages or for going someplace in your app, whereas systemBlue is used for standard actions in your application, or you might even use controlAccentColor if you want to match the system preference, in this case, purple. You may search through your application code and find all the places you're using color and fix it all up and then still end up with a situation like this, where, for some reason, even though your text switched color, there's something behind it that's still drawing wrong. And if you're stuck and can't figure out what to do, I suggest taking advantage of Xcode's View Debugger. I'm just going to cover this briefly here, and the advanced Dark Mode session will go into more detail about debugging tips and tricks. So, I can see that light view is actually my scroll view, and I had set in the interface builder to a custom gray color. That's why I couldn't find it searching through my code. So, I switched it to the default background color, and now everything looks great. And while we're here on this text view, I'd like to go through some of the colors in play here that are dynamic system colors that also may be useful to you. First, there's textColor, which is switching between black and white; textBackgroundColors switches from white to black; selectedTextColor goes from black to white; and then selectedTextBackgroundColor, which is blue. But you can see it's two different versions of blue, and this is special, because it follows the system preference for the highlight color. And you can see, when the highlight color changes, the selectedTextBackgroundColor changes with it. Now, I'd like to move along and talk about accessibility. In the Accessibility display preferences, there's the Increase contrast checkbox. If we turn on Increase contrast mode, you'll see that colors and artwork throughout the system change to make the UI easier to see for people to see with low vision. Well, now in asset catalogs, we can add colors specially for the high contrast appearances as well. So, here I've added to my header color a version for high contrast light and high contrast dark. And this is how Chameleon Wrangler now looks in the dark high contrast appearance. You'll notice that not only is my header color using that special color that I defined, the rest of the system colors are also specially defined for the high contrast appearances. And this is true for light high contrast, as well. Now I'd like to move along and talk about images. We'll take a look at the toolbar buttons here. This chameleon color picker button looks pretty good in light, but in dark, that dark outline disappears against the dark background of the button. So, we'd like to have a special version of it for the dark appearance, and, once again, we turn to our asset catalog friend and add a version for the dark appearance. And now it looks much better against the dark button with the light outline. That's great for full color images. How about these other two buttons? Well, they already look pretty good. We didn't have to do anything, so why? Why are these right? Well, that's because we had made these images template images. To tell you a little bit about template images, in case you don't already know about them, it's important to understand that the transparency of each pixel is maintained, but the color is ignored. You can make the starting image any color you like, that's ignored. It's only the transparency. They can be bitmaps or PDFs. And here's an example from the OS of the speaker volume image. The original image is black, and then it has this partially transparent section for the larger soundwaves. And if we put this image into an NSImageView, it's automatically given the Secondary Label Color, and you can see in light, it's a medium gray, and in dark, it's a lighter gray. And then the partially transparent areas have less contrast against the background. Another advantage of using template images is that they automatically pick up on the button state. When a button is disabled, the images are dimmer, and that's handled for you if you use template images. Okay. So, Chameleon Wrangler is now looking pretty good. We got our colors sorted out. We got some of our images sorted out, but there's more to the application than what you see here, so I'd like to invite Taylor to the stage to tell you about the rest of it. Thank you very much. Thank you Rachel. Now that Rachel's covered a lot of the fundamentals of having our colors and images react appropriately for Dark Mode, let's first take a look at applying some of those to some more detailed areas of our application. First up is Moodometer, where we can keep track of Chloe's mood shifts throughout the day. You can see here that the popover's background has automatically adjusted for the Dark Mode. However, the button content is way too dark, still, on top of the background. Taking a look at our Asa Catalog, we can see that we've prebuilt these images with the color defined in the asset themself. So, we can use the technique that Rachel showed us, where we add a dark variant for those images with a lighter color. However, it's kind of unfortunate that we're only varying the fill color of these images but keeping the same shape. So, there's probably a better way, and, in Mojave, there is, using this contentTintColor API. With this, you can create a template image and provide a custom tint color to be used instead of the default appearance sensitive one. That will be used as the base fill color for any of the places that the image appears. So, we can set the content tint color on an NSImageView to tint its image or on an NSButton to tint both its boot image and title. So, let's take a look at adopting this. The first step here is to convert these to simple template images and remember to set the render mode to be the template effect. Taking a look at Chameleon Wrangler now, we can see that these images are now getting the default templating effect, which looks great in Dark Mode, but the next step is going to be applying that color. So, we could just pull the color out of the asset, add a dark variant, and set the contentTintColor on these buttons to be that named color. And this button will automatically pick the right variant based on whether it's appearing in the light appearance or dark appearance. However, Rachel also showed us how there are a number of colorful system colors that already have these light and dark variants, which we could use instead. For instance, we could use systemRed, systemYellow, and systemGreen, which, if we return back to Chameleon Wrangler, we can see looks really great and has those buttons pop on top of that background. What's really cool about this combination is that not only do we get a very specific green color for both Light and Dark Mode, but with the content tint API, the system effects applied on top of that vary based on appearance as well. As Raymond discussed, in the light appearance, there's a darkening effect on top of that base tint color. Meanwhile, in the dark appearance, there's a brightening effect. All of these effects are further customized for the increased contrast setting without any changes from our application. So, it's really great that we didn't have to specify all 12 of these colors. And instead, we specified a single one and, thanks to dynamic system colors and dynamic system effects, we got all of this completely automatically. So, our Moodometer is looking great. Next, we can step over to another type of app where we keep track of very important information about Chloe, such as how stressed she's feeling or more importantly, what's her favorite type of berry? We can see here that this already looks great, because we are using stock Cocoa controls, which automatically adjust for Dark Mode. We can see how it reacts to the accent color feature that Raymond talked about by opening up system preferences, changing the color to a nice chameleon striped yellow, and returning back to Chameleon Wrangler. We can, again, see that the built-in controls automatically adjust to that yellow accent color, including the focus ring and highlight color. However, we can notice that our custom tab bar has remained blue, even though we've changed the accent color. Because this blue doesn't really have any significance for our application, we probably want it to be following that accent color. Like Rachel talked about, we have a few different colors that will follow the users' accent and highlight color selection. New in Mojave is a control accent color, which you can use to directly mimic the accented areas on system controls, such as the sides of popup buttons, default push buttons, or the selected segment of a segmented control. There's also selected content background color, which is what you see in the background of selected table rows, and you can use to indicate custom selection. And selected text background color is exactly what it sounds like. So, going back to our app, we can see that we're using system blue color for this custom tab selection, and, instead, we can just swap that over to using controlAccentColor. The next problem here with the tab bar is a little more subtle. If we bring our cursor over and press down on it, we can see that we're getting a darkening effect instead of the brightening effect that we should in Dark Mode. This is because we are taking that base color and just applying a constant 30% darkening effect to it, assuming that would look fine in the light appearance. Instead, we want to describe something that basically this color is being pressed, and that semantic description can carry whatever effect it needs to in the light versus dark appearance. So, with the new system effect API in Mojave on NSColor, we can describe that exact semantic. We can take our base color of controlAccentColor, say that we want to get the pressed variant, and it will react appropriately in both the light and dark appearance. There's one more area of our application where you can see the accent color, and that's in this photo browser where we draw a selection ring around the selected photo. Here, we were using the selectedContentBackgroundColor, and that, again, automatically adjusts based on the user's selected accent. Let's open up one of these photos and take a look at a nice, big, beautiful photo of Chloe. The first thing that stands out is the fact that the background has not adjusted for Dark Mode. It's still using its light translucency effect similar to a hardcoded color. This effect is provided by NSVisualEfffectView, which provides the translucent blur materials you see across the system. For instance, there's the sidebars which blur onto the desktop and other windows behind it, as well as things like the title bar, which blur the content that's scrolled underneath. NSVisualEffectView also enables the vibrant foreground blending, that Raymond talked about, by taking a grayscale color plus a Blend Mode and making it really pop on top of these backgrounds. The advanced Dark Mode talk is going to go into a lot more detail about how you can use those blend modes effectively. Turns out that a lot of places automatically get these blur materials for free. For instance, every titlebar of an NSWindow will automatically have a titlebar material showing onto the content behind it. Source list table views and sidebar style split-view items will automatically get a sidebar material showing onto the content behind the window. For custom UIs it's pretty easy to build these custom effects ourselves, as well. But the most important quality of them are the material that you choose to have them represent. We have a few nonsemantic materials, which, like hardcoded colors, just describe the exact visual quality of the material and not the intent of what you're trying to represent. These will not be adjusted for Dark Mode and will stay with that exact quality. However, we have a number of semantic materials, as well, which allow you to describe what type of UI your custom UI is intending to represent. For instance, for a custom menu like UI, you would want to use the menu material. And these not only adjust for Dark Mode, but guarantee that UI to always match those materials even when the system changes in the future. We've had a few of these in the past, but in Mojave, we have several more so that you can make sure to really tailor your custom UI to match the material it needs to. Jumping back to our photo here, we can see that we were using the mediumLight material, because we were trying to represent Quick Look style popover windows. And instead, we can just simply swap that over to the popover material and get the exact effect we want to in Dark Mode. Now, this isn't the only area where we get a unique material showing in the background of our UI. Like Raymond talked about, there's this desktop tinting effect that's shown here in the background of our photo gallery. We're getting a little bit of that blue from that desktop behind it, creating this really visually harmonious result. It's a lot easier to see on top of a more vibrant background, such as these beautiful flower pictures. There's three different variants that have semantics with these background materials. There's the window backgrounds that we see on the backgrounds of generic windows; Under Page Background, which appears behind document areas such as in Preview or Keynote. And perhaps most commonly is the Content Background material, which is white in light appearance and, of course, dark in the dark appearance. In fact, all of these are now dark in the dark appearance but also have that wallpaper tinting effect. The great news is that it's really not hard to get these effects. For the most part, it comes completely automatically by using system views. For instance, the default backgrounds of every window has that window background material built right in. In addition, [inaudible] views such as NSCollectionView or NSTableView also automatically come with the content background material built right in. We've seen a number of places across this system that were actually able to improve their Dark Mode support by deleting an override of these default colors that these views have. If you want to customize beyond the default color that these views have, you can explicitly set a variety of system colors as their background color to get the exact material effect you want. One really important note here, though, is that if you were to read the RGB values of these colors, they would not include the desktop tint. This is because, as Raymond mentioned, this tint is extremely dynamic. As you move your window around on screen, that's getting updated asynchronously to the applications rendering itself, creating a really nice experience as the user is moving around windows. So, jumping back to Chameleon Wrangler, we automatically got this content background because we just use a stock NSCollectionView. In the vital section, we got that window background material showing through because we had nothing opaque covering it up. However, in our notes section, where we have this awesome custom stationary, we have a light grey background that isn't appropriate for Dark Mode and hasn't been adjusted with that under page semantic. We can open up Interface Builder and see where we're setting up this background drawing view and see that, indeed, we are filing it with a custom gray color. And pretty quickly in IB, we can change that to using the Under Page Background color that represents that semantic and immediately see that we'll get the right result, making it look just at home with the rest of the document apps in Dark Mode. The next thing that kind of stands out here, though, is the dark foreground on top of light background of our stationary and document area. And because this is a WYSIWYG editor where we, as users of Chameleon Wrangler have complete control over the colors, images, and fonts seen within it, we expect this to stay the same as a light appearing document. However, there are a few subtle details that don't seem quite right. For instance, the text highlight color is still getting the dark variant of the text highlights, as well as the autocorrect bubbles have the dark appearance, which really clashes with the light appearance of our document. We, basically, want to have everything in this area revert back to the light appearance, and we can do that using NSAppearance. It turns out that NSAppearance is what's been providing the magic of having light versus dark variance automatically switch in Dark Mode this whole time. You can conceptualize NSAppearance as a little bundle containing all of the colors, materials, images, and control artwork that you see across the system. Now, when your application does something like request to draw with labelColor, what that does is it looks up in the current appearance for what the exact RGB values should be. The same is true for control artwork or the different material effects. Now, what's new in Mojave is the dark appearance, which our designers have meticulously crafted with redefining everything across the system. When the system is running in Dark Mode, what happens is that the dark appearance gets applied to your application instead of the light appearance. So, when labelColor goes to draw itself, it references the dark appearance as white value instead of the light appearance as darker value. The same is true for all of those materials. And these aren't the only materials in the system. Right, these aren't the only appearances in the system. There are also the high contrast appearances, which get used when the accessibility high contrast setting is turned on, these will get applied to your application instead. And there's actually even more. There are the vibrant appearances that, again, completely redefine all of these colors that appear in sidebars and titlebars. And the advanced Dark Mode talk is going to go into more detail about how you can effectively use the assets that come out of those vibrant appearances to really make them stand out. Now, by default, the appearance of your application will follow the system. However, you can explicitly override it by setting the appearance property to whatever you want. So, here we have Chameleon Wrangler in Light and Dark Mode, and we can see it's following the system, but as soon as we set its appearance to always be darkAqua, we can see it's always getting that dark appearance. The inverse is true by setting the explicitly light appearance, and, of course, setting it back to nil will have it return back to the default behavior of following the system. Now, we know that after a lot of you see your apps running in Dark Mode, you're going to be really tempted to have your app always be dark, because it's just going to look so good. But like Raymond mentioned, it's really important to keep in mind "When it is appropriate to have an always dark app?" This should be generally reserved for very media focused apps like QuickTime player or professional color-sensitive workflow apps, where it's important to eliminate all of the light behind the content. And, in general, it should be left up to a choice for the user. So, in general, this is the system preference. Some of you maybe already have a dark theme for your application controlled via app preference, so your first step would be to tie that back to the system preference so the people using the system can have a consistent experience. And in some ambiguous cases like Raymond mentioned, there may be some additional need for an in-app preference, such as in the mail case, but for the most part, the system preference should be exactly what users go for. If you do decide you need to have an always dark application, it's really important to not to use the Vibrant Dark appearance on the entire window, because that's not appropriate for the opaque window areas of the application and are really only intended for sidebars. Instead, you want to use the Dark Aqua appearance if you want to have that always dark look. In addition to be able to be set on the window as a whole, you can also set it on an individual window or view scoping, and that's inherited by all of the views contained within it. For instance, here, we're setting the sidebar to always have the darkAqua appearance if we want to experiment with something like that. And all of the text and images within that also get that dark styling, but we can also use this to fix the problem we had before with our text area. We can see here that we actually want this entire document area to remain having a light appearance, and so by setting the lightAqua appearance, we can make sure that the text highlighting color gets the correct appearance as well as the autocorrect bubble inheriting that light appearance as well. However, in addition to be able to be used for some really cool effects, it can also lead to some hard-to-debug situations. For instance, here, we're using the vibrantLight appearance explicitly on our table view, and that looks perfectly fine in the light appearance, where it's actually already inheriting the vibrantLight appearance. In Dark Mode, this quickly becomes a very visible problem. We can see that the text and images inside that table view are now inheriting that vibrantLight appearance and look bad on top of that dark background. So, for the most part, we just want to remove these explicit overrides in places where we really do not intend to have an always light appearance. This can also happen in Interface Builder, where, at the very bottom of the Attributes Inspector, we've accidentally overridden the appearance for our outline view to always be the light Aqua appearance. Here, the fix is simply to change it back to the inherited style so that it can make sure to follow Dark Mode when that's turned on, so do be sure to check for all of those. At this point, Chameleon Wrangler's looking really great in Dark Mode. But kind of an understated thing here is that, at this point, we've written 0 lines of code specific to Dark Mode. The types of changes necessary for being a great Dark Mode citizen is that you need to make sure to express the ideas semantically rather than literally. So, for instance, these same sets of changes allowed our application to not only look great in Dark Mode but also in the light appearance. Adding support for things like the accent color as well as making our label colors more consistent with the rest of the system. Further, these same sets of changes needed to be great in Dark Mode are also the same sets of changes needed to have great support for the high contrast accessibility setting. So, to reiterate what those steps were, the first and foremost was linking on the 10.14 SDK using Xcode 10. For a handful of apps that already use system controls and system colors, this is the only step that's going to be needed, and it's already going to look great in Dark Mode. For the rest of us, the next step is to audit our app for the use of static colors, non-template images, and nonsemantic materials, and replace those as appropriate. For instance, this might be using a system color, a semantic material, or, in some cases, defining a custom asset catalog color or image. And, again, it's important to keep an eye out for those accidental appearance overrides. The advanced Dark Mode talk is going to go into a lot more detail here, including how to correctly use NSAppearance, how to do custom drawing and effects within NSVisualEffectView, how to make sure all of the myriad of ways you might be drawing custom views work correctly in Dark Mode. Some tips and tricks for using View Debugger and Interface Builder to really polish your app. And because we know that a lot of you and your apps' fans are really going to be wanting to have Dark Mode as soon as possible, we want to make sure you can do that without sacrificing how far you can back deploy. So, in general, a lot of these features are available for a year or more, but the advanced talk is going to go into details about how to take that even further back. We have a few Cocoa Labs, but I also really want to point out the Dark Mode Lab, where, in addition to engineering support, Raymond and other members of the design team will be there to help answer any design questions you might have, as well. So, that's a really special treat that I encourage you to take a look at. Thanks for staying until 6:00 today, and have a great rest of the WWDC.  Good morning. Welcome to Session 204, Automatic Strong Passwords and Security Code Autofill. My name is Chelsea. You may have seen Automatic Strong Passwords and Security Code Autofill in the State of the Union, yesterday. I'm really excited to tell you more about these features, today. These new features and other features in iOS 12 will help users log into and create accounts with ease in your app. If you have a login screen or an account creation screen in your app, this session is for you. A major pain point for users is dealing with passwords. They can be a pain point, even for people that use the best practice, which is to use a password manager to create and fill passwords for them. For everyone else it can be tempting to do something easy but insecure, like reuse a password they're already using on another service or use an easy to guess password that they can memorize. We know that the most private secure thing for your users is to use strong unique passwords for each service that they use. We've all heard of breaches in services that result in users' passwords being exposed. Users then need to go and change their password on every service where they were using it. The features that we'll discuss today will help users choose strong unique passwords for your apps. This way, you both help users that are trying to consciously use best password practices, as well as your users that would rather not think about passwords. Many of your users use the iCloud Keychain. The features that we're going to talk about today are built on top of the Keychain. It has best in class security and it protects users' passwords behind biometric authentication. Apple does not have access to the credentials stored in Keychain, so users' privacy is preserved, as well. The iCloud Password Keychain Manager can help users log into and create accounts in your app. We've added some new features on iOS 12 to help make account creation and login even easier. I'm really excited to show you these features with a demo. So, I'm going to go to the Shiny app. This is my favorite demo app for creating an account and logging back into it. So, I'm going to go ahead and create-- and tap create account. I'll focus the email field and you'll see that I've been given a suggested username. This is a new feature on iOS 12. These suggested usernames are based on credentials that the user already has stored in the Keychain. Since I always use Chelsea@example.com, I'm going to select that as my username. Without another tap, the password field is focused for me and a strong unique password has been provided. I'm also told that I can go and look up this password in Settings at any time. So, if I need to go and type it onto another device where iCloud Keychain is available, it's available to me. So, I'm going to go ahead and use this strong password and sign up. So, with just a couple of taps I'm logged back into the Shiny, or I'm signed up for the Shine app. Now, let's fast forward through all of the other account setup. I've turned on second factor authentication. And let's look at what logging back into Shiny looks like. So, I'm going to go ahead and tap the email field. As you can see, the credential I just created is suggested right on the QuickType bar. Going to select that. And then, after Face ID, my password and username are filled and I can log in. Now, I've set up second factor authentication. Normally, I'd need to try to memorize this code or go back to Messages. But as you can see, the code that I just received for second factor authentication is right on the QuickType bar. So, with one tap I can fill the code. Yeah. It's really awesome. So, I can fill that code, submit, and again, with a minimal number of taps I'm logged back into Shiny. Back to the slides. So, as you saw, creating an account with Automatic Strong Passwords is quick and easy. I didn't think about the password, since one was provided for me and inserted into the password field. I also didn't think about the password when logging back into Shiny. Passwords have truly become an implementation detail to logging into my app with Automatic Strong Passwords and Password Autofill. Here's today's agenda. First, we're going to do a quick recap of Password Autofill, since many of the features that we'll talk about today are built upon that feature. Next, we'll talk about how to ensure that Automatic Strong Passwords works in your app. Then, we'll talk about Security Code Autofill. Next, we'll talk about federated authentication with a third-party service. And finally, we'll discuss some new password management features that the iCloud Password Keychain Manager provides. In iOS 11, we introduced Password Autofill for apps. Some of the new features that we're going to discuss today are built upon the same adoption you may have done for that feature. Password Autofill helps users log into your app by surfacing credentials right on the QuickType bar. Here's a quick recap of how to ensure that it works in your app. Passwords in the iCloud Keychain Password Manager are stored based on domains on the Web. Like Apple.com. Thus, it's important to have an association between your app and the domain on the Web. This way, we're able to confidently surface credentials on the QuickType bar. You already have this association if you've adopted Universal Links or Handoff. The process of adding this association is pretty simple. You'll have a small change to your apps Entitlements file and you'll serve a file from your domain on the Web. If you'd like to see an in-depth look at how to set this up, see Introducing Password Autofill for Apps from WWDC 2017. It's important to always tag your fields with text content type, so that Autofill can be offered at the right place. Tag your username fields with the username content type. Tag fields where users are going to be filling, or passwords for existing accounts with a password content type. If you've chosen not to use Secure Text Entry for your password fields, it's particularly important that you tag your fields with the password content type. This way, we know we're in a password context. Now, that we've discussed how to get Password Autofill working in your app, let's talk about some improvements that it has. Since, iOS 11.3, WKWebView supports Password Autofill. This helps your users if your login screen is implemented using Web technologies. New to iOS 12, password managers from the app store can provide information to Autofill. This means that any work you do to support the iCloud Keychain Password Manager filling credentials in your app, also, helps users of these other password managers, as well. If you're a developer of a password manager, see Implementing Autofill Credential Provider Extensions. On iOS 12, we now offer to save credentials when a user logs into your app with a new account. This way, users can then use these credentials in your app and website on all of their devices. Let's talk about how to ensure users are prompted to save and update passwords in your app. Here's how saving works. First, Autofill infers that we're in a login scenario. Then, Autofill checks the eligibility of your app based on if there's an association between your app and the domain on the Web. Without this association saving passwords will not be offered. Next, it finds the username and password fields so that it knows which data to save in the new credential. Then, it detects that a sign in action has occurred. And finally, Autofill decided whether to prompt to save or update the password based on if this is as brand new credential that's not yet in the Keychain. Or if the user is updating an existing credential. Now, this may work with no changes to your app on iOS12. But let's talk about some steps you can take to ensure that it works. First, make sure to tag your fields with the username and password content types, like, with filling passwords. Make sure to remove your username and password fields from the view hierarchy when a sign in occurs. This way, Autofill can detect that a sign in is occurring. You could do this by dismissing the view controller that your sign in fields are in. Make sure to only clear the username and password fields after they've been removed from the view hierarchy. This way, we can read out the data and save it into credential. Make sure the Autofill saves the credential to the correct domain. You can do this by saving a password in your app, and then going to Settings to check where the credential is saved. If you notice that Autofill is not saving to the correct domain, you can override where it's saving using the Web credentials associated domain service. Finally, you may have previously been using SecAddSharedWebCredential to manually save credentials when a user logs into your app. However, now that Autofill automatically prompts users to save passwords you may no longer need this. You will still want to use this if your app has a web view for its login screen because saving is not supported there. So, here are some of the key steps that we've discussed so far to make sure that filling and saving passwords in your app works. Make sure to associate your app with the domain on the Web. Tag your fields with the username and password content type. And for saving passwords, ensure that login is detected by making sure that the saving prompt appears when you sign into your app. By ensuring that Password Autofill works in your app, you help your users log in with ease. For new users, one of the first encounters they'll have with your app is creating an account. In my demo you saw how Automatic Strong Passwords made this process super easy. I'd like to invite Reza to the stage to discuss how to ensure Automatic Strong Passwords works in your app. Thank you. Thank you, Chelsea. Hi, everyone. This is Reza and I'm really excited to tell you all about Automatic Strong Passwords. Account creation is a point of frustration for many users. They might even leave your app and never come back, or as many of them do, decide to use a weak password or keep reusing the same one. This significantly reduces their security. Although, it might be an alternative to account creation, some of you might have concerns about allowing social media to be used to sign into your apps. Automatic Strong Passwords makes account creation just as easy. It brings convenience and security to the signup process. Users no longer need to think about or worry about passwords. Autofill even suggests usernames to ease the signup process. With Automatic Strong Passwords, account creation is only a few taps away. So, users are more likely to sign up and user your services. Now, I'd like to talk about how Automatic Strong Passwords works. Similar to what Chelsea explained in a login scenario, when your app presents a view controller Autofill infers its type. In this case, it's a signup view controller. It will then check the eligibility of your-- it will then check the eligibility of your app based on the associated domains to figure out if it can save passwords. If that's the case, Autofill will then detect relevant signup form elements; the username and the password. Once the username field become first responder Autofill suggests usernames. This is a new feature we're adding in iOS 12. The user proceeds with the suggested username. And eventually, the password field becomes first responder. Autofill automatically inserts a strong password into the password field. At this point, the user only needs to proceed with the suggested password and sign up. Autofill takes care of saving the password. In many cases, this happens automatically without any adoption requirements in your apps. However, in order to ensure your app's compatibility with Automatic Strong Passwords there are a certain number of steps you should take. Many of these steps are identical to those Chelsea explained to make your apps compatible with saving. Make sure to tag your username field with UI textContentType username. New in iOS 12, make sure to tag your new password and confirm new password fields with UI textContentType newPassword. If you're using a UI table view to represent your signup form make sure to use unique instances of UITextField for the username and the password field. This is important, because once Autofill detects username and password fields, it expects to be able to reliably read their values later on. Some of you might have changed password forms in your apps. Automatic Strong Passwords is compatible with change password forms if Autofill is able to detect username and password fields on the same screen. Note that the username field can be Read Only. Best practices that we discussed for signup forms, also, apply to change password forms. Now, let's take a moment and talk about the format of these generated passwords. The generated passwords are 20 characters long. They contain uppercase, digits, hyphen, and lowercase characters. This gives you a strong password with more than 71 bits of entropy. We designed this to be a strong, yet compatible with most services. It is, of course, possible for your apps to define their own custom password rules. As I mentioned before, in most cases, you don't need to do this because the default format of Automatic Strong Passwords should be compatible. However, if your app's backend requires a separate set of rules that are not compatible with the default format of Automatic Strong Passwords, you could define your own rules. To do so, use the new password rules language in iOS 12. Following the format of the password rules language, create a rulesDescriptor. Using the rulesDescriptor create an instance of UITextInputPasswordRules and assign it to the password rules property of UITextField. Once you do this, Automatic Strong Passwords will generate passwords based on these rules. We have also created a new web-based Password Rules Validation Tool. Use this tool to ensure the rules that you specify are correct and produce the type of passwords that you expect. I'll talk more about this, shortly. Now, that we talked about the steps it would take to make your apps compatible with Automatic Strong Passwords, I'd like to show you a demo. I'm the developer of the Shiny app and I want to make sure that Shiny is compatible with Automatic Strong Passwords in iOS 12. So, the first thing to do is to run Shiny using the iOS 12 SDK and try it out. So, in Xcode I'll click on Run. And here is Shiny. I'll tap on Create Account. Tap on Email. And I don't see any suggested username. When I tap on the Password field I don't see any Automatic Strong Passwords suggestion, either. So, let's go back to Xcode and investigate this, further. The first thing to make sure of is associated domains for your apps. In this case, I've already ensure that Shiny has an associated domain. In fact, earlier today I was able to get Autofill suggestions when I wanted to logging into Shiny. Next, you should take a look. You should take a closer look at your signup view controller. I'll click on the Email field. And here, under Text Input Traits, Content Type, I see that I'm correctly setting the content type to username. And because I want the format of this username to be of type email address, I'm also correctly setting the Keyboard Type to Email Address. So, this is good. Let's take a closer look at the Password field. I see that the Content Type is set to Unspecified. This should, actually, be set to New Password. So, I'll select New Password, here. And because this is a password field, I'm going to mark it as Secure Text Entry. All right. Let's run Shiny again and try it out. :15 Okay. Tap on Create Account. Tap on Email. Here, I see I'm getting suggested username, now. I'm going to go ahead and proceed with the suggested username. I also, see Automatic Strong Passwords. So, in most cases, this should be enough and you should be done. Let's proceed with the signup process. For the purpose of this demo, I'm requiring Shiny passwords to contain a dollar sign. This means that we need to specify our own custom password rules for Automatic Strong Passwords. And actually, the best way to do so is using the new Password Rules Validation Tool. Using the tool, I will be able to specify correct password rules, and also, ensure the rule will produce the type of password that I expect. So, let's jump right in. This is the new Password Rules Validation Tool that you can have access from developer.apple.com. For the purpose of this demo, I'm allowing uppercase, lowercase, digit, and requiring at least a dollar sign to be present in my password. At the bottom of the page I see some examples of the password generated. I also, have the option to download a number of these passwords in case I need to run some tests in my app's backend. Once I'm happy with the formatting of the passwords I have two options. If I'm making a native app for UIKit, I can go ahead and copy the rules formatter for UIKit. Or if I'm making a webpage I can copy the HTML formatted rules. For this demo, since we are making a native app for UIKit, I'm going to go ahead and copy the UIKit version. And once I've copied it, all I need to do is go back in Xcode, select the password, and paste in the rule-- in the Password Rule text field. Now, let's run Shiny, again. Okay. I'll tap on Create Account. Tap on. I'm going to go ahead and proceed with the suggested username. And at this point, the password should be compatible with the rules that I just specified. Let's proceed with the signup. And just like that, I'm in. Thank you. We just saw how easy it is to make your apps compatible with Automatic Strong Passwords. Than you, Reza. As Reza showed you, these are some steps you can take to ensure that Automatic Strong Passwords works in your app. Make sure to associate your app with the domain on the Web, just like filling and saving passwords. Make sure to tag your fields with the username and new password content types. Ensure that signup is detected by checking that when you sign up for your app the password is saved. Most of you will be done after doing these three things. However, if your service is not compatible with Apple's password generation format you may want to use some password rules on your password text field. To ensure that Automatic Strong Passwords generates a compatible password. By ensuring that Automatic Strong Passwords works well in your app, you're setting your users up for success by encouraging them to use a strong unique password in your app. Some of your apps and services may use SMS, or security code sent via SMS to your users. I'd like to invite Harris to the stage to discuss how you can service these codes right in your app. Awesome. Thank you, very much, Chelsea. Hello, everyone. My name is Harris and I am super stoked to talk to you about Security Code Autofill. But first, this section needs a little bit of audience participation. So, show of hands, how many of you have ever gotten a text message that looks something like this? All right. I think it's fair to say that most of us have to deal with these codes on a fairly regular basis, these days. All right. Question of the second, after getting one of these text messages how many of you go through a process like this? Where you think you have the code memorized, you type it in, you switch the last two digits around, and then you have to do it all over, again. Let me see those hands. All right. Final question. After going through this process, how many of you feel something like this? All right. So, let me tell you it doesn't have to be this way. Because starting in iOS 12 and macOS Mojave, we're introducing Security Code Autofill. A brand new feature which takes all the pain and frustration out of message based second factor flows. Now, it is important to note that Security Code Autofill does not alter the calculus around the security of second factor methods. What it merely does is remove the indignity out of having to type the code in yourselves in the age of computers. Nothing more, nothing less. So, starting in the new iOS and macOS, instead of exercising your memorizing skills you get to do something like this. Just one tap, right on the QuickType bar, and you're signed in. And hopefully, at the end of this process, you feel a little bit more like this. All right. So, let's talk about some technical details. As with other Autofill features, Security Code Autofill will work in your apps right out of the box, most of the time. Still, you can do just a tiny bit of work to ensure that Security Code Autofill works every single time. We've told you this before but tag your fields. In iOS 12 we're introducing a new UITextContentType with the value oneTimeCode. Tag your security code fields with this and Security Code Autofill will work every time. Additionally, it is very important that you allow usage of the system keyboard for inputting these codes. Tempting as it may be, you should refrain from building Bespoke keyboard UI's within your view hierarchy or setting custom input views in your controls. Because if you do so you are preventing iOS from surfacing the necessary UI or injecting the appropriate events to type the code in on behalf of your users. And additionally, you may be hampering the experience of folks that leverage accessibility technologies like voiceover. So, bottom line is let the system keyboard do the heavy lifting for you and Security Code Autofill will work every time. Another step that you can take to ensure that Security Code Autofill works great with your service is to craft your text message, accordingly. iOS and macOS use data detector heuristics to infer that an incoming message carries a security code. Specifically, our heuristics are looking for words like code or passcode in proximity to the actual code string. And you can see a few sample text messages in different languages in the slide behind me. Now, verifying that your specially crafted message properly triggers the iOS and macOS heuristics is super simple. You can be your own best friends and simply text yourselves. Then, when you go into the message transcript and you see the code being underlined and tapping the code offers you a Copy Code option, you know you have it right. Security Code Autofill is available in all supported locales of iOS and macOS. And if after following these best practices you find that you cannot get Security Code Autofill to trigger appropriately inside of your app or website, please let us know by filing a bug report. We'll be looking out for them. So, so far, we've talked about getting Security Code Autofill to work within your native iOS apps. Security Code Autofill is also available in iOS Safari on websites and, and this is pretty cool, if your users are signing into your service in Safari on their Mac, text message forwarding securely pushes incoming messages from their iPhone to their Mac. Now, what this means is that they can fill in a security code with a single click in Safari's completion UI. You no longer have to switch back to Messages to copy the code out or, you know, go find your iPhone all the way across your home. It's really cool. To support Security Code Autofill in Safari, we are introducing a new input autocomplete attribute value one-time-code. In fact, all of the UI Text Content Types that you heard about today have web platform equivalents that you can leverage to ensure a smooth seamless autofill experience in your websites, just as well as in your apps. You've heard this before but tag your form fields this time and Autofill will work great every time. So, this was a brief introduction to Security Code Autofill, a brand new addition to iOS 12 and macOS Mojave that takes all the pain and all the frustration out of message based second factor flows. Now, with the Autofill features that we've introduced today, it becomes super simple for you to build a relationship with your users while safeguarding their security and respecting their privacy. Still, some of you may find that you have to support federated authentication flows with third party providers, such as popular social networks. For those of you among the audience, we are introducing in iOS 12 a new API to support such flows. We call it ASWebAuthenticationSession and I would like to show you how it works. Now, here I am back in the Shiny app. And this time I will choose to log in with example.com, a super popular social network. Now, as soon as I tap the button the app under the hood calls in to the new ASWebAuthenticationSession API. Now, one of the ways in which ASWebAuthenticationSession makes logins faster is by sharing its underlying cookie storage with Safari. Now, of course, before we do any of this, iOS asks for explicit user consent. Upon consenting, the user is presented with a secure view controller and guided through the third party's federated authentication flow. In this case, I'm already signed into example.com in Safari on my iPhone, so all I have to do is tap Allow. There we go. And with just two taps I've gone through a full federated authentication flow with a third-party service. So, in the past you may have used one of the other iOS platform technologies to implement federated authentication inside of your apps. Starting with iOS 12, ASWebAuthenticationSession is the go-to way to implement such flows. As we mentioned, already, ASWebAuthenticationSession makes login flows faster by sharing its underlying cookie storage with Safari upon explicit user consent. Now, what this means is that depending on the implementation of the federated authentication flow by the third party. If your user is already signed into that service in Safari on their device, they may not even have to see a login form, at all. If they do have to see a login form ASWebAuthenticationSession supports Password Autofill and Security Code Autofill to make the experience as painless as possible. And for you developers, ASWebAuthenticationSession offers a straightforward block-based API that makes adoption super simple. So, let's take a look at it. Now, let's see how simple it is to implement federated authentication using ASWebAuthenticationSession. You're going to start off by importing the brand new AuthenticationServices framework. And then, you simply define the oauthURL as it's described to you by your third-party authentication provider. Now, following this you will create an ASWebAuthenticationSession object. You're passing the oauthURL and a callback handler that gets involved at the end of the authentication session. Now, it is crucial that you hold onto a strong reference to the authentication session object while the session is in flight. And by doing so, you also have the ability to cancel an ongoing session should you choose to do so. Finally, you call the nonblocking start method. Upon doing so, first iOS will ask for user consent to share the underlying Safari, the underlying cookie storage that Safari has with your session. And then, the user is presented with a secure view controller and guided through the authentication flow. When the user completes the authentication flow or if they cancel out of the authentication session, your completion handler gets involved and you can handle the result. And that's how simple it is to implement federated authentication in iOS 12, using ASWebAuthenticationSession. It only takes a few lines of code. Starting with iOS 12 this new API is the go-to way to implement federated authentication and it replaces SFAuthenticationSession. If you find that you have to support federated authentication inside of your apps, we strongly encourage you to adopt this new API. And with that, I'll turn things over back to Chelsea, to talk to you about a number of super cool new password management features that we built right into iOS 12 and macOS Mojave. Thank you, very much. Thanks, Harris. I'm really excited to tell you about some of the new features that the iCloud Keychain Password Manager gives you in iOS 12 and macOS Mojave. You may have seen some of these in the State of the Union, yesterday. What we've talked about, so far, is how you can help users log into and create accounts in your app. However, there are some cases when users will be interacting with accounts for your app when they're outside of the context of your app. For these cases we've added some features to the iCloud Keychain Password Manager to make managing and interacting with these accounts a breeze. If a user needs to look up a password, they can now simply ask Siri. They'll be taken right to their password after authentication. This way, they can read off their password to type into another device that doesn't have iCloud Keychain. Some users may share account information for services like utilities. In order to share this information, users can now AirDrop passwords to their contacts. On iOS 12 the password list UI is redesigned so it's more scannable and delightful to interact with. The password list UI in macOS has been redesigned, as well. And it looks great. Another new feature of the iCloud Keychain Password Manager is it now warns users if they're reusing passwords across multiple websites. When the user taps on one of these passwords they'll be given the option to change their password on the website. Automatic Strong Passwords on the web can help them update to using a strong unique password for that service. The Password Reuse Auditing feature is built right into the Safari Passwords List on macOS, as well. Last but not least, your tvOS apps can now offer to autofill from a nearby iOS device. In order, to learn more about this incredibly convenient feature see the Password Autofill section of the What's New in tvOS 12 session. In summary, Autofill is a powerful platform feature that helps users interact with accounts in your apps. As we discussed, the features that we talked about today may work with no changes to your app, however, it's important that you test your app with these new features. Apply the recommendations that we discussed today. If after applying these recommendations, Autofill is not working as you'd expect, please file a bug report. We'd love to hear from you. To use the Password Rules Validation Tool that Reza showed you during his demo, you can visit the URL listed here. Here are some related sessions you might be interested in. If you're a developer of a password manager, see Implementing Autofill Credential Provider Extensions. If you'd like to learn more about Password Autofill for apps, see Introducing Password Autofill for Apps from WWDC 2017. And if you'd like to talk to us or you have questions about any of the content we've covered today, come to the Safari, WebKit, and Password Autofill Lab after this session. We've also got a couple more, later in the week. Thank you, so much, for attending and have a great WWDC 2018.  Hello, everybody. Welcome to Image and Graphics Best Practices. My name's Kyle. I work on UIKit. And today, I'm going to be sharing with you some techniques and some strategies for efficiently working with graphical content in your applications. We're going to take a tour of the framework stack. First, we're going to start with UIImage and UIImageView, which are UIKit's high level tools for working with graphical content in your app. Then, we're going to focus on how you can best do custom drawing inside of your applications with UIKit. And finally, we're going to touch, briefly, on integrating advanced CPU and GPU technologies into your applications. And throughout this talk we're going to be focusing primarily on our use of two scarce resources on the device; memory and CPU. And we tend to think of these things as separate quantities. They have their own tracks in the debug navigator. They have their own instruments in the instruments application. But really, they're intricately linked. And it might be apparent that as your application uses more CPU, that has a negative impact on the battery life and the responsiveness of your application. But what might be less obvious is that as your application and other applications on the system consume more memory, that also causes more CPU utilization. Which, has further detrimental effects on battery life and performance. So, we're going to focus on how to improve our use of these resources. What better context for discussing this problem than an application that works pretty extensively with photographic content, like the Photos app? You see, we're editing a photo here. And as I mentioned previously, UIImages, UIKits, high level class for dealing with image data. So, we have a UIImage representing this rich content. And we tend to divide graphical content in our applications into two categories; rich content like this photograph and iconography. UIImage is also the data type in UIKit that we use to represent things like the icon displayed in this button. And as I mentioned previously, UIImageView is the class that UIKit provides for displaying a UIImage. Now, in classical MVC style UIImage can be thought of as a model object. And UIImageView, of course, as the name implies, is a view. And these objects in their roles as model and view, have traditional responsibilities. UIImage is responsible for loading image content. And UIImageView is responsible for displaying it, for rendering it. Now, we can think of this as a simple relationship that we establish once. It's a one-way relationship. Bu the actual story is a little bit more complicated. In addition to rendering being a continuous process, rather than a one-time event, there's this hidden phase. It's really important to understand in order to measure the performance of your application. And this phase is called decoding. But in order to discuss decoding, I first need to discuss a concept called a buffer. A buffer is just a contiguous region of memory. But we tend to use the term buffer when we're discussing memory that's composed of a sequence of elements of the same size, usually, of the same internal construction. And for our purposes, one really important kind of buffer is an image buffer. This is a term we use for buffer that holds the in-memory representation of some image. Each element of this buffer describes the color and transparency of a single pixel in our image. And consequently, the size of this buffer in memory is proportional to the size of the image that it contains. One particularly important example of a buffer is called the frame buffer. And the frame buffer is what holds the actual rendered output of your application. So, as your application updates its view hierarchy UIKit will render the application's window and all of its subviews into the frame buffer. And that frame buffer provides per pixel color information that the display hardware will read in order to illuminate the pixels on the display. Now, that last part happens at a fixed interval. It can happen at 60 fps. So, every 1/60th of a second. Or on an iPad with ProMotion Display, it can happen as fast as every 1/120th of a second. And if nothing's changed in your application, the display hardware will get the same data back out of the frame buffer that it saw, previously. But as you change the content of the views in your application, for example, you assign a new UIImage to our image view, here. UIKit will re-render your application's window into the frame buffer. And the next time the display hardware pulls from the frame buffer it'll get your new content. Now, you can contrast an image buffer to another kind of buffer, a data buffer, which is just a buffer that contains a sequence of bytes. In our case, we're concerned about data buffers that contain image files. Perhaps, we've downloaded them from the network or we've loaded them from disk. A data buffer that contains an image file, typically, begins with some metadata describing the size of the image that's stored in that data buffer. And then, contains the image data itself, which is encoded in some form like JPEG compression or PNG. Which means that the bytes subsequent to that metadata don't, actually, directly describe anything about the pixels in the image. So, we can take a deeper look at this pipeline that we've set up. We have a UIImageView here and we've highlighted the region of the frame buffer that will be populated by the image view's rendering. And we've assigned a UIImage to this image view. It's got a data buffer that represents the content of an image file. Perhaps, downloaded from the network or read from disk. But we need to be able to populate the frame buffer with per pixel data. So, in order to do that UIImage will allocate an image buffer whose size is equal to the size of the image that is contained in the data buffer. And perform an operation called decoding that will convert the JPEG or PNG or other encoded image data into per pixel image information. And then, depending on the content mode of our image view. When UIKit asks the image view to render it will copy and scale the image data from the image buffer as it copies it into the frame buffer. Now, that decoding phase can be CPU intensive, particularly, for large images. So, rather than do that work every time UIKit asks the image view to render, UIImage will hang onto that image buffer, so that it only does that work once. Consequently, your application, for every image that gets decoded, could have a persistent and large memory allocation hanging out. And this allocation, as I mentioned earlier, is proportional to the size of the input image. Not necessarily, the size of the image view that's actually rendered in the frame buffer. And this can have some pretty negative consequences on performance. The large allocation that is in your application's address space could force other related content apart from content that it wants to reference. This is called fragmentation. Eventually, if your application starts accumulating a lot of memory usage the operating system will step in and start transparently compressing the content of physical memory. Now, the CPU needs to be involved in this operation so in addition to any CPU usage in your own application. You could be increasing global CPU usage that you have no control over. Eventually, your application could start consuming so much physical memory that the OS needs to start terminating processes. And it'll start with background processes of low priority. And, eventually, if your application consumes enough memory, your application itself could get terminated. And some of those background processes are doing important work on behalf of the user. So, they might get started up again as soon as they get terminated. So, even though your application might only be consuming memory for a short period of time, it can have this really long-tail effect on CPU utilization. So, we want to reduce the amount of memory that our application uses. And we can get ahead of the curve with a technique called downsampling. Now, here we see a little bit more detail about our image rendering pipeline. Including the fact that the image view we're going to display our image in is actually smaller than the image we're going to display inside of it. Normally, the core animation framework would be responsible for shrinking that image down during the rendering phase, but we can save some memory by using this downsampling technique. And what we're going to do, essentially, is capture that shrinking operation into an object called the thumbnail. And we're going to wind up with a lower total memory usage, because we're going to have a smaller decoded image buffer. So, we set up an image source, create a thumbnail, and then capture that decoded image buffer into UIImage. And assign that UIImage to our image view. And then, we can discard the original data buffer that contained our image. And we're left with a much smaller long-term memory footprint for our application. The code to do that has a few steps. So, I'm going to walk you through them. I'm not going to do extremely low-level detail. But I'll highlight the important bits. First, we're going to create a CGImageSource object. And CGImageSourceCreate can take an option dictionary. And the important option we're going to pass here, is this ShouldCache flag. And this tells the Core Graphics framework that we're just creating an object to represent the information stored in the file at this URL. Don't go ahead and decode this image immediately. Just create an object that represents. We're going to need information from this URL. Then, we're going to calculate on the horizontal and vertical axis. Based on the scale that we're going and point size we're going to render at, which is the larger dimension in pixels. Calculate that information, and then create an options dictionary for our thumbnail. There are a couple of options listed here. You can look in the documentation for exactly what these options do. But the very important one is this CacheImmediately option. By passing this option here, we're telling Core Graphics that when I ask you to create the thumbnail that's the exact moment you should create the decoded image buffer for me. So, we have exact control over when we take that CPU hit for decoding. Then, we create the thumbnail, which is a CGImage, that we get back. Wrap that in the UIImage and return it from our helper function that we've written here. So, to give you an idea of the magnitude of savings that this technique gives us, we're just displaying the full screen image here. This is a photograph. It's 3,000 by 2,000 pixels. If we do no optimization, just throw UIImageView in the storyboard and assign our image to it, this application takes 31.5 megabytes just sitting doing nothing. Now, using this downsampling technique and only producing an image buffer that's the size of the actual display, we can get the memory usage of this application down to 18.4 megabytes. And that is a huge reduction in memory usage. Thanks for the applause, but you should all get the applause for implementing this technique in your applications. You can imagine how much of a big deal this is for an app that's displaying a lot of potentially large input images in a small space on screen. For example, the Camera Roll. You might implement such a view using UICollectionView. So, here we've implemented cell for item at indexPath. And we're using our helper function that we wrote earlier to downsample the images to the size that they're going to be displayed at when the cell is actually put on the screen. So, you think this is a pretty good thing to do, right? Like rather than having these large allocations hanging around, we're reducing our memory usage. Unfortunately, that doesn't save us from another problem that's common in scrollable views like table views and collection views. It's a, probably seen this before. You scroll through an application and it starts hitching as you scroll. What's happening here is that as we're scrolling the CPU is relatively idle, or the work that it does can be done before the display hardware needs the next copy of the frame buffer. So, we see fluid motion as the frame buffer is updated and the display hardware is able to get the new frame on time. But now, we're about to display another row of images. And we're about to ask Core Graphics to decode those images before we hand the cells back to UICollectionView. And that could take a lot of CPU time. So much so, that we don't get around to re-rendering the frame buffer. But the display hardware is operating on a fixed interval. So, from the user's perspective the application has just stuttered. Now, we're done decoding these images, we're able to provide those cells back to UICollectionView. And animation continues on, as before. Just saw a visual hitch, there. Now, in addition to the obvious responsiveness consequences of this behavior, there's a more subtle detrimental effect on battery life. Because iOS is very good at managing the power demand on the batter when there is a smooth constant demand on the CPUs. And what we have here are spikes. As new rows are about to come into view on the scroll view, we're spiking the CPU usage. And then, returning back down to a low level. So, there are two techniques we can use to smooth out our CPU usage. The first one is prefetching. And if you want to know a whole lot about prefetching check out the A Tour of CollectionView Talk from this year's WWDC. But the general ideas here, is that prefetching allows CollectionView to inform our data source that it doesn't need a cell right now, but it will in the very near future. So, if you have any work to do, maybe, you can get a head start. That allows us spread out CPU usage out over time. So, we've reduced the maximum size of the CPU usage. Another technique we can use is performing work in the background. So, now that we've spread out work over time we can, also, spread it out over available CPUs. The consequences of this are that your application is more responsive and the device has a longer battery life. So, to put this in action here, we've got a implementation of the prefetch method on our data source. And it's going to call our helper function to produce a downsampled version of the image that we're about to display in this CollectionView cell. And it does this by dispatching work to one of the global asynchronous queues. Great. Our work is happening in the background. This is what we wanted to do. But there is a potential flaw here. And it's a phenomenon that we like to call thread explosion. And this is what happens when we ask the system to do more work than there are CPUs available to do it. If we're going to display a whole number of images, like 6-8 images at a time, but we're running on a device that only has 2 CPUs, we can't do all of that work at once. We can't parallelize over CPUs that don't exist. Now, to avoid deadlock when we dispatch asynchronously to a global queue, GCD is going to create new threads to capture the work we're asking it to do. And then, the CPUs are going to spend a lot of time moving between those threads to try and make incremental progress on all of the work we asked the operating system to do for us. And switching between those threads, actually, has a pretty significant overhead. We'd do a lot better if one or more of the CPUs just got a chance to get images out the door. So, we're going to borrow a technique that was presented last year in the Modernizing Grand Central Dispatch Usage talk. And we're going to synchronize some work, or I'm sorry. Not synchronize, we're going to serialize some work. So, rather than simply dispatching work to one of the global asynchronous queues, we're going to create a serial queue. And inside of our implementation of the prefetch method we're going to asynchronously dispatch to that queue. Now, it does mean that an individual image might not start making progress until later than before. But it also means that the CPU is going to spend less time switching between bits of work that it can do. Now, these images that we're displaying can come from a number of places. They might come with our application, in which case they might be stored in an image asset. Or they might be stored in a file instead of our application wrapper. Or they could come from the network. Or they could be in a document that's stored in the application documents directory. They could be stored in a cache. But for artwork that comes with your application, we strongly encourage you to use image assets. And there are a number of reasons why. Image assets are optimized for name based and trait-based lookup. It's faster to look up an image asset in the asset catalog, than it is to search for files on disk that have a certain naming scheme. The asset catalog runtime has, also, got some really good smarts in it for managing buffer sizes. And there are, also, some features unrelated to runtime performance that are exclusive to image assets. Including features like per device thinning, which mean that your application only downloads image resources that are relevant to the device that it's going to run on and vector artwork. The vector artwork was a feature that was introduced in iOS 11. And you enable it by checking the Preserve Vector Data checkbox in the editor for your image asset. And the upshot of this is that if your image gets rendered in an image view that is larger or smaller than the native size of the image it doesn't get blurry. The image is, actually, re-rasterized from the vector artwork so that it has nice crisp edges. One place that we use this in the operating system. If you turn on dynamic type to a very large size in the Accessibility settings. And then you tap and hold on an item in the tab bar a little HUD shows up as a magnified view of the item that you're currently holding your finger over. So, if you want your artwork to look good in places like this check the Preserve Vector Artwork checkbox in. I'm sorry. The Preserve Vector Data checkbox in the image asset inspector. Now, the way this works is very similar to the pipeline we saw before. Rather than a decode phase, we have a rasterize phase that's responsible for taking the vector data and turning it into bitmap data that can be copied to the frame buffer. Now, if we had to do this for all of the vector artwork in your application we would be consuming a lot more CPU. So, there's an optimization we make here. If you have an image that has Preserve Vector Data checked, but you render it at the normal size. The asset catalog compiler has, actually, already produced a pre-rasterized version of that image and stored it in the asset catalog. So, rather than doing the complicated math of rasterizing your vector artwork into a bitmap, we can just decode that image that's stored in the asset catalog and render it directly into the frame buffer. If you're planning on rendering artwork at a few fixed sizes. Maybe, you have a small version and a large version of an icon. Rather than relying on the Preserve Vector Data checkbox, create two image assets that have the two sizes that you know you're going to render your image at. That will allow the optimization to take the CPU hit of rasterizing your artwork at compile time, rather than every time the image is drawn into the frame buffer. So, we've seen how to work with UIImage and UIImageView. But that's not all of the graphical work that your application does. Sometimes, your application draws content at runtime. The example of this happening might be seen in something like this editing view in the Photos application. The UIButton that's displaying an icon and UIButton can use UIImageView directly. But UIButton doesn't support the style of this Live button, here, that you can tap to enable or disable the Live Photo. So, we're going to have to do some work here, ourselves. And one implementation of this might be to subclass UIView and implement the draw method. And this implementation here draws a yellow roundRect, draws some text, and an image on top of it. Don't recommend this approach for a couple of reasons. Let's compare this view subclass to our UIImageView. Now, as you may already be aware, every UIView is, actually, backed by a CALayer in the Core Animation runtime. And for our image view, the image view creates the, asks the image to create the decoded image buffer. And then, hands that decoded image over to CALayer to use as the content of its layer. For our custom view that overrode draw, it's similar, but slightly different. The layers responsible for creating an image buffer to hold the contents of our draw method, and then our view, excuses draw function and populates the contents of that image buffer. Which is then, copied into the frame buffer as needed by the display hardware. In order to understand how much this is costing us and why we should, perhaps, pursue alternative ways of implementing this UI. The backing store that we're using here, the image buffer that's attached to the CALayer, the size of that is proportional to the view that we're displaying. Now, one new feature and optimization that we have in iOS 12 is that the size of the elements in that backing store will, actually, grow dynamically depending on whether you're drawing any color content. And whether that color content is within or outside of the standard color range. So, if you're drawing wide color content using extended SRGB colors, the backing store will, actually, be larger than the backing store would be if you used only colors within the zero to one range. Now, in previous versions of iOS, you could set the contents format property on CALayer as a hint to Core Animation saying, ''I know I am not going to need to support wide color content in this view'', or, ''I know I am going to need to support wide color content in this view''. Now, if you do this, you're actually going to be disabling the optimization that we introduced in iOS 12. So, check your implementations of layerWillDraw. Make sure you're not going to accidentally defeat an optimization that could benefit your code when running on iOS 12. But we can do better than just hinting at whether we need a wide color capable backing store. We can, actually, reduce the total amount of backing storage that our application needs. We can do that by refactoring this larger view into smaller subviews. And reducing or eliminating places that override the draw function. This will help us eliminate duplicate copies of image data that exist in memory. And it will allow us to use optimized properties of UIView that don't require a backing store. So, as I mentioned, overriding the draw method will require creating a backing store to go with your CALayer. But some of the properties in UIView can still work, even if you don't override draw. For example, setting the background color of a UIView doesn't require creating a backing store, unless you're using a pattern color. So, I recommend not using patterned colors with a background color property on UIView. Instead, create a UIImageView. Assign your image to that image view. And use the functions on UIImageView to set your tiling parameters appropriately. When we want to clip the corners of that rounded rectangle, we want to use the CALayer cornerRadius property. Because Core Animation is able to render clipped corners without taking any extra memory allocations. If we, instead, use the more powerful maskView or maskLayer properties we'd wind up taking in extra allocation to store that mask. If you have a more complicated background that has transparent areas that can't be expressed by the cornerRadius property, again, consider using a UIImageView. Store that information in your asset catalog or render it at runtime. And provide that as an image to the image view, rather than using maskView or maskLayer. Finally, for that icon, the Live Photo icon, UIImageView is capable of colorizing monochrome artwork without taking any extra allocations. The first thing you want to do is either check the, not check the checkbox, but set the property in the image asset editor, the render mode property to always template. Or use the withRenderingMode function on UIImageView to create a UIImage whose rendering mode is always template. Then, assign that image to an image view and set the tintColor of that image view to the color you want the image to render in. UIImage, as it's rendering your image to the frame buffer, will apply that solid color during that copy operation. Rather than having to hold on to a separate copy of your image with your solid color applied to it. Another optimization built into UIKit provided view, UILabel is able to use 75% less memory when displaying monochrome text than when displaying color text or emojis. If you want to know more about how this optimization works in detail and how to apply it to your custom subclasses of UIView, check out the iOS Memory Deep Dive session. Goes into great detail about this backing store format called A8. Sometimes, you want to render artwork offscreen stored in an image buffer in memory. And the class UIKit provides to do that is UIGraphicsImageRenderer. There's another function that's older; UIGraphicsBeginImageContext. But please, don't use that. Because only Graphics Image Renderer is capable of correctly rendering wide color content. What you can do in your applications is use UIGraphicsImageRenderer to render to an offscreen place. And then, use UIImageView to display that, efficiently, on the screen. Similarly, to the optimization that we've introduced in CALayer backing stores. We've, also, made UIGraphicsImageRenderer capable of dynamically growing the size of its image buffer, depending on the actions you perform in the actions block. If you are running your code on a operating system prior to iOS 12, you can use the prefersExtendedRange property on UIGraphicsImageRendererFormat to tell UIKit whether you plan on drawing wide color content or not. But there's a medium middle ground here. If you're primarily rendering an image in to a graphic image renderer, that image may use a color space that required values outside of the range of SRGB. But doesn't, actually, require a larger element size to store that information. So, UIImage has a image renderer format property that you can use to get a UIGraphicsImageRendererFormat object preconstructed for optimal storage of re-rendering that image. Lastly, we're going to talk a little bit about how to integrate advanced CPU and GPU technologies that we provide in iOS into your applications. So, if you've got a lot of advanced processing to do to your images, perhaps, in real time, consider using Core Image. Core Image is a framework that allows you to create a recipe for processing an image and handle that on the CPU or on the GPU. If you create a UIImage from a CIImage and hand that to UIImageView, UIImageView will take care to execute that recipe on the GPU. This is efficient and it keeps the CPU free for doing other work in your application. In order to use it create your CIImage as normal, and then use the UIImage ciImage initializer. There are other advanced frameworks for processing and rendering graphical content that are available on iOS, including Metal, Vison, and Accelerate. And one of the data types that is common among these frameworks is CVPixelBuffer. And this is a data type that represents a buffer that can be in use or not in use on the CPU or on the GPU. When constructing one of these pixel buffers make sure to use the best initializer. The one that's closest to the representation you have at hand. Don't unwind any of the decoding work. It's already been done by the existing UIImage or CGImage representations. And be careful when moving data between the CPU and the GPU, so that you don't just wind up trading off work between the two. You can, actually, get them to execute in parallel. Finally, check out the Accelerate and simd session for information on how to properly format your buffers for being processed by the Accelerator framework. So, to summarize a few key points. Implement prefetch in your table views and collection views, so that you can get some work done in advance and avoid hitching. Make sure that you're not defeating any optimizations that UIKit is providing to reduce the size of the backing stores associated with your views. If you're bundling artwork with your applications store it in the asset catalog. Don't store it in files that are associated with your app. And finally, if you're rendering the same icons at different sizes don't over-rely on the Preserve Vector Data checkbox. For more information there is a couple of related sessions, including one about actually investigating your performance problems. And we'll also have labs, tomorrow and Friday. And if you have any questions, come see us in the labs. Thanks for watching.  Good afternoon and welcome everybody to our session, what's new in user notifications. I'm Kritarth Jain [inaudible] on the iOS notifications team, and we're very excited to be back at WWDC to share with you all the new and exciting features around user notifications that your applications can start using with iOS 12. Today, we will be going over a range of topics as you can see from the list here. We will start with talking about grouped notifications, a new paradigm that we've introduced to iOS notifications when presented in the user's notification list. Then we'll talk about notification content extensions, which are existing extension points with notifications, and discuss new APIs that you've added around these. Then, we'll cover notification management and talk about all the new ways in which your application users can now tweak your notification settings and what you need to do to respond to these new options. Then, we'll cover provisional authorization, which allows your applications to have a trial phase for sending notifications to users without their explicit permission but do it quietly. And lastly, we'll cover critical alerts, which allows your applications to send important notifications to the users, which bypass certain system settings if your users allows your applications to do so. So there's a range of topics to be covered today, and let's begin with looking at grouped notifications. Now up to iOS 11, all new incoming notifications for the users were inserted in a chronological order in the notification list. So these would be interspersed across multiple applications, and it would be hard for the user to find a certain notification or triage multiple notifications together. So starting in iOS 12, we've decided to improve this by introducing notification grouping, so now, as you can see here, all notifications across different applications get grouped into their unique groups. Let's take a deeper look at how grouped notifications works. Now all these notifications will be automatically grouped, so there's nothing explicit that you need to do to start using notification grouping. However, if you do want to have your own custom groups then you can use the thread identifier, which is an existing property on the UN notification content object. So some of you might already be using the thread identifier and for a local notification. You can set it on the UNMutableNotificationContent object as seen here. And for a remote notification payload, you can also include it as part of your notification payload. Now the thread identifier might be familiar to some of you already. We use it today for forwarding notifications to a notification content extension that is presented for your application, which has the exact same thread identifier, allowing the content extension view to update it based on the new incoming notification. Starting in iOS 11, we started using the thread identifier for doing grouping of notifications, if the user had turned on [inaudible] notification previous. So we're just taking this concept and expanding it for all notifications in general. So how does this grouping work? So when a new notification comes in to you, the user's device, if there is no thread identifier set on this notification, then this notifications gets grouped with the application bundle. We can see that from our sample application here, that as new notifications are incoming, they are getting bundled with the same group and the group is getting updated with the latest content. And then the user can simply expand this notification group to see all the notifications that are present in that group. On the other hand, if the notification does have a thread identifier set on it, then it gets grouped with all the other notifications from that same application with that exact same thread ID. What this also means is that the same application can then have multiple different custom groups, depending upon the unique thread identifiers that you're setting on them. A good example of this is the messages application, where here you can see there are two different threads, and as new notifications are incoming, they are going to their own respective groups. And then the user can expand a specific group to see all the notifications that are part of that group. So by using the thread identifier, messages is able to do so. Now, your application users also have the option of tweaking this notification grouping setting from your per application notification setting's page. Here, they get three options. If they choose automatic, then they get the behavior that we just described. However, the user also has the option of just grouping by application, where the system will ignore your thread identifier and group all notifications into a single group. And if the user wants the same behavior as it exists in iOS 11 today, then they can simply turn off grouping for your applications' notifications. So do keep this in mind when you're creating your own custom groups that they create enough value for users when they receive your applications' notifications. Now what are the different components of a notification group? The content that we show is for the latest notification that was received as part of that group. And then the user can simply see all the notifications by tapping on this group, and we expand all the notifications' content. And then the user can interact with all these notifications individually as well. The two buttons at the top give the users much greater control like collapsing the stack as well as clearing all these notifications together. Now, notification grouping also makes triaging of notifications much better. For example, in this case, the user can clear all these notifications together by simply swiping to the right and tapping clear all. Apart from the content of the notification group, we also show a summary text. Now, this summary text, by default, shows the count of all the notifications that are part of that group. However, you can also create a custom summary text so you can give your users much better context of what kind of information is included in that group. Now, we will cover this API and go over much larger use cases of how you can create your custom groups in the advanced session around using group notifications, which will follow this session. So let's do a quick summary of group notifications as we saw them today. Starting in iOS 12, all application notifications are going to be grouped automatically. You can start using the thread identifier if you want to create your own custom groups for your applications, but the user does have the option of changing this grouping setting for your applications' notifications. And lastly, you can use the summary text for customizing the information you want to provide the user around the notification groups that you're creating. All right, so that was group notifications. Now, let's move on to the next topic and talk about notification content extensions. Now, some of you might already be familiar with these content extensions that we included with iOS 10. Content extensions allow your applications to present a rich notification view around the user's notifications, so you can have a much more customized and interactive interface for the notification that the user is seeing. Let's do a quick recap of setting up these content extensions. Xcode gives you a standard template to add a target for the content extensions to your applications and once you set that up, we create a default class for the notification view controller, which implements the UNNotificationContentExtension protocol. Here, the did receive notification method is important because this is your entry point for setting up the view associated with the content extension, and you can use the notification object past here to get all the information around that notification to set up your custom view. The info.plist file associated with your content extension gives you more options. The important thing here is the category identifier. Now, this identifier needs to match the same category identifier you're setting on your notification requests because that's how the system knows which content extension to launch with which notification. Along with this, you can do some quick configurations of your content extension such as setting the initial content size ratio, hiding the default content, as well as overriding the title of this content extension. Now, the primary way in which your users interact with these content extensions is through notification actions, and these actions are presented right below the content of the content extension. Let's summarize how we can set up these actions as well. So doing so is fairly trivial in code. For example, here, we have two actions here for like and comment, and we create a simple UNNotificationAction for like and a text input action for commenting. And once we've created these actions, we create a new category giving it the same identifier as the content extension where we want these actions to be presented. And then, we pass it, the two new actions that we created. Once we've set up this category, then we call setNotificationCategories on the UNNotificationCenter object associated with our class, giving it the new category that we created. So by simply doing so, the next time when the user goes to your content extension we can see that these actions are now available for them to interact with your notification content. Now let's take a look at how we can handle the responses from these actions. There are two ways to do that. Firstly, you can handle this response in the AppDelegate that is associated with your application that implements the UNUserNotificationCenter Delegate protocol. Here, the function UserNotificationCenter did receive response, includes the response object which includes information about the request, the notification request from which the user took this action. However, the content extension also allows you to intercept this action response so that you can update your view and make a much more interactive and dynamic experience for users for the content extension. So for our sample here, we enter the did receive response method and checked the action identifier for the like action. And then we update our UI with the new label as well as update our application state. Finally calling the completion block we do not dismiss. If you do want to dismiss your content extension view here, then you can simply change the parameter you're passing to the completion block to dismiss or dismiss and forward, where we will forward this response to your AppDelegate function as well. All right, so now that we set this up, we can see that when the user takes the like action, the content extension content gets updated right there and then. So it's a much more interactive experience for your user and they're getting real-time feedback. However, if you look at the current state of the content extension, we see that there is some redundant information. The user has already taken the like action, so having the action there doesn't really serve a purpose anymore. Now notification actions, in general, have certain limitations. They are not very dynamic and can't be updated based on the context of your content extensions. Also, these tend to be tied to the notification categories that you have to define at the time of your application setup. So we wanted to address these issues and we have introduced a new API around notification actions, where now we're exposing these notification actions as part of the NSExtensionContext tied to your content extension. What this API allows you to do is access the currently presented notification actions to the user as well as replace these actions by setting a brand new array of notification actions for your content extension. So going back to our sample, what if after the user took the like action we wanted to replace it say with the unlike action so that they can do the reverse of the action they just took? So using this new API, let's take a look of how we can set this up. So we go back to our did receive response method and again identify the like action and update our application state. This time, we also create a new action for unlike, giving it a unique identifier as well as a title. We can also take a look at the currently presented actions so that we can extract the comment action from there without having to create it again. Then we create a new array of these new actions that we've created and simply set that on the notification actions variable. So once we've done this and the user takes the like action, then the UI will automatically update to show them the new action, and then the user can then toggle between the two actions, depending upon how you handle that state in your content extensions. Now, this API can be used in multiple other ways as well. For example, now you can set your actions at the time you're setting up your content extension view in the did receive notification method. What this means is your notification requests are no longer tied to the category to define the actions that you want to present around these notifications. You can also now present a secondary set of actions by replacing the currently presented actions. For example, if the leading action was rate, then you can provide a secondary list of the different types of ratings that you want your user to take. And you can also remove all these notification actions if you feel it does not make sense anymore for your content extension to present these actions. So that's the new API around notification actions. And we feel this will really help you enhance the experience that your users have around your content extensions with the different actions now you can present to them. Let's move on and talk about user interaction with these content extensions. Now notification actions were important up till this point because till iOS 11 we did not allow user interaction touches with your content extension view. Now we received a lot of feedback around this. And I'm happy to announce that we're taking away this restriction with iOS 12. So now your content extensions have the option of opting in to receiving user interaction [inaudible] touches, and setting this up, it's fairly trivial. All you have to do is add a new key value option to your info.plist file. And the key that we've added is the UNNotificationExtensionUser InteractionEnabled. So going back to our sample, what if we want to remove the like action from a notification action and make it a UI interaction touch that's part of the view itself? So once we've configured our info.plist file, we can go back to our content extension view and create our own custom button to handle the like gesture. We add a target for our own private method and inside that function, we update the UI as well as update our application state. So here, it's important that since you're implementing your own user interactions that you are responsible for handling all these actions, responses, and callbacks from the users yourself. So once we've set this up, now when the user goes to your content extension we see the Like button, part of the UI itself, and the user can simply interact with that button right there and then. So that's the new functionality that we've added around content extensions. And coupled with notification actions, along with user interaction touches, you now have a much richer set of tools for creating much more interactive and dynamic content extension experiences for your applications notifications users. Now let's talk about launching your application from the notification content extension. So today the user can launch your application if touches were not allowed by simply tapping the content extension view. They could also do so by tapping your application icon in the top left corner. Or you could create a foreground action, which then would in turn launch the application. But what if you wanted to do this from your own custom control? What if you wanted to launch the application programmatically? To enable this, there is a new API on the NSExtensionContext called performNotification DefaultAction, which would allow you to do this now. Now, what does the default action mean? So, as we said, it launches the application, but at the same time, it calls the UserNotificationCenter did receive response method in your application delegate. Now the UNNotificationResponse object contains the information of the notification from which the user came, so you can update your application state based on the notification. And the identifier that's passed here is the UNNotificationDefault ActionIdentifier. So going back to our sample, let's see how we can set this custom control up. Now again, we create our own UI button for the all comments and then tie it up with our own private function. And in that function, we're simply calling PerformNotification DefaultAction. So by simply doing that, you get this functionality to call this method programmatically from anywhere in your content extension code. So that was launching the application. What about dismissing the content extension view? Again, let's take a look at how the user can do that today. They can do that by tapping the Dismiss Button in the top right corner, or you can create your own custom notification action, which would in turn dismiss the content extension view. Which you can set up, as we saw before, by passing dismiss to the completion block. But again, what if we want to dismiss the view through our own custom buttons, and we want to do this programmatically? Say that when the user taps the Like button, then the view dismisses because we feel the user's done interacting with the content extension. To enable this as well, there's a new API called dismissNotificationContent Extension that's on the NSExtensionContext. We go back to how we set up our Like button, and now this time, we also call the new function that we added for dismissing the content extension view. And once we set this up, now when the user takes the like action, the view of the content extension gets dismissed. Now one thing to note here. That calling this method does not withdraw the notification that was posted to the user. If you want to do that then use the existing API for removing delivered notifications with identifiers to get that functionality. All right, now let's summarize all the new APIs that we've looked at today around the notification content extensions. We started with talking about notification actions where now you can access these notification actions as well as replace them dynamically from anywhere in your content extension code. You can now opt in to having user interaction based on touches within your content extension views. You can programmatically launch the application from anywhere in your content extension code as well as dismiss the content extension view, depending upon where you feel it serves best your user's experience around the content extensions. So that's a varied list of APIs around content extensions, and we hope this really helps you enhance your user's experience around your content extensions and then you start using these APIs. So that was notification content extensions. Now, the next topic today we're going to look at is notification management, and to tell you all about that, let me invite my colleague Teja to the stage. Thank you. Thank you Kritarth. Hi everyone. My name is Teja Kondapalli, and I'm also an engineer on the iOS Notifications' Team. And, of course, I'm here to talk to you about a couple of the new APIs that we have. The first of which is notification management. But before I dive into this API, I want to cover some of the user facing features to give you some more context and then we can deep dive into the API. As our users get more and more apps on their phones, notifications become the primary way that they interact with these apps. But often, they find themselves in a situation like this. With far too many notifications. And it becomes hard to sift through and find the important ones. So perhaps to make this easier, this user has decided that notifications from podcasts don't need to be shown on the locked screen. Right now to configure that they'd have to launch the settings app, find notifications, find the podcast app, and then they can configure their settings. We wanted to make this easier. So, in iOS 12, we're introducing a new management view where the users can configure their notification settings directly from the notification without having to launch the settings app. There's three really easy ways to get into this management view. The first is what we just saw. You simply swipe over a notification, tap manage, and the management view comes up. The second is if you can go into the rich notification, you can tap in the right corner, and you can also launch the management view. And the third is actually in the list itself. Depending on how your users are interacting with their notifications, they will occasionally see suggestions, like this one, that ask them if they want to keep receiving podcast notifications. And from here, as well, they can tap manage and bring up the management view. Let's take a closer look at the management view. And we obviously have options here where users can configure their notification settings directly from this view. But if they want to go into the settings app and configure in a more detailed manner, they have a quick link to the settings for this application, the notification settings. A And also from this view, they have some actions they can take directly, the first of which says deliver quietly, which is probably a concept that's new to all of you. Some of these management views will also have an option that says deliver prominently, so let's talk about what this means. In iOS, we have a lot of settings that users can configure, and this is really great for the power user. The can customize their settings to every detail, but for the regular user, we think that we can help them out by categorizing their notification settings into two big categories. Notifications that are delivered prominently and notifications that are delivered quietly. Notifications that are delivered prominently are what we're used to. They show up on the locked screen. They show up in notification center. They roll down as banners. They badge the AP icon and they can play a sound. Notifications that are delivered quietly only show up in notification center and they don't play a sound. And from the management view, in addition to configuring whether they want their notifications delivered prominently or quietly, users also have the option to turn off their notifications. Now, I know that you might worry that your users are going to turn off the notifications for your app, so we've added this extra confirmation sheet just in case they do tap turn off. And from here, they can also turn off their notifications. But we've also added an API to add a second option to this confirmation sheet, and podcast has taken advantage of this API, so it says configure in podcast. This is a link that will deep link within the podcast app to a custom settings view that allows the user more granular control about what kind of podcast notifications they want. And as your apps send more and more notifications and various type of notifications, we think it's really important to allow them this granular level of control over what kind of notifications are important to them. This link can also be accessed from the systems settings app from your apps' notification settings. And you can see for podcasts it's right at the bottom. It says podcast notifications settings. Let's see how we do this in code. In the class that conforms to UNUserNotificationCenter Delegate, we have a new delegate method. Open settings for notification, and as long as you implement this delegate method, those links that we talked about from the management view, or from the settings app, will automatically be populated by the system for you. So when the user taps on any of these links, this delegate method will be called. And it's really important when this delegate method is called that you immediately take your users into the view where they can configure their notification settings within your app. And if you notice, we have [inaudible] parameter to this method, and it is notification. So depending on where the link was tapped from, if it was tapped from one of the management views, it will have the value of the notification that that management view came from. If it was tapped from the settings app, the value of notification will be nil, and you can use this information to show the appropriate notification settings when this delegate method is called. So that's what we have for notification management. It's a new way for your users to configure whether they want their notifications delivered prominently or quietly, or turn them off, or even configure them at a granular level within your app. And to encourage your users to keep getting your notifications delivered, we think it's really important that you make the content in the notifications relevant. We also encourage you to use thread identifiers to group the notifications when you think it's appropriate. This will help the users organize their lists better and will make sure that they're not overwhelmed by the notifications from your app. We also think that as your apps send various types of notifications, it's really important to provide that custom settings view within that app so that users have more granular control about what kind of notifications are important to them. That's what we have for notification management. And the next big feature I want to talk to you about is provisional authorization. Right now, when a user installs your app, before they start receiving notifications, at some point they'll have to respond to a prompt that looks like this, which is asking them if they want these notifications. And the biggest problem with this is, at this point, the user doesn't know what kind of notifications this app is going to send, so they don't know if they want them or not. So, in iOS 12, we're introducing provisional authorization, and this is an automatic trial of the notifications from your app. This will help your users make a more informed decision on whether they want these notifications or not. So you can opt into this, and if you do, your users will not get that authorization prompt that we just saw. Instead, the notifications from your app will automatically start getting delivered. But these notifications will be delivered quietly, and if we recall, notifications that are delivered quietly only show up in notifications center, and they don't play a sound. Notifications that are delivered with provisional authorization will have a prompt like this on the notification itself. And this will help the users decide after having received a few notifications whether they want to keep getting these notifications or whether they want to turn them off. And this turn off confirmation sheet will also have the custom settings link if you have provided it. Let's see how you can do this in code. In the location where you regularly request authorization, in addition to whatever options you might be requesting, you can add a dot qualifying option called .provisional. And if you include this, you will automatically start participating in the trial. It's really important to note that the .provisional option is in addition to whatever other options you may be providing. That's because if the users decide to keep getting your notifications delivered, we want to know how you want them delivered, with badges or sounds or as alerts. So that's what provisional authorization is. It's an automatic trial of the notifications from your app to help your users make a more informed decision about whether they want these notifications. And again, to encourage your users to keep getting your notifications delivered, it's really important to make the content in your notifications relevant. And also, it's really important to use .provisional as a qualifier option in addition to whatever other options you're requesting. That's what we have for provisional authorization. And the last big feature that I want to talk to you about are critical alerts. Often when I'm in the middle of a meeting or attending something important, my phone looks like this. And as you can see, I have do not disturb turned on. Or at least I have the ringer switch turned off so that I don't hear any sounds when I get notifications. And usually this is really good, but I would have missed a really important notification like this one. This is a health-related notification. That's from a glucose monitor that's warning me of low blood sugar, and this is something I would want to see immediately. Scenarios like this made us realize that we need a new type of notification, and this is what we call critical alerts. Critical alerts are medical- and health-related notifications. Or home- and security-related notifications. Or public safety notifications. And the key to a critical alert is that it requires the user to take action immediately. The way that critical alerts behave is that they bypass both do not disturb and the ringer switch, and they will play a sound. And they can even play a custom sound. But what that means is that these are very disruptive, and for that reason we don't think that all apps should be able to send critical notifications. Critical alerts. So in order to start sending a critical alert, you will need to apply for entitlement, and you can do that on the developer.apple website. This is what a critical alert looks like, and you can see that it has a unique icon indicating that it's critical. And it would have also come in with a sound. Critical alerts also have their own section in notifications settings. This means that a user can choose to allow critical alerts for a particular application but choose not to allow any other type of notification. And before users start receiving critical alerts, they will have to accept a prompt that looks like this that's asking them specifically whether they want to accept critical alerts from a particular application. So, of course, in order to start sending critical alerts, you'll have to request authorization. So after you apply for entitlement and get it, in the place where you regularly request authorization, in addition to whatever other options you want to request, you can also request a .criticalAlert option. And this will give your users the prompt. And let's see how it actually set up and send a critical alert. It actually behaves very similarly to a regular notification. You can see that I just set up a notification with the title body and category identifier, but what distinguishes this as a critical alert is that it plays a sound. So I need to set a critical alert sound. And here, you can see that I'm setting the default critical alert sound that's provided by the framework. I can also set a custom sound. And I can also set a custom audio volume level. And of course, critical alerts can also be push notifications so all of this information can be set in the push payload as well. So that's what we have for critical alerts. They're a new type of notification that requires the users to take action immediately. And they're very disruptive, so you need entitlement to be able to send them. So that's all the new exciting APIs that we have for you today. I just want to quickly go over all the things that we covered. We talked about how you can use thread identifiers to group your notifications to help your users organize their notification lists better. We talked about all the great new APIs around notification content extensions, which will help you make your rich notifications much more interactive. We talked about how you can provide a custom settings view within your app to allow your users more granular control over what kind of notifications they want to receive. We also talked about provisional authorizations, which is an automatic trial of the notifications from your app, which will help you users make a more informed decision about whether they want these notifications or not. And last, we talked about critical alerts, which are a new type of notification that requires the user to take action immediately and that are disruptive. So we hope you take advantage of all of these great APIs and make the notification experience for your users even better. You can find all the information about this session on our sessions' page at developer.apple.com. We're session 710. We have another session just following this one in hall three called using grouped notification where we'll help you determine how to best group notifications for your app. We have two notifications labs, one today and one tomorrow, where you can come and ask the engineers on our team any questions that you may have. And on Friday morning, we have an interesting session called designing notifications. That's going to be hosted by the designers who helped us come up with the designs for these, and they'll be talking about notification best practices. Thank you and have a great dot dot.  Hello and good afternoon everyone. Welcome to our session on natural language processing. I'm delighted to see so many of you here today, and I'm really excited to tell you about some of the new and cool features we've been working in the NLP space for you. I'm Vivek, and I'll be jointly presenting this session with my colleague, Doug Davidson. Let's get started. Last year, we placed your app center stage and told you how you could harness the power of NLP to make your apps smarter and more intelligent. We did this by walking you through the NLP APIs available in NSLinguisticTagger. NSLinguisticTagger, as most of you are familiar with or have used at some point, is a class and foundation that provides the fundamental building blocks for NLP. Everything from language identification to organization, part of speech tagging, and so on. We achieve this in NSLinguisticTagger by seamlessly blending linguistics and machine learning behind the scenes. So you, as a developer, can just focus on using these APIs and focus on your task. All that's great. So what's new in NLP for this year? Well, we are delighted to announce that we have a brand-new framework for NLP called Natural Language. Natural Language is now going to be a one-stop shop for doing all things NLP on device across all Apple platforms. Natural Language has some really cool features, and let me talk about each of those. First, it has a completely redesigned API surface, so it supports all the functionalities that NSLinguisticTagger used to and still does but with really, really Swift APIs. But that's not it. We now have support for custom NLP models. These are models that you can create using Create ML and deploy the model either using Code ML API or through Natural Language. Everything that we support in Natural Language, all of the machine learning NLP is high performed. It is optimized for Apple hardware and also for model size. And finally, everything is completely private. All of the machine learning in NLP that is powered in Natural Language is done on device to protect user's privacy. This is the very same technology that we use at Apple to bring NLP on device for our own features. So let me talk about each of these features of Natural Language. Let's start with the Swift APIs. As I mentioned, Natural Language supports all the fundamental building blocks that NSLinguisticTagger does but with significantly better and easier to use APIs. In order to provide some of these APIs and go over them, I'm going to illustrate them with hypothetical apps. So the first app that we have here is an app that you wrote, and as part of the app, you enable a social messaging or peer-to-peer messaging feature. And an add-on feature in this app that you've created is the ability to show the right stickers. So based on the content of the message, which in this case is "It's getting late, I'm tired, we'll pick it up tomorrow morning, good night." Your app shows the appropriate sticker. You parsed this text, and you bring up the sticker. The user can attach it and send it as a response. So all of this is great. This app has been doing really well. You've been getting rave reviews. But you also get feedback that your app is not multilingual. So it so happens that users are bilingual these days. They tend to communicate in several different languages, and your app, when it gets the message in Chinese, simply doesn't know what to do with it. So how can we use natural language to overcome this problem? Well, we can do this with two simple calls to two different APIs. The first is language identification. With the new Natural Language framework, you start off by importing Natural Language. You create an instance of NLLanguageRecognizer class. You attach the string that you would like to process, and you simply call the dominant language API. Now this will return the single best hypothesis in terms of language for the string. So the output here is essentially simplified Chinese. Now in Natural Language, we also support a new API. There are instances where you would like to know the top-end hypothesis for a particular string. So you'd like to know what are the top languages along with their associated probabilities. So you can envision using this in several different applications where there's a lot of multilinguality, and you want that leeway in terms of what could be the top hypothesis. So you can do this with a new API called Language Hypotheses. You can specify the maximum number of languages that you want, and what you get back is an object with the top-end languages and their associated probabilities. Now in order to tokenize this Chinese text, you can again see the pattern is very similar. You again import Natural Language. You create an instance of the NLTokenizer, and in this particular instance, you specify the unit to be word because you want to tokenize the string into words. You attach the string, and you simply call the tokens method on the string, on the object. And what you get is an array of tokens here. Now with this array of tokens, you can look up the particular token here is goodnight, and lo and behold, you have multilingual support in your app. So your app can now support Chinese with simple calls to language identification and tokenization APIs. Now let's look at a different sort of an API. I mean language identification and tokenization are good, but we would also like to use auto speech tagging, named entity recognition, and so on. So let me illustrate how to use named entity recognition API again with the hypothetical app. So here's an app. It's a news recommendation app. So as part of this app, your user has been going and reading a lot of things about the royal wedding, so a really curious user. They want to find everything about the royal wedding. So they've perused a lot of pages in your app, and then they go on to the search bar, and they type Harry. And what they see is completely things that are not pertinent to what they've been looking for. You get Harry Potter and so on and so forth. What you'd like to see is Prince Harry, something related to the royal wedding. So now you can overcome this issue in your app by using the name entity recognition API. Again, as I mentioned, the syntax here is very familiar. Those of you who have been used to using NSLinguisticTagger, they should look familiar and feel familiar, but it's much easier to remember and to use. You import Natural Language. You now create an instance of NLTagger, and you specify the scheme type to be name type. If you want part of speech tagging, then you would specify the scheme type to be lexical class. You again specify the string that you want to process. In this particular instance, you specify the language, so you set the language to be English. So if you were not familiar or if you're not sure about what the language is, Natural Language will automatically recognize the language using the language identification API under the hood. And finally, you call the tags method on this object that you just created. You specify the unit to be word and the scheme to be name type. And what you get as an output is the person names here, Prince Harry and Meghan Markle, and the location to be Windsor. Now if the user were to go back to the search bar, based on the contextual information of what the user has been browsing, you can significantly enhance the search experience in your app. So there's a lot more information about how to use these APIs. You can go to the developer documentation and find more information. What I'd like to emphasize here is NSLinguisticTagger is still supported, but the future of NLP is in Natural Language. So we recommend that and encourage you to move to Natural Language so that you can get all the latest functionalities of NLP in this framework. Now let's shift gears and look at a situation where you have an idea for an app, or you need a functionality within your app that Natural Language does not support. What do you do? So you can certainly create something, which will be great, but what if we gave you the tools to make that much easier? To talk about custom NLP models and how to build custom NLP models using Create ML and use the subsequent models, which are essentially Code ML models in Natural Language, I'm going to hand it over to Doug Davidson. Thanks Vivek. So I'm really excited about the new Natural Language framework, but the part I'm most excited about is support for portraying and using custom models. And why is that? I'd just like you to think for a second about your apps, maybe the apps you've written or the apps that you want to write, and think about how they could improve the user experience if they just knew a little more about the text that they deal with. And then think for a second about how you analyze text. So maybe you look at some examples of text, and you learn from them, and then you understand what's going on, and then you can look at a piece of text, and at a glance, you can figure out something about what's going on with it. Well, if that's the case, then there's at least a reasonable chance that you can train a machine learning model to do that sort of analysis in your app automatically for you, giving it examples that it can train and learn from and produce a model that can do that analysis. Now, there are many, many types of machine learning models for NLP, and there are many different ways of training it. Probably many of you are already training machine learning models, but our task here has been to produce ways to make this sort of training really, really easy and to make it integrate really well with the Natural Language framework and APIs. So with that in mind, we are supporting two types of models that we think support a broad range of functionality and that work well with our paradigm in NLTagger of applying labels to pieces of text. So the first of model we're supporting is a text classifier. A text classifier takes a chunk of text, maybe it's a sentence or a paragraph or an entire document, and applies a label to it. Examples of this in our existing APIs are things like language identification, script identification. The second type of model we support is a word tagger, and a word tagger takes a sentence considered as a sequence of words, and then applies a label to each word in a sentence in context, and examples of existing APIs are things like speech tagging and named entity recognition. But those are sort of general purpose examples of these kinds of models. You can do a lot more with them if you have a special purpose model for your specific application. Let me give you some hypothetical examples. So, for text classification, suppose you're dealing with user reviews, and you want to know automatically whether a given review is a positive review or a negative review or somewhere in between. Well, this is the sort of thing you could train a text classifier to do. This is sentiment classification. Or suppose you have articles or just article summaries or maybe even just article headlines, and you want to determine automatically what topic they belong to according to your favorite topic classification scheme. This again is the sort of thing that you can train a text classifier to do for you. Or going a little further, suppose you're writing an automated travel agent, and when you get a request from a client, the first thing you want to know probably is what are they asking about? Is it hotels or restaurants or flights or whatever else that you handle. This is the sort of thing that you could train a text classifier to answer for you. Going on to word tagging. So we provide word taggers that do part of speech tagging for a number of different languages, but suppose you happen to need to do part of speech tagging for some language that we don't happen to support quite yet. Well, with custom model support, you could train a word tagger to do that for you. Or named entity recognition. So we provide built-in named entity recognition that recognizes names of people and places and organizations, but suppose you had some other kind of name that you were particularly interested in that we don't happen to support right now. So, like for example, product names. So you could train your own custom named entity recognizer as a word tagger that would recognize names or other terms of whatever sort you are particularly interested in. Even further, for your automated travel agent, once you know what the user is asking about, probably the next thing you want to know is what are the relevant terms in their request. For example, if it's a flight request. Where do they want to go from and to? So a word tagger can identify various kinds of terms in a sentence. Or another application, if you need to take a sentence and divide it up into phrases, noun phrases, verb phrases, propositional phrases. With the appropriate sort of labeling, you could train a word tagger to do this, and many, many other kinds of tasks can be phrased in terms of labeling, applying labels to portions of text, either words in sequence or chunks of text in the text classifier. So these are supervised machine learning models, so there are two phases always involved. The first phase is training, and the second phase is inference. So training is what you do in part of your development process. You take labeled training data, and you feed it into Create ML and produce a model. Inference is then what happens in your app when you incorporate that model into your application at run time when it encounters some piece of data from the user, and then it analyzes that data and predicts the appropriate labels for it. So let's see how these phases work. So let's start with training, and training always starts with data. You take your training data, and then you feed it in to in this case Create ML in a playground let us say or a script [inaudible] as you may have seen in the Create ML session. Create ML calls the Natural Language framework under the hood to do the training, and what comes out is a core ML model that's optimized for use on device. So let's look at what this data might look like. So Create ML supports a number of different data formats. Right here we're showing our data in JSON because JSON makes things perfectly clear, and this is a piece of training data for a text classifier that's a sediment classifier. So each training example, like this one, consists of two parts, a chunk of text, and the appropriate label for it. And so this for example is a positive sentence, so the label is positive, but you can pick whatever label set you want. Now, then when you start using Create ML, the Create ML provides a very, very simple way to train models in just a few lines of code. First line, we just load our training data from our JSON file. So we give it a URL to the JSON file, create a Create ML data table from it. Then in one line of code, create and train a text classifier from this data. All you have to tell it is what the names of the fields are, text and label, and then once you have it, one line of code writes that model out to disk. Now for training a Word Tagger, it's very similar. The data is just a little more complicated because each example is not a single piece of text. It's a sequence of tokens, and the labels are, again, a sequence of labels, the same number of labels, one label for each token. So this, for example, is training data for a Word Tagger that does name identity recognition, and each word, each token, has a label, either none, it's not a name, or org, it's an organization name or prod, it's product name, or a number of different other labels for whatever kinds of names you're recognizing. So each token has a label, and each sample consists of one sequence of tokens and their corresponding labels. And then the Create ML to train this is almost identical. You load the training data into a data table from the JSON. Then you create and train a Word Tagger, in this case instead of a text classifier, and then you write it out to disk. Now, there are a number of other options and APIs available in Create ML. I encourage you to, if you haven't already, take a look at the Create ML session, which happened yesterday, and Create ML documentation for more information on that. Now, once you have your model, we then go to the inference part. So you take your model, you drag it into your Xcode project. Xcode compiles it and includes it in your applications resources, and then what do you do at run time? Well, it's a Core ML model. You could use it like any other Core ML model, but the interesting thing is that these models are able to work well with the Natural Language APIs just like our built-in models that provide the existing functionality for NLP. So what will happen is data comes in. You pass it to Natural Language, which will use that model and do everything necessary to get all of the labels out and then pass back either a label, single label for a classifier or a sequence of labels for a tagger. And so how do you do this in Natural Language API? First thing you have to do is just locate that model in your application's resources, and then you create an instance of a class in Natural Language called ML Model from it. And then, well, the simplest thing you can do with it, at least for a classifier, is just pass it in a chunk of text and get a label out. But the more interesting thing is that you can use these models with NLTagger in exactly the same way that you use our built-in models for an existing functionality. So let me show you how that works. In addition to the existing tag schemes that we have for things like named identity recognition part of speech tagging, you can create your own custom tag scheme, give it a name, and then you can create a tagger that includes any number of different tag schemes. Your custom tag scheme or any of our built-in tag schemes or all of them, and then all you have to do is to tell the tagger to use your custom model for your custom tag scheme. And then you just use it normally. You attach a string to the tagger, and you can go through and look at the tags for whatever unit of text is appropriate for your particular model, and the tagger will automatically call the model as necessary to get the tags and return the tags to you and will do all the other things that NLTagger does automatically, like language identification, tokenization and so on and so forth. So I want to show this to you in a simple hypothetical example. And this hypothetical example is an app that users will use to store bookmarks to articles they may have run across and then might be intending to read later on. But the problem with this application as it currently stands is that the list of bookmarks is just one long list with no organization to it. Wouldn't it be nice if we could automatically classify these articles and put them into some organization according to topic? Well, we can train a classifier to do that for us. And the other thing is that the articles, when we look at them, it's a long stream of text. Maybe we'd like to highlight some interesting things in those articles like for example names. Well, we have provided built-in name identity recognition for names of people, places, and organizations, but maybe we also want to highlight names of products, so we could train a custom word tagger to identify those names for us. So let me go over to the demo machine. And so here's our application as it stands before we apply any natural language processing. As you can see, even just one long list of articles on the side and a big chunk of text for our article on the right. Well let's fix that. So, let's go into -- so the first part of training a model is data, and fortunately, I have some very hard-working colleagues at Apple who have collected for me some training data to train two models. The first model is a text classifier that will classify articles according to topic. So this is some of what the training data looks like. Each training example is a chunk of text and the appropriate label by topic, entertainment, politics, sports, and so on and so forth. And I also have some training data to train a word tagger that will recognize product names in sentences. So this training data is pretty simple. Each example consists of a sentence considered as a sequence of tokens and then a sequence of labels, and each label is either none, it's not a product name, or prod, it is a product name. So, let's try to train with these. So first thing I want to do is bring up a playground that I have using Create ML, and this playground will just load. In this case, this is my product word tagger. It'll load the training data, create a word tagger from it, and write it out to disk. So let me just fire that off, and let it start running. It's loaded the data, and so under the hood, we automatically handle all of the tokenization, the feature extraction. We do the training. This is a fairly small model, so it doesn't take all that long to train, and I have it set to automatically write my model out to my desktop, and there it is. All right. So that's one model. Now I have another playground here that's set up to train my text classifier. As you can see, it looks very similar. Load the training data, create a text classifier from it, and write it out to disk. So I start that off. And again, automatically natural language is loading all the data, tokenizing it, extracting features from it. This one is a bit larger model. It takes a couple minutes to train. So let's just let that go, and in the meantime, take a look at some of the code we have to use these at run time. So I've written two very small classes to do what I need to do at run time. The first one uses the text classifier by finding that model in my apps resources and creating an NL model for it. And then when I run across an article, I just ask the model for a predicted label for that article, and that's really all there is to it. Slightly more code for use of my word tagger. So as you saw before, I have a custom tag scheme for my product name recognition, and the only tag I'm really interested in is the product tag. So I create a custom tag for that. Again, I have to find the model in my bundles resources, create an NL model for it, and then create an NLTagger, and this NLTagger I'm specifying two schemes. The first is the built-in name type scheme to do name identity recognition, and the second one is my custom product tag scheme, and they'll both function in exactly the same way. And then I just have to tell that tagger to use my custom model for my custom scheme. Now if I supported multiple languages, I might have more than one model in here for this scheme. And then what I'm going to do is highlight text in this article that is located, determined to be a name of one sort or another. So I'm going to get a mutable attributed string, and I'm going to add some attributes to it. So I'll take this string of that mutable attributed string, attach that to my tagger, and then I'm going to do a couple of enumerations over tags. The first one uses the built-in name-type scheme for name identity recognition of people, places, and organizations, and if I find something that's tagged as a person or place or organization, then I'm going to add an attribute to the attributed string that will give it some color. And then we can do exactly the same thing with our custom model. We're going to enumerate using our custom product tag scheme, and in that case, if we find something that's labeled with our custom product tag, then I can add color to it in exactly the same way. So you can use custom models with Natural Language API just in the same way that you use built-in models. Now, let's go back to our playground, and we see that the model training has finished, and in fact there are now two models showing up on my desktop. So all I need to do is drag those into my application. Let's take this one and drag it right in. Okay. And let's take this one and drag it in, and Xcode will automatically compile these and include them in my application. So all I have to do is build and run it. And let's hide that. Here's my new application, and you'll notice that my list of articles is all neatly sorted automatically by topic, and if I go in and take a look at one of these articles, you'll notice that names are highlighted in it, and you can see, using our built-in name identity recognition, we highlight names of people, places, and organizations, but if you look a little further, you can see that it has used our custom product tagger to highlight the names of products like iPad, MacBook, iPad mini, and so forth. So this shows how easy it is to train your own custom models and to use them with the natural language APIs. So now I'm going to turn things back over to Vivek to talk about some important considerations for training models. Thank you, Doug, for telling us how to use these custom NLP models. We are really excited to sort of have a very tight integration of natural language with Create ML and the Core ML [inaudible], and we hope that you do some really unbelievable things with this new API. So I'd like to shift attention now again and talk about performance. So as I mentioned before, Natural Language is available across all Apple platforms, and it also offers you what we call as standardized text processing. So let's take a moment again to understand what we mean by this. Now if you were to look at a conventional machine learning pipeline that didn't use Create ML, where would you start? You would start with some amount of training data. You would take this training data. You would tokenize this data. You'd probably extract some features. This is really important for languages like Chinese and Japanese where tokenization is very important. You would throw that into your favorite machine learning toolkit, and you'd get a machine learning model out of it. Now in order to use that machine learning model on an Apple device, you'd have to convert that into a Core ML model. So what would you do? You would use a Core ML converter to do this. This is sort of the training procedure in order to get from data to a model and deploy it on an Apple device. Now, at inference time, what you do is you drop the model in your app, but that's not it. You also have to make sure that you write the code for tokenization and feature extraction that is consistent with what happened at training time. It's a lot of effort because you have to think about maximizing the fidelity of your model. It's absolutely important that the tokenization featured extraction is identical at both training and inference time. But now with the use of Natural Language, you can completely obviate this. So if you look at the sequence at training time, you have training data. You just pass it to Create ML through the APIs that we've discussed so far. Create ML calls Natural Language under the hood, which does the tokenization feature extraction, chooses the machine learning library, does all the work, and returns a model which is a Core ML model. Now at inference time, what you do is you still drop this model in your app, but you don't have to worry about tokenization feature extraction or anything else. In fact, you don't have to write a single line of code because Natural Language does all of that for you. You just focus on your app and your task and simply drag and drop the model in. The other aspect of Natural Language as I mentioned before is it's optimized for Apple hardware and for model sizes. So let's look at this through a couple of examples. So Doug talked about named entity recognition and chunking, and here are two different benchmarks. So these are models that we built using an open source tool kit called CRF Suite, and through Natural Language. The models were built from identical training data and tested on identical test data. The same sort of features were used. The accuracy obtained by both these models is the same. But you look at the model sizes that Natural Language is able to generate. It's simply just about 1.4 megabytes of data size, model size for named entity recognition and 1.8 megabytes for chunking. That saves you an enormous amount of space within your app to do other things. In terms of machine learning algorithms, we support two different options. We can specify this for text classification. So for text classification, we have two different choices. One is maxEnt, which is an abbreviation for Maximum Entropy. In NLP, we call maxEnt is essentially a multinomial logistic regression model. We just call it Maximum Entropy in NLP feed. The other one is CRF, which is an abbreviation for Conditional Random Feed. The choice of these two algorithms really depends upon your task. So we encourage you to try both these options, build the models. Now in terms of word tagging, that is one default option, which is a conditional random feed. When you instantiate an ML word tagger, specify data to it, the default model that you get is a conditional random feed. Now as I mentioned, the choice of these algorithms really depends on your task, but what I'd like to emphasize is sort of draw [inaudible] between your conventional development process. So when you have an idea for an app, you go through a development cycle, right. So you can think of machine learning to be a very similar sort of a work flow. Where do you start, you start with data, and then you have data, you have to ask a couple of questions. You have to validate your training data. You have to make sure that there are no spurious examples in your data, and it's not tainted. Once you do that, you can inspect the number of training instances per class. Let's say that your training a sentiment classification model, and you have a thousand examples for positive sentiment, you have five examples for negative sentiment. You can't train a robust model that can determine or distinguish between those two classes. You have to make sure that the training samples for each of those classes are reasonably balanced. So once you do that with data, the next step is training. As I mentioned before, our recommendation is that you run the different options that are available and figure out what is good, but how do you define what is good? You have to evaluate the model in order to figure out what suits your application. So the next step here in the work flow is evaluation. Evaluation in convention [inaudible] for machine learning is that when you procure your training data, you split your data into training set, into a validation set, and into a test set, and you typically tune the parameters of the algorithm using the validation set, and you test it on the test set. So we encourage you to do the same thing, apply the same sort of guidelines that have stood machine learning in good stead for a long time. The other thing that we also encourage you to do is test on out-of-domain data. What do I mean by this? So when you have an idea for an app, you think of a certain type of data that is going to be ingested by your machine learning model. Now let's say you're building an app for hotel reviews, and you want to classify hotel reviews into different sorts of ratings. And the user throws a data that is completely out of domain. Perhaps it's something to do with a restaurant review or a movie review, is your model robust enough to handle it. That's a question that you ought to ask yourself. And the final step is well in a conventional development workflow you write patches, you fix bugs, and you update your app. How do you do that with machine learning? Well, the way to do that or fix issues with machine learning is to find out where your models do not perform well, and you have to supplement it with the right sort of data. By adding data and retraining your model, you can essentially get a new model out. So it's, as I mentioned, it's very similar to sort of the development workflow, and they are very [inaudible]. So you can think of it as part of your fabric if you're employing machine learning models as part of your app, you can just combine it with the word process itself. The last thing I'd like to emphasize here is privacy. So everything that you saw in this session, all of the machine learning and Natural Language processing happens completely on device. So we at Apple take privacy really seriously, and this is a remarkable opportunity to use machine learning completely on device to protect user's privacy. So in that vein, Natural Language is another step towards privacy preserving machine learning but in this case apply to NLP. So in summary, we talked about a new framework called Natural Language framework. It's tightly integrated with the Apple machine learning [inaudible]. You can now train models using Create ML and then use those models either with the Core ML APIs or with Natural Language. The models that we generate using Natural Language and the APIs are highly performed and optimized on Apple hardware across all the platforms. And finally, it supports privacy because all of the machine learning in NLP happens on user's device. So there's more information here. We have a Natural Language lab tomorrow, so we encourage you to try out these APIs and come talk to us and ask us questions about where you'd like enhancements or perhaps some sort of consultation with respect to your app. We also have a machine learning get together, and there's a subsequent [inaudible] Create ML lab that's happening right now. So you can continue coming and talking to us as part of that lab. Thank you for your attention. Thanks.  Hello. Good afternoon and thank you for coming to What's New in WatchOS. In this session, we're going to give you a high-level overview of the new features you as developers, designers and product leaders can take advantage of to create great Watch experiences. And we're also going to dip into the details of the new APIs and interface builder options. My name is Lori Hylan-Cho, I work on the Watch frameworks team, and I am beyond thrilled to be here, to share with you the awesome new options for creating great Watch experiences in WatchOS 5. We've come a long way since our humble beginnings in 2015. And a great Watch experience now consists of several components, of which a Watch app is likely to be only one. Notifications, complications and Siri shortcuts work together to create a Watch experience that allows for brief and meaningful interactions with the right information at exactly the right time. And of course, with Series 3 Watches that support cellular, and the new Wi-Fi options that let you join networks directly from the Watch, more and more Watch wearers are venturing out without their phones. So, it's important to create a Watch experience that feels complete. Let's take a look at the features we've added to WatchOS 5 to help you deliver this kind of a great experience on the wrist. Ask any Apple Watch user what they like most about their Watch, and one of the first things they'll say is notifications. We're making notifications even richer this year with a few key improvements. Creating dynamic notifications with images and text has been an option since WatchOS 1. But if the user missed the notification when it arrived, chances were they'd never see it at all, as the Notification Center would display the static version. This was intentional, as we have more time to display the notification when it arrives than we do when displaying it from Notification Center. In WatchOS 5, we'll do our best to show that beautiful dynamic notification with no code changes or recompilation required on your part. We worked hard to make this possible for its own sake but being able to show dynamic notifications from Notification Center also makes possible the next two features I'm going to tell you about. The first of these is grouped notifications. You probably already know that you can have multiple categories and different notification interfaces for each one. And you might already be specifying a thread ID and the push notifications that get forwarded from iOS to the Watch. In WatchOS 5, this will cause notifications to be grouped automatically in Notification Center as you saw in the previous slide. Specifying a category and thread ID also opens up another possibility. Getting the same behavior as the built-in messages app, where new notifications with the same thread ID that come in when the original one is still on screen are appended to the existing interface. We made this kind of grouping behavior optional to give you a chance to handle being called multiple times for the same notification interface controller. To opt into this behavior, select your notifications category in the interface storyboard for your Watch app. And check the handle's grouping box in the attributes inspector. If the original notification is on screen when additional notifications that have a matching category and thread ID arrive, did receive notification will be called again on the existing notification interface controller. So, you should be prepared to append content to the existing notification interface. Perhaps by adding a blank line and the new body content to an existing label. Or by adding a new row to a table, as you saw on the previous example. And speaking of did receive notification, its signature has changed in WatchOS 5. This version, with the completion handler has been deprecated, in favor of this simplified version. We did this, so we'd have a clear boundary for when you're done processing your notification data. And we'd know that everything is ready to be shown on screen. We encourage you to do only the work necessary to display the notification and to do it as quickly as possible. By the way, it's not necessary for the user to keep their wrist up to see all the notifications with the same thread ID as they come in. If you're handling grouping and the dynamic interface for your notification category, all the messages with the same thread ID will expand to a single platter when the user taps on the group and Notification Center. And if additional messages on that thread come in, they'll be appended live is when the long look was first on screen. So, here's a big one. In WatchOS 5, you can now bring some of your apps' functionality directly into the notifications you send by including elements that let the user interact with content in the notification itself. For example, you could let a user pay for their ride and rate their driver, notify a user that their meter is about to expire, and let them extend their parking time, or give a diner an opportunity to not only confirm their dinner reservation, but to let you know they'll be a party of three instead of four. So, let's see how all this works. If you're creating a new notification interface controller, a dynamic interactive interface will be created for you automatically. If you already have a dynamic interface for your notification category, select the category in your Watch App's interface storyboard and check that has interactive interface box. In both cases, you'll notice that the old dynamic interface is still there, in addition to the dynamic interactive one. The dynamic interface will be shown in WatchOS 5 when the notification first arrives, and now in Notification Center as well, as I mentioned before. The dynamic interface will be shown in earlier versions of WatchOS so, you'll want to keep that one around for backwards compatibility. Once you have a dynamic interactive interface controller you can add buttons, switches and other interactive elements in the object library, which is now available at the top of the screen as a popover menu. You can even add gesture recognizers with your notification, though you should be aware that the system gestures will take precedence over any gestures you add in the same areas. Once you've designed your notification interface wire up the interactive elements to your code, the way you would any other interactive element in your app. As you can see here, I'm dragging my IB action for up button tapped to the button in my dynamic interactive interface. So, the amount of time will increase by 15 minutes every time the button is tapped. And with all this button tapping you may have already forgotten that notifications have always launched your app when tapped. This is still true for a regular dynamic and static notifications. But, we obviously had to disable this behavior for dynamic interactive notifications. Because we have interactive elements in them now. You can still launch your app from an interactive notification, if you need to. You just have to do it explicitly by calling a new method, performed notification default action. Similarly, if you include a button in your notification interface that should dismiss the notification after taking action, as in this case where tapping the extend button should both commit the changes to my rental time and dismiss the notification, you do this by calling perform dismiss action at the end of the IB action function associated with the button. You could also process the changes made in the body of the notification using a standard action button. Since action buttons always dismiss the notification. Wait, you're probably thinking, action buttons are shared between all the notification interfaces. And some of those buttons wouldn't make sense if the interactive elements weren't there. Well, new in WatchOS 5, you can now adjust the action buttons that are shown at runtime. There's a new notification actions property on the notification interface controller that returns the array of actions that will be displayed for the notification. You can set this property to a new array of UN notification action objects in your did receive notification call back. which gives you the flexibility to add or remove buttons to suit your interactive interface. And in an upcoming seed, we're going to allow changes to actions any time after did receive notification is called. Which will give you the flexibility to change the action buttons based on how the user interacts with the elements in the interactive notification interface. So, that's a lot of information about notifications and the options for creating them so far. But I have two more things to tell you about before we move on. Critical alerts are a new kind of notification that will cause a prominent haptic and a sound to play even when your Watch is silenced or in do not disturb mode. And thus, they can be used to deliver extra urgent information. If your Watch app integrates with a medical device or is used by emergency responders, this kind of alert might be for you. Critical alerts require an app entitlements and explicit permission to be granted by the user separate from regular notification. The opposite of a critical alert is one that can be delivered quietly. That is, rather than interrupting the user with an in the moment notification that takes over the screen, you can now choose to send notifications directly to Notification Center. You'll notice that no indicator will appear unless a prominently delivered notification is also in Notification Center. But the swipe from the top of the screen will still reveal the quietly delivered notification. If you choose to deliver notifications quietly in your app, you don't have to prompt the user to allow notifications when the app is first launched. Instead, you can request provisional permission. This gives the user a chance to see what kinds of notifications your app sends before having to decide whether they want to see them as they arrive. Whichever option you choose as a developer, the user still has ultimate control over how they receive your notifications. They can choose to deliver notifications quietly by swiping left from a notification in Notification Center or change their notification preferences in settings. To sum up, notifications are now more dynamic. Users will see your dynamic interface from Notification Center in addition to when the notification first arrives. We're also offering grouping by thread ID for the first time on the Watch. So, you can design notifications that behave like the ones like the built-in messages app. You can now bring more of your app experience directly into the notification your app delivers with interactive controls and action buttons that can be defined at runtime. And you can also choose to deliver notifications with the appropriate level of urgency. To learn more about notifications you can attend one of these sessions. I highly recommend Designing Notifications, that will give you some great tips for how to design effective interactive notifications. Okay, now that we have your user's attention with notifications, let's turn our attention to the features we're adding to make your Watch apps more awesome. The first has to do with local audio playback. Some of you have already made forays into audio playback on the Watch by taking advantage of URL session for downloading files to the Watch. And WK audio file queue player to play them. With WatchOS 5, we're going to make developing audio apps for the Watch even easier, and the resulting experiences more delightful. In WatchOS 5, we're offering a new background mode for playing local audio files. So, you can now focus on your app's main purpose, audio, rather than trying to figure out how to build something else like a workout app just to be able to play audio. We also exposed AVAudio player and AVAudioEngine directly, which means you can use the same methods and properties you're already familiar with if you've been building iOS apps that play audio. In fact, you can share your code between your iOS app and your Watch app by moving your playback related code to a framework. One thing that's different from iOS, is that playing longform audio on the Watch requires headphones or an external speaker to be connected just as when playing from the built-in music app. For this reason, Bluetooth routing is part of the session activation process. If you set your route sharing policy to longform, as you should, will automatically connect to the headphones with Apple wireless chip, like AirPods or Beats Studio3, if they're already in use. Or, will show a route picker to let the user choose other headphones or Bluetooth speakers when you call the activate with options completion API in a session that's new. You could also use the MP now playing info center API to populate the now playing app with information about what your app is playing. Which means your apps info will show in the now playing complication. And you can handle the media remote commands that make sense for your app; from play and pause, to next and previous, to even like and dislike. And last, but not least, it's now possible to control the volume from your custom playback UI with the new volume control view. Available in the object library and interface builder. The control automatically takes on the tint color of your app when at rest and responds with the color changes you're used to from the system volume control when turning the digital crown. There's a whole session devoted to creating audio apps for WatchOS which I highly encourage you to attend to get more details about the new APIs and best practices for working with audio. And we'll also be prepared to talk about background audio at the WatchOS runtime and conductivity lab on Thursday. If the primary function of your app isn't playing audio, but your app would benefit from being able to control audio playing elsewhere on a system, whether it's on a Watch or on phone, as in the workout app, where you can swipe left during a workout to see now playing controls. You'll be happy to hear that were exposing a now playing view that you can embed in your apps. You can find the now playing view in the object library in interface builder. It's designed to fill the screen. So, it works best in apps with page layouts. Note that the now playing view is a systemwide control that's meant to control audio coming from other apps. So, it will show whatever the user is currently listening to, whether it's on Apple Watch or iPhone. While we're here, I want to point out a couple of other new options and one change in behavior. In previous versions of WatchOS, if you added a 38-millimeter asset but forgot to add the same asset for a 42-millimeter device, the asset would be missing on the larger device. Now it will automatically fall back to the 38 millimeter size if the 42 millimeter size is missing. But you can skip worrying about providing assets of different sizes altogether by instead adding a PDF in the universal selection of your asset catalog and setting the new auto scaling option to automatic. This will cause the right sized asset to show up at the right place at the right time. We've also exposed the title textiles in the font menu to give you more options for differentiating your text within your interfaces. These textiles are compatible with dynamic type, so they'll increase or decrease. They'll scale up and down, as the user changes their font size in settings. And the large title style is available in both interface builder and as an API. Since I'm in the process of building an app for skating workouts, now would be a good time to talk about the improvements we're making to workouts in WatchOS 5. We completely rewrote the workout API in this release to make it simpler, more reliable, and more resilient. And we've moved the built-in workout app to use it. We're hoping you'll move your fitness apps to it to. It's now easier than ever to start a workout and collect the right data with a new initializer from HK workout session and the new workout builder API. You create the workout session, get the builder from the session and start collecting data. It's that simple. This is what it looks like in code. You create and configure the session with a health store and a workout configuration, which includes the activity type. Grab the HK live workout builder that's associated with the session and begin collection. The relevant data for the workout type will be collected automatically, even across pauses and resumes, to give you a correct and consistent HK workout artifact with a correct elapsed time. And because no app is perfect, if your fitness app crashes during an active workout session it will automatically be relaunched. Just use the HK health store recover active workout session API and the session and builder will be restored in their previous state. To learn even more about the new workout APIs, as well as options for collecting health and fitness data check out New Ways to Work With Workouts or visit the health and fitness technology lab tomorrow afternoon. Next up, your apps now have a place on the Siri Watch face with Siri Shortcuts. As you saw on the keynote, Siri Shortcuts are meant to help users accomplish tasks that they perform frequently with more ease than ever before. And the Siri Watch face surfaces these, the common tasks, at the right place and time with a little input from you as app developers. I want to focus on how to use shortcuts to make a great Watch experience and how that experience differs depending on whether your Watch app is installed. First, a few words about what makes a good shortcut. As the name implies, shortcuts are about helping Watch users see useful information and perform common tasks quickly. Whether it's launching your app to a specific screen with options preselected, or getting to the awesome outcome your app enables, such as ordering your morning coffee, booking a fine dining reservation, or re-upping your chocolate supply. Think about glanceable information and one or two tap interactions. Shortcuts can be user activity based, or intent based. User activity based shortcuts are great for when the goal is to launch your app with a specific context, such as navigating straight to a screen where the user can log a meal they've eaten, or in this case, delivered. Intent based shortcuts make more sense for tasks that don't necessarily require your app to be launched. Such as placing your regular coffee order. If your shortcut is intent-based, when the user taps your shortcut platter on the Siri face, they'll receive a quick confirmation screen. Did you mean to order that coffee? If the intent supports background execution, it will run without launching the app, when the user confirms. Otherwise, the confirmation will launch or restore the app and hand it your intent. How the user has interacted with your app in the past is taken into account when predicting what shortcuts to show. You should donate intense or user activities as the user performs the main functions of your app. For example, an audio app like Audible would donate an IN media, play media intent when the user starts or resumes playing an audiobook. Shortcut surface on the Siri Watch face based on relevance with the most relevant items appearing at the top of the up next section and less relevant ones at the bottom. Previous user interactions intense performed or donated and user activities that have been marked both eligible for search and eligible for prediction inform relevance. But to appear on the Siri Watch face, you must also give the system explicit hints about when or where a shortcut is relevant by creating a relevant shortcut. A relevant shortcut consists of a user activity or intent-based Siri shortcut, an optional Watch template that defines the title, subtitle, and image that should appear in the shortcut platter, and an array of relevance providers that define the time, location or situation in which your shortcut would be most relevant. We've already talked about user activity and intent-based shortcuts, so let's talk about the other two components of a relevant shortcut. A Watch template is an optional IN default card template, that consists of a title, an optional title, and an optional image. If you don't provide a Watch template, we'll pull the necessary information from the IN shortcut, but you should take the opportunity to provide informative strings so the user understands what they get when they tap your platter. A relevance provider focuses on the date, location or situation in which the shortcut would be most valuable. Think about when and where your shortcut would be relevant. Is it something the user might glance at or tap on at any time during the day? Is it relevant at a specific date and time, or is it based on location? Since a relevant shortcut takes an array of relevance providers it's possible to specify more than one kind. For example, if your shortcut would be relevant at both a time and a place. Again, these relevant providers act as hints to the system. As the user interacts with your content, actual usage will inform its placement. If the user fails to interact with shortcuts that appear on the Siri face, they'll drop in relevance. Once you have relevant shortcuts, the next step is to supply them to the relevant shortcut store so they can be considered for the Siri Watch face. Relevant shortcuts can be supplied by both your iOS app and your Watch app. iOS relevant shortcuts are synced periodically from iPhone Apple Watch and they're merged into consideration with the Watch relevant shortcuts. If your Watch app supports the iOS shortcut, the Watch app handles execution. If it doesn't, or if the Watch app is not installed, the shortcut executes on the phone, over Internet. A subset of relevant shortcuts can surface on the Siri Watch face, without Watch OS support. If they are intent-based shortcuts that support running in the background and can run without accessing encrypted data, which is separate from off. Again, these shortcuts are going to execute on iPhone over Internet. The richest experience for users will always be with a Watch app that can handle the shortcut locally, either by launching the app or by executing the intent in the background. You'll want to update your relevant shortcuts periodically. If the user launches your app, for example, by tapping one of your shortcuts on the Siri face, you'll have the run time to accomplish the update. But shortcuts that provide glanceable data are useful to the user without launching the app. Think about the carrot whether example from a couple slides ago, were you could tell that it was 72 degrees and cloudy just by looking at the platter. You didn't have to tap. To help with this case, we added a background refresh task called WKRelevantShortcut RefreshBackgroundTask. When you get this task before sure to check if your data needs to be updated and then supply new relevant shortcuts. We factor in user engagement when scheduling these background tasks and glances count as engagement. If you have an intent-based shortcut, your intent might update applications when it executes in the background. Since the intents extension is in a separate process from your Watch Kit extension, we're now supplying a new refresh background task called WKIntentDidRun RefreshBackgroundTask to let your main extension run and update any snapshots or complications that may be stale after your intent executed. Now that you have your shortcuts available at the turn of the wrist and a tap, you should know that users can set up shortcut phrases on the phone and use them to execute your shortcuts at any time. These shortcut phrases are synced to Apple Watch. So, you can use raise to speak to say, for example, mint me up and kick of an order of a mint mojito from Phil's Coffee. Your shortcut platter does not need to be showing on the Siri Watch face for your shortcuts to be invoked with a shortcut phrase. So, in summary, use Siri shortcuts to provide relevant information and quick interactions to your users. Create relevant shortcuts on both iPhone and Apple Watch and add them to the shortcut store so they can be considered for the Siri Watch face. And finally, your users will have the best experience with a WatchOS app where they'll be able to do more with your shortcuts directly on the Watch. For more on Siri shortcuts, I recommend checking out one of these sessions or labs, especially the one about Siri shortcuts on the Siri Watch face, which will go into much more detail about the APIs I covered today. So, we covered a lot of ground today. Here's what we talked about. We want you to make your notifications more engaging and actionable by creating dynamic interactive interfaces for your notifications. You can now bring audio into the background with AV foundation APIs, media remote commands and access to now playing info. You can make your fitness apps more robust and reliable with the new workout builder API. And you can provide value on the Siri Watch face by creating relevant shortcuts. If you have any questions about what you've heard today what you built for Watch OS so far, or what you're going to build next come visit us in the labs. We'd love to see what you're working on. Thank you for coming and have a great WWDC.  Hi everyone. Welcome to session 221, TextKit Best Practices. I'm Donna Tom. And I'm the TextKit engineer. And my colleague Emily Van Haren from authoring tools will be joining me today. And we're both really excited to share with you some best practices for working with TextKit. So let's get started. First, we're going to review some key concepts for working with TextKit. Then, we'll dive into some examples to illustrate how to apply the key concepts in your app. And finally, we'll wrap up with some best practices in the areas of correctness, performance, and security. So let's start with the key concepts. Now to make sure we're on the same page, we're going to start at the very beginning. What is TextKit? And your first instinct might be to open a shiny new playground in Xcode and type import TextKit, except if you've ever actually tried this, you found that it doesn't work. And that's because TextKit is a little different than other frameworks you might have used. You don't have to import anything special to use it. The text controls in UIKit and AppKit are built on top of TextKit. And so if you've ever used a label, the text field, or a text view, you've actually used TextKit. And TextKit pulls together powerful underlying technologies such as Core Text, Core Graphics and Foundation to make it simple and seamless for your apps to show text. And every time you use one of these built-in controls, you're using the power of TextKit to show or edit text in a fully, internationalized, localizable manner without having to directly use these underlying technologies or understand the intricacies of complex scripts. And there are a lot of things you get for free too like all of these display features that you see here. And for editing, you'll also get access to all the tech services that are supported by the OS like accessibility, spell checking and more. And you can take advantage of all of these great features without having to write a single line of code and that's pretty awesome. And so with all of this functionality at your fingertips, how do you decide which control to use? So let's talk about that, choosing the right control for your situation. And the options are going to be a little bit different depending on whether you're using UIKit or AppKit. So let's review them separately. All right. Let's start with UIKit. And first you're going to consider whether you need text input. And if you don't need text input, then consider whether you need selection or scrolling. And if you don't need these, then you should use UILabel. UILabels are intended for small amounts of text like a few words or a few lines. And so if you have more text than that or if you need these selection or scrolling capabilities then you should use a UITextView with editing disabled. Now going back to the top. If you do need text input, then consider whether you need secure text entry. And this would be like a password field where the text is obscured and copying is disabled. And so if you need that, then use UITextField because this is the only control that supports secure text entry. Otherwise, think about how much text you expect to be entered. And if you want something that's like a form field input that only needs a line, then use UITextField. And UITextField only supports one line of text entry. Otherwise, if you need more than that, you can use UITextView. And so now here's that same decision process for AppKit. And it's similar to the UIKit process but there's a few small differences. So, again, you're going to start by considering whether you need text input. And AppKit doesn't have a label control. So if you need to display text, use an NSTextField and you can disable both editing and selection to get that label behavior. Now going back to the top here. If you do need text input, again ask if you need secure text entry. And if so, you can use NSSecureTextField. Otherwise, we're going to ask our favorite question, how much text do you expect? So NSTextView is optimized for performance with large amounts of text. And so if you're expecting a lot of text, you should use NSTextView otherwise you can use NSTextField. Now, unlike its UIKit counterpart, NSTextField does support multiple lines of text, but it's still optimized for shorter strings and so you should still use NSTextView if you have a lot of text. Now those of you who have been around the block a few times with TextKit might notice that the flow charts are missing an option and that's string drawing. And you use string drawing by directly calling draw in point or draw in rect methods under NSString or your NSAttributedString. And many of you may be using this for the performance benefit to avoid the overhead of view objects at the kit level. And so if you're going to go this route, please keep the following tips in mind. You want to use it for small amounts of static text. And you want to limit how frequently you call the draw methods. Now if you're calling the string drawing methods a lot, you might actually get better performance out of a label or a text field because these controls provide better caching, especially if you're using auto layout. And if you're drawing an attributed string with a lot of custom attributes, this could also be slowing down your string drawing because the text system needs to validate all of the attributes before rendering and so for best performance, you should strip out extra attributes before drawing and only pass in the ones that are needed to determine the visual appearance like font or like color. And finally, remember that by using string drawing, you'll miss out on all of this free functionality that's offered by the text controls, so you should use the text controls whenever possible. So now you know what you can do with TextKit just by using the built-in controls. But if you want to go beyond what these controls provide, you'll need to find the right customization point within the text stack. And like much of Cocoa, TextKit is based on the model view controller design pattern. And the text system can be divided into three phases that correspond it directly to NBC and that's storage, display, and layout. And so now let's take a closer look at the TextKit objects that make up each of these phases. And we'll start with the storage which corresponds to the model. Now NSTextStorage holds your string data and your attributes. It's a subclass of mutable attributed string and so you can work with it in the same way that you already know how to work with attributed strings. And my colleague Emily will show you some really powerful ways to customize the text storage a little bit later so stay tuned for that. Now NSTextContainer models the geometry of the area where your text will be laid out. And by default, it's a rectangle but you can customize the flow or the shape of the text layout as shown here. And for more detailed information on working with the storage objects, check out these great past WWDC sessions and documentation. And they'll be available from the more information link at the end of the session. And next up is the display phase and that corresponds to the view. And we've already talked about the display phase quite a bit when we talked about choosing the right control. And so for additional information, you can again check out these documentation resources. And they'll also be accessible from that more information link at the end of the session. And finally, we have the layout phase which corresponds to the controller. And NSLayoutManager is the only component in this phase. And let me tell you it is a beast. And I mean that in a good way because it's so awesome at what it does. So it's the brains of the whole operation. It coordinates changes between all of the phases, and it controls the layout process itself. So here's a quick overview of how that layout process works. So text layout happens after the system fixes attributes in the text storage to remove inconsistencies like making sure that all the characters in the string are covered by fonts that support displaying those characters. And so in this example, the Times New Roman font is specified for the entire string, but this font doesn't support displaying Japanese kanji or emoji. And so after attribute fixing, your text storage will look something like this with an appropriate Japanese font assigned to the Japanese characters and the emoji font assigned to the emoji character. All right. So once the attributes are fixed, the layout process can begin. And we can think of layout in two steps: glyph generation followed by glyph layout. And once they're laid out, they're ready for display. But wait a minute. What's a glyph? Let's back up and review that. A glyph is a visual representation of one or more characters. And as you can see here, the mapping between characters and glyphs is not always one-to-one. So here this string ffi has three characters, but it could be represented by a single glyph for the ligature. And you can go in the other direction too. Here we have n [inaudible] which is a single character that can be represented by multiple glyphs: one for the n and one for the tilde. And so going back to our diagram here, we have NSLayoutManager performing glyph generation and glyph layout. And glyph generation is where the layout manager takes the characters and figures out what glyphs need to be drawn. And glyph layout is where the layout manager positions those glyphs for display in your view. And there's a lot more to learn about the layout manager from these past WWDC sessions and documentation. And you can access them from, you guessed it, the more information link at the end of the session. Okay. So now you understand the phases of the text system. And you know the TextKit components that make up each phase. So now let's explore choosing the right configuration of these components to create different effects. So this is your standard configuration. And when you drag and drop a text view from Interface Builder, you'll automatically get one of each component as shown here. And most the time this is all you're going to need. If you want a multiple page or a multiple column layout, you can use pairs of text containers and text views, one pair for each page or column. And you can hook all of these up to the same layout manager in the same text storage so that they share the layout information in the backing store. And if you want different layouts in each of you, you can do that too, just use multiple layout managers. And, again, since the text shares the same backing store, updating that text will update all of the views. Now we didn't go into too much detail about these configurations because there's a great past session that's already done that so check out WWDC 2010 session Advanced Cocoa Text Tips and Tricks. And this will be accessible from that more information link at the end of the session. All right. So we've looked at the built-in text controls. We've looked at the components in TextKit. And we've look at how to configure those components to achieve different effects. And there's a lot that you can do with that knowledge already, but if you need even more, you'll need to extend and customize parts of TextKit yourself. And so now we'll talk a little bit about choosing the right approach for doing that. And choosing the right approach is like building up your text toolbox. It's like going to the store because you need a hammer. And then when you get there, you encounter this giant wall of hammers to choose from. And you want to pick your hammer that can do the job and ideally the least expensive one that will do what you need. And so these are the hammers that are available to us. Delegation is like your standard hammer with the claw on the end, and it's used to perform multiple tasks. So the delegates have a lot of different customization hooks and most of the time they'll get the job done for you. Notifications is like a ball-peen hammer. And this has a ball on the end instead of a claw so it's more specialized and it's better suited for certain tasks, but it's not as versatile as the standard hammer of delegation. And finally, subclassing is your sledgehammer. The sledgehammer is very powerful, and you can use it for just about anything that you would need a hammer for but it's probably overkill for a lot of things. And with that, I'd like to invite Emily up to show us how to use these different kinds of hammers. Emily. Thank you, Donna. So, as developers, we have a collection of controls to choose from, various configurations, and a wide range of customization options to achieve what we need. So our tool chest is stocked full, but how do we know what tools to choose? So we're going to take a look at some examples of apps that harness the power of TextKit. And we don't have to look very far because almost every app that we use displays or edits text. We're going to start by looking at two apps that we're all familiar with and then go through the steps of building our own. So the first app we're going to look at is Apple News on iOS, which is a beautiful app that displays text in a personalized and curated articles. So here's an example of an article that is featured in the spotlight tab. Now the top of the app shows some details about this article. Now how could we use TextKit to re-create this look and feel? So let's consider the flow chart that Donna showed us earlier to pick the control that's best suited for this example. So we have a handful of text controls to choose from, but since we want to display small amounts of text, each on a single line, we'll use a label. Now we can see that there is a ton of customization options in the inspector panel. So we're going to go ahead and change the text to spotlight. We're going to change the font to use the body style. And we're going to enable dynamic type, which allows those with accessibility settings enabled to see text in a font size and style that is appropriate for their needs. Now it's great that we can customize this label in Interface Builder, but we can also see all these properties in Swift. So we can set the text and the formatting properties dynamically at runtime. Now back in Interface Builder, we'll go ahead and add two more labels. Now everything fits pretty well, but we have one more thing we need to do here. So looking back at Apple News, we can see that the text on the right is actually displayed with two different colors. Part of it's black and part of it's white. Now we could achieve this with two separate labels, but if we wanted to use just one label, we wouldn't be able to do this in Interface Builder. So how could we do this? Well, we can take advantage of the power and flexibility of attributed strings. Now an attributed string is a run of characters that can have attributes applied to ranges of characters. Now some attributes you get for free like the default font and text color, but we can override these attributes with our own values. In this case, we're going to set part of our string's text color to white. Now to see attributed string in action, we'll use the add attribute method on NSMutableAttributedString to set the text color to white just for the range that we want. And this time we'll set the attributed text property on our label. At runtime, this looks pretty spiffy. Now UILabels were a great choice for this sort of text. Now if we look at the bottom of the screen, we'll see a headline. Now this is also text, but it's a little bit bigger and it spans multiple lines. Another thing that makes this text different is that it's selectable. So which control should we use this time? Now both text field and text view support selection but text field is meant for usually just one line. So in this case, since our headline can span multiple lines, we're going to use a text view. Now when we put a text view onto our storyboard, we can see that we get a lot of lorem ipsum text by default. So we're going to go ahead and change the text in the inspector panel. We're also going to change the font to look a little bit more like Apple News. And we want to disable the editing feature because the headline isn't really editable. Now UITextView scroll by default because they are a subclass of UIScrollView. But if we want our text view to play well with auto layout, we want to disable scrolling. So this will allow the bounds of our text view to resize to fit the text. Last but not least, this white background really needs to go, so we're going to set it to transparent. Now Interface Builder made it really easy to customize this text view but just like our labels before, we can set all this in code. So here in Swift, we can set the text and the formatting properties dynamically at runtime. So we looked at Apple News to pick the right control, but now we're going to look at a different app that we're all familiar with to choose the right configuration and that's TextEdit. Now TextEdit is an app on macOS that handles display and editing of rich text content. Now what most people don't know is that TextEdit is actually a really thin wrapper around NSTextView. So I want to take a moment to marvel at just how much we get for free with TextKit. So this is the inspector bar, and we get this for free just by checking a checkbox in Interface Builder. And right below it is a ruler view which we also get for free just by enabling it. And everything below that is just a text view. Actually, it's a text view, text container, layout manager, and text storage. Now this is the standard configuration for both NSTextView and UITextView, but the similarities mostly stop there. So, for example, tables are only supported in NSTextView. And marveling again at the power that we get for free, TextKit provides a table editor that does all the heavy lifting for us. Now when we use TextEdit, we're often editing large amounts of text. Sometimes we paste in a lot of lorem ipsum to see that we also get a spell-checker for free. But really what we want to see is that when we use the format menu to choose wrap to page, we end up with it looking a little bit more like a page. We can see that the text container has been resized to match the dimensions of a piece of paper. Now if we scroll down, we can see that the text jumps from the first page to the second. Now the standard configuration doesn't really support layout like this. Sure enough, this layout uses two text views and text containers. Now they're still managed by the same layout manager and text storage, which allows the text to freely jump from one page to the next. Now if you'd like to see more about how TextEdit works, you can actually find its source in the guides and sample codes library. So we've picked the right controls, we've picked the right configuration, but sometimes we actually need to hammer on these to achieve what we want. But how do we decide which hammer to use? So we're going to try and pick the right hammer for the job when we go through the steps of building a journal app together. We'll start by putting today's date on to the window. Now we don't have UILabels in AppKit, but we can make a text field behave like a label. All we need to do is disable editing. Now for the journal entry part of the window, we're going to use a text view. So in the inspector, we can make sure that the text view is editable and selectable and supports rich text and undo. We're going to add a couple of text fields to the bottom of the window as well so that we can show how many words have been written. Now when we run our app, we want the word count at the bottom to change, so let's find the right hammer for this job. Now we can either conform to a delegate, handle a notification or subclass. But in this case, we're going to use a small hammer. And we're going to listen for a notification from text storage. Now we can get the number of words from the text storage. And when we hear the notification, we can update the string value property of our text field. And when we start typing, we can see the word count change. Now if we want to emphasize part of our text, we can use keyboard shortcuts or the menu to apply formatting like bold. But it would be great if we could support modern text formatting like Markdown, which uses control characters to specify formatting. So if we start inserting asterisks before and after, we want it to be bold. But which hammer should we use for this? Well, we want to know when a change happens, and we want to know where a change happens. But notifications don't really give us much information about this change. So we're going to use a bigger hammer and implement the text storage delegate, specifically the didProcessEditing method. Now we can make a new bold font from our existing one. And we can add that font directly to our text storage for the range that we want to be bold. And now when we insert that last asterisk, we can make it bold. Now we're feeling pretty good about this whole Markdown thing so what if we try inserting a code snippet? Now in Markdown it looks like this. And if we add this last back-tick, we want it to look like a code block. It should have a background and a header that says Swift Code. Now this is actually a complex task, so we're going to need two sledgehammers. And the first is a subclass NSTextStorage. Now when we subclass NSTextStorage, we need to implement four required methods. And we can do this by operating on a private instance of a mutable string. Now let's pay attention to the replaceCharacters method. Now we can add an NSTextBlock to our paragraph style. And then we can add that paragraph style to our text storage over the range of that code block. Now NSTextBlock by itself doesn't do any custom drawing by itself. So we'll need to subclass that too. Our NSTextBlock subclass needs to have some padding with some extra padding on the top and a light gray background. We'll override drawBackground and use string drawing to draw the header Swift Code. Now this is actually all we need to do to make a text block look like a code snippet. Now back in our custom text storage, we can create an instance of our new code block instead of using a plain text block. Now, last but not least, we need to tell our text view to use one of our custom text storages, so we'll replace the text storage on the layout manager. Now this is turning into a real WYSIWYG Markdown editor. Now a popular feature of most Markdown editor's is a side-by-side view with an editing version on the left and a rendering on the right. Now we can do this with two text views side-by-side. We'll disable editing for the one on the right. And now we have two text views but we want them to display the same content but look a little different on the right. So we want a configuration like this where we have one text storage but two of everything else. To do this, we will replace the text storage on the right with that from the left. Now let's see what this looks like. Now this is actually really cool. If we add any characters to the left, they'll show up immediately on the right-hand side. Now usually the right-hand side doesn't really show the Markdown characters but since this is a shared text storage, it means we have to hide the characters during the layout process. Now since we need to do it this way, we really only have one option and that's to implement the shouldGenerateGlyphs method on the NSLayoutManager delegate. This will allow us to intervene in the glyph generation process. So we can take the glyphs that are about to be laid out and if they represent a Markdown control character, we can apply the null property to that glyph. Now this will eliminate the glyph altogether during the layout process without changing the underlying text storage. Then, we will use the new glyphs and tell the layout manager that we want to present these glyphs with our new properties. Now this is actually really cool. So the left-hand side shows an editable version with all the Markdown characters included. And the right-hand side shows no Markdown characters all, all using the same text storage. Now building a side-by-side Markdown editor is not something all of us do every day, but it was really good to see how customizable TextKit is with real world examples. If you'd like to learn more about how to use and customize TextKit, check out our amazing programming guides. And with that, I will hand it back to Donna. Thanks, Emily. Those are some really cool examples. And I really hope you'll be able to take some of the techniques that she showed off and use them in your own apps. But now let's shift gears a bit and talk about some best practices for working with text. So on the topic of correctness, if your text doesn't render the way you expect, it could be related to incomplete or incorrect attributes on your attributed string. And so let's take a look at an example to see this in practice. Let's say we have a UITextView with some attributed text that says don't hate. And it says this in the font Comic Sans 24 point. And we want to programmatically apply a bold typeface to the word don't because if there's any font more universally hated than Comic Sans, it's Comic Sans bold. And so at first blush, it might seem reasonable to write code like this. Now here we have our original font. And we're going to use a font descriptor to create a bold version of this original font. Then, we're going to initialize our mutable attributed string using the original text. We're going to apply our new font or new bold font to the word don't and that's going to be the first five characters. And then we're going to set the attributed text property of our UITextView to use this new attributed string except when we do that we'll see that our new bold font applied to the word don't just as we expected but the rest of the string somehow lost the original font. And now those of you who despise Comic Sans might be happy about that, but the result is wrong and so that warrants a sad face. So why did this happen? And to answer that, let's take a closer look at how we're initializing our attributed string. So notice that we're using a plain text string to initialize it, and we're using the initializer with no attribute information. And when you create a new attributed string and you don't provide any attribute information, that new attributed string, we use the default attributes. And the default font is Helvetica 12 point. And so to recap what happened, we started with this original attributed string with the font Comic Sans 24 applied to the entire range. And then we created this new attributed string, and it got initialized with the default attributes. And we applied our bold font to the word don't on this new string, and we ended up with this incorrect result here where the word don't is in Comic Sans bold 24, and the rest of the string is in the default font of Helvetica 12. And so there are two different ways that we could do this correctly and one way is to avoid mixing the plain and attributed text altogether. So by initializing our new attributed string using the original one, we're going to keep those original attributes. And then we can apply our new attributes without getting this reset effect with the default ones. But it's not always feasible to just avoid mixing plain and attributed text. So if you've got to mix it up, you can explicitly supply the attributes when creating that new attributed string from the plain text string. And if we make sure to apply the same attributes from the original text, we'll get the correct result. But you should be aware that this reset effect can happen with any attributes that have default values and not just fonts. And as you can see, there are a lot of attributes with default values. So I'd like to call out the paragraph style here in particular as being a sneaky reset point. And to see why, we'll revisit our earlier example. But instead of changing the font, we're going to change the paragraph style to truncate the word hate because nobody likes hate. So we want our text to look like this, but when we run this code, we'll get a result like this with all of the text in Helvetica 12 and using the default paragraph style with the default line break mode of word wrapping. And, again, this is really great for those of you who loathe Comic Sans because it's been totally eliminated from the string but it's wrong. And it's wrong in a different way from last time. And to understand the difference, let's recall that attribute fixing happens before layout and this is where the system repairs the inconsistent attributes. And so here in our attributed string we have a single paragraph with multiple paragraph styles and that's pretty inconsistent. So when the system fixes the attributes of this string, it's going to take the first paragraph style it finds and apply it to the entire paragraph. And that's how we ended up with our attributed string displaying with the default paragraph style. And the key take away here is to be explicit with your attributes, especially when you're mixing plain and attributed text. So by doing this, you're going to avoid this reset effect with the default attributes. And for AppKit developers, this is actually super important if you're updating your app for dark mode. So by using the explicit attributes with the dynamic colors like NSColor.textColor, you'll ensure that your text is drawn with the correct colors for the context. So moving on. The next topic is performance. If you're working with large amounts of text, a good way to improve your apps performance is to use noncontinuous layout. And to understand what that means, let's revisit our old friend the layout process. We said that the layout process consists of glyph generation followed by glyph layout. And so with continuous layout, the layout manager is going to perform glyph generation and glyph layout starting at the beginning of the text storage. And it goes in order from the beginning to the end. And so if someone using your app scrolls to some point in the middle of your text view, the layout manager has to generate and layout the glyphs for all the glyphs that come before that point as indicated by the red rectangle. And note that this also includes the text that you can't see that's been scrolled off the top of the screen all the way back to the beginning of the text storage. And so if you have a lot of text, that poor person might have to wait a while for your app to finish layout but luckily, we can avoid this situation by using noncontinuous layout. And so as the name implies, with noncontinuous layout, the layout manager doesn't have to do glyph generation and layout in order from the beginning of the text storage. So now when that person, using your app, scrolls to the middle of your text view, the layout manager can perform glyph generation and layout for that middle section right away. So if your text storage has a lot of text in it, using noncontinuous layout is a huge performance win. Great. So how do you turn this on? Well, noncontinuous layout is a property of NSLayoutManager. And so for NSTextView, you can access the text to use layout manager and then you can set that property there. For UITextView, you usually don't have to do anything because this is turned on by default, but there's just one important thing to remember. Since UITextView is a subclass of UIScrollView, noncontinuous layout will require scrolling to be enabled. And this is because when you disable scrolling, asking for the intrinsic content size of your text view is going to require laying out all the text and then you wouldn't get the performance benefits of noncontinuous layout in the first place. And that brings me to a really important point. You should avoid requesting layout for all or most of the text at once when you're using noncontinuous layout since that kind of defeats the purpose of using it in the first place. So if you have only one text container, don't ask for the layout of the entire thing. And don't ask for layout for large ranges of characters or glyphs that include the end of the text. And we didn't dig too deeply into the topic of text performance here because I gave a great talk on this last year at WWDC 2017, Efficient unteractions with Frameworks. And you can access the video from that more information link at the end of the session. All right. Now it's time to talk about everyone's favorite topic, security. So you may have noticed that there have been incidents in the recent past where some people on the Internet have exploited bugs in our software to cause problems for people who use our products. And in response, we're continuing to devise techniques for mitigating these kinds of attacks, but today I'd like to talk about how we can work together to provide a stronger defense against these attacks. So you may have heard of the concept defense in depth. And in case you're not familiar with the terms, defense in depth refers to creating multiple layers of protection to defend against threats. And this concept has been around for centuries. You can see it in the design of medieval castles. The land around the outside is clear of trees so you can see attackers coming. And there's a moat to make approaching the castle more difficult and to prevent tunneling underneath it. And the walls are another defense. They're built tall so that they're difficult to climb. And there are arrow slits in the walls and crenellations at the top to allow defenders to fire on attackers from protected locations. Now any one of these individual protections might not be enough to fend off an attack but collectively they provide a strong defense. And like the castle, we here at Apple provide multiple layers of defense against attacks, but there's nothing stopping you from also taking your own defensive measures in your app or framework. And by doing this, you're adding another layer of protection and improving your product security. Everyone wins. So let's talk about what you can do here. And something I'd like you to consider is setting limits on text input in your app or framework. And now I'd like to emphasize that this might not always make sense to do. So, for example, if your app is an authoring tool like that journal app that Emily showed earlier, it wouldn't really make any sense to set a limit on the length of the text there. So if it doesn't make sense, you shouldn't do it. But in contrast, if your phone app has a text field for assigning a nickname to an account, you probably have some idea what a reasonable limit would be there. And it's a good idea to set these limits because all text input is potentially untrusted. When you allow text input, you allow copy and paste. You don't know what kind of text can be pasted in there. It could be anything. It could be a string with malicious character combinations, or it could just be a string that's really, really, really long. And even though long strings like that may not be malicious in themselves, it could cause your app to freeze or hang. So if you have a text field that's intended for one line of input and someone pastes the entire contents of "War and Peace" into it, which is about 3.1 million characters in English, is that reasonable? Probably not. So this is a great example of a case where it makes sense to impose your own limits. And here are the recommended approaches for setting these kinds of limits. You want to validate the input string before it's set on the text field. And so for UITextFields, you can do this by using UITextFieldDelegate. And for NSTextFields, you should use a custom NSFormatter to implement your validation logic. Oh, and we've also got some additional security enhancements coming your way. So keep an eye out for them in the release notes and come see us at the labs this week if you have any questions. All right. We're just about out of time so let's recap. You know how to choose the right control, customization point, and customization approach and you know the best practices to follow in the areas of correctness, performance, and security. So use this knowledge to go forth and create great things with TextKit. Oh, and before you go, here's that super important more information link where you can find all of the great past sessions and documentation that we've referenced today. And come visit us at the labs on Thursday and Friday. Thank you and enjoy the rest of the conference.  Good morning, everybody. My name is Pavel and Marin and I will be talking to you about ClassKit today. Our talk is split in three parts. In first part, we will give you a general overview of ClassKit. We will show you how data flows through the system. In the second part of the talk, we will list all the classes we have in our system and we'll mention a few interesting things about each one class. And in the end, Marin is going to show us how to adopt ClassKit in an existing cap. So, let's get started with the Overview. ClassKit is an education-based framework which means that it is designed to work with in a school environment. It allows you to define assignable content from your app, so that teachers can assign it to students as homework. Then it allows you to report progress on those pieces of content that were assigned by teachers. And this is done with privacy in mind. What I mean by that is that you can report progress on any assignable piece of content within your app, but teachers will only see progress on the things that they have explicitly assigned to students. So, why would you adopt ClassKit? Well, for one, this will get a better student experience. Teacher workflow, because they will be able to know exactly what's assignable within your app and assign it to students. It gives teachers insights within how students are doing in your app. And this enables personalized learning. If a teacher knows how students are doing within your app exactly, they'll be able to better cater future homework assignments. Lastly, well, it also gives you a competitive advantage. This is because with teachers while the type of information is surfaced back to them, chances are that they will be advocating for your framework to -- for your app to be used within the school. Speaking of schools, there's a related technology called Apple School Manager, and school admins and IT use this to create managed Apple IDs for everybody within their organization. So, every teacher will get a managed Apple ID and all students will get managed Apple IDs. Admins also use it to create classes within a school, so for example, in mathematics class we will have a teacher with -- which is a person with a certain managed Apple ID and students, which are people with other managed Apple IDs. This is also where schools manage all of their content and devices. And as we mentioned for competitive advantage, this is where schools will be able to enable student progress reports in feature of ClassKit. Also, this is where schools are able to purchase apps in bulk. And any app that has enabled ClassKit support, we'll have works with Schoolwork checkmark next to its name. So, Schoolwork is the new education app that is coming out soon. Students use this app to view handouts that were assigned to them. And teachers use it to create homework assignments. And homework assignments, we call handouts within our system. Handouts is just a collection of tasks. For example, a piece of content might be a task within a handout. So, this is also the place where teachers will view progress reports of how students are doing within your app for a given handout. So, let's actually take a walk, what the life cycle of a handout is. But before we can create the handout, your app actually needs to be able to declare what pieces of content are assignable. Within our framework, this is done using a cross code CLS Context. And what a CLS Context is, is that it allows you to represent your content as a tree structure. So, there is one main app context redefined for your app. And this acts as the root context of the content tree -- or the context tree. Also, all of your content is just a descendant of the main app context. So, we ask that you define your content as accurately as possible because the sooner you define your content, the sooner it will be available to teachers in Schoolwork to assign to students. Assuming that you've created your context tree, a teacher will be able to go to Schoolwork, they will be able to tap the Create New Handout button, which will create a new instance of a handout. Then they will point the handout to a specific piece of content within your app, and once they've done that, they will assign the handout to a student. Once a student receives that handout, they will then tap on the handout which will open your app. At that point, your app should create the necessary contexts. And for a better student experience, we ask that you navigate the student to the same piece of content. Marin is going to show us later how this is done in code. So, assuming that now the student started working on the homework assignment, your app starts reporting progress back to students and all of this progress will be sent to the teacher's device and bundled in what we call a progress record -- a report, or activity report. So, teachers, once this is done, will have access to that report from within the assigned handout. So, this entire flow involves you having a teacher managed ID, a student managed ID, and maybe a couple of devices or even one device from which you'll be logging in and out, so you can test. But with all that, we can actually make this a little better and we created something that we call Developer Mode. What Developer Mode does is that it allows you to act as a teacher, so that you can create handout assignments, assign them to students, and view progress reports for your students, and it also allows you to act as a student, so you can consume those handouts and your app can report progress back to teachers. There's also a way to reset the development data and I personally had to use this button quite a bit, to be honest. With that, let's take a look at the classes we have in our framework. At the very top, we have a Data Store. It is used to maintain the Context Tree, within our app. This is also where the main app context is exposed as a property. And the Data Store also gives tracks of all modified objects within the system. So, if you'd like to save those object, you can call CLS Data Store, Save with Completion. I'd like to mention also that there is one shared instance of the Data Store that you use to interact with. So, next, let's talk about contexts. We saw that contexts are a three hierarchy, and Marin is actually going to go in detail later about contexts, how to use them and mention some interesting properties about them, but I'd like to concentrate on Context Identity. So, identity of a context in ClassKit is two things. The first one is the Identifier Property, and the Identifier Property allows you to uniquely identify contexts across siblings. What I mean by this is that it's okay for you to have a context with the same identifier, multiple contexts with the same identifier, as long as they don't have the same parent, which will be fine. So, that [inaudible] is something we call Context Identifier Path. Context Identifier Paths are what we actually use to uniquely identify context within the tree, within the context tree. And what Context Identifier Paths are, well, it's just an area of Context Identifiers. The way we use them is that we traverse the Context [inaudible] following the path, until we reach a note there -- the last note within the path. Let's actually take a walk how this looks in action. In this example, we have a context card that's pointing to Section 1 Context within some chapter, within some book. And we also have our Context Tree. To find Section 1, we will first visit the App Context. Then we'll visit the Book Context. We will find the [inaudible] of the Book Context with Identifier Chapter 2. We will visit that one, and we will do the same for Section 1. Since Section 1 is the last thing on our path, then this is what the path is referring to. So, we have a few ways to look up contexts with our system. One is using absolute context paths, and to do that, you would call the CLS Data Store Context Matching Identifier Path Method. The completion block of this method will return all contexts among the paths. So, for example, it will be an array of the Up Context, Book Context, Chapter 2 Context, and Section 1 Context. If for some reason, Chapter 2 in Section 1 was missing, then that array in the completion block will just continue to Up Context and the Book Context. And that gives you a chance to create the missing context as something else. There's also a way to look up contexts using relative paths. This is useful if you already have a reference to a context, but you would like to find a descendent of it. And the method for that is called CLS Context Descendent Matching Identifier Path. The difference is that here you will either get the context [inaudible] if it doesn't match the path. There's also a genetic way to [inaudible] contexts and this is done using CLS Data Store Context Matching Predicate. On the example from the slide, we're showing you how to find all children of a given context. So, there's also a Data Store delegate. This is used in conjunction with the path quitting methods that we just saw. The definition of the decoration of the delegation is on the screen. It's usually useful to use this when your contexts are downloaded on demand to -- when all of your context is not available, so you want to create things on demand. The way it works is that as we start querying for a path, for our contexts among the path, if you don't find a context on that path, we will call the Create Context for Identifier Method under Delegate that you've defined, giving you a chance to create the missing context. And if you do create a context, we will take the context and go add it to the correct place on the tree for you. So, as I mentioned, it's useful for app for -- with dynamic content. Let's actually take a look at how this work in action. Again, we have the same identifier path, but in this case, our tree is incomplete. And we also have a delegate. So again, we will visit the App Context. Then we'll visit the Book Context. Then we are going to try to visit the Chapter 2 Context, but it doesn't exist. So, we are going to ask a delegate to create it. If a delegate creates it, we will then attach it to the correct place on the tree, and we'll visit that context. Then we're going to build the same thing for our Section 1. Again, since section 1 is the last thing on the path, that means that this is what a path is referring to. Next, let's talk about Activity Objects. So, Activity Objects are actually the objects which you will use to report progress back to teachers. And they actually translate to Activity Reports. Activity Objects are always associated with contexts. You can officially allocate one on your own. And the way you would create a new one is by calling CLS Context Creating New Activity, which will return a new activity object associated with the receiver of the method. If you would like to see if there's currently an activity associated with your context, you can do so by querying for the current activity property on the CLS Context. I'd like to mention that every time you co-create a new activity, this is the same thing as starting a new attempt at that activity, meaning that a teacher would receive a new progress report. Let's take a look at how would you add progress to an activity. One way is to directly set the progress property. And that one is to add a progress range, from a start to an end. Also, know that setting the progress property directly is the same thing as adding a progress range for the start of zero. I'd like to also mention that it's perfectly fine for you to add overlapping progresses arranges or even the same progress arranged multiple times. Underneath, we take care of the details to make sure that the correct progress is reported to teachers in the end. So, there's one more object that we should talk about and that's Activity Items. What an Activity Item is, is a quantity that you would [inaudible] to represent to teachers as part of your report. Each activity can have one primary activity item, and this would be the main piece of information you would [inaudible] to teachers that's apart from progress. For example, this is useful for a final score of a quiz. You can also have multiple additional activity items, which you would use to add and show extra information to teachers. Like for example, how many hints were given to a student, answer to individual questions, and etcetera. So, Activity Itemization and Abstract Class, and in our system, we have defined at the moment, three subclasses. One of them is CLS Quantity Item. This is useful for simple scalar quantities like hints, experience points, and things like that. We also have a score item which is useful to represent X out of Y quantities. For example, final score of a quiz might be useful for this. And we also have a Binary Item. And this one is used to answer Yes or No questions, type of questions. Let's take a look at how would you add a primary activity item to an activity. Well, you will do that by first creating an activity item. In the [inaudible], we're showing you how to create a Score Item with a title of Total Score. And then to associate it as a primary activity item [inaudible] activity, you will set it -- you will set the primary activity item property of an activity. To add an additional activity item, you will again create a new Activity Item. In the [inaudible] size, we're showing how to create a quantity item with a type of hints, and then some quantity. To associate it with an activity, you would call CLS Activity Add Additional Activity Item after passing the item you just created. At that point, this activity item will be associated with activity. So, there's some best practices when dealing with activity items. One of them is to always save the subclass for a primary activity item. What I mean by that is that imagine a teacher assigns the same context to two students. You know, Student A device, you set the primary activity item as a score item. On Student B device, you set the primary activity item as a binary item. Well, if you do that, we can't actually generate an aggregated report back to teacher because there's no clear heuristic of how to convert from a score item to a binary, or vice versa. Because of that, it is best to always have the same subclass of an activity item as the primary activity item. Also, please provide clear, concise, but descriptive titles to your activity items. This is because the titles that you set on your activity items will be visible to teachers in their report. And please make use of additional activity items. They're a great way to provide the extra information that teachers will actually need to truly understand how students are doing within your app. With that, I'd like to invite Marin to the stage who is going to show us how all of this works in action. Hello, everyone. My name is Marin, and I'm an engineer on the ClassKit Team. And I would like to show you how to adopt ClassKit into a preexisting app. So, to do this, Pavel and I went ahead and built a sample app. The sample app is called Quizler. It's a simple, math, quiz-taking app. The very first screen that you get asks you to select what type of math you'd like to be quizzed on. So, let's go ahead and select the addition quiz. Once we do that, we'll get presented with another view that asks us if we'd like to view the scoreboard of all the users' high scores, or if we just want to start the quiz. So, let's go ahead and tap Start Quiz. Then, we get presented with each one of our questions. We're just going to answer them, and we get a total score at the end. So now that we understand what this sample app does, let's talk about the steps that we'll take to adopt ClassKit. So first we will talk about what type of activity -- context data makes sense for our sample app. After we do that, we'll discuss what type of student generated activity data, might also make sense. We'll make sure that we support deep-linking. Now, there's two ways to do this. The first way is to use universal links. So, if you already adopted universal links within your applications, you can simply just set the universal link property that lives on CLS Context. Now, our sample app does not support universal links, so I'm going to show you the second approach, which is to use NS User Activities Continue Restoration Handler. Then we'll make sure that we test our implementation using Developer Mode and the Schoolwork app. So, now let's talk about what type of context structure might make sense for this app. As a first approach, we might decide to do a one-to-one map of RUI to a Context Tree. And if we did that, we would end up with a structure that looks something like this. At the top level, we have our main app context, which is our Quizzler app. Beneath that, we have our Addition Context and a Multiplication Context. And then beneath each one of those, we have the Scoreboard Context and then the actual Quiz Context. Now, Pavel told us that a context is a part of our application that a teacher will assign to their students. So, if we keep that in mind, let's first talk about the Scoreboard Context. Well, what is the Scoreboard Context? It's all the users' high scores. Now, does that really make sense for a teacher to assign to their students? Not really. So, we're going to go ahead and remove that. Now, we're left with a structure that looks something like this. So, next let's focus on the Addition Context. What is the Addition Context within our application? Isn't it really just the addition quiz? It's really just one thing. It's not two. So, those really should be combined into one. So, let's go ahead and do that and then the same goes for the multiplication context. Now, we're left with a structure that looks something like this. Now, Pavel and I talked about it, and we decided that we may add a subtraction quiz and a division quiz in the future. And if we did that, we could easily add those as siblings. Now, when you all are thinking about which structure makes sense for your applications, don't just think about your current feature set, but also consider what you might be doing in the future, and make sure that your structure will easily grow with you. So now, let's talk about the context themselves. We want to make sure that we have clear titles, and that's because the title is the one piece of information that both students and teachers will have to know what this context represents. We also want to define our context as early as possible. For our sample app, we have static content. And so, we'll know what context to write as soon as the app launches. So, that's when we'll define ours. Now, we might also want to always display our context in a particular order for teachers that makes sense. So, for our sample app, that might mean our addition context should probably also be displayed above the multiplication context. So, to do that, we'll take advantage of the Display Order Property that lives on CLS Context. Now that we've determined what type of context data we're going to write, let's talk about the student generated activity. Here, I have a screenshot of the Schoolwork App, and this is a sample of what some activity data might look like. For our quiz, I think it definitely makes sense to track the total amount of time that a student spends taking this quiz. So, to do this, we can go ahead and call the Start and Stop Methods that live on CLS Activity. Now, it might also be nice to show what the total score was that the student received. And if we think about it, the total score is probably the most important piece of information about this whole quiz. So, we probably want to highlight it within the Schoolwork UI like we have here. Well, to do this, we can go ahead and create a CLS Score Item, and then we'll set it as the primary activity item. That way, it will get highlighted within the Schoolwork UI. Now, it might also be nice to indicate whether an individual question was correct or not. So, to do this, we can go ahead and create CLS Binary Items and we'll add each one of those as an additional activity item on our activity. So, now that we've determined what type of ClassKit close we're going to write, let's go ahead and see what that actually looks like in action. So here, I'm mirroring my display for you and have Xcode on screen. The first thing that I'm going to do is select my project's target. Once I do that, I'm going to select the Capabilities Pane, and we're going to find the ClassKit APIs right here. Then, all we need to do is make sure that we toggle that on, and we're all set to start writing some ClassKit code. So, first we're going to write our context and we said that we could do that as soon as the app launches. So, let's go ahead and open up the App Delegate. And in here, let's go ahead and create a function that will publish our context for us. Here we have our published context function, and what we're going to do is we're going to instantiate an instance if CLS Context, passing in the type of context that this is, giving it a unique identifier, and then a nice, clear, and concise title. Then we make sure to set our display order because we want our addition quiz to display above our multiplication quiz. Then, we're going to do the same for our multiplication quiz and [inaudible] a context and make sure to set the display order. Once we do that, we're going to create a dictionary of our context that we will need to create. Then, we're going to grab the parent context that we want to add these to. Since our structure is flat, we know that our parent context is always going to be our main app context. Now, let's issue a query to see if any of these contexts already exist. So, we're going to create a predicate, looking for all the context where the parent is the parent we just defined. Then, we're going to issue that query to the CLS data store's shared instance. For all the context that match, this predicate we just defined. This will return an array of context. We're just going to iterate over all of the context that we know already exist. Then, for each one of those contexts that's already there, we're going to remove it from our dictionary of context that we need to create. Then, we're going to iterate over the remaining contexts that do not exist, and for each of one those contexts, we're just going to add it as a child context to our parents. Then we call Save to Save our changes. Now, what we need to do is call this function when our app launches. So, let's go ahead and run this code to test that our contexts were actually created. Here, I have a device and I'm mirroring my display for you. So, awesome. Our Quizzler app launched. But everything looks the same. So, how do we know that these contexts were actually created? Well, this is where we use Developer Mode and Schoolwork. So, I'm going to hit the Home button, and open up the settings up. Now, I'm going to scroll down and I'm going to look for the Developer Settings. When I select the Developer Settings, I'm going to see some ClassKit APIs. If I select the ClassKit APIs, we can see that we can switch and act as the role of the teacher. So, I just make sure I select that. And now, we can hit the Home button and open up the Schoolwork app. Now, when the Schoolwork app launches, I first get a Welcome screen. I'll go ahead and dismiss that. And then if we look in the top right-hand corner, I can see that I have a plus button. That's how I know that I'm logged in as a teacher. If I tap on that plus button, I'll get the Create a New Handout view. And I can add an activity. Then I can select Apps, and we can see our Quizler app is showing up. If I select that, awesome, our contexts are there. So now, we've been able to validate that our contexts actually got created. I'd also like to point out that the addition quiz is displaying above the multiplication quiz. We've also now been able to validate that our display order was set correctly. So, let's go ahead and select the addition quiz. That's going to add that context to this handout. And now, let's just go ahead and send this handout to our class. I'm going to tap on the To field. Select my class. And let's just give a title to this handout. Now, I'm going to go ahead and hit Post. What this does is it sense this handout to my class, and it authorizes the context that's on this handout. So now, that context can start recording progress data. We can see that my handout posted successfully, and I can tap on it. Here we see the context that I've added, and I see my nice, app icon is right there next to the context name. If I tap on the icon, it should navigate me straight into the addition math quiz. So, let's go ahead and tap on that. Uh-oh. Schoolwork launched our app, but this is not the addition math quiz. Oh, yes. That's right. We forgot to add in deep-linking. So, let's go back to XCode and add that in. So, I'm going to go back to the App Delegate, and let's add in that Continue Restoration Handler. So here, the Continue Restoration Handler will process some user activity. We're just going to grab that User Activity and get the Context Identifier Path. Once we have the Context Path, we're going to instantiate our own internal quiz model that's associated with this context. Then, we're going to make sure that we call our same published context function, and that so, if the context has not been created, we'll make sure to create them. This will return an optional error, and we'll make sure to handle any errors. Then, we're just going to sync back up with our main thread and instantiate our storyboard and also instantiate the correct view controllers for this quiz. Then we're going to set our quiz on our view controller, and push on the appropriate views. So, now let's go ahead and run this code and test that our deep-linking is working. So, I'm going to switch back to my device, and then we're going to navigate back to the Schoolwork app. Here, let's tap on the icon one more time. Awesome. It launched us straight into the addition math quiz. So, now we're ready to start writing our user generated activity data. So, let's go back to XCode, and let's navigate to the part of our code that gets called when a quiz first starts. So, here, we said that we wanted to start the timer for our quiz. So, what we're going to do is we're going to issue a query to our CLS Data Source Shared Instance. And then we're going to query in our main app context for all of the descendants, that match the identifier path that's associated with this quiz. This will return an optional context. We're going to grab that context, make sure that we called the Become Active, and then we're going to instantiate a new instance of CLS activity. And that's because when this part of our code gets run, we know that the student is taking a new attempt at the quiz. Then we're just going to cash that activity and a property that we have defined. Then, we call Start on Activity to start our timer. We also said that we wanted to report the answer for an individual or question. So, I'm going to navigate to the part of my code that gets called when a student taps on an answer. So here, what we're going to do, is we're going to get that same currently running activity that we just created. Once we have that, we'll instantiate a CLS binary item, giving it a unique identifier, and then a nice, clear title. Then we'll make sure to pass in the type of binary item that this is. Now, we're going to set the value of whether the student got this question correct or not. Then we're going to add this binary item as an additional activity item on our currently running activity. The last thing that we said we wanted to do, was stop the timer and set the total score. So, we're going to navigate to the part of our code that gets called when a student ends an attempt. Here, we're going to get that same currently running activity. Once we have that, we're going to create a CLS score item, passing a unique identifier, title, and then we're going to pass in the score the student received out of the total maximum possible score. Then, we're just going to add that score item as the primary activity item on our activity. Notice, every time I'm doing this, I'm always setting the same subclass of CLS activity item. It's always a score item. Then, we're just going to call Stop to stop our timer and save to save all of our changes. So, now let's go ahead and run this code to test that our student generated activity data gets set. So, I switch back to my device, and now we have to switch and act as the role of a student. So, I'm going to go ahead and go back to my Developer's Settings. And the settings are all up here. We can tap on student and now I'm switched to act at the role of a student. Now, we can go back and open up the Schoolwork app. Here, if you look in the top right-hand corner, there no longer is a plus button, and that's how I know I'm logged in as a student. And we can see that I have the handout the teacher assigned to me. If I tap on that, we can see the context that I need to complete, and I can tap on the icon and it navigates me straight into the quiz. Now, I can an alert saying that progress data is going to be recorded on my behalf and sent to my teacher. So, I'll dismiss this. Then we just go through, answer all of our questions, and we get our total score at the end. So, we can see we got 100%. So now, let's go back to the Schoolwork app. Here we can see our activity data showing up. We can see the time and the total score. Notice the total score is highlighted right there in the main part of the [inaudible]. Now, we've been able to validate that our activity item was set correctly. I can also tap on the cell, and I get a popover of all the activity data I've written. I can see the total score and in light gray, I see the title of Total Score that I set. And then at the bottom, we can see the time, and then each individual question. And then also in light gray, we can see the title that I set for each one of those individuals questions. So now, we've been able to validate that our activity data was getting written. So, with that, I would like to reintroduce Pavel back on stage to go ahead and summarize everything that we've seen. Thank you, Marin. So, I'd like to mention some best practices about ClassKit. For one, declare your contexts as early as possibly so that they're available to teachers to assign. And also, Marin showed us that not everything [inaudible] needs to be a context [inaudible]. If it makes sense, take advantage of CLS Data Store Delegate. And also, please take advantage of additional activity items. They really are a great way to provide extra information that teachers might need to understand what the student is doing within your app. There's also some general best practices about education apps. One of them is to StoreKit dependence. And this is because in-app purchases don't really work within a school environment. That one is to support purgeable storage. This is a good idea in general, but it's real important in school environments where shared iPods are common, and space is a premium on those. Lastly, implement setting access via Managed App Config. Doing that, makes school admin's jobs a lot easier to configure devices. You can find links about all of this and the going details at developer.apple.com/education. Now, with that, have a wonderful rest of WWDC. Thank you for coming.  Good afternoon. Welcome. My name is Ken Ferry. Today Kasia Wawer and I are going to talk to you about performance in Auto Layout. Last time I was up here talking about Auto Layout was in 2011 when we first introduced it. So it's really great to be back up here chatting with you all today. OK, so Auto Layout. This is how we position content on iOS on the Mac. The main object involved as we know are views and constraints, constraints giving the relationships between views. When it comes to performance, I think the challenge here is that if all you've said is the distance between these two buttons should be 20, it can be hard to understand the step-by-step process that the machine goes through to do that and that means it can be hard to understand the expectations around performance and understand what's fast and what's not and generally how things are working. So that's our goal for this talk. It's to really understand that, to have a good feel for how things are going to work. So we're going to start things off by first showing some of the work that we've been doing in this release for iOS 12 around performance. We've been doing a lot of work, that's we get to give this talk. When that's done we're going to move on to trying to build that step-by-step understanding that I was talking about. So we have good intuition and good expectations around performance. To do that we're going to do something very unusual for us, which is to go into internals. So enjoy that please. Last, if you only ever rely on our intuition for performance, it's not going to work out very well. So we will then look -- Kasia will take over and we'll analyze code and we'll sort of show how to merge your intuition with, you know, practice. But let's get to it. So first as is traditional for an Apple presentation, we're going to look at a bunch of numbers and brag. Here we have, what we're looking at here is a benchmark. So the way we approached this work is we went out and looked a bunch of third party apps as well as our own apps and we tried to isolate what we saw happening into test cases that we could then benchmark. So this one here, what we're looking at, is UICollectionView with self-sizing cells and on the whichever side looks bad is iOS 11, which hopefully looked janky and bad. And on iOS 12 it's perfect. It's hitting full frame rate. So that's just one of the cases we looked at. Here are some more, just another sampling. We have a lot. These ones are all time. So what you're looking at is that the gray bars are iOS 11. How much time it took on iOS 11 and the blue are iOS12. So what you take from this is that we found a lot of places where we could make things really a lot better. And that will only improve your apps. That will make things quite a bit better for you I hope. This is going all the way up and down the stack. So some of it is in the real guts that affect just absolutely everything. Some of it's moving into UI kits. Some of it's up in the client layer so for in how people use Auto Layout. So if you look at for example that UICollectionView benchmark that we were looking at that's all of those. It does include stuff that's in the real guts but it also includes a lot of really important changes in just how UICollectionView uses Auto Layout and is more performance due to it. Which is a good segue to the rest of the talk, which is how you can do that too. So how to use it properly. When we were going through these I think a lot of the time the reason why we were able to make all these improvements is that we have a good mental model for how things are put together and how it performs, how it works. We want to help you develop that model as well. So to frame this we're going to go through an example case, some client code, that is not -- that has some issues and we're going to discuss why. So your code may or may not have this particular issue, but we did choose what we thought was the most common thing that we saw when we went through all these client apps. But even if you don't have this particular issue, the work we do to go through what's happening should be meaningful to everybody and probably new to almost everybody. So let's do it. This is the case we're going to go through so we're going to produce this layout, obviously very simple. Oftentimes I think you would build this in interface builder. That's a great idea. It is such a good idea that it would completely prevent the performance issues that we'd like to go through. So let's say we didn't do that. Let's say that we built it like this. First let's just walk through -- before we try to analyze it, let's walk through what this code is doing. First, we are overriding the UIView method updateConstraints, whatever that does. So we'll talk about it. Next, we have an Ivar called myConstraints. And we are taking everything in that variable and we are deactivating all those constraints. We are then creating constraints, which implement the layout that we were just looking at. That's fairly straightforward. It's using visual format language here. We're then activating those constraints, installing them, and last we're calling super.updateConstraints was an important thing to do because the UIView level implementation that this method does do work. OK, that's the basic structure of what it's doing and it does work, it's functions. But let's talk about what it's doing more concretely now so that we can understand the performance. So the first thing to understand is what exactly is updateConstraints, this method we're overriding. Well, it's one component of the Render Loop. The Render Loop is the process that runs potentially at 120 times every second. That makes sure that all the content is ready to go for each frame. OK, so it consists of three phases -- Update Constraints, Layout, and Display. First every view that needs it will receive updateConstraints. And that runs from the leaf most views up to the view hierarchy towards the window. Next, every view receives layout sub views. This runs the opposite direction starting from the window going down towards the leaves. Last, every view gets draw if it needs it, that kind of thing. OK, what are these for? Why do they exist? Well, they all have the exact same purpose and they have exact parallel sets of methods. And that purpose is to avoid wasted work, which I can explain by example. So a label, a UI label needs to have constraints that describe the size of its text, OK? But there are many properties that contribute to that size. There's the text property itself, there's the font, and the text size, etcetera. One way to do this would be that every time one of those properties changes go re-measure your text. However, that would often be pretty inefficient because you usually change a bunch of these right in a row. When you're first setting up a label, you're probably going to call a bunch of these property setters and if you're re-measuring text on each one, all the intermediate ones are wasted, you really just want to measure at the end. And that's what the Render Loop gives you. Because what you can do instead is that inside a set font you can just call setNeedsUpdateConstraints and then you're guaranteed to get update constraints at the end before the frame goes to the screen. And that's what it's for. So the couple things to understand from this before we move on is number one it runs a lot, 120 frames a second. Number two they're parallel. So you can use that for intuition as well. If you feel like you understand the layout pass or have some feel for that, same deal when you're thinking about UpdateConstraints or you're thinking about display. And then the last thing being that the whole reason it's there is to avoid wasted work, to defer work and possibly skip it entirely. All right, so having looked at that we are now in position to analyze the rest of this method. See how we are -- every time it's called we're deactivating constraints and then activating them again new ones. We are saying this is analogous to layoutSubviews. So if we wrote the exact same code in layout Subviews that is the analog, that would be as if you -- every time layoutSubviews was called you destroyed all your Subviews, created them from scratch and then added them again. And I think a lot of people have the completely accurate intuition that that's not going to perform very well. So the thing to really get is that it's the same. Whatever intuition you take from that apply it to updateConstraints as well. When you are ripping down constraints like that you're doing a lot of extra work. So how do you fix it? Well, you need to use -- as we were saying, you need to make sure that you're not doing it more than once. It's for deferring work. So it should be something like this, we say did we already do this work? If we did then just don't do anything at all. If we haven't done it yet, then sure set up those constraints once. And that will perform well, OK? So this is again, this is actually the most common error that we see in client code, which is we call it churning the constraints. Unnecessarily ripping them down and putting them back up. OK, great. We are going to do more but stepping back for a second now to think about the Render Loop for a little bit. The Render Loop is great if you actually need it. The purpose again, it's really useful for avoiding that redundant work. But it's also dangerous because it runs to often. It's very sensitive code. So in a case like this usually what you want to do about sensitive code is not -- like, you should take care if you're writing it but you should also try to minimize how often you write sensitive code because, you know, you're probably going to screw it up. We all do. So in this case, in fact you might be, you should really think again like could I just do it once and not put it in updateConstraints? And a good way to do that is use Interface Builder. If you can use Interface Builder you should. It's great for all sorts of reasons. It puts you on a good path. OK, so that's great. We've now talked about that. I think we have a better understanding for why that's problematic, at least somewhat by analogy sub use. But for the purposes of this talk we want to do better than that. We don't just want to say this is bad. We want to really understand it and understand the process. So to do that we're now going to peel back the covers and start to really see what really happens. So when we activate these constraints, when we add the constraints, what is the process that occurs? Let's diagram it out at a high level. So if this is the view that we're adding the constraints to, this view is in a window. Hanging off the window is an internal object called the engine. And the engine is the computational core of Auto Layout. When the constraint is added what will happen is that we make an equation, which corresponds to the constraints, and we add that equation to the engine. The last object to understand in the diagram is that the equation is in terms of variables where a variable is like, you know, if I hand you an equation and I say solve for X, X is a variable. The things that we need to solve for in this case is the frame data of a view. So there will be four variables for every view, which is the min X, the min Y, the width, and the height. OK, so let's go into this process. So this was the layout we were going to do. We're going to focus just on the horizontal constraints for simplicity, but we're going to follow through the process. So the first thing that happens, as we said, is we make these equations, which look like this. These are pretty straight forward. The most interesting one is I think the space between the two text fields, which looks like we're saying it looks very, very similar to what you say with the constraint but it's somewhat lower level because it's in terms of these variables. OK, then each of those equations needs to get added to the engine. And we're actually going to follow along that process again with the goal being to have a good feel for the performance characteristics. What is happening when we do this? So the engine is trying to solve for these variables, which is something you may have done in algebra and it actually looks exactly the same. So let's follow it. So first equation comes in, says the first fields minX is 8. Cool. Its width is 100, fine. OK, when this one comes in we say the second field's minX is equal to the first minX plus the width plus 20. What would you do in algebra if somebody asked you to solve for these variables? You would substitute out for the ones that you already had in there. And that's exactly what's going to happen. If you are profiling, you'll see there is a genuine method in the engine that contains the word substitute as well as another 140 characters because we are Cocoa Code Programmers. But and that's what it will do. And then, you know, and the last equation comes in and this looks done. it looks like that was all the work that had to happen at least in this case to solve for those variables and that's true. That's what I want to understand at this point is that the work that happens is both not very complicated. It corresponds very, very, very closely to what you would do if you were doing it by hand. And it's also not very expensive. It's just sort of substituting out like this. That's the work it does. OK, so now we have sort of solved for these variables in the engine but that's not layout. So let's finish the process. What happens for the rest of the process is that whenever the engine sort of assigns a value to one of these variables, it's going to inform the view that the variable came from and say, this thing changed. What will the view do in response to that? Well, if you think about it for a minute it will call it Superview and say hey, setNeedsLayout because it needs to move. OK, that was all happening as part of the update constraints phase. We now just receive setNeedsLayout, so at some point it will move on to the layout phase. Then, OK, so the last piece of the puzzle is that we'll receive, UIView will receive layout Subviews will do is it will copy that data from the engine into the frame. So it will just say engine, what are the values for those variables? Engine will tell it and it will just call set Superview of that view we'll call setBounds at setCenter on that Subview. And that is the entire process. So just step back and think for a second. Like, that is the step-by-step process of Layout. If you can try to internalize that and get a feel for it, you're going to have a much, much, much better feel for performance expectations around this stuff. In fact, let's see how that's going right now, because now when we look at this and we look at this method that we were looking at that where we're deactivating constraints and we're reactivating constraints, think about what we just did and think about what the engine is going to be doing. It's going to look like this. Which we call churning [laughs]. So each operation it's doing is not super expensive, but it's doing a lot of them and it's just completely unnecessary. This work is wasted. So if you can feel this in your heart, if you can really feel that this is what is happening when you do this, then you're going to be in good shape. Then that's -- you're going to be in the same position we are to go through and really get a good feel for this. OK, so I hope that's great. There's one other big topic that we want to cover though. If we want to really have a good performance model is this idea that you only pay for what you use with Auto Layout. And having looked at this, I think we're in a good position to understand what that means, OK? To do this, let's say we double the situation we had before. So we have four text fields in two sort of independent hierarchies. Now something you can do is you can make a constraint that crosses the hierarchy like this. That goes -- that you can say, well text field one should be aligned with text field three even though they don't have the same Superview. I think sometimes people have the impression that because this possible, it means that things are going to be generally quite slow because anything could affect anything at any time and so it's just sort of a giant ball of mud and performance probably sucks. OK, but having looked at what we've looked at, let's see what happens in the common case where you don't have this because most of the time you don't. Most of the time views are only constrained to their parent and to their siblings. What you'll see there is that since we have these two independent blocks, that will give, if you look inside the engine it will be two independent blocks of equations that completely don't interact with each other, that don't have any overlapping variables. What that will do, is that because they completely don' t overlap, they just don't interact. And if we have one of these it will take some amount of time to deal with. If we have two of them it will just take twice the time because they have nothing to do with each other. Three of them, three times, etcetera, the point is you're going to see a line. You're going to see linear performance, which is the best you can get. That's perfect marks for this kind of thing. So I want to stress this again, the reason why it's linear is because there aren't any dependencies between these pieces. If you do have a dependency, then it will tie those blocks of equations together and that will be somewhat more, you know, more computation to deal with but that's only if you use it. And of course if you do have something like that, you know, if you're doing it by hand of course it's going to be a little bit more expensive that's what you expect. You're doing something more complicated. So it's kind of this usual thing that we often aim for in Cocoa, which is that the simple things are simple and the complex things are possible. In this case it's more like they cost a little more. But you're not paying for it if you're not using it, which is actually the right way to think of the whole engine in terms of intuition again, you can think of it as a cache for layout and as a dependency tracker. It's very, very targeted. It understands which constraints affect which views, and when you change things it just updates exactly the things that are needed. And this has implications on how you write code too. Sometimes we see -- one issue we sometimes see is people taking great pains to avoid making constraints because they have the impression it's going to be expensive. But actually, it's very, very targeted. As long as the constraints that you're making correspond closely to the problem that's being solved, it's pretty unlikely that whatever you do, if you tried to dodge it, it's going to be more performance. Oftentimes we'll see people doing very complicated measurement and adding things up and sort of trying to pull information out and then push it back in and that's almost always more expensive than just directly expressing as a constraint what you're after. Now the converse side of that is that sometimes we see hierarchies that look like something like this where we see lots and lots of constraints and lots of priority and it's really not clear what's happening and this is a -- usually this is a telltale sign of this being the situation that there's actually just two completely separate layouts that someone has in mind and we're trying to sort of pack them together into one set of constraints and do it all in one. And that's also not a real good idea. So that will -- that creates a lot of false dependencies, places where it seems like things interact that they really don't. It's also nearly impossible to Debug, if you haven't noticed. So the overall advice is try to model the problem as directly as possible. Kasia is going to walk through this kind of case where you're switching between different layouts and show that a little more explicitly. But that's the general advice. Just use it in a natural way. It's better for both performance and for understandability. OK, so that most of what we have to say. But since we're trying to build an overall mental model of the performance characteristics of Layout, I want to at least make sure we touch on all of the major features. So there are some other things you can do. And let's discuss. So you can say that some particular view should be at least 100 points wide. You can use inequalities. What does that cost? Very, very, very little. Compared to just saying it's equal to hundred points wide. Since we went internals a little bit, it's going to coast exactly one more variable. That's it. You can also call set constant. The example use case for this is something like I have a gesture recognizer and I'm kind of trying to drag a view around and what I'm going to do is every time I receive a call from the gesture recognizer I'm going to take its translation and I'm going to pump it into a constraint by calling set constant on that constraint with that translation value. OK, what that's going to do is we talked about how the engine is a dependency tracker. This exploits that to the maximal degree. So that's sort of a very, very, very fast one step update of just exactly what has to change due to this one constraint changing. So that's a performance optimization. That's why we even have this method set constant. Last to talk about it priority. So here you can say, you know, you can say this view should ideally be 100 points wide, but if something else prevents that just please be as close as possible. This does incur some more work, some amount of work. So let's talk about that a little bit more. Another way to think about that is to say that the width of that field is going to be equal to 100 plus some error and please minimize the error. That's what you're asking for. So there is an error minimization phase I didn't discuss before. So when the view asks the engine as part of layout subviews and says, hey what's the value for these variables? The engine needs to make sure that all of those error terms have been minimized first. And this is actually, this is -- I'm not going to go into how this works but I am going to talk a little bit about performance characteristics and I'm also going to say that's super neat. So you might want to look this up. This is the simple X algorithm. This is what we're really doing. It's super old. It was developed during World War II. What you might note is before computers. In fact, the people who used to be called computers, before there were machines that were called computers, this is kind of what they're doing. They're doing it by hand, which does give you some feel for the performance characteristics. It must be pretty fast if you do it by hand. And it is. It's pretty much the same stuff we've been doing. It's more substitutions. That's how you should think of it. Anyway, but it does -- you know, when you use priority it does cost at this level so that's just something to be aware of. OK, and other than that it's just same as before. So that's what I wanted to talk about. So that is our attempt to build this intuitive understanding of the performance characteristics around Auto Layout. So quick review of what we talked about. Try not to churn your constraints. That's when you're doing all this work that just doesn't matter. So don't do it. When you do work with constraints it's basic algebra and that algebra is happening when you add constraints, when you remove constraints, when you call set constant, that's the primary times. And then also, you know, when we have this error minimization phase. The way to think about what Auto Layout does is that it's a cash for your layout, we saw the engine sort of contains all those solved values and it's a dependency tracker so that when things change we can update those values in a super, super targeted way. Which leads to our last point, which is that you only pay for the features that you're using. That's what we talked about. You know, that's your intuition. And for the rest of the talk I'm going to turn it over to Kasia because if you, again, if you only rely on intuition, things are not going to go well. So she's now going to go into some analysis, avoid we talked about and putting that intuition into practice. So please enjoy. Ok let me get to my slide here. Thank you, Ken. Hi everybody. My name is Kasia Wawer. I am a member of the iOS Keyboards Team and we use Auto Layout and we love it. So I get to talk to all of you about building efficient layouts. All right, let's go back to Constraint Churn real quick here. Constraint churn as we heard happens when you change your constraints but the actual views don't need to move so you're sending extra work to the engine and enough of that can affect your performance. So you tend to want to avoid it. So let's talk about how you might run into this problem and how you might get out of it. So we're going to work with a spec here. This is for a social media type app. There's an avatar view that shows you who is sharing. There's a title, a date, and a log entry view and for that you're going to need some spacing, you're going to need some sizing and you're probably going to need some alignment too. But this is actually not a pure social media app. It is a semi social media app, where you can choose whether you want to share things. So there's also optionally a view that says that you've shared and who you've shared with. And no social media app would be complete without the ability to share cat pictures. So that's another layout that you might have to put in. And maybe you don't even want to share that cat picture because it's just too good, you want to keep it to yourself. So we have four very similar layouts. They're not the same and there's going to need to be some adjustment when these table view cells come on to the screen. If I didn't mention it these are in table view cells. And let's say that you are working on performance in this app and you ran it for the first time and this is the scrolling performance you got. And there are a lot of hiccups there, especially on the scroll back to top. So you're like, OK, how do I improve this app? What's going on? So I get to introduce something new today, a sneak peek into something we're working on. This is not actually available in the beta but stay tuned because we're going to be introducing an instrument for layout. And, OK. I'm glad you are excited. That's good motivation. Anyway, let's take a look at what's here. The top track is your standard how much CPU is being used. And this is sort of your canary in the coalmine view. If there are a lot of peaks here you have an indication that you might have something you need to look at in your layout. And if it's pretty flat, probably your performance problems are originating somewhere else. Below that we will be specifically tracking constraint churn. The height of the bars in this instance correspond to the number of views that are experiencing constraint churn. So when you see a big piece there you know a lot of views are affected. We're also going to show you how to remove and change constraint instances and finally sizing for UILabel and other text views. This one says UILabel because that's what's in this app. It's also going to track other types of text views as well. So this was taken with that app scrolling, so what do we look at here? There are several peaks in the CPU view but let's zoom in on this one because right below it I see a big jump in constraint churn and that's a little concerning. So if you highlight this view and go down to the detailed view in instruments, what you'll see is a list of the views that are affected by churn by view description. And we are grouping them by Superview so that in an instance of say Table View Cells, it's easier to see that it's happening over and over in a specific context and not different ones. So in this instance we see that the avatar view and three labels are experiencing churn. And since I am the one who ran this through the instrument, I know that these labels correspond to the Title Label, Date Label, and our Log Entry Label. That's almost all of our views in this cell. That's a little concerning. Let's see what happened. All right, back to our spec here. So look into the code and find that UpdateConstraints is being overridden. And in that method when anything changes or when UpdateConstraints runs at all, we're removing all of the constraints and then adding back the ones that we think we still need. Well, everything landed back in the same place where it started. So that removal just is contributing to performance issues. So in the instance of the social label here, social avatar thing, being added and removed, we don't actually need to pull it all the way out. When you look at the constraints around this view, you'll see that they don't actually interact with anything else, just that particular view. So here you can use, you know, this neat little feature called setHidden, maybe you've heard of it. And because it's not affecting any of the views around it, it's just going to disappear, it's constraints stay in place and this is a very, very, very cheap way to hide and show views, rather than removing them from the hierarchy. So that's fine. But this is a really simple example. What about the image view? All right, so for the image view, again we might we might want to try removing all constraints and then adding back the ones we already had plus the image view ones. And again, everything is landing in the same place so we're experiencing churn. Well, in a situation like this how I want you to think about it is to look at groups of constraints. So let's start with this group that I'm highlighting here in green. These constraints stay the same in every one of our layouts. Once we're doing the hide and show on the sharing view that doesn't need to change, the avatar view never moves, and the labels never move other than the log entry label being able to get longer. So those green constraints should be added when you create the views and then left in place. Don't touch them. They want to stay where they are. But now we have the four constraints that are controlling the image view. So what do we do with those? Well, let's stick them in an array and let's also take the constraints that are going to be there when there's no image. And I very creatively named these imageConstraints and noImageConstraints so you can keep them apart. And let's, when we're getting to the point where we're going to be putting in this image view or taking it away, let's see what layout we're currently in. Deactivate the noImageConstraints if we need to and activate the ones for the image. If we don't have an image coming in, you know, all of our other constraints are already activated, we just have the one that we're adding. Now I put these both in arrays despite the fact that this is a single constraint because it simplifies things in my code. I don't need to check and see whether I'm dealing with an array or a single constraint, I'm always dealing with an array of constraints. Your mileage may vary though. So the nice thing about this is that if you are tracking your constraints properly like this and you know that you want to add this image view live in front of the user, you can deactivate these noImageConstraints, activate the ImageConstraints and call Layout in needed inside a View Animation block and it's going to animate nicely into your view. If you tried to do this with deactivating all of your constraints and putting them back in, it would look very interesting. Let's say it that way. All right, so now that we've debugged this and we're working with groups of constraints instead of throwing everything at it, let's look at what it will look like. This is what it looked like originally just to remind you. Let's scroll to the top. It's very bad. And this is what it looks like after we've debugged it. And that is much smoother. Thank you. But wait there's more! I actually took this video on iOS 11. This is not taking advantage of any of our performance improvements in iOS 12. This is just the client code doing things more efficiently. In iOS12 it also looks great. And of course fabulous [laughing]. Yes, it's great. So how do we avoid constraint churn? Well, avoid removing all of your constraints. That's usually going to put you into a situation where you have to put a bunch of them back and that's going to land you in a position where you're relaying out frames that don't need to -- or relaying out views that don't need to be laid out again. If you have a set of constraints that going to be common to all of the potential layouts in your app, add them one and then leave them alone. This is a good use for Interface Builder and the initial layout of your app. Changes to the constraints that need changing but don't change the ones that don't need changing. Kind of tautological but it seems good. And then you have a neat trick now for hiding views instead of removing them when that makes sense. All right, so that was constraint churn in the instrument. We also have that view at the bottom that said UILabel sizing. UILabel sizing is tracking the amount of time it takes for the label to calculate its size. So let's talk about intrinsic content size. I'm going to take a walk over here. OK, not all views actually need intrinsic content size. Some views do. Views with non-view content like to return a size for their intrinsic content size based on that non-view content. Two examples of this are UIImageView, which uses the size of its image to calculate its intrinsic content size and UILabel, which measures its text and uses that to return its intrinsic content size. There's nothing really magical about intrinsic content size, it's used to create constraints by UIView. It makes sizing constraints for you and that's it. You can define all of your sizing in constraints yourself and skip this whole thing. There are a few circumstances where it needs to be overridden, that's what it's there, as we saw there are these couple of things plus some other examples in UIView subclasses. But a lot of the times it gets overridden because the assumption that it's either faster or more exact and it is neither of those things. However, there is a circumstance where overriding it might help your performance. Text measurement can be expensive. In our app here the UILabel sizing did not take very long. It was very short durations. So messing around with that is not going to improve the performance of that that much. But if you have a text intensive app and you're seeing a lot of time happening in the UILabel text measurement or you have text view text measurement or whatever else you're using, you might be able to help it along if you have some additional information. If you know the size that the text needs to be without doing all that text measurement, you can return that size and intrinsic content size, or if when you're going to be putting this view on the screen the constraints are going to be fully defining the size regardless of the size of the text inside of it. For instance, the constraints are always going to make it larger than the amount of text you have. Then you can go ahead and return no intrinsic metric for the width and height in intrinsic content size. And what this will do is tell the parent, hey I already have my size, don't bother to do the text measurement. So obviously this only works if you're not trying to detect measurement yourself, but it can help some apps improve their performance. So I wanted you to know that this little trick exists. And we can't talk about intrinsic content size without talking about system layout size fitting size because people often conflate the two even though they're kind of opposites, so that's kind of unfortunate. Intrinsic content size is a way that you communicate size information to be put into the engine. System Layout Size Fitting Size is a way that you get sizing information back out of the engine. They're actually kind of opposites. So this is used in sort of mixed layouts where there's some reason that you need frame information from a view that manages its subviews using Auto Layout. Not very frequent usage but is there and can be used. I want to tell you how this method works because it might be a little more expensive than you think. When you call System Layout Size Fitting Size an engine is created. Constraints are added to this engine, the layout is solved, and then the size of the top views frame is returned, and then the engine gets discarded. So each time you call this method an engine is being created and discarded. While this is fine for small uses, if you're doing it a lot you can see how it might start to build up over time. So be cautious when calling System Layout Size Fitting Size. One of the uses that we sometimes see people do is forwarding that call from their self-sizing collection or table view cell to a content view. And when you do that you're actually overriding some optimizations we've made to make that scrolling, scrolling in that view faster and you're adding extra engines. So if you're currently doing that and your scrolling is no good, maybe look into that. All right, now we come to my very favorite topic in the world. Unsatisfiable Constraints. OK, so what are unsatisfiable constraints? If you haven't run into this before, this is what happens when you do something like, hey this view should be 50 points wide, also it should be 200 points wide. Well, that's not really going to work. These are not actually quantum phones. You know, I can't speak to the future but so the engine has to kind of calculate the fact that there's no layout available and break a constraint in order to generate some sort of layout for you. When it breaks that constraint it sends a very detailed log to your debugger, possibly you've seen it, that says, hey unsatisfiable constraints. Here's the constraint that I broke and here's all the other ones that affected, that caused me to have to break it. So this can sometimes affect performance directly and it can also mask other issues. So it's a good idea to get them debugged. And Mysteries of Auto Layout, Part 2, had some good debugging information so you might be interested in checking that out if you have been having trouble with your unsatisfiable constraints. OK, guys you've graduated. Congratulations. You are all Auto Layout experts and, you know, I hope that you really enjoyed learning about the internals of how it works. Now you know better how to think before you update constraints and understand the process they go through, you've got some tuning information with size and priority and inequality, and you have faster layouts in iOS 12 so that's awesome. We're going to be in the Labs tomorrow if anybody has questions. And we've got the link here for information in related sessions. Enjoy the rest of your week.  Good morning and welcome to What's New in Swift. This has been an exciting year for Swift and the Swift Community. And over the next 40 minutes Slava and I are thrilled to give you an update on the major developments there. The session is roughly broken in to two parts. First I'm going to give an update on the Swift open source project and the community around it. And then we'll dive into Swift 4.2, which is available as a preview today in the Xcode 10 beta. Since late 2015, Swift has been an open source project on GitHub. With a vibrant community of users who discuss, propose, and review all changes to the language and standard library. In that time over 600 individuals have contributed code to GitHub as Swift open source project. And together they have merged over 18,000 pull requests. Since Swift.org was launched Swift has been available for download on Swift.org both for downloadable tool chains for Xcode, as well as various version of Ubuntu. These are both development snapshots and official releases. Now we want Swift to be supported on all platforms so everyone can use it. And a critical part of that is extending the testing support provided to the community. Various folks in the open source project are working on bringing Swift support to other platforms. And we'd like to support those efforts. About a month ago we extended the public continuous integration systems to support community hosted CI notes. So if you are a member of the community interested in bringing Swift to another platform or effort, you can now seamlessly plug in your own hardware support to bring testing there. This is a nice prerequisite for supporting Swift in other places. We've also invested tremendously in the community around the Swift open source project. This is the community that discusses all changes to the language. About six months ago we moved from using mailing lists, which were very high traffic to forums. This was at the behest of the community. Various people were concerned that they wanted to be able to engage in the project at a level that worked well for them but found it difficult to do so. With the forums you can now engage the level that works well for you. The forums have also worked so well that we wanted to extend their utility out to supporting the general open source project. If you maintain a Swift open source project such as a popular Swift library, you can now use the forums for use to discuss things on that project such as discussions with your users or development. We've also looked at ways to continue to extend Swift.org to be of general use the community. This week we've moved to hosting the Swift programming language book to swift.org. Located at docs.swift.org, this will be a natural place for us to extend more documentation for use by the community. Now the really exciting thing about Swift is that people are really thrilled about using it. And they're talking about it in a variety of places. At Podcasts, Meetups, conferences. And we, Apple, thought it was very important for us to engage in those places because that's where a lot of the discussion is happening. Over the last year we have made a very conscious effort to engage in those conferences and present technical presentations on things that we're doing with Swift, or how does Swift work, or how you can get involved in the open source project. And this is something we're very committed to continuing going forward. One of these efforts I'd like to talk about is a event co-located next to WWDC on Friday and that is the try! Swift San Jose Conference. There there will be a workshop with members from the community to help on board people who are interested in contributing to the Swift open source project. And there will be members from Apple's compiler team there to also facilitate those conversations. So that's all about the community update. Let's talk about Swift 4.2. I think the natural place to start is well, what is this release about and how does it fit into the bigger picture? Swift updates occur -- some major updates occur about twice a year. And Swift 4.2 is a major update over Swift 4.1 and 4.0. Now there are in broad strokes two themes to this release. One is a huge focus on developer productivity. You can see this in a variety of ways. The faster builds for projects. But also just through and through massive improvement to the core tooling experience from the Debugger through the Editor. And the community has also focused on language improvements that aim to improve common developer workflows, remove boilerplate. And Apple has continued invested improvements to the SDK so that the Objective-C APIs better reflect into Swift making better use of the language and improving your use of our APIs. And the other side there's been a massive effort on under the hood improvements and changes to the runtime towards this binary compatibility goal, which culminates in Swift 5, which will be released in early 2019. So what is binary compatibility? Binary compatibility means that you can build your Swift code with the Swift 5 compiler and layer. And at the binary level it will be able to interoperate with other code built with that compiler or any other compiler layer. This is a very important milestone for the maturity of the language. And what this will enable is Apple to shift the Swift runtime in the operating system, which means apps can directly use it, meaning that they no longer need it included in the application bundle. So this is a code size win but it's also important that it impacts things like startup time, memory usage, it's an overall huge win for the community. If you're -- we've been very transparent on the progress towards ABI stability or binary compatibility. You can follow along on the ABI stability dashboard on Swift.org. Today's focus is on Swift 4.2, which is an important waypoint toward Swift 5. Let's talk about source compatibility. So just like in Xcode 9, Xcode 10 shifts with one Swift compiler. So if you're using Xcode 10, you are using Swift 4.2. However, just like in Xcode 9, the compiler also supports multiple language compatibility modes. Now in all the modes you can use all the new APIs. You can use all the new language features. What these gate are source-impacting changes. The first two are ones that existed previously in Xcode 9. They're there to provide an out of the box experience that you can build your Swift 3 and Swift 4 code without modifications. The Swift 4.2 mode is almost identical to the 4 mode except it gates those SDK improvements that are talked about. That's it. Just some previous versions of Xcode, there's Migrator Support that you can find in the edit menu to mechanize most of your changes. I want to give an important disclaimer about the Swift 4.2 SDK changes. Later Xcode 10 betas will likely have further SDK improvements. This is to provide opportunities to incorporate you feedback from the betas of how these APIs should be improved and how they reflect into Swift. This means if you migrate to 4.2 early, you should expect there are going to be some changes later. Or you can hold off and migrate later. It's completely up to you. Now with Swift 4.2 we think we are rapidly converging on what Swift code is going to look like going forward. This is an important phase in the maturation of the language. And thus we really think it's important for everyone to move off of Swift 3 and embrace using Swift 4.2. There are important code size improvements there and just overall improvements to the language. And this is Xcode 10 is going to be the last release to support that Swift 3 compatibility mode. So let's talk about some improvements to the tooling. In the State of the Union we mentioned that there are some significant improvements to build improvement for Swift projects over Xcode 9. And so these numbers are run on a 4-Core MacBook Pro i7. Let's look a little bit closer at one of them. This project is a mix and match of Objective-C in Swift. It started out as an Objective-C project and started incorporating Swift. This is a very common scenario. Now what this Build time improvement doesn't really underscore is how much faster building this Swift Code actually became. So if we just focus on how much faster the Swift code built, it actually builds three times faster than it did before. And so that's why the project has that more modest 1.6x speedup. And what you will see is that the overall build improvements will depend on the nature of your project, how much Swift code it's using, the number of cores on your machine, but we've in practice have seen from many projects it's a 2x speedup. And the win comes from observing that because within a Swift target you have cross-file visibility, right, that's one of the great features of Swift where you don't need header files. There was a lot of redundant work being done by the compiler. And what we've done is we've retooled the compilation pipeline to reduce a lot of this redundant work and make better use of the cores on your machine. And that's where these speedups come from. If you're interested in more details there's these two great talks later this week that dive into how the build process works under the book including more details about where this performance win comes from. Now this big win comes from debug builds. I want to focus on how this is surfacing in the Xcode build settings. Recently we separated out compilation mode from optimization level. Compilation mode is how your project builds. So for release builds the default is whole module compilation that means all the files within your target are always built together. This is to enable maximum opportunities for optimization. It's not the amount of optimization done but the opportunities for optimization. And for Debug builds the default is incremental. That means not all the files are all built, re-built always all together. So this is a tradeoff in performance for build times. Optimization level for Debug builds continues to be no optimization by default. This is for faster builds and better Debug information and the release builds are optimized for speed. We'll get back to the optimization level in a few minutes. All right so this separation of compilation mode and optimization level nicely highlights and interesting stopgap measure that various folds discovered that when they combined whole module compilation with no optimization that they sometimes would get faster Debug builds. And this is because that combination reduces a lot of that redundant work that I talked about before that we have no made great efforts to eliminate or significantly reduce. The problem with this combination is, is it impedes incremental builds. So anytime you touch a file within a target the whole target gets rebuilt. Now with the improvements in Xcode 10 to Debug builds, we feel you no longer need to use the stopgap measure and we have observed that the default incremental builds are at least as good as this combination or better. Especially since they support incremental builds. Let's talk about some important under the hood runtime optimizations and this is all part of that march towards binary compatibility. Swift uses automatic memory management and it uses reference counting just like Objective-C for managing object instances. On this slide I've illustrated in comments where the compiler inserts, retains, and releases. This is how it behaved in Swift 4.1. When an object is created there's a +1 reference account associated with it. What the convention was if the object is passed off as an argument to another function, it's the obligation of that function call to release the object. So it's basically you're passing off the responsibility to the call to release it. This provided some performance opportunities to shrink the lifetime of some objects to like their smallest range of use. However, code often looks more like this where you're passing the object off several times to different APIs. And because you have this calling convention, you still have this dance where the initial reference count is balanced out with the final call. But the intermediate calls are expected to have these extra retains and releases because that's what a convention is. This is really wasteful because the object is really just going to be alive during the entire duration of this function. So in Swift 4.2 we changed the calling convention so that it was no longer the callee's obligation to release the object. This means all these retains and releases go away, which is a significant reduction in retained release traffic. This has two implications. It's both a code size win because those calls are gone and it has a runtime improvement. Another important optimization we did was to string. And Swift 4.2 string is now 16 bytes big where it as previously 24. We feel this is a good tradeoff between memory usage and performance. It's also, however, still large enough to do an important small string optimization. If the string fits within 15 bytes then the actual string is represented directly in the string type without going to the heap to allocate a separate buffer to represent the string. This is obviously also a memory win and a performance win. This is as similar to an optimization that exists within a string. We can actually represent larger strings. Finally before I hand it off to Slava we'll talk about the language improvements. I want to talk a little bit more about the efforts to reduce code size. I talked a little bit about that calling convention change, which reduces code size. But we've also introduced a new optimization level, Optimize for Size. This can be useful for applications that care very much about app size limits such as from cellular over the air download limits. Swift is a very powerful language with static knowledge about what your program does. And so compiler has many opportunities to do performance optimizations such as function call inlining, speculative devirtualization, which trade off a little bit of code size for more performance, but sometimes that more performance isn't really needed in practice. This is the result of applying Osize to the Swift Source Compatibility Suite, which contains an assortment of projects from GitHub, frameworks and applications. And what you'll see is a wide range depending on what language features are used about 10% to 30% reduction in code size. Now this, when I talk about code size I'm talking about the machine code that is generated as a result of compiling your Swift code, not the overall app size. The overall app size depends on assets and all sorts of other stuff. In practice we observe that runtime performance is usually about 5% slower. So you're trading off for a little bit of performance. For many applications this is totally fine. So it really depends on your use case. But if this is something you're interested in we encourage you to give it a try. With that I'd like to hand it off to Slava who will talk about all the great language and improvements with Swift 4.2 Hey everybody, I'm Slava Pestov. I work on the Swift Compiler and today I'm going to talk about how the new language features in Swift 4.2 allow you to write simpler and more maintainable code. So before we start talking about the new language changes, let's review the process for making improvements to the language. So as Ted mentioned, Swift is an open source project, but it also has an open design. This means that if you have an idea for improving the language, you can go and pitch it on the forums and if the idea gains enough traction and crystalizes into draft proposal, you can submit it together with implementation to the core team for review. At this point a formal review period allows members of the community to give additional feedback and then the core team makes a decision as to whether to accept the proposal. If you go to the Swift Evolution website, you can see a list of all the proposals that were accepted and implemented in Swift 4.2. And if you look at this list of proposals there's a lot here. There's more than I can cover today. But one thing I really wanted to emphasize was the large number of proposals that were both designed and implemented by the community. What this means is that these proposals address common pinpoints in the language that came up in the real world and you came up with the idea for fixing these pinpoints and you contributed these improvements back to Swift so that everybody benefits. Thank you. So for the first improvement we're going to see how to eliminate a common source of boilerplate when working with enum's. So let's say I have to find an enum. And I want to print every possible value that this data type can have. So in Swift 4, I had to define a property perhaps with a list of all the possible cases. And if I add a new case then I have to remember to update that property, otherwise I just get incorrect behavior or runtime. And this is just not very good because you're repeating yourself to the compiler. So in Swift 4.2 we've added a new CaseIterable protocol and if you state a conformance to this protocol, the compiler will synthesize an all cases property for you. OK, that was short and sweet. For the next improvement we're going to see how to eliminate another source of boilerplate. This time it's when you're unable to make your code sufficiently generic. So in Swift 4 we have this contains method on sequence. And this requires that the element type of the sequence is Equatable so that it can find the element that it's looking for. And of course I could call this within an array of strings because string is Equatable. But what if I call it within an array of arrays. Well array of Int, the element type here is not equitable, which meant that I would just get a compile time error. And you might ask, well why doesn't the standard library make all arrays Equatable. But that doesn't make sense either because if the element type of the array is not Equatable, like a function perhaps, then you can't really make the array Equatable either. But certainly if the element type of the array is Equatable then I can define an equality operation on arrays that just compares the elements pair wise. And this is what conditional conformance allows a standard library to do. So now array gets an implementation of Equatable for the case where the element type is equitable. And in Swift 4.2 this example we saw earlier now works. And in addition to arrays being Equatable the standard library defines a number of other conditional conformance. For example, optional and dictionaries are now Equatable when their element type is Equatable and the same works for Hashable, Encodable, and Decodable conformances. And this allows you to compose collections in ways that were not possible before. So here I have a set of arrays of optional integers and everything just works. If you want to learn more there's a session later this week where you can learn about conditional conformance and some other generics improvements in Swift 4.2 that I won't be covering today. So what about defining your own Equatable and Hashable conformances. Well, a common pattern in Swift is that I have a struct with a bunch of stored properties and all those stored properties are themselves Equatable. And then I want to make the struct Equatable just by comparing those properties of the two values. In Swift 4 previously you had to write this out by hand. And this is just boilerplate. If I add a new stored property to my struct, I have to remember to update this implementation of Equatable and it's easy to make a copy and paste error or some other mistake. So in Swift 4.1 we introduce this ability to synthesize the implementation of equality. If you emit the implementation than the compiler will fill it in for you as long as all those stored properties are themselves Equatable. This also works for Hashable. Now what about generic types? So here I have a data type whose values are either instances of the left type or instances of the right type. And I might now want to make left and right constrained to Equatable because again, I want to be able to use this either type with functions, errors, and other non Equatable types. But certainly I can define a conditional conformance so that if left and right are both Equatable then either is Equatable. But I can do even better than this. Notice how the implementation of equality here there's only really one obviously correct way to do it. You have to check that both values have the same case and if they do you check their payloads for equality. So you might guess, well the compiler should be able to synthesize this for you and it can in Swift 4.2. And this also works for Hashable. So now I can have a set of either Int's or strings as one example. OK. Now, there are cases where you really do have to implement equality and Hashing by hand. So let's look at one example of that. Let's say I have a data type that represents a city and it's got a name, it's got the state that it's located in, and it has the population. And let's say that for the purposes of this example I only have to compare the name and the state for equality and if I know those are equal I don't have to check the population. So if I let the compiler synthesize the implementation of equality here it's going to do unnecessary work because it's going to be comparing that population field. But I certainly write it out by hand and maybe in this case it's not too bad. But what about Hashable? So if I want to calculate the Hash code of the city object, then I'm going to calculate the Hash code of the name and the Hash code of the state and I have to combine them somehow. But how do I do that? Well, I can use an exclusive or operation or I could use some random math formula that I found on the Internet or just came up with myself. But neither one of these is very satisfying and it feels like these Hash combining functions have a lot of magic to them. And the cost of getting it wrong is pretty high because the performance properties that you expect to get from a dictionary or a set really rely on having a good high-quality Hash function. There's also a security angle here. So if an attacker is able to craft inputs that all Hash to the same value and send them to your app over the Internet somehow, then it might slow your app down to the point where it becomes unusable creating a denial of service attack. So in Swift 4.2 we've added a better API for this. Now recall the Hashable protocol in Swift 4 and 4.1. It has a single Hash value requirement that produces a single integer value. In Swift 4.2 we've redesigned the Hashable protocol so now there's a different Hash into requirement. And instead of producing a single Hash code value, Hash into takes a Hasher instance and then you can feed multiple values into the Hasher, which will combine them into one Hash code. So going back to our example of the city data type, all we have to do is implement Hash into by recursively calling Hash into on the name and the state passing in the Hasher object instance that we were given. And the Hash combining algorithm in the Hasher, it does a good job of balancing the quality of the Hash code with performance and as an added layer of protection against denial of service attacks, it uses a random preprocess seed, which is generated when your app starts. And we think that it should be pretty easy to migrate your code to using the new Hashable protocol and we encourage you to do so. The one caveat to watch out for is you might have some code where you're expecting that Hash values remain constant from different runs of your app or that if you iterate over a dictionary or a set you're going to get the elements in the same order. And this is no longer the case because of that random preprocess seed. So you will need to fix your code. And to make this easier we've added a build setting, the Swift Deterministic Hashing Environment Variable, which you can enable in the scheme editor to temporarily disable that preprocess random seed. OK, so let's talk about generating random numbers. So how do you generate random numbers in Swift today? Well, you have to use imported C APIs. And this is really not ideal because they are different between platforms and they have different names, different behavior, so you have to use build configuration checks. But also they're quite low level and these common operations that are not quite so obvious to implement. For example, if I want to get a random number between 1 and 6, then I might think to just call this Darwin arc4random function and then calculate the remainder of dividing by 6. But that actually gives you a result that is not uniformly distributed between 1 and 6. So in Swift 4.2 we've added a new set of APIs to make this kind of thing easier. First of all, all the numeric types now define a random method that takes a range and returns a number uniformly distributed in that range. This uses the correct algorithm and it even works for floats. For higher level code we've added a random element method to the collection protocol. And just like min and max this returns an optional so that if you pass in an empty collection you get back no. And finally there's a shuffled method on collection where this gives you an array with a random permutation of the elements of that collection. And we think the default Random Number Generator is a good choice for most apps. But you can also implement your own. So there's a random number generator protocol and once you write a type that conforms to this protocol you can pass it to all these APIs that I talked about which have an additional overload with a using parameter that takes a random number generator. OK, so we saw these build configuration checks earlier. Let's talk some more about them. Well, this is a pretty common pattern in Swift. You have a little piece of Swift code that is shared between iOS and macOS and on iOS you want to do something with UIKit. On macOS you want to do something similar in AppKit. So if you want to do this today you're going to write a #if compile time condition check and then you have to list out those operating systems where UIKit is available. So but what you really care about is not that you're running on this particular operating system, but that you can import UIKit. So on Swift 4.2 we've added a has import Build Configuration Directive so you can better express your intent. And with the new features of Swift 4.2, I can actually improve this code further. So let's say that I'm also going to explicitly check for AppKit and then if neither UIKit nor AppKit is available, for example if I'm building on Linux, I'm going to use the new #error build directive to produce a friendly compile time error message. OK, now here's another similar source of boilerplate. So if I want to compile something conditionally when I'm in the simulator environment, then today in Swift 4 I have to copy and paste this ugly thing everywhere I want to perform that check. In Swift 4.2 you can use the new hasTargetEnvironment condition, to again better state your intent and just explicitly ask the compiler, am I compiling for the simulator or not? And while we're at it, let's replace that FIXME with a #warning build directive to produce a message or compile time so that I don't forget to fix my FIXME. OK, so that about wraps up all the features that I'm going to discuss today, but I have a couple more things to talk about. Let's unwrap, Implicitly Unwrapped Optionals. That's a horrible pun. OK, so Implicitly Unwrapped Optionals can be a little bit confusing and let's first review the mental model for Implicitly Unwrapped Optionals. How do I think about them? Well, so since Swift 3 they're not the type of an expression. Don't think of it as a type. Instead, think of Implicitly Unwrapped Optionals as an attribute of a declaration. And what the compiler does when you reference such a declaration is it will first try to type check it as a plain optional and then if that doesn't make sense in the context where it's used, it goes ahead and unwraps it and then type checks it as the underlined type. So let's look at an example of the first case. So here I have two functions, the first of which produces and implicitly unwrapped optional integer and the second of which takes a value of any type. And I'm going to call the second function with the result of the first function. Now in this case I can store an optional Int inside of an Any and so no forced unwrapping is performed. The value simply becomes a plain optional. Let's look at an example of the second case now. Here, the first function now produces -- sorry, the second function now takes an integer. So when I call the second function with the result of the first function then I cannot pass an optional Int where an Int was expected. So the compiler has to insert a force unwrap and then it all works because now I have an Int and an Int And this mental model makes Implicitly Unwrapped Optionals very easy to reason about. But until recently the compiler had some edge cases where it did not always follow this model. So recall that you cannot have an implicitly unwrapped optional that is part of another type. And this is still the case in Swift 4.2. I cannot have an array of implicitly unwrapped Int's. However, in Swift 4 previously, there is some edge cases like this. I could define a type alias where the underlying type was implicitly unwrapped Int and then I could make an array of this type alias and I would get very confusing behavior from the compiler that made code hard to understand. So in Swift 4.2 we've re-implemented Implicitly Unwrapped Optional so that it exactly matches the mental model I outlined earlier and this confusing code example now generates a compile time warning and the compiler parses that as if it was just a plain array of integers, of optional integers. Now, most code will not be affected by this change to Implicitly Unwrapped Optional, but if you were accidentally relying on these edge cases I encourage you to check out this blog post on Swift.org that goes into a lot of detail and has a lot of examples about what changed and how. OK, now there's only one more thing here today. Let's talk about memory exclusivity checking. So if you recall, in Swift 4 we introduced something called Memory Exclusivity Checking, which was a combination of compile time and runtime checks that restricted certain operations from being performed. In particular we banned overlapping access to the same memory location. What does this mean? Well, let's look at an example. So here's a piece of code that implements a data type for operating system paths. And this is represented as an array of path components. And there's a withAppended method. This method adds an element to the array, then in calls a closure that you pass in and then it removes that element from the array. And this code is totally fine, it's a valid Swift 4 code. But let's look at this usage of our path data type. So here I have a path that's stored and a local variable and then I call withAppended on it and inside the closure I access that local variable again printing it. So what the problem here? Well, it turns out this code is actually ambiguous because when I access that local variable inside the closure, it's already being modified by this withAppended method, which is a mutating method. So the ambiguity is that do I mean the original value of path as it was before I called withAppended or do I mean the current value that is being modified whatever that means. Well, in Swift 4 this was a compile time error because it was an exclusivity violation. And one way to address this is to resolve the ambiguity by telling the complier, hey I really want the new value so I'm going to just pass it in as a parameter to the closure instead of capturing it. OK, but now look at this example. So this is almost the same function except that it's generic, it's prioritized by the return type of the closure. And in this case we can have the same kind of ambiguity by accessing the path value from inside the closure. But previously Swift 4 did not catch this error at compile time. In Swift 4.2 we've improved the static exclusivity checking to catch ambiguities like this in more cases. And in addition to improving -- OK, and you can also fix the ambiguity in the same way by passing it as a parameter to the closure. In addition to improving the static checks, we've also added the ability to use the runtime exclusivity checks and release builds. And this has some overhead but if your app is not performance critical, we encourage you to try this out and leave it on all the time. In the future, we will get the overhead of these dynamic checks down to the point where we can leave this enabled all the time and it will give you an extra level of protection just like array bounce checking or integer overflow checking today. And there's a lot more in Swift 4.2 that I didn't talk about today. And we encourage you to try it out on your existing apps. We also want you to try out the new features and if you have any questions please come to the labs and ask us. Thank you.  Hey guys! Good afternoon. Welcome to Adding Delight to your iOS App. My name's Ben. And, my name is Peter. And, we're going to show you six pro tips to make it magic. We're going to start with external display support, bringing your app's experiences to the big screen. Next, we're going to go through a brand-new programming pattern for you, called layout-driven UI. Then, we're going to show you how to get your customers to your delightful experiences as fast as possible, with laser-fast launches. We're going to focus hard on smooth scrolling, and keeping things feeling great. Continuity is one of the most magical experiences on iOS. And, we're going to show you just how easy it is to adopt Handoff in your applications. And, finally, we're going to teach you some Matrix-level debugging skills, in debugging like a pro. We have a lot to cover, so let's get started. iOS devices are defined by their stunning, integrated displays. And, you can bring your app's experience even further, by adding support for an external display. We've built a demo app to help illustrate this. Built right into iOS, is Display Mirroring, which replicates the entire system UI on the external connected display. Here's our demo app. As you can see, it's a simple photo viewer. When you tap on a photo thumbnail, the photo slides in, and it's full screen. And, this entire experience is replicated on the external display. To take full advantage of the size of the external display, we can rotate the iPhone to landscape to fill it up. And, this is great, with no work on our part, we were able to get this experience. But, we can do better than this. Built right into iOS, are APIs that allow you to create an entirely custom, second user interface on this externally connected display. Let's take a look at a couple of examples of apps that have done this. Keynote is a great example. On the external display, you remain focused on the primary slide at hand. But, on the integrated iPhone display, you can see presenter notes, and the next slide, tools essential to any presentation. Or, maybe you have a game. But, typically you'd have soft, overlaid controls. Well, you could create an entirely custom interface to control your game, and put that on the iOS device's display, and have your full, unobstructed and immersive gaming experience on the external display. When designing your applications for an external display, there are some key things that you should think about. Aside from the obvious size differences, your iPhone is personal. And so, you should consider the kind of information that you show on this display as private. Whereas, the external display will typically be situated in an environment where many people can see it, such as a TV in a living room, or a projection system in a conference hall. So, you should assume that the information shown on this display is public. Additionally, while the displays built into iPhone and iPad are interactive, the external display is not. So, you should avoid showing UI elements, or other interactable controls on the external display. So, let's apply this kind of thinking to our demo app, and see what we can come up with. Here's our optimized version for the external display. As you can see, we're now showing the selected photo full size on the external display. And, on the integrated display, we're showing just the thumbnails, and a new selection indicator to show the photo that is currently being shown full screen. While simple, this is a really powerful use of this design. To show you how we built this into our demo app, we're going to cover three topics. Connectivity, behavior, and connection transitions. Let's start with connectivity. How do you know if you have an external display connected? UIScreen has a class variable, screens, which contains a list of all the connected displays, including the device, built into the iPhone. So, if there's more than one element in this array, you know you have an external display connected. Additionally, because the external display can be connected and disconnected at will, UIKit will post notifications to help you know when this happens. So, you should listen for the UIScreen .didConnectNotification, and the UIScreen .didDisconnectNotifications. And, bring up and tear down your UI accordingly. Peter, can you show our developers just how easy it is to set up a second user interface? Ben, I'd be happy to. Let's jump into our code for our UIScreen connection callback. Here, we'll set a local variable to the last screen in the screens array. We know that this is the external screen, because we're inside of our didConnectNotification callback. Next, we'll make a new UI window to show on this external display. And, we'll assign its screen property to the screen. Next, we're going to want to make sure we set up this window. We factored this into a function, but all we're doing here is making a root view controller, and sticking it on the window, the same way we'd do for the onboard display. And, finally, we're going to mark this window as not hidden to show it on the external screen. So, that's connection. Now, let's look at disconnection, which is even easier. So, here we are inside of our UIScreen .didDisconnectNotification handler, and all we have to do here is hide the window, and nil out our local reference to it, to free up any resources. And, that's it. We've implemented screen connection and disconnection in our app. Wow, Peter, that was really easy. The next thing you're going to want to think about is changing your app's default behavior for when it has an external display connected. Let's look at an example of some code from our demo app. This is the code that's called when we tap on a photo in your collection view. When we're in single display mode, we create our photoViewController and push it onto our navigation stack. But, when we have an external display connected, we're already showing that photoViewController full screen in that second UI, so we just tell it to present that photo. Really easy. The third thing you should think about when designing for an external display is you should handle connection changes with graceful transitions. Let's go back to our demo app to illustrate this. Here you can see our demo app is currently showing a photo full size. And, we haven't yet connected the external display yet. Watch what happens when we plug it in. What happened here, is we popped our viewController back to the thumbnail view while simultaneously showing that previously selected photo full size on the external display. And, it's these graceful transitions that really help preserve the context, and help your customers understand where they are in your app's flow. So, that's external display support. It's really easy to set up. Just consider the different display contexts when designing your application, and be sure to handle connection changes gracefully. To learn more about this, check out this talk from WWDC 2011. Thank you. Layout-driven UI is a powerful way to write your app to make it easier to add features, and easier to debug. Layout-driven UI helps us deal with the number one cause of issues in iOS apps, and that's managing UI complexity. I'm sure you've been here before. I know I have. You add some code, and a gesture callback. You add even more UI update code in a notification callback. And, more when you get a value trigger from a UI control. And, suddenly your app is in this weird, and hard to understand state. And, you have to follow these strange orders to reproduce these unusual bugs. And, as you add more features to your app, the problem gets worse and worse. If we instead follow a simple recipe, and push these UI updates into layout, we can get rid of these bugs, and make it much easier to add features. Let's walk through the recipe for adding layout-driven UI to your app. The first thing you should do, is you need to identify and track all of the state that affects your UI. Then, every time that states changes, you should dirty the layout system by calling setNeedsLayout. Finally, you'll want to update your UI with this state in layoutSubviews. And, that's it. So, what I love about this recipe is just how easy it is to follow. And, if we apply layout-driven UI to our app holistically, while considering the three core components of an iOS app, layout, animations, and gestures, you'll find that our implementation of all three of these works harmoniously, in a really awesome way. Let's start with layout. Layout is the process by which you position your application's content onscreen. But, we're also recommending that you do all other UI updates in layout. Let's look at a simple sample app that we wrote to highlight this. Ben, can you take us through this app? Sure, Peter. So, there's a really simple sample app, with this cool guy in the middle. He shows when we're feeling cool. When we're not, he hides away. But, we're feeling quite cool right now, Peter. So, let's bring him back in. Great. So, while this is a simple example, it's really important to walk through, so we can understand how layout-driven UI works. So, let's go through a skeleton of this app, and follow through the layout-driven UI recipe that Ben showed us earlier. So, here we've got our managing view that hosts this cool guy view, in our coolView, which I wrote ahead of time. So, Ben, what's the first step in our recipe? Well, we need to identify and track that state that affects our UI. So, remember what Ben said. The cool guy is there when we're feeling cool. And, he's not there when we're not. So, I guess we'll have a variable called feelingCool. OK. Ben, what's the second step in the recipe? Well, now, every time this state changes, we need to dirty the layout system by calling setNeedsLayout. But, we need to make sure that every time this state changes this happens. And, this state could change from various places in our application. So, Peter, how can we ensure that we're always dirtying the layout system when there's changes? I'm happy you asked, because I think I've got a good idea for this. We can use a feature called Swift property observers. These let us run code before or after a property is set. So, we can use the didSet property observer to call setNeedsLayout. This is a really excellent use of Swift property observers in your app. OK. So, we're almost done. Ben, what's the last step in the recipe? Well now, Peter, using this state, we need to update our UI in layoutSubviews. OK, easy. We'll override layoutSubviews, and we'll updated the isHidden property of our cool guy view based on the value of feelingCool. And, that's it. That's all you need to do to add layout-driven UI to your app. Now, while this works really well for this simple example, it works well for some more complex ones earlier. Ben and I were up really late last night playing this new macOS The Gathering trading card game, which is sweeping the Apple campus. We built this fun little deck builder app to help us win the tournament next weekend, which we're really going to do. It lets you pick up and drag these cards around, and you can fling them into this little deck area. And, it's really fast, and fun, and fluid. We can pick up two cards at the same time. I think with this app we can really show you how layout-driven UI works, and importantly, we can beat Ben's officemate next weekend. So, let's walk through the other two core aspects of an iOS app, and how we can apply layout-driven UI to them, starting with animations. Animations are the hallmark of any great iOS experience. The life-like motion of your user interface truly makes your apps feel alive. UIKit has some great API available to help you create these delightful animations. The UIViewPropertyAnimator API is a really powerful tool, and it was turbocharged last year with a bunch of new features. To learn all about how to use it, check out Advanced Animations from UIKit from WWDC 2017. In addition to this, the tried and true UIView closure API is also a great way to make these animations. So, great. We can use UIViewAnimations with our layout-driven UI-based app. One thing to keep in mind, is we're always going to want to use the beginFromCurrentState animation option. This tells UIKit to take the current onscreen position of your view, even if it's mid-animation, when doing animation. And so, this lets us do these really wonderful, interruptible interactive animations. Let's look at an example in our macOS The Gathering trading card game app. So, here we've got a variable that tracks what cards are in our deck. And, using those Swift property observers we talked about earlier, we're dirtying the layout system every time this changes by calling setNeedsLayout. Next, when we want to put a card in the deck, all we have to do is add that card to this array, which will dirty our layout, and then inside of an animation block, using this beginFromCurrentState option, we call layoutIfNeeded. This will trigger a call to our code in layoutSubviews, which will move all of our views around, play the appropriate animation state transitions. And, what's really excellent about this, that I really want to highlight here, is notice how we didn't add any special case for these animations. We just kind of got this animated layout for free by doing our layout inside of an animation block. This is really, really awesome. So, that's how we can add animations to our layout-driven UI app. Finally, let's take about the third core aspect of an iOS app, and that's gestures. And, of course, we can't talk about gestures without talking about UIGestureRecognizer, UIKit's awesome API for adding gestural interactions to your app. UIKit provides a number of great concrete subclasses of UIgestureRecognizer. Everything from pans to pinches to swipes to rotation. You should be able to create the kind of interactions you want, using these. And, they're highly customizable as well. If you want something really crazy, you can always subclass UIGestureRecognizer yourself as well. When looking between the built-in UIKitGestureRecognizers, it's important to understand the difference between discrete and continuous gestures. Discrete gestures tell you that an event happened. They start in the Possible state, and then they don't pass go, they don't collect $200. They move immediately into the Recognized state. These are really useful for fire and forget interactions in your app, but won't tell you at every phase during the interaction. The other type of gestures are continuous gestures. These provide a much greater level of fidelity to you. Like discrete gestures, they start out in the Possible state, but as they begin to be recognized, they move to the Began state. As they track, they enter the Changed state. And, at this point you're receiving a continuous stream of events as the gesture moves around. Finally, when the gesture is complete, it moves to the Ended state. One of our favorite types of continuous gestures is the UIPanGestureRecognizer. And, there are two great functions available to help you get the most out of it. translationInView will tell you where the gesture is tracking, relative to your views. And, velocityInView will tell you how fast your gesture is moving. And, this is really powerful for handing off velocity between a gesture and a subsequent animation. To learn all about how to build really great gesture interactions using these, check out Building Interruptible and Responsive Interactions from WWDC 2014. So, I also love UIPanGestureRecognizer. And, we used it to help build that card dragging behavior you saw earlier. Let's look at how we did this using layout-driven UI. Again, we have a local variable, which tracks the offsets for each of our cards that the gesture has applied. And again notice every time this variable changes, we're triggering setNeedsLayout, using one of Swift's property observers. Next, inside of our panGestureRecognizer callback function, we're just going to grab the current translation and view out of the gesture, and associate this gesture to one of our cards. We'll then increment the offset for this card in this dictionary. And finally, in layoutSubviews, we'll make sure to update the position of our card views based on their offsets from this dictionary. Notice again how we're not really doing anything special, other than -- besides the traditional layout-driven UI case. We just have this piece of state that happens to be driven by a gesture, and we're responding to it in layoutSubviews. In fact, if you follow this pattern throughout your app, you'll find that it makes a lot of these types of interactions much easier to adopt. So that's layout-driven UI. Remember the recipe. Find and track all of that state that affects your UI. And, use Swift property observers to trigger setNeedsLayout whenever this state changes. Finally, in your implementation of layoutSubviews, make sure to update your view state based on this state that you tracked. Thank you. The iOS experience is all about being responsive. And, you want to get your customers to your delightful experiences as quickly as possible. There is one step between them tapping on your icon, and being delighted. And, that one step is your launch time. To help you optimize this, we're going to take you through the five high-level components that make up the anatomy of a launch. Starting with number one, process forking. Peter, what can we do in this phase of the launch? Well, for process forking, it's really complicated. You're going to want to read the Man pages for fork and exec, and familiarize yourself with POSIX fundamentals. No, no, I'm just kidding. iOS will take care of process forking for you. We'll deal with number one for you. Let's look at number two. [ Audience Laughter ] Dynamic linking. At this phase, we're allocating memory to begin executing your application. We're linking libraries and frameworks. We're initializing Swift, Objective-C, and Foundation. And, we're doing static object initialization as well. And, typically we see this can take 40 to 50% of the typical launch time of an app. And, one key thing to remember is at this point, none of your code has run. So, it's vital that you understand how to optimize this. Peter, do you have any great advice for our developers? I'm happy you asked, Ben. It's important that you take great care when optimizing the linking phase of your app's launch time. Because it takes up such a large amount of your launch time. My first piece of advice for you is to avoid code duplication wherever possible. If you have redundant functions, objects, or structs, remove them from your app. Don't repeat yourself. Next, you're going to want to limit your use of third-party libraries. iOS first-party libraries are cached, and may already be in active memory if another application is using them. But, third-party libraries are not cached. Even if another app is using the same version of a library as you, we'll still have to pull in the framework if your app uses it. So, you should really limit your use of these third-party libraries as much as possible. And, finally, you're going to want to avoid static initializers, and having any behavior in methods like plus initialize, and plus load. Because these have to run before your app can do any meaningful work. To learn more about this vital part of your launch time, check out App Start-up Time: Past, Present, and Future from WWDC 2017. The next phase of your launch is your UI Construction. At this point, you're preparing your UI, building up your viewControllers. The system is doing state restoration, and loading in your preferences. And, you're loading in the data that you need for your app to become responsive. So, Peter, what can we do at this phase of the launch to optimize? You'll want to optimize the UI construction phase to be as fast as possible for your app. This means returning as quickly as possible from the UI application activation methods. WillFinishLaunching, didFinishLaunching, and didBecomeActive. Because UIKit waits for you to return from these functions before we can mark your app as active. Next, you're going to want to avoid any file system writes during your application launch, as these are blocking, and require a sys call. Hand in hand with these, you're going to want to avoid very large reads during app launch as well. Instead, consider streaming in only the data you absolutely need right now. And, finally I encourage you to check your database hygiene. It's always a good idea to stay clean. If you're using a library like CoreData, consider optimizing your schema, to make it as fast as possible. And, if you're rolling your own solution with SQLite, or similar technology, consider vacuuming your database at a periodic interval. For example, every time your app gets updated. Thanks, Peter. The next phase of the launch is when we create your very first frame. At this point, core animation is doing all the rendering necessary to get that frame ready. It's doing text drawing. And, we're loading and decompressing any images that need to be shown in your UI. So, Peter, do you have any more sage advice for this phase of the launch? Oh, do I. When preparing your first frame, it's really important that you take great care to only prepare the user interface that you absolutely need during launch time. If your user hasn't navigated to a particular section of your app, avoid loading it. And, instead, pull it in lazily when you absolutely need it. Also, you should avoid hiding views and layers that should not be visible when we first navigate to your app. Even when views and layers are hidden, they still have a cost. So, only bring in the views and layers that you absolutely need for your first frame. The final phase of your launch is what we call extended launch actions. These are tasks that you have potentially deferred from your launch path to help you get responsive faster. So, while your app may be responsive at this point, it may not be very usable yet. This phase is really all about prioritizing what to do next. Bring in the content that needs to be onscreen right now. And also, if you're loading content from a remote server, be sure to consider that you may be in challenging network conditions. And, have a placeholder UI ready to go if necessary. So, those are the five high-level components that make up the anatomy of a launch. We've one more thing for you today. ABM. A: Always. B: Be. M: Measuring. Coffee is for quick apps. It's essential that you understand where your launch time is going. So, measure it regularly using the Time Profiler. Any time you change code in the path -- in your launch path, you'll want to remeasure. And, take statistical averages. Don't depend on a single profile to check your launch time. So, laser-fast launches. Get responsive fast. Use only what you need. And, measure, measure, measure. Thank you. Scrolling is a key part of the iOS user experience, and a huge part of the experience inside your apps. iPhone and iPad are magical sheets of glass that can transform into anything your app would like them to be. So, it's important that you work to help preserve this illusion of your app's content sliding on this magic sheet of glass. At Apple, we've got a phrase that we like to say, that your app should feel smooth like butter. But, from time to time, you have some hitches and stutters that make it feel less like butter, and more like peanut butter. And, you've seen this before. Your app feels choppy or stuttery. Ben, what are some causes of the slow behavior in apps? Well, Peter, this slow behavior that you're describing is really that we're dropping frames. And so, we need to understand why that might be happening. And, there are two key areas where this can happen. The first is you could be doing too much computation. And, the second is you could be doing too much complex graphics compositing. Let's look at each of these in turn, starting with computation. How do you know if you're doing too much computation? Well, the Time Profiler, built into Instruments, is the ultimate tool for this. It can give you down to the line measurements for just how much CPU time your code is using. It's a really powerful tool, and we encourage you to go and check out Using the Time Profiler in Instruments from WWDC 2016. So, once you've identified these hot spots, using the Time Profiler tool, we've got some great tips for you to optimize them. The first is to use UICollectionView and UITableView prefetching. These are pieces of API that will tell you while the user is scrolling towards particular cells, and can give you an opportunity to preload that data. There was a wonderful talk given by two very handsome presenters on this in 2016 that I encourage you to watch. The next tip that I have for you, is to push as much work as possible off of the main queue and onto background queues, freeing up the main queue to update your UI and handle user input. Ben, what kind of work can we push off the main queue? Well, there's some usual stuff that you might expect, like network and file system access. These should never be running on the main thread. But, maybe some other stuff that you might not expect, like image drawing and text sizing. UIGraphicsImageRenderer, and its distributed string, both have functions available that are safe to use on a background thread, that might just help you move some of that complex computation off of your main queue. Wow, Ben, those are great tips. I never would have thought to do that off of the main queue. So, let's say I've run the Time Profiler. I've used prefetching, just like those guys told me to. And, I've pushed as much work as possible off of the main queue, but my app is still slow. Surely this isn't my problem, right Ben? Well, Peter, we may not be out of the woods just yet. While we may have optimized our computation, we could still be having problems with our graphics system. Fortunately, there's another great tool available here. The Core Animation instrument is a really powerful way to see exactly what your frame rate is doing. And, also at the same time, looking at your GPU utilization. It's another really powerful tool. And, to learn all about how to use it, check out Advanced Graphics and Animations for iOS Apps from WWDC 2014. Once you've identified that your app is graphics-bound, we've got some great low-hanging fruit for you to investigate. Usually, you have a graphics-bound app due to overuse of one of two things: visual effects, or masking and clipping. Visual effects, like blur and vibrancy, are expensive, and so you should use them tastefully within your apps. You should also definitely avoid things like blurs on blurs on blurs, as this will cause the GPU to work in overdrive, slowing down your app. Also, you should avoid masking and clipping wherever possible. Instead, if you can achieve the same visual appearance by just placing opaque content on top of your views, then I would encourage you to too that, instead of using the masked view, or masked property of UIViewer CA Layer. So, that's how we can optimize smooth scrolling performance. Make sure to run the Time Profiler, and Core Animation instruments on your apps. Use prefetching, and push as much work as possible off of the main queue. And, try to use visual effects, and masking and clipping sparingly. For even more information about profiling, check out this great talk from WWDC 2015. Thank you. Continuity is one of the most magical experiences across Apple platforms. And, Handoff is a fantastic way to truly delight your customers. The ability to take a task from one device, and seamlessly transition it to another device is an absolutely awesome experience. Handoff works between iOS, macOS, and watchOS. It doesn't require an internet connection because it uses peer-to-peer connectivity, and best of all for all of you, is it's really easy to set up. So, how should you think about using Handoff in your applications? Well, let's go through a few examples of how we do it in some of our Apple apps. Let's say I get a message from my handsome co-presenter, and I want to reply on my iPhone X with a humorous animoji. Well, I can get right back into that conversation, right from the App Switcher on iOS. Or, if I'm editing a document in Pages on my Mac, and I have to run, and I want to hand it off to my iPad, I can do so by tapping the icon in the Dock. Or, again, if I'm casually browsing photos on my watch, and I find one from a previous WWDC, and I just want to go and look at all the photos in that album, I can get right back into my Photo library on my iPhone, without having to go search for that one photo. Handoff is really powerful, and it can save your customers so much time when moving from device to device. So, we're going to show you just how easy it is to adopt. And, it's all built on top of the NSUserActivity API. NSUserActivity represents a current state or activity that you're currently doing. In this case, we're composing an email. When this activity is created, all of the nearby devices that are signed into the same iCloud account all show that Handoff is available. On the Mac, you'll see an icon down in the Dock. When you click on this Mail icon, the activity will be transferred over to the Mac, and Mail can launch and continue exactly where you left off. So, let's look at the code necessary to set this up. On the originating device, you start by creating your NSUserActivity with a given type. And, this type represents the kind of activity that your user is currently doing. You then set a title, and set isEligibleForHandoff to true. And you then want to populate your userInfo dictionary. And, you need to fill this with all the information necessary to be able to continue the activity. In this case, our example is a video, and we're including a video ID, and a current play time. Finally, you'll want to set this activity to your viewController's userActivity property. This will cause it to become the current activity, whenever that viewController is presented. And, that's all you need to do on the originating device. On the continuing device, first of all, your app needs to declare support for the type of activity that you created. Then, you need to implement two UIApplicationDelegate callbacks. The first is application willContinueUser ActivityWithType. And, this is called as soon as you click or tap on the icon to initiate the handoff. At this point, we don't have the NSUserActivity object ready yet, but you know the kind of activity that's going to be continued, so you can begin preparing your UI. Very shortly after, you'll receive applicationContinueRestoration handler, which will contain the fully reconstructed NSUserActivity object. From that point, you can set up and continue the experience right on that device. If you've got more information than can fit in a userInfo dictionary, there's great feature of NSUserActivity that you can use, called continuation streams. All you have to do is set the supportsContinuationStreams property to true. Then, on the continuing device, you'll call the getContinuationStreams method on the NSUserActivity, which will provide you with an input and an output stream. Back on the originating device, the NSUserActivity's delegate will receive a callback providing it with input and output streams as well. And, through these channels, you can do bi-directional communication between the originating and the continuing device. But, you're going to want to finish this as fast as possible, because the user may be moving the devices apart to leave. For more about streams, check out the Stream Programming Guide on developer.apple.com. Now, this is great for moving things that wouldn't be appropriate to put in the userInfo dictionary, like images or video content, such as in our email handoff example earlier. But, for document-based apps, the handoff story is even easier. Because you get much of this behavior for free. UIDocument and NSDocument automatically create NSUserActivity objects to represent the document that is currently being edited. And, this works great for all documents stored in iCloud. All you have to do in your applications is configure your info.plist accordingly. In addition to app-to-app handoff, we also support app-to-web browser handoff. If you have a great web experience to go alongside your native app experience, and the continuing device doesn't have your native app installed, you can handoff to Safari, and continue the activity right in the web browser. Handoff also supports web browser-to-app handoffs. You need to configure a list of approved app IDs on your web server, and then you need to add an associated domain entitlement in your iOS app. And then, the user can seamlessly continue from your web experience to your app on iOS. For even more about this, check out this great Handoff talk from 2014. So, that's Handoff. Go out and implement it in your applications. Truly delight your users, and as an added bonus, the NSUserActivity API is used all over the system experiences. In things like Spotlight search, and the new Siri Shortcuts feature. For more about these, check out these talks, from previous WWDCs. Thank you. You write amazing apps and experiences. But, from time to time, you have problems that you need to investigate. And, for that, we're going to teach you some Matrix-level debugging skills. But first, a word of warning. Before we give you this red pill, and show you just how deep the rabbit hole goes, I want to let you know that the methods that we're going to show you in this section are great for debugging, but must not be submitted to the App Store. If you do, your application will be rejected, and you'll have a bad day. So, with that warning, let's get started. We're going to start with a detective mindset. How you should approach problems that you find in your program. Next, we're going to talk to you about how to debug issues with your views and view controllers. We're going to teach you about LLDB, and how you can use it to identify state issues in your app. And finally, we're going to look at some techniques for some great memory issues that you might run into that make you feel less than great. So, let's start with a detective mindset. When you're looking at a problem in your program, you want to make sure to verify your assumptions. What do you expect your program to be doing? And then, verify that it's actually doing that. This can be a great step, when you start to debug issues in your app. Once you're pretty sure which of your assumptions is being violated, you can start by looking for clues. You'll use the tools that we'll show you during this section to poke and prod at your objects and structs. And then, you're going to want to test your hunches, by changing state in your app, and verifying that you found the issue. Let's start with a sample bug that's a real bug. One of the great things that I get the privilege of working on here at Apple, is the screenshot editor. Recently, we were debugging an issue, where my screenshot pen tools were missing, which is pretty bad. Ben, are there any tools we can use to help diagnose this issue? Absolutely. Built right into Xcode, is the View Debugger. You can launch it by simply clicking on this icon in the bottom toolbar. And, Xcode will show you a 3D representation of your entire view hierarchy. As you can see here, our pencil controls are still there, but they're being occluded by this full screen view in front of it. So, we need to go and look at where we're building this UI, and see what's happening with the ordering there, Peter, I think. That's great. The Xcode View Debugger is a wonderful tool for debugging view issues in your app. There are even more tools that you can use to help out with this. UIView recursiveDescription, UIView parentDescription, and the class method UIViewController printHierarchy are great tools for debugging view and view controller issues in your app. Again, they're also great things to not include when you submit to the App Store. It's important to note that these are Objective-C selectors. So, before using them, you'll want to put your debugger into Objective-C mode, using this command. We're going to walk through each of these debugging methods, step-by-step, and how they can help you, starting with UIView recursiveDescription. So, UIView recursiveDescription will print the view hierarchy of the receiver -- the subview hierarchy. And, some associated properties to help you understand the layout attributes. Let's take a look at an example. We have another bug in our screenshots UI with a missing view. So, we're going to call recursiveDescription on our viewController's view. Now, this might look like a wall of debug text, and that's because it is. But, we know what we're looking for. Our screenshots view is there. We can see it. And, on inspection, we can see that it is currently hidden. So, we need to go and look at everywhere we're setting the hidden property on this view, and really understand why it's not showing. In addition to recursiveDescription, UIView also has parentDescription, which will walk up the view hierarchy to the parent views, until it reaches no -- to a nil parent. It'll print the same kind of debugging information. RecursiveDescription and parentDescription are great for UIView issues. But, sometimes you have a problem with UIViewControllers. And, for that you can use the great class method, UIViewController printHierarchy. Recently we had a bug in our screenshot editor, where one of our viewControllers had not yet received the viewDidAppear message. And so, it hadn't set up its state appropriately. By running UIViewController printHierarchy, we can get an output of all of our presenting viewControllers, our presented viewControllers, our parentViewControllers and childViewControllers, and even our presentationControllers. It's Controllerpalooza. So, let's run printHierarchy in our screenshot UI. Here we can see our viewController hierarchy. And, when we inspect the viewController that we're having the problem with, we can see that it's stuck in the appearing state. So, we had missed a callback. And so, we need to look into our app to where we're calling this callback, and then we found the issue. So, great. Using these methods, you can identify view and viewController issues. But, sometimes you have a more fundamental issue with your app. And, for that we can use some great state debugging tips that we have for you. LLDB's expression command can let you run arbitrary code in the debugger. Think about that. Any code that you can run in the source editor, you can write right in the debugger, and run while your program is running. This is so useful for debugging. You can call functions on your structs, get properties on your objects, and better diagnose what your program is doing. For even more about debugging, check out this great talk on how to debug with LLDB from 2012, and how to debug with Swift from 2014. There's some great functions that you can run inside of LLDB with the expression command, that we're going to teach you. And, the first one, is dump. Dump will print all of your Swift objects and structs properties. Let's go through another bug that we have in some of our custom UI. We have a view with a number of subviews, including some labels, and an imageView. And, right now one of our labels is missing. So, we're going to run dump on our parent view, and take a look at what's going on here. So, we've found our missing label. It is here, but it's -- if we bring up and look at the imageView that's alongside it, we notice that the frame of these two things, they both have the same origin. So, what's likely happening here, is that the label is obstructed by the imageView. So, we need to go and look at our layout code again, I think. In addition to dump for Swift objects, if you still have some Objective-C code lying around, NSObject also has the ivarDescription method. This will print all of the instance variables of your Objective-C objects. We have another bug in our screenshot's code, where our crop handles aren't working for some reason. If we call ivarDescription on our screenshot's view, we can see by looking closely, that our cropEnabled ivar is currently set to no. So, we have a good place to start investigating this bug. That's great. Using dump and ivarDescription are great ways to diagnose problems with your app. Another wonderful debugging tip and trick that we have for you is breakpoints. Breakpoints let you pause the program at arbitrary states of execution, and run commands. And, using the LLDB command line, or the Xcode UI, you can even add conditions before these breakpoints are run, and commands to run every time the breakpoint is hit. Breakpoints are an essential part of your debugging workflow. And, you can use the expression command, and dump, and ivarDescription, with the breakpoints that you set up in Xcode. I really encourage you to use breakpoints next time you're debugging an issue with your app. But, sometimes we don't have an issue with views or viewControllers. We don't have an issue with state, instead we have a really hairy memory management issue. Ben, are there any tools we can use for this? Well, I'm glad you asked Peter, because yes, there is another great tool built into Xcode. The Xcode memory debugger. This tool will help you visualize exactly how your application is using memory. Peter and I were debugging an issue the other day, where we had a leaking viewController. And, we could see that here that it's being held onto by a block. By enabling Malloc stack logging, we were able to see the full backtrace of exactly when this block was allocated. By zooming in, we can see that this block was actually created by that viewController. And so, that block is holding onto that viewController. But that viewController is also holding onto the block. And, there's our retain cycle. Wow. Great! The Xcode memory graph debugger is such a great tool for diagnosing issues like this in your app. For even more, check out the Debugging with Xcode 9 talk from 2017. So, that's how you can debug your app like a pro. Make sure to think like a detective whenever you encounter problems with your program. Use the Xcode view debugger, and memory graph debugger to dive deep on view- and memory-related issues. And, use LLDB's expression command with dump, and all the other great debugging methods we talked about here. Thank you. So, we've covered six really exciting topics this morning, this afternoon. But, we've barely scratched the surface. We encourage you to go out and check out the talks that we referenced throughout this presentation, and add even more delight to your applications. For more information, check out our page on the developer portal, and thank you. We hope you had a great conference. Thank you. Hey, everyone. Welcome to the AR design session. My name is Grant Paul, from the human interface team at Apple. And, I hope you've been having a great WWDC this week. So, for this session, we're going to start off by, I'm going to talk about how to design great AR apps and games with the user interfaces and interactions. And then, Omar's going to come up, and he's going to tell you all about making 3D models that look great in AR, and feel great in AR. So, before we get to that, I want to quickly talk about the basics. So, if you're new to it, you might be wondering exactly what it is that we mean when we talk about AR. But, even if you're an AR expert, I want to talk about what we mean for this session. So, AR, of course, stands for augmented reality. And, let's break down what that means a little bit. And, we can start with reality, because it's a little bit easier to get into. So, reality means is that AR deals with things in the real world. And, that's a little bit different than other things we might have done on our devices, that take place on the device, or they take place on the internet. But, with AR, things happen in the real world. They happen in the room around you, in the environment you're in, or in your place on the map. Where you are. But, the important thing is, it's a little bit different. And the other part of augmented reality, is augmented. And, that can mean a few different things. So, augmented can mean -- it can mean augmenting what you know about the world. It can mean getting information that the device can understand about the world, and giving that to you. It can mean placing virtual things out into the world, giving the virtual this physical context. And, it can mean taking something that is real, like your face, when you put an animoji on as a mask. Taking something that is real, and enhancing it, augmenting it. So, that's what we mean when we talk about AR, when we talk about augmented reality. And, with that, I want to get to the first half of this session. I want to start talking about how to design interfaces, and design interactions for your AR apps and your AR games. And, the first part of that is to talk about how to get people into AR. How to help guide people into AR. So, I'll show you how iOS 12's built-in AR experiences help guide people into AR. And then, how you can take those principles, and apply them in your own apps. Next, we'll talk about the different ways you can present content in AR. The different possibilities that ARKit opens up, and also some tips and tricks that are great no matter what kind of AR app you're making. And finally, we'll talk about interactions in the world. It's a little bit different when you're building an AR app than when you're building a 2D app, for what kind of interactions make sense, and what kind of interactions you're going to want to use. So, we'll figure out what still works great in AR, and then, where we might need some new kinds of interactions. But first, I want to get started with talking about how to get into AR. And, what I'm talking about here is after somebody's downloaded your app, after they've found the AR experience in the app, they've opened it up, what I'm talking about here is when ARKit needs to understand the world. Because for every AR app, ARKit needs some level of understanding of the world in order to start the AR experience. In order to get it going. Because every AR app needs to -- needs that understanding in order to place its objects into the world, or show the user that information. And, the way ARKit builds that understanding of the world, is by getting you to move your device, by having you move around. And, that's a little bit different from other places that you might have looked through a device, seen a camera preview, in the past. Like, for example, when you're taking a photo, you just need to point the device where you want to frame the shot. But, in AR, you really do need to start moving, looking at the same place from different positions, and looking at it from some different angles. So, the trick here is to let people know what they need to do. Let them know how to move, that they need to move their device. And, the way to do that is to give them a fixed reference. Give them something to base that understanding off of. And, let's talk about that. Let's take a look at an example. So, this is the game Euclidean Lands. And, what it's showing here, is the device moving around inside of a room. And, that's really great, because you can see here, without any text exactly what it is you need to do, that you need to move the device within that room, and just turning the device to look at a different angle isn't going to be enough. So, without any text, it's really clear from that fixed reference of the room, exactly what you need to do. And, usually, most of the time, that is all you're going to need. ARKit is even faster to build that understanding of the world in iOS 12. So, in even more cases that is all you're going to need. You just need to start moving, and you're ready to go. But, of course, there are some situations that aren't quite as well suited to AR. Maybe you're in a dark room, or it's really reflective, and it's not as easy to -- for ARKit to start building that understanding. So, it can take ARKit a little bit of time to get ready. It might not happen immediately. And, in those situations, if you're still being told to move your device, you're still looking at something that says to you, move your device around, people might start to wonder, why is the app not working? Does the app not understand the movement that they're making? And, it might get -- start to get confusing. So, what you need is some kind of feedback that lets people know that they're not doing anything wrong. They're doing exactly what they should be doing to get into AR. And, they should just keep doing it. So, let's take a look at an example. So, this is how iOS 12's built-in AR apps help guide people into AR. You can see the device moving over the fixed reference of that surface, by showing you that you need to move it. You can't just keep it in one place, or rotate it. And then, once you've started moving the device, the surface transitions into a cube. And, that cube spins with the movement of the device, giving you real, connected, direct feedback that you're on the right path, and you're doing the right thing. And then, once ARKit has built that understanding, it's all ready, the cube spins away, and you're ready to go in your AR experience. So, that's how the built-in apps in iOS 12 help guide people into AR. And, your apps should follow those same principles. They should help people know that they need to move their device, and give them that feedback that they're doing the right thing. But, your apps don't need to follow that same style. They don't need to look like line drawings. AR should feel like an integrated part of your app. It should feel native to your app's style. And, it shouldn't feel like something that's attached on, or this flow is something that's added on after the fact. So, the important part is to help people down the right path, but it should feel like part of your app while you're doing that. And, the last thing I want to talk about for helping get people into AR, is that it's important to balance your instructions, your guidance, how you're helping people get into AR with being really efficient when people already do know what to do. So, if somebody already knows what to do, they're just going to start moving their device right away. They don't need any kind of instructions. They don't need anything to tell them to do that. So, don't make them sit through any kind of guidance. Since ARKit is so much faster, in a lot of cases, they'll start moving, ARKit will understand the world, and they'll be ready to go right away. Alright. So, that's getting into AR. That's how you help guide people into your AR experiences. Now, let's talk about how to present your content in AR. The different ways that you can show things in AR, and tips and tricks for all kinds of AR apps and AR games. And, I want to start with some of the more radical possibilities of things you can build in ARKit. Because not every AR experience has to look like looking through your device, seeing a camera preview of the world, and then placing objects out into space. There are other options as well. So, one of those options is to build an AR experience entirely in two dimensions. You don't need to show a camera preview. You don't need any kind of 3D graphics. You're using that information that ARKit gets about the world to present a great AR experiences entirely in 2D. So, let's take a look at an example of that. This is the game "Rainbrow." And, in Rainbrow, the way you control you character is by moving your eyebrows up and down. [ Game Music ] So, there's no need of any kind of camera preview here. There's no need for any 3D graphics. It's a lot more fun entirely in 2D, but it's still a great AR experience. So, that was AR in 2D. But, another way, another thing you can do with ARKit is you can build things that I would consider to be full virtual reality experiences. And, what I mean by that, is that the experiences take you to a different place. They really make you feel like you're somewhere else. You can walk around the environment. You can move around within it. You can look in all directions. And, to me that makes something really into a virtual reality experience, even if it is through a device. And, there's some benefits to that. You don't need any extra equipment. It works everywhere. You don't need to wear a headset on your head. It works everywhere you are with the device you already have. You don't need any kind of trackers or anything else. And, there's some benefits to looking through a device rather than being fully immersed. Like, you're never going to accidentally walk into a wall because you always have your peripheral vision. So, ARKit can be a great way to make virtual reality experiences. We'll take a look at one. So, this is the virtual reality experience called "Enter the Room." And, it was developed by the International Committee of the Red Cross. And, in this experience, once you walk into the room, you can look around in all directions. You can move closer to things. You can inspect them. You can move farther away. And, the sound comes in, and it feels like it's coming in from outside the room, from all around you. And, that makes it into a really powerful experience. And, that's what you can do with virtual reality using ARKit. So, those are some of the more radical options of things that you can build with ARKit. We can also look at some tips and tricks that make sense no matter what kind of app it is that you're building. Whether it's a game, or a productivity app, these can work for everything. So, the first thing I want to talk about here is showing text in AR. Because text is really important. All kinds of AR apps have different reasons to show text. If it's a game, maybe you want to show the title of a level, or some instructions. Or, maybe for other kinds of apps, you want to label something in the world, label a virtual object. Maybe show some annotations. But, the important part, no matter what reason it is that you're showing this text, is to keep that text readable. Make it really easy to read. So, the simplest way to show text in AR, is to put it out into the world, to put it in perspective. And, you know, that can look really cool. But, it also has some drawbacks. And, one of those drawbacks is what happens when you look at it from an angle. The letters can, sort of, squish together. It can be a little hard to read. And, another issue is what happens when you take a step back, when you're looking from further away. The text can get really small. It's like trying to read a piece of paper from all the way across the room. So, if you're showing titles, or other kinds of things that maybe people already know, or could get in another way, it can be really cool to show that in perspective. But, if you're showing something that people really need to read, need to get information from, you might want to try a different option. So, a different option that you can use, is to show your text in screen space. And, what I mean by screen space, is that the text is always the same size. It's always facing you. It's always head-on. And, that makes it really easy to read. It doesn't have those problems of what angle you're looking at, or how far away from it you are. But, the important part of showing text in screen space, is that it's still attached to a position in the world. It's still fixed to the place in the world that you attach it to an object, or attach it to some physical feature. And, that makes it really feel like part of the AR scene. So, screen space text is a great way to label things, put some annotations into AR, but still make them really readable. So, here's an example of screen space text. And, this is from Measure, which is part of iOS 12. When Measure shows the measurements, it shows them in screen space. So, no matter what angle you're looking at those measurements from, or how far away you are, they're always incredibly readable. So, screen space text is great for readability. But, you should still try and keep how much text it is that you're actually showing in AR to as little as possible. And, the reasons for that is when text is placed in the world, you always have to keep pointing your device at it in order to read it. If you turn your device to a more natural reading position, the text is going to go away. So, if you have more detailed text, if you're showing details about some kind of object, or something in the world, you should show those details on the display. And then, people can use all that experience that they built up using iOS and reading things on their devices, to read those details directly on the display. And, when you're showing those details, when you're coming out of AR, it's important to have a transition. Because those transitions can make it really clear what it is you're looking at. What text, what object it is that those details are referencing. What it is that you're getting details about. So, let's look at an example there. And, this is also from Measure. In Measure, when you tap on a measurement, it comes out of AR to show the details about that measurement flat on the screen. And that's great because it's really easy to read. You don't have to point your device or your phone at the measurement in order to read it. But, it's also really clear, because of the transitions. The transitions show you what measurement it is that you're looking at details for. And, they comes out of the measurement, so you're never going to be confused. So, transitions. They're really great when you're coming out of AR, and going back into AR, for showing details on the display. But, they're also really important when you're just showing objects in AR. And, that's because it makes it feel like there's one version of the object. It makes it feel like that object has its own identity. And, that's important, because things in AR are so physical. They have that physical sense. When you see them in AR, they look real. And, things in reality, you can't just copy them. You can't just make multiple copies. So, it's important to keep that same principle when you're showing objects in AR. So, this is what happens when you Quicklook at objects in AR. And, it shows a great example of keeping that identity. When you switch from the object tab to the AR tab, the object stays in place. It always stays visible. It doesn't disappear and then come from somewhere else. And, even when you're deciding where to place the object here, it always stays visible on the display. So, it's really easy to see that there's one version of this object. And, even when you go back into the app that you were Quicklooking the object from, it still shows the object transitioning back to where it came from. So, it feels like there's one object moving between different parts of your app. And then, into the world, and back out of the world. It doesn't feel like there's multiple copies. Alright. So, that was a bunch of information in a row about the different ways that you can present your content in AR. So, let's do a quick recap of that. So, first, we looked at the different ways you can make AR experiences, and we talked about using AR to create entirely 2D experiences, with no camera preview, and no 3D graphics. We talked about how VR can be a great way to build experiences with ARKit, and make you feel like you're somewhere else, that can be really powerful and really immersive. We talked about using screen space to show text in AR, to make it really readable from any angle, and readable from any distance. We talked about showing your details on the screen, so that they're easy to read without pointing your device at some specific place in the world. And that you can use all of that same knowledge that you've built up reading things on iOS. And finally, we talked about transitioning into AR, and out of AR, for showing details flat on the display. But, also to give objects that sense of identity, that sense of physicality that's so important in AR. Alright. So, that's presenting content in AR. So, different ways that you can present it. And some tips for different types of content that you're going to want to show in your AR apps. Now, let's talk about how we can interact with that content. Let's talk about interactions that make sense in the world. And, let's start with touch, because touch has been really important all the way back to the beginning of iOS. Multi-touch was there right from the start, and it's been the most important way to interact with our devices. And the reason touch is so important, the reason touch is so great, is that it enables direct manipulation. And, direct manipulation is when you interact directly with things on the screen, like they're physical objects. You're not using controls to scroll or to pinch to zoom. You're interacting directly with the content. It's like there's a physical thing that you're manipulating. And, that's even more important in AR. Because in AR, as I said before, things are really physical. Objects feel like they're real. They feel like part of the real world. So, it's really important to use direct manipulation to make it feel like you're interacting directly with those objects. And, direct manipulation is also great because it uses gestures that you already know, that you have experience with from iOS. Because those gestures will be the same as any other content on iOS. They're things that you've been using for probably a long time. So, the first one there is how you can move objects in AR with direct manipulation. If you want to move objects, you just put your finger down and drag them to a new place. And, it feels like you're picking up the object, because it stays under your finger. You get this physical connection to the thing that you're moving. Another gesture you can do, is scaling objects. So, in AR, things start out at their physical size, at their natural size. But, if you want to change that, you can pinch the object to make it bigger. And, you can pinch out on the object if you want to make it smaller. The important things to think about when you're scaling objects in AR, is to give some feedback when you're doing it. Because the change is really, really big when you take an object and scale it up to 4 times the size. So, it's important to give people feedback, so they absolutely know what happened. And, the second thing is to make it really easy to go -- to take the object back to its natural size. Back to the size that it would be in the physical world. So, you can snap back to 100%, maybe with a haptic, to make that really easy. Another thing you can do is rotate objects, by putting two fingers on the display, and twisting to rotate. And, that can be really great, but with all of these two finger gestures, another thing you should think about is your tap targets. Because things in AR are always moving as you're moving the device, and they can get really small if you get further away, or if you scale them down. You should make sure to use really generous tap targets. So, it's easy to land two fingers on the object. And, be sure to use the center of those two finger touch positions to figure out which object to interact with. Because you might not be able to land two whole fingers, even on a generous tap target in AR. Alright, so direct manipulation is really great in AR. It's really important because AR is so physical. But, it's also not quite enough for most AR apps. Because if you have a lot of objects, it can make it hard to touch the right one. As I said before, the objects are always moving on the display as you're looking at different places in the world. And, they're staying fixed to that place in the world. So, it can be a little hard to aim for those objects on the screen. But, the fundamental reason, the number one reason, the real reason that touch is not enough for AR apps, is that it's fundamentally two-dimensional. The surface of the display is two-dimensional, and you're touching on that surface. That is what made it so great, multi-touch so great for flat, 2D apps in iOS. But, AR content, it's placed into the world. It's part of the world. So, that means we need some way to interact with that content also in three dimensions. Because the world is three-dimensional. The answer there is moving your device. Because moving your device, it's natively three-dimensional. It's three-dimensional inherently. You can move up and down. You can move left and right. You can move forward and back. You can turn in all directions. You can stand up and move across the room if you want to go a little bit further. But, the important part is that moving your device is fully three-dimensional, and that really makes it the number one, the primary interaction in AR. In fact, I would say that moving your device is more important than touch for AR apps. In fact, it's so natural, it's built in to every AR app by default. The way that you look at different things in AR is by moving your device to look at it from different angles, and different positions. So, it's really natural, and it's also really powerful. And, moving your device, it accomplishes many of the things that you would have done in a 2D app by using multi-touch. So, in a 2D app, if you want to see more content, the way you do that is by scrolling on the display. You scroll down to see something new. And, that's great in a 2D app, but in AR, you do that in 3D. You see different content, by moving your device to look at the content from different positions, and from different angles. So, it solves that same problem of wanting to see more, but it solves it fully in three dimensions. Similarly, in a traditional 2D app, if you want to see something bigger, you pinch in on it to make it bigger. You pinch to zoom. If you want to see it smaller, you pinch out. You pinch to zoom out. But, in AR, if you want to see something bigger, you can just get closer to the thing you're looking at. You can just move closer in. And, if you want to see more things at once, you want to see things from a wider angle, you can just take a step back, and look at all of the content at once. So, moving your device, it also replaces what you might have done by pinch to zoom to see more content in a 2D application. So, movement is really great. It can replace some of those things you might have used multi-touch for. You can also use it to build totally custom interactions for your AR apps. And, those can be really natural. Just like using your device to look around at different things, or to get closer and further away. So, let's take a look at an example of that. This is Swiftshot, the multiplayer AR sample game that you might have seen in the Keynote, or you might have actually tried it. In order to fire a slingshot in Swiftshot, the way you do it is you move close to that slingshot. You don't have to pick which slingshot you're going to fire from a list. You don't have to reach out and try and aim for it on the screen. You just move close to it, and when you want to fire the slingshot, you just pull back and release. So, it's incredibly precise, because you can fire in three dimensions. You have three dimensions of precision while you're moving that slingshot, while you're pulling it back. And, that's more than you could ever do using touch. So, moving your device, it's not only really natural, but it's also more precise than you could ever do using touch in AR apps. So, moving your device is great, but sometimes we need to combine the best of touch and the best of device movement to create the absolute best interactions. So, let's look at some examples of that. And, let's start with combining direct manipulation with moving your device. So, Quicklook in AR also offers a great example of combining device movement and direct manipulation in AR. If you want to move an object, of course, you can just drag it to the new position, the same as you saw before. Touch on the screen and move it to a new place. But, you could also touch down to pick up the object and then turn the device and release in a new place. And, that's great, because it gives you the full three-dimensional control of moving your device to pick where you're going to place it. And, it lets you move objects to places that you can't see on the screen, by picking them up and turning. But, it still keeps that sense of physical interaction. That sense of direct manipulation that you had from picking up the object directly. So, if you support any kind of moving objects in AR in your apps, you should definitely support picking up an object with direct manipulation, and moving the device to find a new place to put it. Alright. So, moving your device and direct manipulation is one way to combine touch. But another way to combine touch with moving your device is through indirect controls. And, what I mean by indirect controls, are controls that are flat on the display. They're not placed in the world. They're not attached to anything. They're in a consistent position on the display, and you can learn that position. And, that's really great, because that means they get out of the way. Once you've learned that position of that control on the display, it stays in one place. You can rest your finger above it while you're speeding your time focusing on the rest of the app. So, let's look at an example of that. Here's Zombie Gunship AR. You're flying in a gunship above a horde of zombies, and you really need to aim at them. You want to focus on aiming. You don't want to focus on moving your finger, trying to figure out where you need to aim your finger on the display in order to fire. Instead, you can just rest your finger above the fire button, and spend all of your time using that full 3D precision of moving your device in order to aim where it is you're firing, to aim at those zombies. So, indirect controls are really great combined with moving your device. But, they're also really great because they let apps work one-handed. In AR, no matter what, you always have to use one hand to control where it is that you're looking, at least one hand. So, if you want to build a one-handed AR experience, you're going to need to use really reachable controls that are really easy to access. So, an example of that is Measure. Measure uses an indirect control, the plus button, the add button at the bottom of the screen to add points. And, that control is in a really reachable spot. So, even while you're using one hand to focus the reticle on the center of the screen, to precisely place your measurements, you can keep a finger placed above that plus button to easily add your measurements, to easily place those points. So, using indirect controls can be a great way to make it not only easy to use AR experiences, but one-handed ones as well. Alright. So, that's how to pick interactions for AR. You can use direct manipulation to give that sense of physicality, and physical interaction. You can move the device, the primary interaction in AR. And, you can use really reachable indirect controls to focus on the content, rather than on controls or buttons interacting with it. And, that's what I wanted to talk to you about today. Getting into AR, guiding people down the right path, and giving them that direct feedback that they're doing the right thing. The ways you can present your content in 2D and VR, and how you can show content to make it easy to read, and give objects that physical identity transitioning in and out of AR. And, the interactions that you can use in the world, especially and primarily moving your device. So, now I want to bring up Omar, who's going to talk about making your models, your 3D models look really great in AR. Thank you. Thanks, Grant. Hey, everyone. I'm really excited to be up here to talk about some best practices that you need to keep in mind while you're developing your content for your AR experiences. So, we have a lot of different pieces of information to cover today. And, whether you're an engineer, designer, manager, or an artist, we wanted to provide you with a utility belt of tactics and definitions, so that you are best equipped to craft your own exceptional AR contents to delight people with. So, to begin with, let's start with some essential concepts to keep in mind while you're developing your AR experience. AR is incredible. Having the ability to take anything you can imagine, and place it into the real world is absolutely magical. And, because of this, people tend to expect a lot out of their AR experiences. They expect your 3D content to render at a smooth and consistent rate. It's incredibly distracting when you're really engaged with the content, and you start to move a bit closer to it, and then, you want to appreciate those fine details, and all of a sudden, wham! Poor optimization has caused performance to tank, and now it's as if you're watching a slideshow. So, to ensure your -- the most smoothest performance at all times, and to keep people fully engaged with your AR scene, your app needs to render at a recommended target of 60 frames per second. Now, it's really important to maintain this target throughout the entire experience. Really stress test your content. View it from every possible angle. You know, move in close, move back from it, and just make sure that the performance does not ever degrade. You know, maybe someday in the future, batteries will run for days. But, today, make sure that your experience is able to have as minimal of an impact to battery life as possible. Don't give people the chance to blame your app for draining their battery. The more power you save, the more you want people to come back to your experience and try it again. I mean, I don't know about you, but whenever I see a battery indicator in this stake, I seriously feel like I'm going to have a panic attack. And, we definitely don't want to have our AR experiences cause widespread battery chaos throughout the lands. Remember, only you can prevent excessive battery drain. I like to look at AR as having the power to take anything you can imagine and to transport into the real world. People want to explore your content, so you definitely want to bring your A game. Take the time to craft those nuanced details into your 3D content. Build a cohesive story and style. And, remember that every little detail, every little touch is an opportunity to surprise people. So, let's say we wanted to make an AR experience to be about an aquarium. Even in its most abstract form, I think I would be really hard-pressed to find anybody who would believe that this marshmallow blob represents a fish. On a positive note, though, our app will pretty much rock the performance numbers if this little guy's flopping around. So, let's try that again. Ah, now this is much better. That is one properly dead-looking fish. See how it exhibits some nice details, that once it's running in AR will really want to entice people to move closer to it to see all those nuanced details and explore its features. We should strive to maintain this level of quality with all the fishes swimming in our aquarium, or just floating in the top in this case. Finally, it's important to remember that people really want to use your app in a wide variety of environments. You want to avoid having your content stand out in real world locations where potentially the lighting ambient conditions could conflict with the story that you're trying to tell. So, when working with your assets, try to avoid using colors that are either too bright or too dark. And, make sure that you light your AR scene in a way that it casts even lighting on all the objects that you're planning to render, and at no matter what angle you view them from. You want your AR content to work whether it's day or night. Now, spoiler alert, we're going to go over some really nice features in ARKit that will enable you to really delight people when they see your AR content blend and react seamlessly to the real world environment. So, as you're building your AR content, one great tool to help evaluate your progress is by using one of our recent announced iOS 12 features, AR Quicklook. Throw some of your assets up on iCloud drive, view them using the Files app on iOS, and quickly have the ability to project it into AR. Heck, you can even show off your masterpiece by throwing it onto a website so that your friends can look it up, and you can view it anywhere, right from Safari. It's pretty awesome. Definitely go back and check out the session with Dave and David, who go over details of best practices of how you use AR Quicklook, and I'm guarantee you, it'll probably change your life with how you develop your assets. So, now you've taken some time to consider what people expect from our AR experience, let's quickly plan out what type of app we're going to build today. You know, it's always a good idea to start thinking this through before you begin creating your 3D content. As knowing what you want to make will help narrow down how best to optimize your content and your assets for AR. So, you're sitting at your desk, and suddenly you're struck by inspiration. You just came up with the most absolute brilliant AR experience ever. Alright? So, let's step back and ask ourselves a couple questions first. Does this experience really need to render hundreds of AR objects, or will we focus on a single hero asset? How much detail do we actually want? And, what graphical style best represents what we're trying to convey? Have we really paid attention to Grant earlier and are thinking about the level of interaction we want people to have with our experience? Having a clear answer to questions like these will help determine where best to put your rendering budget when you're developing your app. For example, imagine you're building an AR experience similar to the IKEA Place app, where people can preview different pieces of furniture by being able to place them in their home, or in our case, outside on their patio. Now, the stars of the show are actually the different furniture pieces, so you need to present highly detailed objects that closely mirror the real world counterparts. In this case, it is a good idea to make sure that you spend a little extra time in your rendering budget on these single hero assets, because their level of quality can essentially make or break a sale. On the other hand, you decide that you've had enough of accidentally stepping on those little, tiny, plastic bricks of agonizing pain that your kids start laying around the house. So, in order to preserve your sanity, you build an AR experience so they can play around with as many of these blocks as they can possibly imagine, similar to Playgrounds AR. And, save yourself from ever having to experience brick pain again. In an app like this, where you potentially have a lot of objects being rendered and interacted with, you want to basically create very simple, low-poly models with a flat colorful material, so that you can have a ton of them onscreen, and still have really good performance. So, now that we've asked ourselves these important questions, it's time to set up our AR canvas. Just like painters like to set up their canvas in space before they begin work, we would like to have a couple suggestions of how to set up your project up front, in order to position yourself for optimal success. We are big fans of creating a focus square to determine where to start placing your AR content. And, if you're using SceneKit, right there on the bottom of the screen, you have the option to actually activate the statistics panel. This will allow you to see your current frames per second, as well as how many polygons there are visible on the screen at any given time. Which should be very helpful as you start to build out your app and put all the different elements into it. So, now that we have a starter scene up and running, I was thinking, what will be a good example AR app to help get over these best practices? So, I'm not really an outdoorsy guy, but coming to California, I find that here a lot of people are. So, I've been trying to connect with nature. You know, go camping, maybe make a campfire for once, and yet it really didn't happen. So, instead I thought, let's just throw it into an app, and see how it goes. And we're going to call it CampfiAR. I know, it's perfect, right? Now, we can work on building out a detailed, single object, and bring all the joys of being outdoors without the fear of any of those bugs or fresh air. We decided to render with a stylized, semi-realistic, and playful graphical style. And, apply unique details to the use of careful application of some key, physically-based material properties. These choices mean that we could potentially use a lot of polygons to render the content on the screen, but why deprive people of the ability to spend multiple hours staring at our beautiful campfire? Let's avoid going down that route, and work towards optimizing our scene by using a few tricks of the trade. So, we'll begin by focusing on the foundational structure of 3D objects, the mesh. And describe the typical development flow that will allow you to create highly detailed models, but still maintain a low poly count for all the models in your scene. And, for those of you who might not know, poly count is essentially the number of polygons, typically triangles, that a mesh is composed of. So, one of the first things we like to do, is lay out the basic structure of an AR scene by using these simple meshes. We find using this type of white-boxing technique to be really helpful for testing out some basic interactions, as well as seeing how well the objects fit into the real world, what kind of scale are they at? You know, actually, I think this campfire looks really great. I think we're going to call it a day here. Let's just call this done and ship it. Thanks everyone. I'm off to the afterparty, and -- wait. So, that didn't look like a campfire to you guys? Oh, alright. Sorry. Really? My bad. Let's jump back to actually building out this campfire mesh. I want to give a quick high/low review of what a mesh is, and the basic data structures that comprise it. So, now you can think of a mesh as being a collection of triangles that are arranged in 3D space that will form a surface for you to apply materials on. And, the corners of these triangles are actually made up of points called vertices, which hold different pieces of information, such as its position in space, UV coordinates for texture application, as well as a very important property that we'll go over later called the normals. Well, since I got busted for trying to ship early, I wanted to redeem myself by building out one of the most gorgeous campfires in the world. Take a look at the details of this camp. Look at that fish and those branches. You can see all the intricate details of the scales, as well as the etchings found on the bark. But, man, my performance has tanked. And, that poly count has gone up to almost about a million polygons for this screen. So, I'm already in hot water, and I don't want to get in any more trouble, so we better go back and fix this. As I'm concerned about the impact that this will have on battery life, as well as how people are able to perceive and interact with this AR scene. So, let's see what we can do to, kind of, help reduce the number of polygons here. Most 3D authoring tools have specific functions that make it easy for you to reduce the actual complexity of your models. Here, we've reduced the number of polygons associated with the high-density model of the fish. But, notice that, as we zoom in, many of the details have been lost. But, don't fret. We can use certain material properties later on that will bring back a lot of those missing details. The key here is to build a solid foundational mesh that uses a minimal amount of polygons. So, let's put aside the high-density mesh for now. And, let's focus on building up this low-density mesh as we move forward. Alright, so I'll admit that this isn't looking quite as good as before, but man, you can take a look at that performance. Not only have you saved a crazy amount of overhead by reducing the number of polygons found on the screen, but we're able to add a bunch of 3D objects to this scene to make it even more robust. And, if you recall, in our previous high-density mesh, we were running at about 30 frames per second. But, now we're back to 60 frames. And, we were close to about, I think about a million polygons, and now it's down to 9,000. This is incredible. Because of this, we are well on our way to having those performance specification that we desire. A really solid frame rate with minimal impact to the battery life. So, now that we have this optimized model in our campfire scene, let's see how we can bring back some of those details that we lost by working on what some of these different material properties and techniques that will ensure that our model looks as good as possible while still maintaining that great level of performance. So, you may have heard this time before, physically-based rendering thrown about regarding modern 3D rendering. It's a pretty complex topic that will take a lot more time than we actually have in this session to go over. But, the basic concept describes the ability to take the application of your different material properties onto your mesh, in order to have it react realistically to the simulated lights found in your AR scene. And, moving forward, all the materials that we'll be discussing will conform to this shading technique. If you want further details about this concept, there's a great WWDC 2016 talk that goes over details about physically-based rendering as it applies to SceneKit, called Advances in SceneKit Rendering. Now, with that said, let's start talking about our first material property, albedo, or what is sometimes lovingly referred to as the base or diffuse color property. So, let's jump back into CampfiAR. Previously our base measure's looking a little bit boring, with the gray material associated with it, but after you apply albedo, you start to see that it looks a lot better. But, the campfire is still missing a lot of those small, finite details that were originally found in that high-density mesh. As you move closer to the campfire, you'll notice that all the surfaces are relatively flat. And, this is something that we'll definitely correct later, but first let's dive into the albedo property a little bit more. Think of the albedo as basically being the base mesh of the various objects in your AR scene. This is the material property that you typically use to apply textures to the surface of your model. If you recall, your mesh contained different vertices, that held different pieces of information. The ones you see here are actually called UV coordinates, which help map out how pixels from various texture maps are actually applied to the model. And, after we've worked with these textures, we've applied the abel material to this property on the fish. So, now we've essentially applied a texture to our fish, I want to remind you about the fact that you never know where people are never going to experience your app. You want to be able to have your content fit in as many different scenarios as possible. So, you want to take care in selecting the right albedo value that are neither too bright or dark. As, you want this to work in a wide variety of different situations. So, our fish has got a skin, but we're still missing a lot of details from this, and the other objects found in the scene. So let's go back into how we can start bringing back a lot of those details through the use of the normal material property. So, as we jump back into CampfiAR, let's see how we can bring back some of those details that we moved from optimization. This can be done through the use of a special texture called a normal map, which you can see here as the blue tinted maps that are now applied to our AR scene. These maps allow you to add those fine surface details back into your models, without the need to add any additional geometry. Now, after applying the normal maps, you can see how the fish exhibits some scales, as well as making the branches show just a tiny bit more detail. Also if you take a look at the statistics panel, you'll notice that there was absolutely no change in the number of polygons related to this model. It's magical, isn't it? So, how do we make this normal map? Let's see a closer look at one of the branches, and see what we can do to make this work. In most modern 3D modeling applications, artists have the ability to generate these normal maps by projecting details from a high-density mesh over to a low-density one. So, here you can see what the normal map looks like on the branches after they've been generated from this high-density mesh. And, after applying the normal map, you start to notice all those nice details that were originally lost come back into this model. But we still are able to maintain that high performance, of the low-poly mesh. So, you may be wondering why the normal map looks kind of strange, well the colors of the normal map actually represent visual representations of the vector data. And, determine how the normals on the surface model will be offset in order to change the way that light is being reflected, and our key to making this effect work. That was a bit of a mouthful, so let's dive a little bit more into this property, because normals we feel are a really important topic. And, we want to spend a couple more minutes just going into how truly spectacular these can be. The art of manipulating normal vectors are one of the key tools that AR creators have in order to add a lot of the significant details back into their model. So, what the heck is a normal vector, and are there strange vectors as well? Well, no there's no strange vectors unless you forgot your high school trig, but normal vectors lie perpendicular to the surface of the mesh, and are associated with each mesh vertex. So, why do we need these normals? Well, in order to see our object, you need to place simulated lights into your 3D engine. Normal vectors allow 3D engines to calculate how these lights are actually reflected off the surface of these materials. Similar to how light behaves in the real world, and are essential to making sure that your AR scenes mimic reality. What's interesting is that by modifying these normals, you can trick the engine to thinking that your surface is actually more detailed than it really is, without need to add any additional geometry. If you take a look at this example, you can see a simple sphere, being rendered with flat shading. What this means, that the normals associated with each face of the mesh are pointing in the exact same direction, as seen by the 2D diagram. Now, when you -- when light reacts to this surface, you'll be actually able to notice all the different polygons comprising this mesh due to being evenly lit across each face. Here, though, we're using the exact same model, but leveraging a technique called smooth or phong shading. Notice that the normals are actually gradually changing as you move across the surface of the polygon. With the engine calculates the reflection off of this model, it'll give the impression of a smooth, curved surface due to the gradual interpolation of these normals. What's really awesome is that these two models have the exact same number of polygons associated with them. But, through this normal manipulation, the object will seem to have a much more smoother and detailed surface, without the need to, again, add or change any of the geometry associated with this mesh. Whew. Alright, so we've gone through enough about normals. Let's, kind of, go over what it takes to add a little bit more shiny to your scene. CampfiAR is definitely starting to look better. But, some of these normal maps [inaudible] but some of these parts seem a bit dull, especially the objects that you expect to be shiny or reflective, kind of like the kettle or the fish in this scene. What we see here is the results of applying a metal map to our AR scene. A metal map is used to determine which object surfaces should exhibit reflective properties. Once the material property's activated, notice how shiny and reflective the area's we've designated as being metallic are. And, on the kettle as well as the scales of the fish. So, let's focus specifically on the kettle. We'll begin by taking the original albedo map, and then apply a metal map to the metalness property of this material. After the application of the metal map, the 3D render will actually designate the surface to be reflective in the areas on the map that were actually white. And, despite begin called metalness, it doesn't necessarily mean that your object needs to contain metal. Rather, we're just letting the 3D engine know that this object should exhibit, kind of, a reflective surface. Now, it's best to use the metalness map on your model, like our kettle here, when it has both a mixture of metallic and non-metallic surfaces. It's a simple grayscale map, where the level of metalness ranges from being black, for non-metal surfaces, to white, for metallic surfaces. And, it allows for a single material to be used to represent both reflective and non-reflective surfaces on your single object. However, this kettle's a bit crazy reflective. And, doesn't really quite provide the look that we're hoping for. So, in this case, we want to potentially vary the amount of reflectivity, as well as simulate the fact that all surfaces are not perfectly smooth. And, they actually might exhibit slight small, microabrasions to their surface. And, this is where the roughness material property comes into play. So, returning back to CampfiAR, you can see how the reflective surfaces are a bit too smooth. As we layer on the roughness maps, you can see that we are going to modify both the kettle and the fish to adjust the way that they're reflecting. And then, after we apply the roughness material property to both of these objects, you can clearly see the reduction of the reflectivity. This combination of roughness and metalness properties are another important concept to focus on. So, let's dive a little bit deeper into the roughness material property. Use roughness to simulate micro surface details, which in turn will affect the way light is being bounced off that surface. If the roughness property is set to completely smooth, then light will bounce off of your surface like a mirror. As you increase the roughness of the material, light reflects over a wider range of angles. Here, we're slowly scaling a constant roughness value between no roughness to max roughness on the kettle itself. And, this is a good way of being able to simulate the concept of having microsurfaces, and blurring reflection to the point where you might not see any reflectivity depending on which range you put your value to. So, for the kettle, we've taken the original metal surface, and instead of just applying a constant roughness value to it, we actually applied a roughness map. And, this will help designate the surfaces where we will be scattering the light more often, and more than others. Once we've applied the roughness map, we get to see the real final look of the reflectiveness of this kettle, which is a lot less shiny as before. Now, a combination of this metalness property and this roughness property really make your reflective AR models look phenomenal. Roughness can be used to tweak how much of your objects will reflect the environment. And, can really be used to add a lot more realism to your metallic surfaces. You can use this roughness map to add additional minor details as we've done here, to make our kettle look a little more scuffed up. Now, to close out materials, we have two more material properties that I want to discuss to further refine your models, and have a good balance between performance and aesthetics. Ambient occlusion is a material property that is used to provide your model with self-shadowing, which in turn can lead to adding additional depth and details to your AR models. Now, while normal maps are great for applying significant amount of details back into your AR model, you can use ambient occlusion to really hammer in those details. Here, we're visualizing the ambient occlusion maps for CampfiAR, and it's a bit of a difficult effect to demonstrate, as it's meant to be relatively subtle. But, see if you can notice the additional shadows on the logs, as well as certain areas near the bottom of the kettle. In our case here, it's kind of like playing Where's Waldo? with shadows, so let's focus in on the logs in the scene. So, here we're showing you the normal maped logs before. Now, there's some great details found in the ridges, but we can definitely improve some of these areas. Now, as we look at the ambient occlusion map, you can see how we've added some regions of self-shadowing. So, lower portions of the log, as well as around the small embedded stumps. And, after we apply the map to our ambient occlusion property, I hope you can see the benefits of adding those baked detail shadows without the need of using expensive dynamic lights in your scene. When working in AR, we recommend that you actually bake your ambient occlusion into a map. Which is what we've done for CampfiAR. Rather than use the alternative, such as screen space ambient occlusions, which is a camera-based post-process effect, and can potentially lead to poor rendering performance in your scene. And, last but certainly not least, be frugal with the use of your transparency in your materials materials. If you must use transparencies, we recommend that you use separate materials for objects where you see a combination of transparent and non-transparent surfaces. In general, when working with AR content, the use of a lot of transparent surfaces can potentially have a huge impact on performance, especially if you are having transparent surfaces that are, kind of, stacked in front of each other when you're viewing them. This is known as overdraw, and it's something that you definitely want to avoid when you're working in AR. Whew. Alright. I hope everybody's still with me, as that was quite a lot to go through. So far, we've focused mostly on how the AR content will react to the simulated spotlights found in our 3D engines. But now, it's time to focus on some ways to make our content seem like it's actually part of the real world. A fantastic option to use to compensate for varied lighting conditions is to leverage one of ARKit's well known features, and light estimation. Let's begin by activating this functionality and see how it affects our kettle. Notice that how when the ambient light changes in intensity in the real world, a similar adjustment is made to the ambient light in our AR scene. The way this works is that ARKit analyzes each frame of the video, and uses them to estimate the lighting condition of the real world. It is an absolutely magical feature that helps assure that the amount to light applied to your AR content matches what you see in the real world. Now, that we have magical light wizards living in our AR scene, let's discuss shadows. Shadows in AR are really, really hard to get right. Your shadow needs to be able to work in a wide variety of situations. Remember that people can be potentially using your app anywhere. And, if your AR shadows differ from the ones that are seen in the real world environment, it can be relatively jarring for your experience. Here, we have tried to incorrectly cast a dynamic shadow by using a directional light as a sharp angle in our 3D engine. Shadows are a great way to make objects actually feel grounded in the real world, but in this case, it doesn't really match the shadows being seen in the surrounding environment. It's like we're purposely trying to defy the laws of physics here. Instead, we suggest that you place your directional light directly overhead, and play a little bit around with the intensity of that effect to make sure that it actually feels a little bit more subtle. This will allow your shadows to work in a lot more different situations, and a lot more different scenarios. An alternative to this is actually using a method where you can create your own drop shadow, rather than using dynamic lights in your scene, which can get expensive, and can severely affect performance if you're rendering a lot of 3D content. Take the time to craft those really good shadows. And ensure that they will fit in as many real world situations as possible. Ah, environment maps. If you really want to astound people, you definitely want to use these maps, especially on those AR objects that exhibit reflectivity. It'll make it seem like your AR content exists right here, in the real world. And, to make it super easy to leverage their powers we want to show you how one of the new features in iOS 12 and ARKit 2.0, automatic environmental mapping, can help you achieve this effect. If you take a close look at the kettle, originally we're using a baked environment map that has a kind of blush tinge to it. Once we've activated the automatic environmental mapping, notice how the kettle now reflects a lot of the ground, and the surrounding color of the current environment that we're in. You can even see a little bit of the green from the grass that this kettle's sitting in. This is a fantastic feature that'll really help ground your objects into the real world, and with careful use of roughness, can really enhance the believability of your scene. So, now why is automatic environmental mapping so actually incredible? Typically, these maps are used to simulate the ability of metallic surfaces to mirror the environment around them. You can see example of a cube environment map here. Now, before ARKit added automatic environmental mapping, you had to provide your own image, and hope that it was generic enough to work in a large variety of situations that your app may be used in. But now, with ARKit 2.0, you can kiss those sleepless night, fretting about whether your environment map is actually ruining your AR experience. For more information about environmental mapping, and other new features found in ARKit 2.0, please check out the talk by Arsalan and Reinhard called What's New in ARKit 2. And now, to close out CampfiAR, let's put all this together, and add the final touches to the campfire. With a little bit of animations and [inaudible] effects, along with the applications of all the techniques discussed today, CampfiAR is ready for primetime. Now, if I ever get the crazy urge to go outside, I can suppress those urges, and stay safely at my desk, enjoying the joys of simulated outdoors with CAmpfiAR. Who needs EpiPens? Not this guy. We went through lot of different topics today. So, I quickly want to reiterate some of the important things to keep in mind while you're developing your app. Remember that your app can be used in a wide variety of real world conditions, so always make sure aesthetic choices allow your content to blend almost anywhere. Once you've decided what kind of AR experience you wish to build, adhere to what you think your rendering budget is, and work as many optimizations as you can to get that smooth performance and efficiently use your power. And, finally leverage the use of various material properties, as well as the built-in features of ARKit to get your AR content looking great, in order to delight people who use your app. And, for your reference, here's a table of all the material properties we worked with today to build out CampfiAR. You can get additional information at the link provided. Thank you.  Hello, everybody. I'm very excited to be here today to talk about understanding ARKit Tracking and Detection to empower you to create great augmented reality experiences. My name is Marion and I'm from the ARKit Team. And what about you? Are you an experienced ARKit developer, already, but you are interested in what's going on under the hood? Then, this talk is for you. Or you may be new to ARKit. Then, you'll learn different kind of tracking technologies, as well as some basics and terminology used in augmented reality, which will then help you to create your very own first augmented reality experience. So, let's get started. What's tracking? Tracking provides your camera viewing position and orientation into your physical environment, which will then allow you to augment virtual content into your camera's view. In this video, for example, the front table and the chairs is virtual content augmented on top of the real physical terrace. This, by the way, is Ikea. And the virtual content will appear always virtually correct. Correct placement, correct size, and correct perspective appearance. So, different tracking technologies are just providing a difference reference system for the camera. Meaning the camera with respect to your world, the camera with respect to an image, or maybe, a 3D object. And we'll talk about those different kind of tracking technologies in the next hour, such that you'll be able to make the right choice for your specific use case. We'll talk about the already existing AR technologies' Orientation Tracking, World Tracking, and Plane Detection. Before we then have a close look at our new tracking and detection technologies which came out now with ARKit 2. Which are saving and loading maps, image tracking, and object detection. But before diving deep into those technologies, let's start with a very short recap of ARKit like on a high level. This is, specifically, interesting if you are new to ARKit. So, the first thing you'll do is create an ARSession. An ARSession is the object that handles everything from configuring to running the AR technologies. And also, returning the results of the AR technologies. You then, have to describe what kind of technologies you actually want to run. Like, what kind of tracking technologies and what kind of features should be enabled, like Plane Detection, for example. You'll then, take this specific ARConfiguration and call run method on your instance of the ARSession. Then, the ARSession, internally, will start configuring an AVCaptureSession to start receiving the images, as well as a call motion manager to begin receiving the motion sensor, so, data. So, this is, basically, the built-in input system from your device for ARKit. Now, after processing the results are returned in ARFrames at 60 frames per second. An ARFrame is a snapshot in time which gives you everything you need to render your augmented reality scene. Like, the captured camera image, which would then be, which will be rendered in the background of your augmented reality scenario. As well as a track camera motion, which will then be applied to your virtual camera to render the virtual content from the same perspective as the physical camera. It also contains information about the environment. Like, for example, detected plates. So, let's now start with our first tracking technology and build up from there. Orientation Tracking. Orientation Tracking tracks, guess what? Orientation. Meaning it tracks the rotation, only. You can think about it as you can only use your hat to view virtual content, which also, only allows rotation. Meaning you can experience the virtual content from the same positional point of view, but no change in the position is going to be tracked. The rotation data is tracked around three axles. That's why it's also, sometimes, called the three degrees of freedom tracking. You can use it, for example, in a spherical virtual environment. Like, for example, experience a 360-degree video, in which the virtual content can be viewed from the same positional point. You can also, use it to augment objects that are very far away. Orientation Tracking is not suited for physical world augmentation, in which you want to view the content from different points of views. So, let's now have a look at what happens under the hood when Orientation Tracking is running. It is, actually, quite simple. It only uses the rotation data from core motion, which applies sensor fusion to the motion sensors data. As motion data is provided at a higher frequency than the camera image, Orientation Tracking takes the latest motion data from commotion, once the camera image is available. And then, returns both results in an ARFrame. So, that's it. Very simple. So, please note that the camera feed is not processed in Orientation Tracking. Meaning there's no computer version under the hood here. Now, to run Orientation Tracking you only need to configure your ARSession with an AROrientation TrackingConfiguration. The results will then be returned in an ARCamera object provided by the ARFrames. Now, an ARCamera object always contains the transform, which in this case of Orientation Tracking, only contains the rotation data of your tracked physical camera. Alternatively, the rotation is also represented in eulerAngles. You can use whatever fits best to you. Let's now move over to more advanced tracking technologies. We'll start with World Tracking. World Tracking tracks your camera viewing orientation, and also, the change in position into your physical environment without any prior information about your environment. Here, you can see on the left side the real camera's view into the environment, while on the right side you see the tracked camera motion while exploring the world represented in the coordinate system. Let's now explain better, what happens here, when World Tracking is running. World Tracking uses a motion sensor, the motion data of your device's accelerometer and gyroscope to compute its change in orientation and translation on a high frequency. It also provides its information in correct scale in Metal. In literature, just this part of the tracking system is also called Inertial Odometry. While this motion data provides good motion information for movement across small time intervals and whenever there's like, sudden movement, it does drift over larger time intervals as the data is not ideally precise and subject to cumulative errors. That's why it cannot be used just by its own for tracking. Now, to compensate this drift, World Tracking, additionally, applies a computer version process in which it uses the camera frames. This technology provides a higher accuracy, but at the cost of computation time. Also, this technology is sensitive to fast camera motions and this results in motion blur in the camera frames. Now, this version only part of the system is also called Visual Odometry. Now, by fusing those both systems, computer vision and motion, ARKit takes the best of those both systems. From computer version, it takes a high accuracy over the larger time intervals. And from the motion data it takes the high update rates and good precision for the smaller time intervals, as well as the metric scale. Now, by combining those both systems World Tracking can skip the computer version processing for some of those frames, while still keeping an efficient and responsive tracking. This frees CPU resources, which you can then, additionally, use for your apps. In Literature, this combined technology is also called Visual Inertial Odometry. Let's have a closer look at the visual part of it. So, within the computer version process interesting regions in the camera images I extracted, like here, the blue and the orange dot. And they are extracted such that they can robustly all to be extracted and other images of the same environment. Those interesting regions are also called features. Now, those features are then matched between multiple images over the camera stream based on their similarity and their appearance. And what then happens is pretty much how you are able to see 3D with your eyes. You have two of them and they are within the sidewise small distance. And this parallax between the eyes is important as this results in slightly different views into the environment, which allows you to see stereo and perceive the depth. And this is what ARKit now, also, does with the different views of the same camera stream during the process of triangulation. And it does it once there's enough parallax present. It computes the missing depth information for those matched features. Meaning those 2D features from the image are now reconstructed in 3D. Please, note that this reconstruction to be successful, the camera position must have changed by a translation to provide enough parallax. For example, with the sidewise movement. The pure rotation does not give enough information here. So, this is your first small map of your environment. In ARKit we call this a World map. In this same moment, also, the camera's positions and orientations of your sequences are computed, denoted with a C here. Meaning, your World Tracking just initialized. This is the moment of initialization of the tracking system. Please note that also in this moment of this initial reconstruction of the World map, the world origin was defined. And it is set to the first camera's origin of the triangulated frames. And it is also set to be gravity aligned. It's denoted with a W in the slides. So, you now have a small representation of your real environment reconstructed as a World map in its own world coordinates system. And you have your current camera tracked with respect to the same world coordinate system. You can now start adding virtual content to augment them into the camera's view. Now, to place virtual content correctly to an ARSession, you should use ARAnchors from ARKit, which are denoted with an A here. ARAnchors are reference points within this World map, within this world coordinates system. And you should use them because the World Tracking might update them during the tracking. Meaning that, also, all the virtual content that is assigned to it will be updated and correctly augmented into the camera's view. So, now that you've used the ARAnchors you can add virtual content to the anchor, which will them be augmented correctly into the current camera's view. From now on, this created 3D World map of your environment is your reference system for the World Tracking. It is used to reference new images against. And features are matched from image to image and triangulated. And at the same time, also, new robust features are extracted, matched, and triangulated, which are then extending your World map. Meaning ARKit is learning your environment. This then allows, again, the computation of tracking updates of the current camera's position and orientation. And finally, the correct augmentation into the current camera's view. While you continue to explore the world, World Tracking will continue to track your physical camera and continue to learn your physical environment. But over time, the augmentation might drift slightly, which can be noticed like you can see in the left image, in a small offset of the augmentation. This is because even small offsets, even small errors will become noticeable when accumulated over time. Now, when the device comes back to a similar view, which was already explored before, like for example, the starting point where we started the exploration, ARKit can perform another optimization step. And this addition makes, a Visual Intertial Odometry system, makes the system that ARKit supplies to a Visual Inertial SLAM System. So, let's bring back this first image where the World Tracking started the exploration. So, what happens now is that World Tracking will check how well the tracking information and the World map of the current view aligns with the past views, like the one from the beginning. And will then perform the optimization step and align the current information and the current World map with your real physical environment. Have you noticed that during this step, also the ARAnchor was updated? And that is the reason why you should use ARAnchors when adding virtual content to your scenario. In this video, you can see the same step again with a real camera feed. On the left side you see the camera's view into the environment, and also, features which are tracked in the images. And on the right side, you see a bird eye's view onto the scenario, showing what ARKit knows about it and showing the 3D reconstruction of the environment. The colors of the points are just encoding the height of the reconstructed points with blue being the ground floor and red being the table and the chairs. Once the camera returns back to a similar view it has seen before, like here the starting point, ARKit will now apply this optimization step. So, just pay attention to the point cloud and the camera trajectory. Have you noticed the update? Let me show you, once more. This update aligns the ARKit knowledge with your real physical world, and also, the camera movement and results in the better augmentation for the coming camera frames. By the way, all those computations of World Tracking, and also, all this information about your learned environment, everything is done on your device only. And all this information, also, stays on your device only. So, how can you use this complex technology, now, in your app? It is actually quite simple. To run World Tracking you just configure your ARSession with an ARWorldTrackingConfiguration. Again, its results are retuned in an ARCamera object of the ARFrame. An ARCamera object, again, contains the transform, which in this case of World Tracking contains, additionally, to the rotation, also, the translation of the track camera. Additionally, the ARCamera also contains information about the tracking state and trackingStateReason. This will provide some information about the current tracking quality. So, tracking quality. Have you ever experienced opening an AR app and the tracking worked very poorly or maybe it didn't work at all? How did that feel? Maybe frustrating? You might not open the app, again. So, how can you get a higher tracking quality for your app? For this, we need to understand the main factors that are influencing the tracking quality. And I want to highlight three of them here. First of all, World Tracking relies on a constant stream of camera images and sensor data. If this is interrupted for too long, tracking will become limited. Second, World Tracking also works best in textured and well-lit environments because World Tracking uses those visually robust points to map and finally triangulate its location. It is important that there is enough visual complexity in the environment. If this is not the case because it's, for example, too dark or you're looking against a white wall, then also, the tracking will perform poorly. And third, also, World Tracking works best in static environments. If too much of what your camera sees is moving, then the visual data won't correspond with the motion data, which might result in the potential drift. Also, device itself should not be on a moving platform like a bus or an elevator. Because in those moments the motion sensor would actually sense a motion like going up or down in the elevator while, visually, your environment had not changed. So, how can you get notified about the tracking quality that the user is currently experiencing with your app? ARKit monitors its tracking performance. We applied machine learning, which was trained on thousands of data sets to which we had the information how well tracking performed in those situations. To train a classifier, which tells you how tracking performs, we used annotations like the number of visual-- visible features tracked in the image and also, the current velocity of the device. Now, during runtime, the health of tracking is determined based on those parameters. In this video, we can see how the health estimate, which can be seen-- which, is reported in the lower left, gets worse when the camera is covered while we are still moving and exploring the environment. It also shows how it returns back to normal after the camera view is uncovered. Now, ARKit simplifies its information for you by providing a tracking state. And the tracking state can have three different values. It can be normal, which is the healthy state and is the case in most of the time. It's the case in most of the times. And it can also be limited, which is whenever tracking performs poorly. If that's the case, then the limited state will also come along with the reason, like insufficient features or excessive motion or being currently in the initialization phase. It can also be not available, which means that tracking did not start yet. Now, whenever the tracking state changes, a delegate is called. The camera did change tracking state. And this gives you the opportunity to notify the user when a limited state has been encountered. You should, then, give informative and actionable feedback what the user can do to improve his tracking situation, as most of it is actually in the user's hand. Like for example, as we learned before, like a sidewise movement to allow initialization or making sure there's enough adequate lightning for enough visual complexity. So, let me wrap up the World Tracking for you. World Tracking tracks your camera 6 degree of freedom orientation and position with respect to your surrounding environment and without any prior information about your environment, which then allows the physical world augmentation in which the content can actually be viewed from any kind of view. Also, World Tracking creates a World map, which becomes the tracking's reference system to localize new camera images against. To create a great user experience, the tracking quality should be monitored and feedback and guidance should be provided to your user. And World Tracking runs on your device only. And all results stay on your device. If you have not done it already, try out one of our developer examples. For example, the Build Your First AR Experience, and play a bit around, just 15 minutes with the tracking quality in different situations; light situations or movements. And always remember to guide the user whenever he encounters a limited tracking situation to guarantee that he has a great tracking experience. So, World Tracking is about the camera-- where your camera is with respect to your physical environment. Let's now talk about how the virtual content can interact with the physical environment. And this is possible with Plane Detection. The following video, again, from the Ikea app, shows a great use case for the Plane Detection, placing virtual objects into your physical environment, and then interacting with it. So first, please note how, also, in the Ikea app the user is guided to make some movement. Then, once a horizontal plane is detected, the virtual table set is displayed and is waiting to be placed by you. Once you position it, rotate it as you want it, you can lock the object in its environment. And did you notice the interaction between the detected ground plane and the table set in the moment of locking? It kind of bounces shortly on the ground plane. And this is possible because we know where the ground plane is. So, let's have a look at what happened under the hood here. Plane Detection uses the World map provided by the world I just talked about, just talked about a moment ago, which is represented here in those yellow points. And then, it uses them to detect surfaces that are horizontal or vertical, like the ground, the bench, and the small wall. It does this by accumulating information over multiple ARFrames. So, as the user moves around the scene, more and more information about the real surface is acquired. It also allows the Plane Detection to provide and like extent the surface, like a convex hull. If multiple planes belonging to the same physical surface are detected, like in this part now, the green and the purple one, then they will be merged once they start overlapping. If horizontal and vertical planes intersect they are clipped at the line of intersection, which is actually a new feature in ARKit 2. Plane Detection is designed to have very little overhead as it repurposes the mapped 3D points from the World Tracking. And then it fits planes into those point clouds and over time continuously aggregates more and more points and merge the planes that start to overlap. Therefore, it takes some time until the first planes are detected. What does that mean for you? If your app is started, there might not directly be planes to place objects on or to interact with. If the detection of a plane is mandatory for your experience, you should again guide the user to move the camera with enough translation to ensure a dense reconstruction based on the parallax, and also, enough visual complexity in the scene. Again, for the reconstruction, a rotation only is not enough. Now, how can you enable the Plane Detection? It's, again, very simple. As the Plane Detection reuses the 3D map from the World Tracking, it can be configured by using the ARWorldTrackingConfiguration. Then, the property planeDetection just needs to be set to either horizontal, vertical, or like in this case, both. And then, just call your ARSession with this configuration. And the detection of the planes will be started. Now, how are those, the results of the detected planes returned to you? The detected planes are returned as an ARPlaneAnchor. An ARPlaneAnchor is a subclass of an ARAnchor. Each ARAnchor provides a transform containing the information where the anchor is in your World map. Now, a plane anchor, specifically, also has information about the geometry of the surface of the plane, which is represented in two alternative ways. Either like a bounding box with a center and an extent, or as a 3D mesh describing the shape of the convex hull of the detected plane and its geometry property. To get notified about detected planes, delegates are going to be called whenever planes are added, updated, or removed. This will then allow you to use those planes, as well as react to any updates. Now, what can you do with planes? Like what we've seen before on the Ikea app, these are great examples. Place virtual objects, for example, with hit testing. Or you can interact with some, for example, physically. Like we've seen bouncing is a possibility. Or you can also use it by adding an occlusion plane into the detected plane, which will then hide all the virtual content below or behind the added occlusion plane. So, let me summarize what we've already gone through. We've had a look at the Orientation Tracking, the World Tracking, and the Plane Detection. Next, Michele will explain, in depth, our new tracking technologies, which were introduced in ARKit 2. So, welcome Michele. Thank you, Marion. My name is Michele, and it's a pleasure to continue with the remaining topics of this session. Next up is saving and loading maps. This is a feature that allows to store all the information that are required in a session. So, that it can literally be restored in another session at a later point in time to create augmented reality experiences that persist to a particular place. Or that could, also, be stored by another device to create multiple user augmented reality experiences. Let's take a look at an example. What you see here is a guy; let's name him Andre, that's walking around the table with his device having an augmented reality experience. And you can see his device now is making this seem more interesting by adding a virtual vase on the table. A few minutes later his friends arrive at the same scene. And now, they're both looking at the scene. You're going to see Andre's device on the left and his friend on the right now. So, you can see that they're looking at the same space. They can see each other. But most importantly, they see the same virtual content. They're having a shared augmented reality experience. So, what we have seen in these examples can be discovered in three stages. First, Andre went around the table and acquired the World map. Then, the World map was shared across devices. And then, his friend's device re-localized to the World map. This means that the object was able to understand in the new device that this was the same place as the other device, computed the precise position of the device with respect to the map, and then, started tracking from there just like the new device acquired the World map itself. We're going to go into more detail about these three phases. But first, let's review what's in the World map. The World map includes all the tracking data that are needed for the system to be localized, which includes the feature points as Marion greatly explained before. As well as local appearance for this point. They also contain all the anchors that were added to the session, either by the users, like planes, for example. I mean by the system-- like planes. Or by the users, like the vase, as we have seen in the example. This data is serializable and available to you so that you can create compelling persistent or multiple user augmented reality experiences. So, now let's take a look at the first stage, which is acquiring the World map. We can play back the first video where Andre went around the table that you can see his device on the left, here. And on the right, you see the World map from a top view as acquired by the tracking system. You can [inaudible] is the table and the chair around it. There's a few things to pay attention to during this acquisition process. First, everything that Marion said during tracking also applies here. So, we want enough visual complexity on the scene to get dense feature points on the map. And the scene must be static. Of course, we can deal with minor changes, as you have seen the tablecloth moving by the wind. But the scene must be mostly static. In addition, when we are specifically acquiring a World map for sharing we want to go around the environment from multiple points of view. In particular, we want to cover all the direction from which we want to later be localized from. To make this easy, we also made available a world mapping status which gives you information about the World map. If you guys have been to the What's New in ARKit talk, [inaudible] greatly expand this to quickly recap. When you start the session the World map status will start limited. And then, will switch to a standing as more of the scene is learned by the device. And then, finally, we go to mapped when the system is confident you're staying in the same place. And that's what you want to save the map in the mapped state. So, that's good information. But this is mostly on the user side applied to acquire the session. So, what does this mean to you as a developer? That you need to guide the user. So, we can indicate the mapping status and even disabling the saving or sharing of the World map until the mapping status goes to the mapped state. We can also, monitor the tracking quality during the acquisition session and report to the user if the tracking state has been limited for more than a few seconds. And maybe even give an option to restart the acquisition session. On the receiving end of the device, we can also guide the user to better localization process. So, when we are, again, in the acquisition device, when we are in the map state we can take a picture of the scene and then, ship that together with the World map. And on the receiving end we can ask the user find this view to start your shared experience. That was how to acquire the World map. Now, let's see how you can share the World map. First, you can get the World map by simply calling the getCurrentWorldMap method in the ARSession. And this will give you the World map. The World map is a serializable class. So, then we can simply use NSKeyedArchiver utility to serialize it to a binary stream of data, which then, you can either save to disk in case of a single user persistent application. Or you can share it across devices. And for that, you can use the MultiPeerConnectivity framework, which has great feature like automatic device, nearby device discovery, and allows efficient communication of data between devices. We also, have an example of how to use that in ARKit called Creating a Multiuser AR Experience that you can check out on our developer website. On the receiving end of the device, once you've got the World map let's see how you can set up the World Tracking configuration to use it. Very simple. You just set the initial World map property to that World map. When you run the session, the system will try to find that previous World map. But it may take some time, even because the user may not be pointing at the same scene as before. So, how do we know when localization happen? That information is available in the tracking state. So, as soon as you start the session with the initial World map, the tracking state will be limited with reason Relocalizing. Note that you will still get the tracking data available here, but the world origin will be the first camera, just like a new session. As soon as the user points the device to the same scene, the system will localize. The tracking state will go to normal and the world origin will be the same as the recorded World map. At this point, all your previous anchors are also available in your session, so you can put back the virtual content. Note here that because of what's happening behind the hood, behind the scenes, is that we're matching those feature points, there needs to be enough visual similarity between the scenes where you acquired the World map and the scene where you want to relocalize. So, if you go back to this table at night, chances are it's not going to work very well. And that was how you can create multiple user experiences or persistent experiences using the saving and loading map. Next, image tracking. So, augmented reality is all about adding visual content on top of the physical world. And on the physical world, images are found everywhere. Think about [inaudible] the world, magazine covers, advertisements. Image tracking is a tool that allows you to recognize those physical images and build augmented reality experiences around them. Let's see an example. You can see here; two images being tracked simultaneously. On the left, a beautiful elephant is put on top of the physical image of the elephant. On the right, the physical image turned into a virtual screen. Note also, that the images can freely move around the environment as tracking around at 60 frames per second. Let's talk about looking at what's happening behind the scenes. So, let's say you have an image like this one of the elephant and you want to find it in a scene like this. We're using grayscale for this. And the first type is pretty similar to what we do in tracking. So, we'll track those interesting points from both the reference image and the current scene. And then, we try to go in the current scene and match those features to the one on the reference image. By applying some projected geometry and linear algebra, this is enough to give an initial estimation of the position orientation of the image with respect to the current scene. But we don't stop here. In order to give you a really precise pose and track at 60 frames per second, we then do a dense tracking stage. So, with that initial estimate we take the pixels from the current scene and warp them back to a rectangular shape like you see on the right-- top right there. So, that's a reconstructed image by warping the pixels of the current image into the rectangle. We can then compare the reconstructed image with a reference image that we have available to create an error image like the one you see below. We then optimize the position orientation of the image, such that that error is minimized. So, what this means to you that the post would be really accurate. Thank you. And will still track at 60 frames per second. So, let's see how we can do all of this in ARKit. As usual, the ARKit API is really simple. We have three simple steps. First, we want to collect all the reference images. Then, we set up the AR Session Configuration. There are two options here. One is the World Tracking configuration that gives, also, the device position. And this is the one we have talked, so far. And in iOS12, introduced a new configuration, which is a standalone image tracking configuration. Once you start the session you will start receiving the results in the form of an ARImageAnchor. We're now going into more details of these three steps, starting from the reference images. The easiest way to add reference images to your application is through the, called asset catalog. You simply create an AR Resource Groups and drag and drop your images in there. Next, you have to set the physical dimension of the image, which you can do on the property window on the top right. Setting the physical dimension is a requirement and there's a few reason for that. First, it allows the pose of the image to be in physical scale. Which means, also, your content will be in physical scale. In ARKit, everything is in meters, so also, your visual content will be in meters. In addition, it's especially important to set the correct physical dimension of the image in case we combine the image tracking with the World Tracking. As this will give immediately consistent pose between the image and the world. Let's see some example of this reference images. You can see here, two beautiful images. These images will work really great with image tracking. They have high texture, high level of contrast, well distributed histograms, as well as they do not contain repetitive structures. There are, also, other kinds of images that will work less good with the system. You can see an example of this on the right. And if we take a look at these top two examples, you can see that the good image we have a lot of those interesting points. And you can see that the histogram is well distributed across the whole range. While, [inaudible] image, there's only a few of those interesting points and the histogram is all skewed toward the whites. You can get an estimation of how good an image will be directly in Xcode. As soon as you drag an image in there, the image is analyzed and [inaudible] to you in the form of warnings to give you early feedback, even before you run your application. For example, if you click on this bottom image that could be a magazine page, for example, we can see that the Xcode says that the histogram is not well distributed. In fact, you can see there's a lot of whites in the image. And it would also say that this image contains repetitive structures, mainly caused by the text. Another example, if you have two images which are too similar and are at risk of being confused at detection time, also, Xcode warns you about that. You can see an example of these two images of the same mountain range, the Sierra. There's a few things that we can do to deal with this warning. For example, let's go back to this image that had repetitive structures and not well distributed histograms. You can try to identify a region of this image which is distinctive enough, like in this case, for example, the actual image of the page. And then, you can crop that out and use this as the reference image, instead. Which will give you, of course, all the warnings are going to be removed and will give you better tracking quality. Another thing that we can do is use multiple AR Resource Groups. This allow many more images to be detected. As with the commands to have a maximum of 25 images per group to keep your experience efficient and responsive. But you can have as many groups as you want. And then, you can switch between groups programmatically. For example, if you are want to create an augmented reality experience in a museum that may have hundreds of images. Usually though, those images are actually physically located in different rooms. So, what you can do is put the images that physically will be present in the room into a group. And images of another room into another group. And then use, for example, core location to switch between rooms. Note also, that you can have similar images, now, as long as they are in different groups. So, that was all about reference images. Let's now, see our two configurations. The ARImageTrackingConfiguration is a new standalone image tracking configuration, which means it doesn't run the World Tracking. Which also, means there is no world origin. So, every image will be given to you with respect to the current camera view. You can also combine image tracking with a World Tracking configuration. And in this case, you will have all the scene understanding capability available like Plane Detection, light estimation, everything else. So, what is more appropriate to use which configurations? Let's see. So, in the ARImageTrackingConfigurations is really tailored for use cases which are built around images. We can see an example on the left here. We can have an image that could be a page of a textbook. And to make the experience more engaging, we are overlaying [inaudible] graph. In this case, on how to build an equilateral triangle. So, you can see that this experience is really tailored around an image. If you have, let's see this other example. Image tracking is used to trigger some content that then goes beyond the extent of the image. In this case, you want to use the ARWorldTrackingConfiguration as you will need the device position to keep track of that content outside the image. Also, note that the image tracking doesn't use the motion data, which means it can also be used on a bus or an elevator, where the motion data don't agree with the visual data. So, let's see now, how we can do this in code. You can easily recognize those three steps here. The first one is to gather all the images. And there's a convenience function for that in the ARReferenceImage class that gathers all the images that are in a particular group. In this case, it's named Room1. We can then simply set the trackingImages property to those images in the ARImageTrackingConfigurations. And run the session. You will then start receiving the results, for example, to the session, didUpdate anchors delegate method, where you can check if the anchors is of type ARImageAnchor. In the anchor, you will find, of course, the position and orientation of the image, as well as the reference image itself. Where you can find, for example, the name of the image as you named it in the actual title so that you know which image has been detected. There's also another Boolean property, which tells you if this image is currently being tracked in the frame. Note here that other than these use cases that we have seen so far when you build [inaudible] around images, image detection and tracking allows a few more things. For example, if two devices are looking at the same physical image, you can detect this image from both devices. And this will give you a shared coordinate system that you can then use as an alternative way to have a shared experience. Another example, if you happen to know where an image is physically located in the world, like for example, you know that the map of this park is in the physical world. You can use image tracking to get the position of the device with respect to the image and, therefore, also the position of the device with respect to the world, which, you can then use, for example, to overlay directions really attached to the physical world. So, that concludes the image tracking. Let's now go and look at the Object Detection. So, with image tracking we have seen how we can detect images, which are planar objects in the physical world. Object detection extends this concept to the third dimension allowing the detection of more generic objects. Note, though, that this object will be assumed to be static in the scene, unlike images that can move around. We can see an example here. That's the Nefertiti bust. It's a statue that could be present in a museum. And now, you can detect it with ARKit. And then, for example, display some information on top of the physical object. Note also that in the object detection in ARKit, we are talking about specific instances of an object. So, we're not talking about detecting statues in general, but this particular instance of the Nefertiti statue. So, how do we represent these objects in ARKit? You first need to scan the object. So, really, there's two steps to it. First, you scan the object and then you can detect it. Let's talk about the scanning part, which mostly is going to be on your side as a developer, to basically, create that representation of the object that can be used for detection. Internally, an object is represented in a similar way as the world map. You can see an example of the 3D feature points of the Nefertiti statue there on the left. And to scan the object, you can use the Scanning and Detecting 3D Objects developer sample that's available on the website. And note here, that the detection quality that you will get at runtime, later, is highly affected by the quality of the scan. So, let's spend a few moments to see how we can get the best quality during the scanning. Once you build and run this developer sample you will see something like this on your device. The first step is to find the region of space around your object. The application will try to automatically estimate this bounding box, exploring different feature points. But you can always adjust this box by dragging on a side to shrink it or make it larger. Note here, that what is really important that when you go around the object you make sure that you don't cut any of the interesting points of the object. You can also, rotate the box with a two-finger gesture from top. So, make sure that this box is around the object and not cutting any interesting part of it. The next part is the actual scanning. In this phase what we want to do is really go around the objects from all the points of view that you think your users will want to detect it later. In order to make it easy for you to understand which part of the objects have been, already, acquired like this beautiful tile representation. And you also can see a percentage on top which tells you how many tiles have already been acquired. And it's really important in this phase that you spend time on the regions of the object which have a lot of features that are distinctive enough. And you go close enough to capture all the details. And again, that you really go around from all the sides. Like you see here. Once you're happy with the coverage of your objects, you can go to the next step, which is allows you to adjust the origin by simply dragging on the coloring system. And this will be the coloring system that will be later given to you at detection time in the anchor. So, make sure that you put it in a place which makes sense for your virtual content. So, at this point, you have a full representation of your object, which you can use for detection. And the application will now switch to a detection mode. We encourage you to use this mode to get early feedback about the detection quality. So, you may want to go around the object from different points of view and verify that the object is detected from all these different point of view. You can point your device away, come back from another angle, and make sure that the scan was good to detect the object. You can also, move these objects around so that the light condition will be different. And you want to make sure that those are still detected. This is particularly important for objects like toys that you don't know where they're actually going to be physically located. We, also, suggest that you take the object and put it in a completely different environment and still make sure that it is detected. In case this is not detected you may want to go back to the scanning and make sure that your environment is well lit. We really like, well lit environment during the scanning is very important. If your Verilux meter, it will be about 500 lux will be best. And if that is still not enough, you may want to keep different versions of the scans. So, at this point, once you're happy with the detection quality you can simply drop the model to your Mac and add it to the AR Resource Groups, just like you did for the images. Also note that there are some objects that will work really great with this system. Object like you can see on the left. First of all, they are rigid objects and they are, also, rich of texture, distinctive enough. But there are also certain kinds of object that will not work well with the system. You can see an example of this on the right. And for example, metallic, transparent, or metallic or reflective objects will not work. Or transparent objects like glass material object will also not work because the appearance of these objects will really depend on where they are in the scene. So, that was how to scan the objects. Again, make sure that you have well-lit environment. Let's now see how we can detect this in ARKit. If this looks familiar to you, it's because the API is pretty similar to the one of the images. We'll have convenience metered to gather all the objects in a group. This time is in the ARReferenceObjects class. And to configure your ARWorldTracking configuration, you simply pass this object to the detectionObjects property. Once you run the session, again, you will find your results. And in this case, you want to check for the ARObjectAnchor, which will give you the position and orientation of the object with respect to the world. And also, the name of the object as was given in the asset catalog. So, you guys may have noticed some similarities between the object detection and the world mapping relocalization. But there's also few differences. So, in the case of the object detection we are always giving the object position with respect to the world. While in the world map relocalization is the camera itself that adjusts to the previous world map. In addition, you can detect multiple objects. And object detection works best for objects which are tabletop, furniture sized. While, the world map is really the whole scene that's been acquired. With this side, we conclude the object detection. Let's summarize what you have seen, today. Orientation tracking tracks only the rotation of the device and can be used to explore statical environments. World Tracking is the fully featured position and orientation tracking, which will give you the device position with respect to a world origin. And enables all the scene understanding capabilities like the Plane Detection, which will make you able to interact with the physical, horizontal, and vertical planes where you can then put virtual objects. We have seen how you can create persistent or multiuser experiences with the saving and loading map features in the architecture. And how you can detect physical images and track them at 60 frames per second with the image tracking and how you can detect more generic objects with the object detections. And with this, I really hope you guys have a better understanding, now, of all the different tracking technology that are present in ARKit and how they work behind the scenes. And how you can get the best quality out of it. And we're really looking forward to see what you guys are going to do with that. More information can be found at the session link in the developer website. And we have an ARKit Lab tomorrow, 9 a.m. We will both, me and Marion will be there answering any question on ARKit you may have. And with that, thank you, very much and enjoy the bash.  Good afternoon everyone. My name is John Hess. Today I'm going to be joined by Matthew Lucas, and we are going to be talking to all of you about practical approaches to great app performance. Now, I'm an engineer on the Xcode team, and I've had the luxury of spending the last several years focused on performance work. First, with Project Find, and Open Quickly, two areas of Xcode that treat performance as the primary feature. Most recently, I've had the opportunity to do a survey of Xcode GY responsiveness, and I want to share with you the approaches that I take to performance work, both in code that I'm intimately familiar with, and in code that I'm just experiencing for the first time. Now, if I could get everyone in today's presentation to just take one lesson away, it is that all of your performance work should be based on measurement. Before you start solving a performance problem, you should measure, to establish a baseline so you know where you stand. As you iterate on solving a performance problem, you should measure it each step of the way to ensure that your performance changes are having the impact that you expect. When you're done solving a performance problem, you should measure again, so that you can compare to your original baseline, and make a quantified claim about just how much you've improved the performance of your application. You want to share this with your boss, your colleagues, and your users. Now, when you think about improving performance for your users, you need to think about what I like to call the total performance impact. If you improve the functionality and performance of one area of your application, by 50%, but it's something that just 1% of your users encounter, that does not have nearly the breadth of impact as improving some other feature by just 10% that all of your users use all the time. So make sure you're not optimizing edge cases, and make sure that your changes are impacting all of your users. Now how do we fix performance bugs? Well, how do we fix regular bugs? Normally it starts with some sort of defect report from users, and we take this report of the application not behaving the way that people expect, and we find some way to synthesize steps to reproduce so that we can cause the failure at will. Once we've done this, we attach a debugger to our program, so that we can see just what our program is doing while it is misbehaving. We combine that with our knowledge of how the code is supposed to work, to modify it as necessary and eliminate the undesired behavior. We verify that we haven't introduced any unwanted side effects, and we repeat as necessary until we've completely solved the bug. I've fixed performance bugs in just the same way. Except instead of using a debugger, I use a profiler, and a profiler is just a fancy tool for measuring. I find some set of steps to reproduce the program being slow. And I run those steps with a profiler attached, so that I can get an insight into what my code is doing while it's running slowly. I combine that knowledge with what my program has to do to accomplish the task at hand, and I find steps that are happening and remove them, because the primary way you make your code faster is you remove redundant steps from whatever is that is calculating. Now, I make the modifications to the source code, and I repeat and measure as necessary until I'm happy with the total result. When I'm doing this type of performance work, I often find myself in one of a handful of scenarios. And these different scenarios change the way that I go about testing the code in question to reproduce the bugs. Sometimes I'm up against a big performance regression, right? Everything was moving along smoothly, then someone checked something in on our team, maybe it was me, and performance has fallen through the floor, and now we have to go back and find out what caused this regression. If this regression is very pronounced, or it's in an area that I don't think it's likely to regress again in the immediate future, I may just test it with my hands, manually, with the profiler attached. However, your performance victories are going to be hard-won battles, and they can easily be lost through a slow stream of regressions. I would encourage all of you to write automated performance tests to capture your app's performance, so that you can ensure that it's not regressing over time. Another scenario I often find myself in, is, are applications performing the same as it has been for a long time? Maybe it is running at 45 frames a second in some drawing test, but we expect it to run at 60. It needs to be improved marginally, and we have reason to believe through our previous performance work that we can get there through spot fixes and incremental changes. Now, in this type of scenario, I probably also have automated tests already in play, because I understand my performance over time. And a third scenario, our application is just suffering from a poor design and performance is orders of magnitude worse than it should be. We know that we can't improve it with simple spot fixes, because we've tried them in the past, and we are still stuck here with a very sub-par performance. In a situation like this, you'd want to do a total performance overhaul, where you are redesigning some core part of the feature, or the algorithms in question, so that performance is a primary constraint. And definitely in these cases, you would have performance tests to measure that you're actually hitting your performance targets. Now, it is important that you know just what to test. I want to caution you that I don't ever immediately jump to these sort of performance overhauls as a way of fixing a performance problem. I love to do that. It's sort of Greenfield engineering, where you get to design things from the ground up, but it's very risky. You're going to end up with a better product at the end, but it's going to be a turbulent path getting there as you rework an entire feature. When you're doing this style of work, it is imperative you understand not only the functional constraints of the code in question, but also the performance constraints, and the typical use patterns that your users are most frequently applying to this feature, and you only get that by having done performance work in the area in the past. I'd like to share an anecdote about our work on a situation like this, within Xcode. In Xcode 9, we reworked Project Find, with performance as a primary goal. It was our goal to deliver search results in just tens of milliseconds. When we were going to discuss this feature with our colleagues, we were often challenged to perform searches across large projects for things like string, or even the letter E. Things that produce millions of results, right? And certainly if our application could produce millions of results quickly, it would be fast on anything. But if you consider what typical patterns are, we search for APIs we use, the names of our own classes, the names of, you know, images that we're referencing. Things like that. They produce dozens, maybe hundreds of results. Certainly, it is essential that the application works decently when you get a million results, but the normal use case is hundreds of results. Now, some of your work in doing a task like search is going to be proportional on things like generating the raw results, and other work is going to be based on how efficiently you can index the text in the project, and avoid work in the first place. In these two scenarios, you're likely to have completely different targets for what you would optimize to make one of these searches faster than the other, right? So it's essential that you understand how your users are going to use the product, so that you can optimize for the right cases. Now, in all of these cases, I need to do some form of testing, whether it's manual, or automated. I want to share with you two types of performance tests that I will typically write to measure the performance of Xcode. We will either do unit tests, or integration tests. Let's compare and contrast them. In a performance unit test, it's your goal to isolate some feature of your application and measure it all by itself. You might mock out its dependencies, and you might launch it in a context where it has been isolated. If I were to write performance unit tests for Xcode's code completion, I might write a series of three small tests. One of these tests would measure talking to the compiler and getting the raw results, the raw set of code completion candidates back. Another performance test would measure correlating, ranking and scoring those results, so we knew which ones to display to the user. A third test might take those already prepared results, and measure putting them into UI elements for final display. And in covering all three of these areas, I would have pretty good coverage over the major components of code completion in the IDE. Now, there are some great aspects to these performance unit tests. They're going to be highly focused, which means if they regress in the future, I'm going to have a very good idea on where the regression is, because the code that is running has been scoped so well. They are also going to produce much more repeatable results from run to run. They're not going to have a big variance in the times that they produce. Again, because the code is so focused. Now, let's contrast that to an integration test. In an integration test, your job is to measure the performance of your application as your users experience it. Holistically. So, if I was writing code completion unit tests for Xcode, I'm sorry, integration tests, I would launch the full Xcode app. I would open a source file. I would navigate to the source file, and I would type, and I would bring up code completion over and over again. When I profile this, to see what Xcode is doing, and how much time it is taking, I am going to find that this test is anything but focused and quiet. Xcode is going to be doing drawing and layout as I type. It is going to be doing syntax coloring as I type. In the background, it might be indexing, fetching get status, deciding to show new files in the Assistant Editor, and all of these things are going to be competing for CPU resources, along with code completion. Maybe when I look in the Profiler, I'll see that we spend 80% of our time syntax coloring, and 20% of our time in code completion. And with this data, I would know that the best way to improve code completion performance would be to defer syntax coloring. I will never gain that type of knowledge with a highly focused unit test. So if I can get everyone here to take two things away from this presentation, the second one should be that your performance investigations should absolutely start with these wide integration tests that measure how the users experience your application. So I'm talking about testing, measuring and profiling. And right now, I'd like to introduce you to profiling in Xcode with instruments. Let's head over to the demo machine. Today we are going to be looking at a performance problem that we fixed between Xcode 9 and Xcode 10. I want to show it to you. I'm going to launch Xcode 9, and open our solar system application. Now the problem that we are going to be looking at is creating tabs. I'm going to just press Command-T quickly a couple of times, and as you can see, the whole screen flashes black, and it takes several seconds to create those tabs. That definitely doesn't meet my expectations as far as performance goes, and we need to fix this. So let's take a look at how you would do that. First, I'm going to launch Instruments. That is our profiling tool. You can do that from the Xcode menu, under Open Developer Tool, Instruments. Now, I'm currently in Xcode 9, so if I choose this, it's going to launch the Instruments from Xcode 9, and of course, I want the Instruments from Xcode 10, which I've put here in my doc. So I'm going to hide Xcode, and bring up Instruments. Now, when Instruments launches, we're presented with a list of profiling tools that we could use to measure our application. There's all kinds of tools here. They can measure graphics utilization, memory consumption, IO, and time in general. It can be intimidating to know which one of these profilers to start with. I would encourage all of you, if you just learn one of these tools, it should be the Time Profiler. I use it for 95% or more of my performance work. When your users complain about your app being slow, they're complaining about it taking too long, and long is time. If it turns out that you're slow because you're doing too much IO, that is going to correlate with time, and you will be able to see this with the Time Profiler. So if you learn just one instrument, it should be the Time Profiler. Let's take a look at how that works. I'm going to launch the Time Profiler by just double clicking on it here, and make Instruments take the full best op. Now, we'd like to record Xcode. In the upper left-hand corner of the Instruments window, you can control which process you're going to attach to and record. By default, hitting this record button would record all processes on my Mac. I just want to focus on Xcode. I'll switch this popover to Xcode and hit record. Now, I like to keep an eye on this area of the window to track view while I'm recording. So I'm going to resize the Xcode window to be a little shorter, so I can still see that, and then I'm going to do the thing that was slow. I'm going to create a couple more tabs. And you can see the graph changed here. Now, I'm going to go ahead and quit, and return to Instruments. So what just happened? While the Profiler was running, it was attached to our process like a debugger. And it stopped it, thousands of times per second, and as it was stopping it, it gathered back traces. Now, just a reminder, a back trace is a description of how your program got to where it currently is. So if you're on line 6 of function C and you got there because main called A, called B, called C, then your back trace is Main, A, B, C. When Instruments captures one of these back traces, it notes, hey, we just spent one millisecond in function C. It says one millisecond, because that is our sampling interval for recording once every millisecond. Now, on the main thread, all these back traces are going to start with the Main function, and they're probably going to call Application Main, and they're going to branch out, all through your source code after that. We can collapse these back traces together, and overlay them into a prefix tree, so they start at Main and work their way out. And we can bubble up those millisecond counters that we captured at the top, so that we can hierarchically see how much time was spent in all the different areas of our source code. And we are going to look at this data to try and find redundant and unnecessary operations that we can make faster, and that is our primary method that we are going to use to improve the performance of our application. Now, as you can imagine, we're capturing thousands of back traces per second. There is an overwhelming amount of data for you to wade through in instruments. My primary advice to you is that you want to filter this data as much as possible so that you can see the course grain performance leads, and not focus on minutia. All right? So I want to show you how to apply a bunch of powerful filters and instruments. So as I did the recording, you remember, I had the track view visible. I did that because I wanted to see how the CPU utilization changed and where it was changing, while I was creating new tabs, and I noted to myself that it was right here. I simply dragged and selected over that area of the trace, and I've caused instruments to only focus its back trace data on just that time interval. Everything over here, this is before I was creating tabs. Everything over here, this is after I was creating tabs, when I was quitting the application. That's not what I'm trying to optimize right now, so I don't need to see that data. Now, in the bottom area of the Instruments window, Instruments is showing me all the traces it collected. By default, there is one row per thread that was running. And in this example it looks like there was only four threads running. Sometimes you'll have much more. Depends on how concurrent your application is. I often like to collapse these in the name of focusing, and I also like to collapse them so they're based on the top level functions executing in each of the threads, rather than the thread IDs, because that corresponds better with how I use Grand Central Dispatch. Down in the bottom of the Instruments window, I'm going to click on this button that says Call Tree, and I'm going to zoom in on it, so you can see what I'm about to do. There are several filters available here. One of them is separate by thread. It is on by default. I am going to go ahead and disable that, and instead, all of the threads are going to be grouped by their top level entry point, rather than their thread ID. Now, looking at this trace, I can see that of all these threads running, which by the way, below the main trace, which is the aggregate CPU usage, the CPU usage is broken down per thread, I can see that almost all the other threads were largely inactive during this trace. I can focus on just the main thread by selecting it here, and now I'm only looking at traces from the main thread during this time period. I'm ready to start digging into this call hierarchy, so I can see what my application was doing. Often, I'll walk this with the keyboard, by just pressing right arrow and down, over and over again. But I'd like to show you the heaviest back trace inspector that Instruments offers. If your Inspector is not visible, you can toggle it with this button, and the heaviest back trace will be available here, in this tab, Extended Detail. Now, the heaviest back trace is just the trace that occurred most frequently. It's the back trace that happened most frequently while we were recording under the current selection. And you can use this to quickly navigate many frames deep at a time. I typically look through here, looking for my own APIs, and things that would surprise me for taking up this amount of time, or for areas where we make a significant branching point in the number of samples. Now, looking through here, I see this call, which is to IDE Navigator, replacement view, did install view controller. Now, I'm familiar with this API, because it's an internal API of Xcode. And in the trace, I can see over here on the left-hand side of the window that it is responsible for 1.19 seconds of the total time we're recording, or 45% of the time. That is far and away above my expectations for how much this method should cost. However, it's hard to focus on what is happening here. Right? I'm, there is all this other stuff at the bottom of the trace, and it looks like I'm, you know, 30 or 40 stack ranges deep. That can be intimidating. I want to show you how to focus. The first technique is back here in that call tree popover again. I'm going to use this popover to choose the flattened recursion. Let's go ahead and do that. And now you can see that, that repeated set of method calls that was right here, oops, has been collapsed. I'm sorry, let me scroll down. That has been collapsed. In fact, I'm confident that I want to continue my performance investigation inside of this IDE Navigator area, API call, and I can refocus the entire call tree by context, clicking here, and choosing Focus on Subtree. And Instruments is going to take that symbol up to the top of the call graph, it's going to remove everything else, and it is going to reset the percentages at 100% so I can focus on just this. Now, I can continue to walk this sample with the arrow keys to see what we're doing. And I'm familiar with these APIs. And it looks like we're doing state restoration. And as I continue to expand this, I can see that we are sort of deep inside the table view, and in addition to there being this sort of hot call path, you know, that is taking large number of the total percentage, there's all these other incidental samples as well. It's easy to get distracted by these. One of them here is OPC Message Send. This can occur all over your tracers if you're writing objective C. Even if you're writing Swift code, as you work your way into the system libraries, you'll see this. You'll often see its counterpart functions, OPC, Load Strong, Load Weak, etc., Retain, you can remove all that content from the call tree by context clicking on it, and choosing Charge OPC to Callers. That's going to tell Instruments to take all the samples that came from lib OPC and remove them from the call data, but keep the time as attributed to the parent frames that called them. I tend to treat those objective C runtime functions as just the cost of doing business when writing objective C code. It's rarely the case that I'm going to attempt to optimize them out, so I just prefer to remove them from the data, so I can focus on the things that I'm likely to take action on. Another very powerful filter that you can apply, and one that I'm going to use to remove all these small samples that occurred during this set of frames, is here in the call tree constraint section. Let me show you. I'm going to tell Instruments that I would only like to see areas of the trace that accounted for let's say 20 or more samples. I'm picking 20 because I know that I've selected about a two second interval and 20 milliseconds is going to represent about 1% of the total work, and that is about the granularity that I like to work at by default. So with call tree constraints set to a minimum of 20, I now focus this down much more significantly. Now, I mentioned here that we were expanding out my view items. I see that in the fact that we're calling NS outline view, expand item, expand children. Now, a lot of people would stop with the call graph at this point. They'd see I'm calling into a system framework, and I'm spending a lot of time there. This isn't my fault, right? What can I do about this? I can't optimize NS Outline View, Expand Items. You absolutely have the power to influence these situations. For example, the system framework could be spending all of this time because it's operating on data that you provided it. It could be taking a lot of time because you are calling this method thousands or millions of times. It could be taking a lot of time because it's calling back into your code through delegation. And most importantly, you can get an insight into what the system framework is doing by expanding down through the Instruments tree, and looking at the names of functions that are being called. In fact, that's exactly how I learned to fix this bug. As I expand the trace into the outline view, I can see that it is calling these two methods here. Batch Expand Items with item entries, expand children, and do work after end updates. Now, those are big clues to me that there is probably some opportunity for efficiency through batching. As you could imagine, the outline view starts with a small set of items, and then we are trying to restore expansion state in this area of our code, and so we are telling it to open, for example, the top item. And when we tell it to open the top item, internally you might imagine that it moves all the other items down. Then you ask me to expand the second item. It moves all the items down again. And the third item, and so on. And by the time you're done, you've moved those bottom items down thousands of times. That is all redundant work, and that is exactly the sort of thing I'm looking to eliminate when I'm trying to improve performance. Now the fact of these method calls talk about batching leads me to believe that there is probably some API where I can ask the outline view to do the work in bulk so it computes all the positions just once, instead of over and over again as I make the calls. I also see a call that says to do the work after end updates. Now, sometimes an API will offer sort of bulk method that operates on an array, and other times, it will offer a sort of transactional API that says I'm going to begin making changes, then you make a bunch of changes, and then you say you're done, and it computes something that happened for the whole range of your changes, more efficiently than if it had done them all individually. So at this point, I would head over to the NS Outline View, or NS Table View API, and I would look for some such method. And there is exactly one there. In NS Table View, there is methods for beginning and end updating, that allow the table view to coalesce, and make all this work significantly more efficient. Of course, we adopted that in Xcode 10. Let me show you. I'm going to launch Xcode 10. I'm going to open the source as an application, and I'm going to create a couple of tabs. And you can see, there is no awful flashing, and the tabs open much more quickly. Now, I'd like the tabs to open even quicker than that, right? So what am I going to do next? I got lucky here. It's not every day that you're going to go into the trace, and find something so obvious and easy to fix, that is responsible for 50% of the sample. Right? In fact, there is not going to be any other huge lead sitting there waiting for me. Instead, what I'm going to need to do is go through that whole sample, with those course filters applied, so I'm only looking at operations that take about 1% of the time or more, and I'm going to look for every single thing that I see that I think I can come up with some mechanism for making a little bit faster. I'm going to note them all down on a piece of paper or in a text document or something, and then I'm going to start solving them. Now, I need to pick an order to solve them in, right? Because sometimes the fifth thing on the list, fixing it with an obsolete, whatever fix you would do for the second thing on the list, and it feels bad to do them in the wrong order, such that you did redundant work, because that's the whole thing we're trying to remove in the first place, is redundant work. But it's very hard to predict how these things are all going to play out. And you often can't know until you've already done the work. So do not let this stop you from getting started, because you're going to get your second 30% improvement by stacking 10 3% improvements. Okay? Now, I want to go back to the slides, and show you some of the techniques we typically use to make those continued improvements. Far and away, the thing that comes up the most frequently is using those same techniques the outline view was using. Batching and deferring, right? You have an API, and when the API is called, it has some side effect. And then you have some code calling your API in the loop. That's what you're doing-- the primary piece of work that is being requested, and having a side effect. Well, if no one was reading the result of the side effect, then you're doing that work redundantly, over and over again. You can often get a much more efficient interface by using a batch interface, where a client gives you an array or some sort of collection of all the work to be done, so that you can compute that side effect just once. Now, sometimes you have many clients, right? And they can't batch across each other, and you can get even-- you can still get that same style of performance through deferring the work and doing it lazily. A third easy way to improve performance is you look through that instrument's trace, is to find areas where you see the same thing being computed over and over again. For example, you have a method in its computing, the size of some text, then you see the same thing happening several frames later, for the same text, and again, and again. Now, in this situation, of course, you want to try to just compute that value one time. Compute it at the top, and pass it down or maybe cache it. Another technique you have available in your UI applications is considering how many views you are using to render your UI. It can be very great for your source code organization to use very small views, with small sets of functionality, and to compose them together into larger pieces. But the more views you use, the harder you tax the rendering and layout systems. Now, this is a two-way street, because smaller views often led you to have more fine-grain caching, which can be good for performance as well. But generally, you can tweak the number of views that you have in order to have a significant impact on performance. It is not always best to have fewer views, otherwise all of our applications would just have one giant view for the whole thing. Another technique that comes up pretty frequently is using direct observation. We often have two areas of our source code that are loosely coupled. Maybe one area knows about the other, and they're communicating with each other through some indirect mechanism. Maybe they're using NS Notification Center, some block-based call backs, delegation, or key value observing. Now something that I see very frequently is we'll have some model code, and it's going in a loop, being changed, and every time it is going to that loop, it is firing lots of KVO notifications. You can't actually see that in the model code, of course, but over in some other controller, it's madly responding and trying to keep up with whatever is changing in the model, and you're burning lots of CPU time doing this, that ends up being redundant when you consider the whole scope of changes. Now, if this was direct callouts from the model code, either through notifications, delegation or manual block-based call backs, it would be much more obvious that this was happening as you edited that model code. And you might decide that it is totally appropriate to pull some of those notifications out from inside the loop to outside the loop, to have a big impact on performance. Now, alternatively, on the controller side, you could use one of these deferring and batching techniques to avoid the redundant work and just not respond synchronously. Last, this is an easy one. Once your code is already on the [inaudible] happy path, you know, it's already linear, and it's not going to get any better than linear. That's sort of the minimum performance that you're going to get. You're after all the constant time improvements that you can. Now, an easy one is that if you're using dictionaries like they were objects, then you probably know you're doing this, if you have a bunch of string constants for all the keys, then you can get a big improvement to code clarity, to code completion, to re-factoring, to making the validating your source code, by using specific types. It couldn't be easier with strucks and swift with their implicit initializers and conformance to equitable hash. And this can just be hands-down an improvement to your source code, and you'd be surprised at how much time you're spending in string hashing and string equation if you were doing this millions of times on lots of small objects. So with that, I'd like to turn it over to Matthew to talk to you about how we've applied these techniques inside of photos. Thanks Jim. Hi everyone. I'm Matthew Lucas, an engineer in the photos team, and today I want to give you some practical examples on performance from directly from photos. So first, let's talk about photos for a second. We are all familiar with this app. It lets you store, browse, and experience your favorite moments. So you can browse your favorite moments from the moments view, that you can see here. It's is the default view. But you can also get another view from the collection, or the years. And I'll talk more about this view later. Now, libraries today can go from 1,000 to 100,000 assets previous depending on your love for photography. And we all love capturing those fun and precious moments we live every day. So we are patient enough to capture them, but we are less patient when something like this appears. How would you feel if something moments like this would be displayed in Photos the first time you launch the app? Now, you may also experience something like this, where we are showing a lot of placeholders, and that's really not great. Maybe you're soft scrolling, you'll be lost in this gray area, the [inaudible] would start to load, but then you'll keep scrolling and then you'll experience some frame drops because the views are being updated. Well, our goal is to not show views like this. We think this is not providing a great user experience, but we understand that sometimes it's unavoidable. But when it's too frequent, this isn't really great. Now, when you work on an app, you want to make sure that it's responsive, and usable at once. You also want to make sure that the animations are smooth. And these two attributes are really crucial to providing a great user experience. If the users don't find your app relatable or pertinent, they might stop using it. Now, to illustrate these two points, I would like to give you two examples. And the first one is going to be how we optimize launching to this moment view. The second one is how we build the collections and years view for good scrolling preference. First, let's do launching [inaudible]. So what is launch? There are three kinds of launches. The first and more expensive one is the find referred as called, and it depends the first time you are going to relaunch your app after it reboots. So basically, nothing has been cached yet, and it might require some bug run processes or some libraries to load. Now, it also happens when the system goes under memory pressure and starts reclaiming some memory. Now, if you kill an app, it might not trigger a code launch, because the system decides when the resources should be paged out. And when you kill an app, and you relaunch it a few second later, it's almost guaranteed that you'll hit a warm launch. And we call it warm, because the resources or the dependents are still in the cache, so it's faster to launch. Now, the last type is-- we call it hot, and it's basically a resume, because it's when your app is already running and is being brought back to the foreground. So when you start measuring launch, you should start by measuring the warm launch. And the time it takes to launch during this warm is less variable than the cold launch, and the test iteration is much faster as you don't need to reboot your device. Now, the way we measure launch is by evaluating the time it takes from the moment you hit the application icon, and until you can start interacting with the app. And what I mean by interacting is that it's really using and not interacting with a spinner. A common pattern is to dispatch some work and display a spinner in the meantime, well that doesn't make the app usable sooner, so we are trying to avoid that here. Now there are three goals that we are shooting for at Photos, and the first one is that we want to instant, we don't want to display any spinner, and we don't want to display any placeholder or [inaudible]. And I Have to be honest with you, we-- you might see some placeholders the first time you synchronize with iClub, but when the data is local, we really try our best to not display any. Now, what do we mean by instant? Well, the time it takes to launch should be the same time as the zoom animation from the home screen. That is usually between 500 and 600 milliseconds, and that way, the transition from the home screen to the application is seamless for the user, and the user can start interacting with it, as soon as the animation is done. And by the way, this is the lowest recommendation, not something just for photos, so it's valid for any apps. Now, let's look at how photos launches today. If we look more closely at what is happening exactly, you can see that photos is all set up and ready before the animation is done. And if we dive into the launch anatomy, you will see there is mainly two parts. The first part is being spent in DYD, this is the loader that is going to load and link all of your dependent libraries, but it's also going to run your static initializers. And your control over that part is limited, but it's not impossible. I would encourage you to watch the DYD session from last year in order to get more details on that part. Now DYD is also calling Main in your object table, which leads us to the second part here, where you have lots of control over, and this part, you need to make sure that it stays under 500 milliseconds. Now, the first [inaudible] pass that is being scheduled right after the Did Finish launching will mark the end of your launch, and this is basically when your app should be usable. There are a few principles that we will be referring to during this session, and these are really the common pillars of the performance work that we achieved. The first one is that we want to be lazy and defer the work that we don't need. The second one is that we want to be proactive, and it's valid for two things. It's valid for being proactive in order to anticipate the work that we are making it later, we also want to be proactive and catch regressions quickly, so you should make sure that you have continuous integration testing in place. And the last point is we want to be constant, regardless of the total amount of data that we need to load. Now, if we were taking a naÃ¯ve approach, and we were loading everything we needed during launch, this is how long it would take roughly for a 30,000 item library. First you need to initialize the database, then you need to prepare some view controllers. You need to configure the data sources, load some library images, and fetch the cloud status. And keep in mind that this might vary as the data grow, and in fact, the data will grow forever as people takes pictures every day. So at Photos, really keep in mind that we are dealing with a non-bonded data sets. Now, let's see how we optimize each of these steps for Photos, and let's start with initializing the database. So first, usually, the database is initialized and loaded when the first query is being fired. One optimization that we have found was to do it as early as possible in the background thread, so that it doesn't have to do the initialization when the first query has been fired. And this is an issue, especially if the first query is being done from the main thread. Now, we spend a lot of time and we are still spending a lot of time reviewing all the queries that we're doing during launch, and we want to make sure that the work that we are doing is only the necessary one, and we are not doing more. Now, lastly, we want to ensure that all the queries that we are doing are efficient as possible, and we want to avoid the complex query as much as possible as well. And we sometimes we understand that we need this, and for these cases, we are setting up some indexes, so that we can speed them up. Now we are aiming for, at most, 30 milliseconds spent in that initialization. So next, let's look at how we are preparing our view controllers. So we have four tabs representing the main features of the app. And so the first thing that we need to be careful of is we want to minimize the work that is being done in the initialization of these three non-visible ones, and the rule that we are trying to follow here is to do as little work as possible in the initializers. We really want to do the bare minimum, and note all the data in the view that loads. This also allows us to initialize our controllers in constant time. Now, lastly, we also want to ensure that only the visible views are loaded. It's easy, and we often regress on that part, so you should really be careful about that. So preparing the view controllers, we are now aiming for 120 milliseconds. But preparing view controllers implies configuring the data sources, and let's look at that chunk next. So the Moments view is a representation of these things, events in your life, and the UI represents that by having this group of photos, and these headers. In this library, for example, we might have 500 moments, and in order to build a view, we need to load all the moments up front. But the only thing we need really for these moments is only the meta data so we can build the view. We don't need your content. So the first thing we do is we fire that query, which is super fast. And then we are only loading the content that we need here. In that case here, we are only going to load the visible content, which in our case is going to be between 7 to 10 Moments. Since our deficit is limited, and finite, we can allow ourselves to do it synchronously on the main thread. Now, we also want to anticipate and schedule the work so that we can start loading the remaining data as synchronously. And we do that on the bug run thread, with the right quality of service to make sure that it doesn't preempt the main thread from running. Now we are aiming at 100 milliseconds here. So lastly, our data sources are also providing some images and let's see how we optimize that part. So this was by far the biggest chunk here that we are all attacking, and when we realized that we were spending multiple seconds loading this image during launch, we realized that we were doing too much work. So the first thing that we did is that we evaluated the number of images that we needed during launch, and we are only loading that during that first transaction. In that case, that can be up to 60 including some piling above and below. And next, in order to load those images firstly, we need to make sure that we are all loading only low-resolutions one. That way we are loading fewer pixels in memory, and it is much more efficient. That chunk is now representing 200 milliseconds. And this is, by far, the biggest gain that we had. Which I need to be a constant time, and that's really great. Now, sometimes you have to ask yourself the question, is this really needed during launch? And one of our examples here is this footer view. That pulls information via the network or the database, and literally first our design was to not show it during launch. To prioritize all the images that we are seeing here. We wanted to show as much images as possible. So that may be simpler. We are now only scheduling that work post-launch, and we cache to process information for raising later. Now, if we would have had the requirement of displaying this information, one approach could have been to leverage the register background at refresh API from UA kit, that will proactively clear your app so that you can start preparing some content when the user is going to launch your app. So now, that part has gone from launch, and that saves us 400 milliseconds of CPU time. If we look at the updated breakdown here, we can see that we now have only 450 milliseconds worth of work. We are now fitting into that 500 millisecond time window, and regardless of how things can be represented concurrently here, the most important part of that is to really make sure that you think about the cost of preparing your content. And what I mean by think is really measure it. Now, you should strive for doing work in constant time, regardless of the total amount of data you are loading. In our case, really have unbonded data assets, and we need to stay constant. Now that we have launched the app, we need to start using it. And let's see how we did collections and [inaudible] for good [inaudible] performance. So as I mentioned earlier, our users can seamlessly transition with animation from the Moments, through the collections, to the years view. And this is a complex hierarchy. We have thousands of pictures to display. We need to support live updates, we need to also support animation between these layers, and we also have some gestures. Now, we also have some goals here. For the experience we want to provide to our users. The first one is the same as before, we don't want to have any spinner. We don't want to have placeholders, but we also want to have smooth animations. And by smooth animations, I mean 60 or 120 frames per second, depending on the screen you're running on. Now, remember the principles that we've seen before. Well, they are all applicable here. We want to be lazy and defer the work we donate up front. We want to be proactive, and catch regressions quickly, but we also want to be constant in our layout passes, and regardless of a lot of data that we are loading. Now, this time, we also want to be timely, and we want to remember the rendering loop cycle. And what I mean by that is that I want you to remember that we only have 8 or 16 milliseconds to render that frame, so we need to make sure that we are not going over that time, otherwise we would start dropping frames. Now, let's take a step back, and look at what we are trying to achieve here. We wanted to have this portable view, with sections and mini cells in it. And that is basically what your Collection view is providing, right? Except that in this extreme case, we are restricting the limit of what we could achieve with a basic approach. And that resulted in too many views, too many layers. But also in an increased layered complexity, and that also had an increased memory cost. So we needed to innovate here, and we did that by restricting the number of views drastically while still using a collection view. We used a technique more commonly used in video games, that is called atlasing. And it basically consists of combining a set of images into a single one. We do that efficiently by using only very small thumbnails first, then we stamp all the raw image data on the canvas we are using as a strip. Now, we use image raw data so that we can avoid decoding each thumbnail as we send. So basically we are displaying a random strip of images. Now, we generate and cache them on the fly so that we can be more flexible. And as we render multiple images into a single one, we are registering the number of cells, layers, objects drastically, which simplifies the layout and the time spent building it. Now, the separate works well, but it has trade offs to consider as well, and this is one of them. So if someone tries to long press or force search an item here, we will need to figure its position so that we can achieve the preview correctly. And as we display a single image, we need to maintain the mapping of each individual image, and its render strip. Now, you might be thinking, why are we generating them on the fly? Well, we need to support live updates, that's the reason. We need also to support different view sizes. For example, we have landscape here. But we also have portraits. And also we can do that because we can [inaudible] because our user's labor typically grows organically over a long period of time, and the cases where we might need to generate thousands of them are pretty rare. Now, you may be wondering also why are we not generating the whole section then? Well the answer is that our design record is to do this cool animation, where you can see that the collections are expanding into their own sections or collapsing into group ones, and the other way around. So if there is one thing that you should also remember from that second part is you should really think about the layout course of your hierarchy and measure it. Lastly, you should always think about performance. At Photos, we care deeply about it, and this is really part of our daily job. For more information, you can come and see us in these three labs that are mentioned here, and I hope that you have a great conference. Thank you.  Hello, everyone, and welcome to Building Voice with Siri Shortcuts. My name is Amit, and I'm here today with my colleague Ayaka Nonaka. And we are so excited to be here and talk to you about Shortcuts. And you all know, shortcuts allow you to accelerate your users to the things they value the most in your apps. What we are really excited about is that your users can add these shortcuts to Siri and perform them with their voice wherever they are. So, it's paramount that we build a great voice experience for all of our users accessing them through voice. And that's what we'll focus on today. So, let's get started. First, we'll talk about how your users can add shortcuts to Siri and how simple it is to write -- use it right away. Then, we'll talk about how you can provide downloads that can be spoken by Siri, through custom responses. Then Ayaka will go over some of the best practices that you should adopt to create great voice and UI experiences for your shortcuts. And finally, we'll go over how you can bring the experience of setting shortcuts to your apps. So, before we dive into all the exciting details of adopting shortcuts and custom responses, let's take a look at the experience of setting a shortcut with Siri. My teammates and I have been working on this app called Soup Chef. It lets you order soup. Now Soup Chef adopts all our new shortcut APIs, and every time a user places an order, Soup Chef lets the system know by donating a custom intent to Siri. So, yesterday, I ordered some tomato soup. And it was really great. So much so, that I would like to order it again. And wouldn't it be great if I can just do that from Siri? Well, now we can. So, when we go to Siri's Settings UI, you see that Siri's already suggesting that donated intent to be created into a shortcut. So, let's tap on it. And we are presented with this screen, where we can set up a phrase that you can say to Siri and perform your shortcut. There are also a lot of details on this screen from Soup Chef that convey to me as a user what this shortcut would exactly do. How to get these right, we'll talk about that in a little bit. But for now, I would like to associate a phrase with this shortcut. Something that is short. Something that's memorable and I can say it again and again. It's not always easy to come up with that right in the moment, so I figure that Soup Chef is suggesting I call it, Soup Time. I like that. I'll take it. Now, when I record the phrase, I have a shortcut to order soup with Siri. Let's try it out. Now, when I say to Siri, "Soup Time," I'm asked to confirm whether I would like to run the intent, and when I say, "Yes," the order is placed. And it's that easy. Your users will set up shortcuts for things they do most often. And they can use these shortcuts wherever they are, from their iOS devices, from their wrist using the Apple Watch, in their homes using the HomePod, as well as on the go with CarPlay. So, we should pay a lot of attention to the voice experience that they encounter. Towards that goal, let's talk about custom responses. Shortcuts allow you to provide dialogues that can be spoken by Siri, and this gives you the ability to provide relevant information, important information, right with voice. Fundamentally, shortcuts are -- custom responses are template strings whose structure you define in the Intent Definition File. And based on the type of information you provide in your responses, shortcuts can be one of four types. The first is a confirmation custom response. This gives you the opportunity to provide critical information before your users commit to running the shortcut. Next, is a success response. And it gives you a chance to provide an auxiliary relevant information, after the shortcut has run, and the structure of a success response consists of Siri letting the users know that their order was successful, or their shortcut ran successfully, followed by any information that you provide attributed to your app. The third type of custom response is an informational response. And we are really excited about this. Informational response allows you to provide information like transit schedules or ski forecast right within Siri, with voice. This opens the door for a whole new category of apps which was not possible before with Siri. And finally, when things do not work as expected, custom error responses give you the opportunity to explain what went wrong instead of just saying, "Something didn't work." The structure consists of Siri communicating to the users that their intent did not successfully execute, followed by information from your app, and the interaction punctuated by Siri, advising users to continue in your apps. Now, that we understand what custom responses are, let's take a look at how you would adopt them in your apps. Custom responses work hand in hand with custom intents. So, the first step is to define an intent that models the use case you are trying to create a shortcut for. For Soup Chef, we created an intent called Order Soup, and it lets you, you guessed it, order soup. Alongside the intent, you have to pick a category for your intent. And the category you pick influences how Siri speaks about it or what you see across the OS like the order buttons. So, pick something that feels natural, that works well with your use case. Once we have defined our custom intent, let's go over to the Responses Tab. And in the Responses Tab, you see that we already have a success and failure response codes. But these are just a generic response codes which don't convey much more information besides the status of the operation. What I would like to do, is let our users know how long it will be until their order is ready and they can to the store and pick it up. So, what I want to do is define a custom success response for that. Towards that goal, the next step is to declare the response properties that you will use in your templates to provide information at run time. So, if you go over to the Properties Section, I'm going to add a Wait Time and a Soup Property. With that in place, the only thing left to do in our definition of custom responses is to define the template. The template is what is spoken out by Siri. So, when we look at the template section of our Intent Definition File, now I'm going to add my success template. Not just that. Our store is getting a little popular and we tend to run out of soup these days. So, it will be great if instead of just throwing a random error, I can let the users know that the soup is out of stock. So, I will add another response code called, "Failure Out of Stock," with a template that communicates that the soup is out of stock. With that, we have defined our intent responses. And when you do that, XCode automatically cogenerates a custom response subclass. This subclass has all the properties that you defined as well as custom constructors for your templates, which take all the important properties that you need to construct that template. Now the last step that is left to do, is to use these responses in our Intent Handler. Handling a custom intent comprises of two steps: confirm and handle. In confirmation, we are asked whether we are ready to handle this intent at this time. So, we'll first find out whether the soup the user has requested is in our menu. If it is, we'll let the system know that we are ready to place the order. But this is our chance to also check that the soup is available right now, and that we can place the order of that. So, let's do that. First, I'll grab a reference to my menu item. Then check if it's available. And if it isn't, I'm going to call the Completion Handler with our new custom Failure Out of Stock Response Code. If everything looks great, we'll continue as before. This is also our chance to provide custom response -- custom confirmation response. Support for those will be coming in upcoming [inaudible]. Alright, that was confirmation. Now, it's time to handle the intent. For us, that means to place the order for the soup. So, I'm going to build an order from all the information that is present on my intent. And then place the order with our Order Manager. And if it's successful, we'll let the system know, by calling the Completion Handler with the Success Response Code. However, since we defined the Custom Success Response Template, this is our chance to provide the wait time. So, let's do just that. And with that, we have added support for two really useful custom responses to our custom intent. Let's see how it looks. So before, all we could let the users know was, "Your order was placed." But now, Siri can speak out that it will be about 10 minutes before they should come down to the store. And that's really helpful. The improvement to the usability is even more stark if you look at the error case. Before, "Sorry, there was a problem with the app." But now, we can let the users know that the soup is out of stock and then they can decide what they would like to do. Either, they can go into our app and order something else or try again later. So, you saw that Siri and shortcuts allowed you to provide custom dialogues, but that's not all. As soup engineers, we work really hard to build superb experiences for our users. And wouldn't it be great if we can bring those experiences from our apps to our shortcuts wherever they are run? With Intents UI extensions, you can do just that. Intents UI extensions allow you to provide custom UI, familiar UI from your apps, to shortcuts. And it is displayed all across the OS, on Lock Screen, in Search , and of course, within Siri. To learn more about how you can build great, Intents UI extensions, please take a look at the WWDC 2017 session, "What's New in SiriKit?" So, we built a custom Intents UI extension for Soup Chef app. And let's see how that looks. Again, this is where we started out with. But with a custom Intents UI Extension, now I'm able to show the users the entire invoice and convince that we got all the details right, that it will be 2.95, before they place the order, which is incredibly helpful. Once they place the order, instead of just saying, "It's done," we can now show them the receipt and reaffirm that we got everything right and it will be ten minutes before they should come down there. Alright, so you saw how easy it is to add support for Custom UI and Custom Responses. Now, let's take a look at how we did that in Soup Chef. Alright, what you see here is our app Soup Chef. If we go to the Intents Definition File, you can see that we have a custom order soup intent defined. This intent is off-category order. Now, let's look at the response section that is associated with my intent. Here, I have properties and template sections. What I would like to do is add the custom success response that we just talked about. So, let's start by adding the wait time property. I'll click the Plus button, and name my property Wait Time. I'll leave it of the Type String, so I can format the date however I see fit before providing it into the dialogue. To reaffirm which soup the user ordered, I'm also going to add the soup property. Now, soup is a custom object in our app. So, I'm going to select the type "Custom." Now, let's take a look at the response codes. We have a failure and a success code. Here, I'm going to define the template for our Wait Time Response Template. So, let's do that. "Your Soup Will Be Ready in Wait Time." I would also like to add our Out of Stock Error Code. For that, I'll click the Plus Button and create another response code. I'll call it "Failure, Out of Stock." Since this is an error code, I'll uncheck the Success checkbox. And now, finally, let's define the template for it. "Sorry, Soup is Out of Stock." And with that, we have defined our custom responses. Now, let's head over to the Intent Handler file. Alright, here we are. As you see here, we have a Confirm and a Handle Method. Let's look at Confirm first. We find out which soup the user has requested, and then check with our menu manager whether that soup exists on our menu. If it does, we return the response code "Ready." What I would like to do here, is to check whether that soup item is available currently to order. So first, let's grab reference to the menu item from our menu manager. Then, I'm going to check if it's available. If it isn't, I'm going to call the Completion Handler with our new Failure, Out of Stock Error. And provided the soup object that the user requested. Alright, so we have provided the custom error code here in completion. Let's take a look at Handle. Handling this intent means ordering our soup. So first, we build the Order Object from Intent. Then we place the order, using our order manager. And finally, let the system know that the order was successful. But this is our chance to provide users with more helpful information like wait time. So, let's call the Completion Handler with our New Custom Response Code. This response template takes the soup and wait time strengths. So, I'll provide that from my intent. And we can grab the wait time from our order. And that's it. So, you saw how easy it is to add support for custom responses to our custom intent. You start by defining a custom intent that works for your use case. You follow it up by defining custom responses that you want Siri to speak out. And finally, you provide them in your Intent Handler. Now, I placed that order for that soup earlier, so I better head back and get it. So, please welcome Ayaka Nonaka, who will talk to you about some of the best practices that you should adopt in order to create great voice and UI experiences in your apps. Thank you and welcome Ayaka. Thanks so much. Hi, everyone. So, as Amit just showed you, it's super easy to get started with Siri Shortcuts. Once a shortcut is donated, it is available for the system to suggest as a shortcut to use with Siri straight out of the box. I'm going to be showing you some more new shortcuts APIs, and the best practices to help bring the greatest experience to our users. And I know you care about this because your users are going to notice these small details and the large amount of care that you put into your apps. If you saw the first session, "Introduction to Siri Shortcuts," you learned that there are two ways to donate shortcuts. The first way is through NS User Activity. User Activities are great if you want to do a basic integration, where you just want to open the app and show your user a particular piece of content. And the second way is through Intents. Intents are great if you want to do a deeper integration where you can run things in the background and inline within the Siri experience, so you don't have to context with your users out of that to get things done. And for both NS User Activities and Intents, there are several parameters that we can configure. The first one is the title. So, in this case, Order Clam Chowder, with the subtitle underneath it and image next to it, and finally, a suggested invocation phrase which you'll get to later. For each of these, we're going to go over how best to configure them. So, let's get started with the title and subtitle. So, when you're thinking about titles and subtitles, there are two high-level things that you should keep in mind. First, the title should represent what happens when the user runs the shortcut. Your user is going to want to know exactly what happens before they run the shortcut or add that shortcut to use with Siri. And secondly, the subtitles will provide more information, but only if you need it. So, let's dive into a little bit more of the details. The first easy thing to do is see a sentence case. This is to make sure that we provide a consistent experience across all different apps and their shortcuts. Next, it's important to keep the title concise. This is because we're dealing with potentially a limited amount of [inaudible]. And you can do this by dropping articles like "a" and "an" if your language allows for it. Next, it's super important to include critical information. For example, if you have a payments app that lets you send money to your friends and you have a shortcut for that, it's important to put the dollar amount or your currency amount in that. Probably in the title in this case. Next, if we're working with Intents, it's important to include a verb. And if you're working with English, we should put that at the beginning and prepend the verb to your phrase. And last but not least, to make sure that we provide the shortcuts experience to everyone around the world, it's very important to localize. So, not let's take a look at some things that we should avoid. So, the first thing that we don't need to include, is the name of your app. This is because our RUI already attributes the name of your app, and by not including this, you can save even more space. And second, we don't need to put duplicate information in the title and subtitle for similar reasons to save space and leave room for critical information. And lastly, we don't want to use quotation marks unless you're actually quoting something that's going to be used verbatim in the shortcut. For example, if you have a shortcut to send a message, you can put the content of the message in the quotation marks. So, let's take a look at an example. This example says, "A clam chowder from Soup Chef. The best way to get soup, Soup Chef." And just based on what we learned about, I think we can all agree that this is not a good example. So, let's try to make it better. So, the first easy thing that we can do is to drop the "From Soup Chef," because as you see, Soup Chef is already attributed in this UI. So, now we have "A Clam Chowder." But when I'm looking at this, I'm not sure what's going to happen when I run the shortcut. Am I ordering the soup or just looking at it? Who knows? So, let's fix that by adding a verb. And while I'm doing that, I'm also going to drop the article "a" to save some space. So now I have "Order Clam Chowder." So, when I'm looking at this, I'm thinking, "I want to order a Clam Chowder, not a Clam Chowder, which doesn't sound very delicious and we're trying to sell soup here, so that's really not ideal. But the good thing is, we can fix this really easily by simply dropping the quotation marks. And now we have, "Order Clam Chowder." And I think at this point, our title's looking pretty good. But let's take a look at the subtitle. The subtitle says, "The best way to get soup." But that's basically Soup Chef's tag line. I already know that Soup Chef is the best way to get my soup. And it's not adding any extra information that is specific to the shortcut. And, so that's already not good, but a more critically bad thing is that Soup Chef recently started offering delivery service, and I don't know where I'm getting my soup delivered to. Am I getting it to my home, my office, someone else's office, no idea. So, let's fix that by adding a delivery location. So, now I have "Order Clam Chowder to 1 Apple Park Way." And now I know exactly what happens when I run the shortcut. So, that is great. And in the Settings UI, it might look something like this. And this is looking pretty good. But I think we can do a little bit better. So, as you can see, the Soup Chef app icon is used for both the Clam Chowder and Tomato Soup cases, but wouldn't it be great if we can show an image of a clam chowder for the clam chowder case and an image for the tomato soup for the tomato soup case? Sort of like this. And for this, we provide two APIs. One for User Activities and another for Intents. So, let's take a look. So, to set some context, the user has just viewed one of their past soup orders and we want to donate it, so we can associate it -- we can associate that with a shortcut. So, the first thing that we want to do is create a CS Searchable Item Attribute Set which by the way is the same thing that you want to use if you want to provide a subtitle to your user activity, but in this case, I want to set an image [inaudible]. First, make an image with chowder. Get the PNG data out of it and set it as the thumbnail data property on my Attributes. And next, I'm going to take those attributes, set it as the Content Attribute Set on the User Activity, and finally I'm going to donate the User Activity by setting it as the User Activity property on my view controller. So, let's take a look at how it works during Intents. For Intents, I'm actually able to set multiple images. One for each parameter that is defined as an intent. So, I'm going to set an image of a chowder for the parameter named "Soup," and an image of an office for the parameter named "Delivery Location." And once I have that, I'm going to create an IN Interaction out of it and donate it. So, at this point, you might be wondering, "If I have a shortcut that is based on a shortcut type that contains multiple parameters that have images, how do I know which image gets used?" For that, we're going to go to our Intent Definition File. And in particular, the Parameters Section. So, in the Parameters Section, we want to make sure that these parameters, at least the ones with images are ordered from least specific to most specific. So, in my case, I care more about the soup image than the delivery location. So, I'm going to make sure the soup comes below delivery location. And once I do that, my list of shortcuts might look like this. So, in the first two cases, where the shortcut type contains both a soup and a delivery location, I see an image of a soup, but in the last case, where the shortcut type just contains a delivery location, I just see the image of a delivery location. Now, these shortcuts are looking so great, that I just want to tap on one and add it to Siri, so I can use it. And our users are super excited about this new feature too, and they're all so eager to start setting up new shortcuts to use with Siri. But as more users starting using this feature, we noticed something. We noticed a problem. When they got to the recording screen, they had no idea what to say. They could use some creative inspiration in choosing a great suggested shortcut phrase. And as Soup Chef app developers and general soup enthusiasts, we think we have some pretty great ideas on what would make a great soup ordering phrase. You know, something like "Tomato, Tomato" or "Chowder Time." So, would it be great if we can show a suggestion like this? "Hey user, you can say something like, 'Chowder Time.'" And of course, we have an API for this, so let's take a look. So, building on a previous example, all we have to do is add one line. So, User Activity has a property called Suggested Indication Phrase. And I'm going to set that to Chowder Time. And for Intents, it's the exact same thing. Intents also have a property called Suggested Indication Phrase which I'll also set to "Chowder Time." So, the API for this as you can see, is super simple. But what actually makes a good suggested invocation phrase? So, the most important thing to keep in mind is that the suggested invocation phrase is both short and memorable. You have to remember that your users are going to have to say this phrase exactly every, single time they run their shortcut. So, ideally it would be somewhere between two to three words. Next, it's important to not include the phrase, "Hey Siri," in your shortcut phrase, because if your user accidentally sets their shortcut phrase as "Hey, Siri, Chowder Time," when they actually run the shortcut, they might have to say, "Hey Siri, Hey Siri, Chowder Time," and no one wants to do that. So, let's take a look at some examples. This example says, "Hey, Siri, please place an order, thank you." Although this example is polite, it's not a good example. First, it contains, "Hey, Siri," which is pretty obvious, but it's a little bit too long for me to want to repeat it every, single time I want to run the shortcut. So, that's out. What about this example? This example says, "Order a clam chowder to my office." Seems pretty reasonable, but this is still not good because it's still too long. Like, it's not short and memorable. And I don't think I can or want to remember to say this phrase every, single time I want clam chowder delivered to my office. What about this example? This simply says, "Chowder Time." This is both short and memorable and doesn't include the phrase, "Hey, Siri." So, this passes all of our tests and that's awesome. But we can't [inaudible] in order to provide a truly great shortcuts experience to your users, around the world, you have to make sure it's localized. For example, I speak both English and Japanese, and it would be great if I can suggest a Use Shortcuts in Japanese sort of like this. So, instead of "Chowder Time," I can be like [foreign language spoken] which means, "I'll eat chowder." To do this, we can localize. And when we're thinking about localization, there are a couple of things to keep in mind. So, in addition to localizing your code, we want to make sure to localize the content in the Intent Definition Files, because as you can remember, there are a lot of strings in there. And second, we have to think about pluralization. So, even if you're not localizing to any other language besides English, if you have to handle things like, "Order one clam chowder," versus, "Order three clam chowders," you have to think about localization. And to learn about all of this stuff, check out the Localization Session and their labs. They'll teach you everything that you need to know, and more. So, cool. For now, let's go back to English. Now, that we've followed these best practices, when we go to the Settings UI, you are presented with a beautiful list of suggested shortcuts. And right now, these are based on a combination of suggestions based on the user's past routines and the things that the user did recently. But in addition, we provide an API to allow developers to suggest their own shortcuts. And this is useful because this allows you to suggest things for things that the user hasn't necessarily done yet or hasn't done recently. For example, if you have a music app, you might want to offer a shortcut for playing a particular playlist, even if the user hasn't played it before or hasn't played it recently, because presumably, if a user has created the playlist, they probably ought to play it at some point in the future. So, in the Soup Chef app, I think it'd be great if we can suggest a shortcut for ordering the soup of the day, even if they haven't ordered it recently, or have never ordered it, because based on past experience, our users seem to love it because it's both delicious and offered at a special price. So, let's take a look at the API on how to do this. So, for Intents, you can create an IN Shortcut out of Intent, well, and for User Activities, you can also create an IN Shortcut out of it. I'm going to wrap it in a suggestions list and pass it into IN Voice Shortcut Centers Set Shortcut Suggestions Method. And once you do that, you'll see my suggestions alongside all the other system's suggestions. And by the way, the same set of suggestions is shown in the shortcuts app, so you can create custom shortcuts, using your shortcuts. So, now that we put in all this work to provide a great voice experience for your users, we want to make sure that your users know that they can add shortcuts to use with Siri. And the best way to do that is bringing shortcuts into your app, so that users can create shortcuts right from inside your app. So, for example, I just ordered a soup, and I'm going to need a suggestion to add this to Siri. And when I tap, "Add to Siri," I'm presented with the same system UI that allows you to configure a new shortcut to use with Siri. So, let's take a look at the API to do this. So again, I'm going to take my User Activity and create an IN Shortcut out of it, and then using that, I'm going to create an IN UI Voice Shortcut View Controller, set the Delegate, and present it. And it's the same thing for Intents. So, instead of creating the IN Shortcut with a User Activity, I will create it with Intent, create my View Controller, set the Delegate, and present it. And once I do that, your user will see this setup screen, right from inside of your app. In addition, rewrite an API to allow users to delete and edit any existing shortcuts. We also have an API that lets you get a list of shortcuts that are already added to Siri by our users. So, for example I can use this information to indicate which soups on my menu have the user has already added the shortcut to use with Siri. So, as you can see, for clam chowder, I have "Chowder Time" associated and for tomato soup, I have "Tomato, Tomato." Awesome. So, to summarize, let's go over what we learned about today. First, we learned that Custom Responses are critical in providing a great voice experience to our users. And Amit showed us just how simple that can be. Next, we saw how Custom Intents UI can help bring the apps experience into Siri and to the other places in the system like Search and Lock Screen. We also saw that details matter. Even something small like adding an image to your shortcuts, or just dropping the quotation marks from around "clam chowder" to make it a big difference. And bigger details like localization can make a huge difference. Lastly, we learned about the various different APIs that you can use to bring the shortcuts experience into your app, so that your users can create shortcuts to user with Siri at the most relevant moments in your app. So, the documentation on how to do this is all on developer.apple.com, and in addition, we'll be holding shortcuts labs throughout the week, including one later today. And we also have a Watch Session right after this, that will show you how to bring the shortcuts experience to the Watch and to the Siri Watch face so that -- and they're also going to show you how it works, even if you don't have a Watch app. So, it's a total must watch. Alright. Alright, well, now you know everything that you need to know to create a great shortcuts experience for your users. We can't wait to see what you build with our new APIs. Thank you for listening, and I hope you have a great rest of WWDC.  Good afternoon. My name is Ali Ozer. I, with my colleagues, Chris Dreessen and Jesse Donaldson, will be talking to you about what's new in Cocoa in macOS Mojave. As you saw yesterday, we have some amazing new features in AppKit this year. We'll be touching up on many topics, among which are these. We'll be talking about some of the API refinements we've been doing. The amazing new Dark Mode, and related features, some changes in layer backing, and also custom Quick Actions. So, let's dive right in. Now, as you know, we pride ourselves on our API's, and our goal always, is to have API's that are capable, and consistent, and powerful. So, with that in mind, we continue to do refinements that improve both the Objective-C and Swift exposure of our API's. And, this is not just in AppKit or Foundation, but also other frameworks including UIKit, as you might have heard in this morning's "What's New in Cocoa Touch." Now, the API changes we're doing are fully source compatible in Objective-C, and we also expect them to be 100% migratable in Swift by the time we GM the SDK. So, with that, let's look at some of the API refinements within this release. First I'm going to talk about some updates to string types. Last release, we introduced string enumeration types as a way to collect groups of related string constants together. These helped make API's that deal in these types much clearer. And, here are some examples. The first one is declared as NS STRING ENUM. This is used for string enumerations, where we provide some values out of the frameworks with no ability to extend them, so it's a fixed set of values. The next two here, NS EXTENSIBLE STRING ENUM, this is used to declare string enumerations where we might provide some values out of the box, but other frameworks and applications can also add to that set. So, we've done two changes here. First one is a simple one. We've simply replaced NS STRING ENUM and NS STRING EXTENSIBLE ENUM with their typed variants. This is effectively a no-op change. These are just more general forms of the same declarations. So, no changes in your code, or call sites, or anything. Now, the next one, NSImageName, underwent a bigger change, a more significant change, instead of a string enumeration, it's now declared as NS SWIFT BRIDGED TYPEDEF, which is effectively a typedef. Now, here's what the Swift exposure of this looks like. In Swift 4, NSImage.Name came across as a struct, which is the way you would declare the string enumerations. In Swift 4.2, it's as a typealias, a simple, good old, garden variety typealias. Much simpler. So, the question is, why did we do this? Let's look at a call site example. Here in Swift 4 is how you would take a string, and create an NSImage with it, by using the named method. As you'll see here, you'll be taking the string, and we're converting it to an NSImage.Name into the [inaudible] name before we call NSImage named. This does not feel super-streamlined to have to repeat NSImage.Name here. Now, with changes in Swift 4.2, this is all we have to write. You do not have to convert to the NSImage name, which is more streamlined, a little cleaner, less redundancy there. So, we believe this typedef approach is appropriate for passed-through values. Yes, we had heard from some of you. So, we believe this appropriate for passed-through values, such as resource names or identifiers. Basically, values that are not interpreted by the framework but are just used in a passed-through fashion. So, image name, color name, window frame autosave names, and so on. So, these are the types that are appropriate for this. Now, note that you still have the benefits of a specific API declaration however with this new approach. Here's the declaration for NSImage named method. You'll note that the argument is still NSImage.Name, as opposed to a string. So, we still have that come through. Now, here's the full list of types. We did this to NSAppKit. Turns out a lot of types could benefit from this. It's not just this, it's also this set as well. So, a number of types have changed in this fashion. So, next, I'm going to talk about common prefixes. As you've seen in previous years, over time, we've been switching from common suffixes, which is what we used to to many years ago, to common prefixes in Objective-C. Using common prefixes enables names to group together, and become more easily discoverable, and come across better in Swift. So, let's look at an example. Here is NSLineJoinStyle as it appears in the 10.13 SDK. And, here is how it appears in 10.14. You'll note that enumeration values such as MiterLineJoinStyle now have become LineJoinStyleMiter. So, a common prefix. The Swift exposure changes from miterLineJoinStyle to just miter. So, it's-- you know, you don't have to repeat the type any more; it's pretty obvious in the call site, so much cleaner. And, so good it deserves a happy emoji. [ Scattered Applause ] Thank you. And, we did this to a number of other types that we had not done, applied this change to, and here is that list of types. Next, I want to talk about formalized protocols. In the olden days, we used to use informal protocols, which are basically categories on NSObject to group related methods together. And, since then, we added features such as optional methods on protocols and so on, and we've been switching to formal protocols where possible. And, I'll show you an example of one of the ones we did this release. Here is the method validateMenuItem, and it used to be an informal protocol, a categorization object in 10.13. Now, it's a formal protocol, called NSMenuItemValidation, with .method in it. The Swift exposure changes from an extension NSObject to a formal protocol, of course. NSMenuItemValidation in Swift 4.2. Of course, the benefits here are that objects that do menu item validation now have a way to formally declare that they do that by conforming to this protocol. Again, we like this so much, we did it across a bunch of other API's. So, here's the full list of formal protocols we added in AppKit. You'll notice things like color changing, font changing, NSEditor, NSEditorRegistration combines the bindings-related methods, and so on. So, it's a good list of new, formal protocols. Next, I want to talk about direct instance variable access. Now, most-- in our API's almost all of the instance variables are private. And, we've said so, but in a way that they have been declared, especially in some older AppKit classes, subclasses were allowed to touch the instance variables, directly access those instance variables. Now, some of you may not even be aware of this, so please don't go ahead and start using them, because this is probably an old code, code I'm sure you didn't write, but maybe inherited, that may be using instance variables directly. So, for now, we are going to be frowning upon this practice a bit more vigorously by deprecating it. Now, you'll be-- code that accesses instance variables directly will get a warning, and our intent is to break this in future updates. So, as you get the chance, please go ahead and clean these usages. And, the fix is pretty straightforward. Instead of accessing the instance variable directly, please go ahead and call the getter, or the property accessed, or whatever there might be. And, if you have some reason to access the instance variable, and you don't see a way around it, you might want to let us know. Now, speaking of deprecation, we're doing one more thing called formal soft deprecation. So, over the years we have deprecated a lot of API's, and have replaced them with better ones. In cases where the deprecation isn't urgent, we usually go through an informal deprecation phase, where we tell you the API's deprecated, we release note it, we comment it, and so on, before we actually mark the API's deprecated. Usually to reduce disruption. But now, we have a way to mark API's as to be formally deprecated. Let me give you an example. Here we have a symbol, NSBoxOldStyle, which of course happens to be a name that begs to be deprecated. And, you'll note that we've marked it as deprecated. And, the version number for the deprecation is API TO BE DEPRECATED. So, what this does is, it tells the compiler not to generate a deprecation warning or an error, however our intent is that if you try to use the symbol in Xcode, or new code, or in access documentation, you will get a warning that the symbol is deprecated, and it will be pointed at the replacement symbol. So, of course, comes across in Swift as well. As you can see here, one thing to note, the version number 100,000. This is not a pre-announcement, or a leak of some far, future SDK. It's just that's a placeholder number we're using for now to indicate this feature. Now, we use formal soft deprecation in a few other cases. Earlier I showed you this. And, I told you that we renamed MiterLineJoinStyle to by LineJoinStyleMiter. And, I also said that Objective-C source code was 100% compatible. So, you might be wondering, well, what happened to that old symbol that you renamed? Well, we actually declared that old symbol, as you see here, by using this new API TO BE DEPRECATED. So, we declare it as deprecated, as API TO BE DEPRECATED, meaning any new attempts to use it will get warnings, but existing uses will be left alone, because we really don't want to disrupt uses of the symbol in Objective-C source code. Now, turns out there was a lot of symbols that were waiting for this facility, so a lot of API's are marked with API TO BE DEPRECATED. A bunch of these are because we did the common suffix to common prefix naming, and some of the others are symbols that we are de-emphasizing deprecating, because we're bringing new ones in. Especially in support of features such as Dark Mode, which you'll hear about later today. So, the last topic I want to talk about is secure coding. As you may be aware, we introduced the concept of secure coding back in 10.8 and iOS 6. It basically allows class-- when you are archiving, basically allows you to specify what classes are expected, that way it can be an explicit error if those classes are not encountered in the archive. Now, the way we did secure coding, the secure coding was an optional feature. But, we now have new API's that enable both secure coding as a default behavior, and as a bonus, they enable error returns. Our archiver and unarchiver API's worked with exceptions, but of course we prefer error returns. And, the new API's enabled error return behaviors by default. So, I'll show you the API's NSKeyedUnarchiver, since that's the most interesting. Here is an NSKeyedUnarchiver. One new method is in it. It simply creates a keyedUnarchiver, securely, and in a way will return errors. Two other new methods are these convenience API's, unarchivedObject(ofClasses from, and unarchivedObject(ofClass from. These basically unarchive a single object and return it. They do it securely, and they will return an error if some problem's encountered. Now, note the second method here. It's sort of decorated like a crazy fancy peacock. All that decoration enables Swift to infer the return type much better, which is of course a trick Swift is really good at. Now, note that all the way out at this SDK this year, they do work back to 10.13, and iOS 11. So, you can start using them, even with those deployment targets. These methods replace the methods on this slide here. Now, you'll note that these are being deprecated in 10.14 and also iOS 12. Since these are not doing secure coding, we are deprecating them immediately, rather than going through that formal soft deprecation, because we really encourage you, we really want you to switch to the secure coding if you haven't done so yet. Now, one more thing about secure coding is a new value transformer. As you may know, NSValueTransformer is a class used for automatically transforming values from one to another. These two valueTransformers here-- unarchiveFromData and keyedUnarchivedFromData-- the first one does unkeyed archiving, the second one does keyed archiving, but not securely. And so, these are now not working in the way we like, so we're deprecating these two. And, replacing them with this new secure Unarchive FromDataTransformerName, which will do the unarchiving securely. So, we urge you to switch to this one as well. Now, on the secure coding front, we've also gone ahead and adopted secure coding in a number of AppKit classes that didn't do it yet. Note here, NSAppearance, which is a recent, relatively recent class that is increasingly becoming your friend, as you'll see in later talks about Dark Mode and other features we've added in AppKit. We've also added secure coding to a few foundation API's that did not have it. And, here is that list. Now, one more note on secure coding. We have a talk Thursday morning, "Data You Can Trust," where we'll talk about doing coding and archiving and unarchiving in a robust and secure fashion. So, I invite you to attend that. Thursday morning at 9. So, at this point, the rest of the talk is going to be about the new features in AppKit and related areas. And, to kick that off I invite Chris on stage. Thanks, Ali. So, Dark Mode is one of the most exciting new features in macOS 10.14. And, I'm sure you all saw yesterday, I bet some of you are running it right now. But, let's go ahead and take a look. So, we have these great new sets of system artwork. It makes our system UI look great. It makes your application look great. It's going to make your user content look great. And, what we all want to know is what we need to do to adopt this. So, the first step is really simple. We need to relink against the macOS 10.14 SDK. That's easy enough. That might be enough for some of us, too, but most of us are going to need to do a little bit more work to make an app that looks great. So, the next thing we're going to want to do, is we're going to search our application for places we've hardcoded color values. And, we're going to want to replace them with an appearance-sensitive color instead. For, most system UI elements, AppKit actually offers a lot of dynamic system colors that will react with the current appearance, and look great for whatever UI element you're trying to come across with. And, to flesh out the list, we've added even more in macOS 10.14. But, in some cases you're not trying to make a system UI element, you're trying to make some piece of your document model that also looks great in Dark Mode. Now, you can do this by sorting your colors in asset catalogs. So, if you go to the color editor in Xcode, you can configure which appearances you'd like to set up specific colors for, using the sidebar on the right. In this case, we've picked colors explicitly for the light appearance, for the dark appearance, and a general fallback color for any other appearances. Similar to with colors, we're going to want to go through our UI's and find places we can use template images. Template images are great because the image artwork will be tinted with the right color for whatever appearance we're using. And, you might have been skating by with places in your app where you included a dark gray piece of artwork, or solid black artwork, which looked fine in light mode, and is going to look absolutely wrong in Dark Mode. So, you can make template images programatically. You can also set them up in your asset catalog. But, you don't need to limit yourself to template images to make your Dark Mode UI. You can also specify colors-- or, sorry, images that look different in Dark Mode. In this case, for our planet app, we decided that we wanted nighttime view of North America more in Dark Mode, but for other appearances, we're going to use a daytime view. So, something that's really great about Dark Mode is how we handle desktop pictures. And, let me show you what I mean by this. If you take a look at the system preferences UI, it kind of looks like it's just a dark gray at first glance, but it's more complicated than that. If we look behind the window, we can see that we have these gorgeous sand dunes, and there's a lot of blues and light and dark grays in there. And, if we do the masked pick, an average color for this rectangle, we wind up with this nice dark blue color instead. So, when we construct our UI and add back in this gray color, it's not solid gray. We're keeping that dark blue color with us. And, this permeates even when we add back the individual controls in the window. They all have this nice character from the desktop picture. So, let me show you what this looks like with a different desktop picture, in this case, a flower. You can see we have these much brighter purples and greens in this desktop picture. And, that affects the system preferences window here. Likewise, if we used a red flower instead, you can really see in this case, how the System Preferences window is taking that wonderful warm character from the desktop picture and propagating it to all of the UI elements. So, a really common thing you might be wondering with this is, that sounds like a lot of work to dynamically figure out where a window is, what the average color is, and you know, update it live. So, my first advice to you is don't be daunted by this task. AppKit is going to help you. So, there's some great classes you're already familiar with that are just going to do the right thing out of the box. And, it's Window, and it's ScrollView, and it's TableView, and it's CollectionView. All of these will look great in Dark Mode without any changes. But, if you want to get your hands on them, you can also tweak these a little bit. Each of these classes has a background color property. And, there's four very special NS colors I want to mention, the control background color, the window background color, and the underpage and text background colors. And these all, when used with these classes, get that nice bit of desktop picture infusion, and all look slightly different depending on the role of the UI element you're trying to construct. One other class I really want to call out for this purpose is the NSBox class. If you configure a box as a custom style, you can use its fill color property with one of these special NS colors, or really any other NS color, too. But, that's significant, because NSBox can be used to just add wonderful color fills to pieces of your UI, whereas these other classes are a little bit more special case. So, if you really want to get into detail on this, there's another AppKit class I want to mention, which is NSVisualEffectView. And, NSVisualEffectView has this material property that allows you to determine how the visual effect you use is going to process the background behind it, and what sort of blending operations it's going to do. And, we have a few of these to describe where the visualEffectView's being used in your UI. In macOS 10.14, we've added a lot more. So, pretty much whatever sort of UI you're trying to construct, we should have a material that's appropriate for that use case. In previous OS's, you'll note we had some materials labeled explicitly as light or dark. And, you're going to want to stay away from those, as they're not going to look right across our many new appearances. So, that brings me to another topic, which is accent colors. If we go ahead and look at these UI elements, we can see there's this delightful splash of view, of color, in a lot of these elements. And, in macOS 10.14, we've added a number of new accent colors for users to select. And, all of these look absolutely great. But, if you're making your own UI elements-- I'll pause for applause. Thank you, accent colors. Anyway, if you're making your own UI elements, you might be trying to make this motif yourself, and incorporate that splash of color. So, if you've done that in the past, you've probably been using the NSColor.currentControlTint method, which returns this enumeration saying whether the system's running in aqua or graphite mode. So, we have way more colors than that now. That enumeration's not going to do the job. So, in macOS 10.14, we'd urge you to instead, switch to the controlAccentColor method on NSColor. So, NSColor doesn't stop helping you with accent colors. There's a number of other things it does. If you're making a UI element, one of the common features you're going to want to do is adjusting the color of that UI element to reflect user interaction with it. So, NSColor introduces a new method called .withSystemEffect. And, we've defined a number of system effects for interaction, like the pressed state or the disabled state, and we'll go ahead and apply a recipe to a base color to produce a new color that's appropriate for the current appearance as well as a sort of interaction being done with that control. So, this will save you the trouble of having to develop a formula yourself for modifying a color for these states. And, it'll also save you from cases where you might have a really long list of hard-coded colors for different interactions. So, it's a great API to make use of. We're going to talk about color for a bit more. In this case, a new feature of macOS 10.14, is what we call the content tint color. If you look at my mock application here, you can see it's mostly user content, it's mostly text. But, there's a few elements that I want to call attention to. These are things where the user can click on them to perform more actions. And, I didn't want to use the normal button borders because I felt that, kind of, overwhelmed the content. But, in macOS 10.14, we're going to let you tint borderless buttons and image views to call them out, so the user can still recognize these as clickable and interactable. So, that's really easy to do. NSButton and NSImageView, both have a new property called contentTintColor. You can set it to any color you want, to those dynamic colors I mentioned earlier are great candidates. You can also set these up in Interface Builder. So, this is what the UI looks like for configuring buttons. And, this is what it looks like for configuring image views. The tint option is here on the right, in the sidebar. So, we've covered a lot of great stuff about what you can do with the new appearance in macOS 10.14. We have more sessions on it, but they're in the WWDC app, if you look at our latest sessions. They're both absolutely great. Which brings me to my next topic. No discussion of Cocoa is complete without some talk of layer backing. So, I wanted to let you all know that in macOS 10.14, when you link against the new SDK, AppKit's not going to use a legacy window backing store any more. It's going to provide all of this content to the window server using core animation layers. And, a lot of you who do development on iOS are going to think this sounds really familiar to me. But, let's take a look at what actually goes on here. So, if we have a tree of views like this, in UIKit, the relationship between views and layers is really simple. Every view gets exactly one layer. And, the parent/child relationship between views is mirrored in the layer tree also. But in AppKit, we create the layer tree as a process of-- or as a side effect of processing the view hierarchy. So, we can wind up in cases where we might decide to take many views, and use a single layer for that. And, that's great because it can reduce system memory consumption, and GPU memory consumption, and also gives the window server a little less load to process when it's rendering the screen. Something I really want to point out here, though, is that this is dynamic based on the configuration of the view hierarchy. So, it can change moment to moment. So, you really can't rely on having this fixed parent/child relationship between views and layers like you might on iOS. So, programmatically one of the changes you no longer have to care about here is that you don't have to explicitly set .wantsLayer on your views to use layers anymore. AppKit will take care of this for you when you're deploying against macOS 10.14. If you're deploying against-- In fact, we generally encourage you not to even use this property, because if you set it explicitly to true, we're going to make sure your view gets its own layer, and we're not going to do the optimizations we can do, where we render multiple views into a single layer. You might also need to still use this if you're deploying to earlier OS's, but usually you can still get away with ignoring it. I wanted to talk about some other patterns you might have in NSView UI's you are making that use CALayers. So, one of the easiest ways to draw a CALayer, is to just override the draw method in the CALayer class. Or, implement a delegate method. And, this is mostly fine, but NSView actually gives you a lot of functionality you probably don't want to have to replicate yourself. If you use NSView's draw method, it'll go ahead and take care of things like making sure that the appearance works correctly for dynamic colors. It'll manage the backing store resolution for you. And, it's really just as simple as implementing the layer methods. So, I really encourage you to override drawing at the view level instead of the layer level. Sometimes you'll have cases where you were implementing the display method of CALayer instead, and you're updating layer properties directly, because maybe it's more efficient, or really expresses what you're trying to accomplish better. You can still do that using the NSView API by overriding the update layer method, and you get all the same benefits you do by using the NSView draw rect method. A quirk I want to point out, is you can implement both update layer, and the draw methods on NSView. If you do this, when your view has a single layer backing it, we'll go ahead and use the optimal layer version. And, if you're being merged with other views to save memory, we'll go ahead and use the draw rect version. And, we also use that for things like printing. So, it's fine to implement both of these. If you have a view that you really can't express using the CG drawing API's, or the AppKit drawing API's, you can, in addition to the update layer method, override wantsUpdateLayer, and if you just return "true" from that, we know that you need an explicit layer to do what you want to accomplish. There's another way of taking best advantage of AppKit and core animations features here, and that's just to build your UI's out of a very simple vocabulary of basic NSViews. NSImageView, NSBox, and NSTextField, these are all really great building blocks to make complicated UI's, and they'll do the right thing no matter what technologies we pick to actually render to the screen. With our changes to layer backing, there's a few patterns I want to call out that aren't going to work in macOS 10.14 anymore. If you're using NSView lockFocus and unlockFocus, or trying to access the window's graphics contents directly, there's a better way of doing that. You should just subclass NSView and implement draw rect. Both of those methods have been kind of finicky for a while. So, you'll be saving yourself some trouble. The other thing I want to point out is I've actually written these in Objective-C, which is a little weird for a talk that's mostly in Swift. And, the really great news about this is I've never actually seen any Swift code using these. The takeaway from that is I really don't want any of you to be the first to go ahead and surprise me. So, we have one more thing about our changes with layer backing. If you're using NSOpenGL classes to render with OpenGL, and you link against macOS 10.14, some of our implementation details for how we bind the OpenGL system to our layers are a bit different. And, you may notice a few small changes there. But, more importantly, I want to call out that as of macOS 10.14, OpenGL on our platform is deprecated. If you've been using NSOpenGL view, we really encourage you to adopt MTKView instead. And, there's a great session coming up later today about adopting Metal for OpenGL developers. There's one last change I want to talk about, which is a change we've made to font antialiasing. If you go ahead and look at this screen comparison, I have macOS 10.13 on the left, and macOS 10.14 on the right. And, if you look at the text in this window, it's basically identical. But, if we zoom in, all the way to a 48X scale factor, we can see that macOS 10.13 is using this color-fringing effect for its font rendering. In macOS 10.14, we no longer use that effect. And, this means our text looks great on a much wider variety of panel technologies, as well as scaling modes. So, we have a bunch of other great things to cover. And, at this point I'd like to invite Jesse onstage to go over those. Thanks, Chris. Hi everyone. It's great to see you here today. I have a bunch of topics to cover. And, I'd like to start with the user notifications framework. This has been available in iOS for some time now, and with macOS Mojave, we're bringing it to the Mac. This allows for better control of user notifications. And, it also means that your apps can interact with them the same way that they do on iOS. They should do that using the NSApplication method, registerForRemoteNotifications, as well as the requestAuthorization method on userNotificationCenter. As a part of this work, we're also deprecating some existing user notification-related API's. Specifically, in NSApplication, we're deprecating the remoteNotificationType OptionSet, as well as the registerForRemoteNotifications method and the enabledRemoteNotificationTypes property. We're also deprecating all of NSUserNotification. So, as you rebuild with the new SDK, you should try to update to the user notifications framework. Next, I'd like to talk a little bit about NSToolbar. When you wanted to center an item in the toolbar, you've maybe been tempted to put a flexible space on both sides of your item. And, this works, but it has some drawbacks. Notably, when you add extra items to the toolbar, it'll push your item off-center. So, NSToolbar now exposes a new property, the centeredItemIdentifier. You can set this to the identifier of an item you'd like to remain centered, and NSToolbar will put it there. It should stay there unless other Toolbar items actually force it to be out of place. There's another change here worth noting as well, which is that auto layout is now used to measure toolbar items when the minimum and maximum sizes are not specified. This applies only to apps on the 10.14 SDK, but it means that you can do things like change the size of the button, and the measurement will happen for you. The centeredItemIdentifier behavior is also available through Interface Builder. So, here's the inspector pane for a Toolbar item. You can see there's a new checkbox at the bottom, "Is Centered Item." You can click this instead of setting the property from your code, and so there's no need to fall back to the programmatic API. You can continue to do all your UI work inside Interface Builder. And, speaking of Interface Builder, I can't tell you how excited I am about Interface Builder's new support for editing, NSGridViews. If you're not familiar with gridView, we introduced it a couple of years ago, and it's a layout primitive for rendering your views in a grid-like pattern. This is an example from a keychain access app. And, you can imagine how many little constraints would be necessary to create this layout by hand. You could also build it with stackViews, but NSGridView makes the whole thing much easier, and the new editing support in Interface Builder is just fantastic. Let me show it to you. So, here's some UI from a storyboard file. You can select these controls, and embed them in a grid view. And, once you've done that, you can go through and adjust the padding and the alignment of the cells in order to achieve the layout that you want. The editing UI works a lot like the numbered spreadsheet app. So, you can drag and drop views into cells. You can select cells in rows and columns, and adjust their properties. You can even merge cells, as you see in the bottom two rows here. Here's an example where we select a column. And, this is what the inspector pane looks like, so you can see you can adjust the placement of the cells in that column. You can adjust the leading and trailing padding. If we switch over to the Size Inspector, you can specify an explicit width for the column. Or, if you don't do that, then the column will be sized automatically based on the content. And, one of the other really nice things about this feature is that it's backwards-deployable. GridViews authored in Interface Builder can be used back to macOS 10.13.4, or if you're not using merged cells, you can actually go all the way back to 10.12. So, if you need to deploy your app to an older version of macOS, there's still no reason to wait to use this great new functionality. The next topic I'd like to cover is some changes to NSTextView. First off,there's a few new factory methods. The first one here, fieldEditor configures a textView to act as the fieldEditor for an NSTextField. These all provide a much easier way to configure textViews for common use cases. The latter three provide textViews wrapped in scrollViews. This is by far the most common use case for a textView, but if you have to do additional configuration on the textView, it's important to remember to look at the scrollView's documentView. These are also available through Interface Builder. So, again, there's no need to fall back to the programmatic API here. So, let's see what they look like. Here's a sample window that shows all four. TextViews are sometimes misconfigured when clients need to override the fieldEditor in a textField. And so, using a fieldEditor factory method can help avoid problems there. The next one, scrollableTextView should be used for textViews that are for auxiliary text in popovers and inspector panes. Things like that. And then, the bottom two are for text that's main document content. The one on the left is for rich text; the one on the right is for plain text. You might be wondering at this point what the distinction is, because they all look fairly similar. The main benefit is that you don't need to worry about the system configuration. For example, if the system's in Dark Mode, they begin to look more distinct. The rich text's textView retains its white background, and the plain text turns dark to match the rest of the system for example. So, in general if you use these factory methods, it'll help keep your application consistent with the specifications for the rest of the system. The other change to textView that I'd like to talk about is a new method for modifying the text, PerformValidatedReplacement. The idea behind this method is that it gives you a very easy way to manipulate the text in the textView, and it gives you behavior as if the user had performed the change themselves. So, it performs all the appropriate delegate methods, as you'd expect. But, the really interesting part is that any attributes that are not specified on the input string are automatically filled in using the textView's typingAttributes. So, let me give you an example. Here's a window with some rich text in it, and a little snippet of code that calls performValidatedReplacement to insert the word "Developers" in the middle. If we run this, this is what we get. The word appears and it matches the style of the surrounding text, and we didn't have to specify any attributes. There's a subtlety here to be aware of, though. And, that's because the fallback attributes come from the typingAttributes. So, if you start with some rich text like this, and the insertion point is in the lighter portion at the end, and we run the same code; this is the result. The style attributes come from the lighter portion at the end. So, for this reason, you may find that you need to set the selective range for the range you're about to replace before you call performValidatedReplacement. If you do that, this is the result that you get. So, the next topic I'd like to cover very briefly, is continuity camera. This is another fantastic feature in macOS Mojave. And, if you're just using the standard framework classes like NSTextView, there's nothing special you need to do in order to take advantage of it. So, framework will handle everything for you. But, if you have a more application-specific need for this, it is possible to use it more directly. And then, it's important to understand that it's implemented using the existing services API's. So, all you need to do is tell a framework that your responder class is able to handle image data. And, you can do this by implementing validRequestor. If you want to try this out, I'd encourage you to check out the documentation for validRequestor and some of the related methods. Next, I'd like to talk about custom Quick Actions. You heard a little bit about Quick Actions from the State of the Union session yesterday. And, they make it very easy to perform simple actions like opening a favorite app, for complex ones like filtering files, or invoking scripts. You can build custom Quick Actions using app extensions or action bundles from Automator. They're useful in so many different places that there's a variety of ways to invoke them. But, my favorite by far is the Touch Bar. If you put your Quick Actions in the Touch Bar, it just makes it very easy to get to them, wherever you are, whenever you need them. And, you can get this behavior by looking in the keyboard preferences panel, and reconfiguring your Touch Bar to either always show them, or to flip to them when you hold down the function key. Or, you can customize your Touch Bar and drag the workflows button into your control script. It's also worth noting that you can go over to the Shortcuts pane, and look under Services. And, here you can turn them on and off to control which ones show up. They don't only show up in the Touch Bar, though, as I mentioned. So, here's a Finder window for example. And, the contextual menu has a Quick Actions submenu where you'll see them. Finder's preview pane also has a couple of Quick Actions at the bottom, and then the full list underneath the "More" button. And, action bundles from Automator will show up inside the Services menu. So, TrimLogs is one that I wrote to filter my debug logs, for example. And, that brings me to the next topic I'd like to talk about, building action bundles, also known as contextual workflows. This is a new feature in Automator. When you go to Automator, and create a new document, there's a new option available now for contextual workflows. They look a lot like regular workflows except there's a new block at the top that allows you to configure the input and output as well as pick an icon and the color. So, let's go though a quick example. I often have a problem where there's some file I want to open in TextEdit, but I can't because it doesn't have a file extension. This is super easy to fix with Automator. All you need to do is look inside the library, and drag out the Open Finder Items action. You can configure it to open the items with TextEdit instead of with a default application. Any file selected in the Finder automatically become the input to this action, and then if you save it with some name, it will automatically show up in the Touch Bar, or in other contexts where it's useful. So, to summarize, we've talked about a variety of new features, and other changes that will make your development experience richer, and your applications even more awesome. Check out the new SDK and begin implementing some of these things in your applications. They'll make your applications shine, and your customers will appreciate it. For more information, you can follow this URL. And, also look inside the WWDC app under this session. All of the related sessions are linked there. Thank you very much. 