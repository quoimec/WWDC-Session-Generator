 Hi, everybody. It's great to see you all here today. I'm Dave, and the next 40 minutes are about understanding and honoring what makes our programs actually work. There will be practical advice, but this is not a talk about tips and techniques or any specific algorithms even, though we will look at a few. It's about revealing something fundamental, the potential of which is already present in your code. I hope that for at least a few of you it marks the beginning of a new relationship to the practice of programming. Speaking personally, when I discovered this approach, it changed the course of my life and my career. It's the reason I care so much about software libraries, but it's also the source of the liability, maintainability, and performance in every piece of concrete code I've written since. But before we get into that, let me introduce you to a friend of mine. This is Crusty. Crusty is old school. He doesn't trust debuggers or or mess around with integrated development environments. No, he favors an 80 x 24 terminal window in plain text, thank you very much. Now, Crusty takes a dim view of the latest programming fads, so it can sometimes be an effort to drag him into the 21st century. He just thinks different. But if you listen carefully, you can learn a thing or two. Now, sometimes his cryptic pronouncements like "programming reveals the real; border on the mystical." And to understand him, I found it helpful to actually write some code. So, lately, I've been working on a little program called Shapes. I'm hoping to make it into a full-featured vector drawing program, but so far it lets you arrange shapes on an infinite canvas. Now, I want to tell you the story of the delete selection command because I learned so much from implementing this one feature. I think we've probably all gone through part of this progression as programmers as we learn how to remove things from an array. Everybody starts out doing something like this. That's the delete selection command. We loop from 0 to count, and when we find something to delete, we [inaudible] remove that, and then we continue with our loop until, ouch, we walk off the end. The array got shorter, but we picked the number of iterations when the loop started. Fortunately, you can't miss this bug if you Swift and test your code because it'll trap. But if you had to learn this lesson as a C-programmer, like I did, you might not be so lucky. Okay. So, we can fix it by replacing the for loop with a somewhat uglier while loop, which lets us examine the count at each iteration. But there's a subtle bug in this one too. If two consecutive elements are selected, it'll remove the first one and then immediately hop over the next one. Now, this bug is a little more insidious because it hides from you unless your tests happen to exercise it. But if we're lucky enough to notice it, we press ahead and we fix the implementation again by guarding the increment in an else block. So, are we done now? Are we sure this one is correct? I think I can prove to myself that it works. Anyway, having gone through this ordeal, what do we do? Well, of course, we lock this nine-line pattern into our brains so we can trot it out whenever we have to delete something. Now, I'm sure many of you have been holding yourselves back from screaming at me because that there's a much more elegant way to do it. I still remember the day I discovered this trick for myself because once you find it, you never do this nine-line dance again. The iteration limit and the index of the next item to examine kept shifting under our feet because remove(at: i) changes parts of the array after i. But if you go backwards, you only iterate over the parts of the array that you haven't changed yet. Slick, right? And this is the pattern I've used ever since because it's clean, and it never fails, until a few months ago. One morning, I had just finished my avocado toast, and I was idly fooling with my app when I tried deleting about half the shapes from a really complicated canvas. My iPad, it froze up for like three seconds. So, I took a sip of my half caf triple shot latte in its bamboo thermal commuter cup, and I considered my options. This was disturbing. I mean, it's a pretty straightforward operation, and my code was so clean, how could it be wrong? Profiling showed me that the hot spot was right here, but beyond that, I was stumped. So, just about then, Crusty walked up behind me carrying a can of off-brand coffee grounds from the local supermarket for his daily brew. "Stuck?" he said. "Yeah," I sighed, and I explained the situation. "Well, did you even look at the documentation for that?" Well, I hadn't, so I popped up the Quick Help for Remove At, and Crusty leaned in. "There's your problem, right there," he said, leaving a smudge on my gorgeous retina display. Now, I carefully wiped off the fingerprint with a handcrafted Italian microfiber cloth, and Crusty said, "What's that tell you, son?" "Well," I said, "it means that removing an element takes a number of steps proportional to the length of the array." And it kind of makes sense, since the array has to slide all of the following elements into their new positions. "So, what's that mean about your Delete Selection command?" he asked. "Uh," I said. That's when he pulled out a pack of mentholated lozenges and lined them up on my desk. "Try it yourself." So, I went through the process as I tried to answer his question. "Well, since Delete Selection has to do order n steps, once for each selected element, and you can select up to n elements, the total number of steps is proportional to n squared." Crusty added, "That's quadratic, son, whether you do it the ugly forward way or the fancy pants backward way." I realized then that for my little 10 to 20-element test cases, we'd only been talking about a few hundred steps, and since the steps are pretty fast, it seemed great. But the problem is that it doesn't scale well. Fifty squared is 2,500 and 100 squared is 10,000. So, if you do all your testing in this little region down here, you'll probably never see it, but scalability matters because people are using their phones and iPads to manage more and more data, and we keep shipping devices with more memory to help them do it. You care about this because scalability is predictability for your users. So, now, I understood the problem, but I still wasn't sure what to do about it. "Now what?" I asked Crusty. "You know, kid," he said, popping a lozenge, "there is an algorithm for that." I told him, "Listen, Crusty, I'm an app developer. You say you don't do object oriented. Well, I don't 'do' algorithms." You pay attention to your data structures in algorithms class because you know when it's time to get a job, your interviewer is going to ask you about them. But in the real programming world, what separates the elite from the newbies is the ability to wire together controllers, delegates, and responders to build a working system. "Bonkey," he said; I don't know why he calls me that, "What do computers do?" "They compute." "Now, where's the computation in all that?" "Well," I replied, "I guess I don't see anything that looks like an algorithm in my code." But Crusty wasn't having it. "Oh, your app is full of them," he said, dropping an old dead tree dictionary on my desk, "Look it up." After I gathered my composure, I carefully slid the book to one side and typed Define Algorithm into Spotlight, which Crusty thought was a neat trick. Hmm. A process or set of rules to be followed in calculations or other problem solving operations. Well, come to think of it, that did sound like most code, but I still wasn't sure. "You ever do a long division?" asked Crusty. "That's an algorithm." I started to type into Spotlight again, but he snapped, "On paper." And not wanting to embarrass myself, I turned the subject back to my code. "Hmm." I asked, "So, what is this magic algorithm that will cure my performance problem?" "Well, if you'll just let me at your TTY there for a minute," he said, "How do you work this thing? Oh, it's a track pad. I'll try not to touch that. So, first, you do away with this foolishness. Now, shapes.removeallwhere the shape is selected. Hmm. Try that on for size." Now, Crusty headed off to wash out the pitcher of his coffee maker, leaving me to figure out what had just happened in my code. First, I checked, and I found that the performance problem was fixed. Nice. And I didn't want another earful from Crusty about looking at the documentation, so I popped up Quick Help for removeAll(where: and I saw that its complexity scales proportionally to the length of the collection just like removeAt. But since I didn't have to put it in a loop, that became the complexity of my whole operation. Now, I want to give you some intuition for the kind of difference this makes. What are n means that the time the algorithm runs scales linearly with the problem size. The graph is a straight line. Now, the orange line is the shape of order n squared. As you can see, a linear algorithm may do worse on a small problem, but it's eventually faster than a quadratic algorithm. The cool thing is though, it doesn't matter how expensive you make the steps of the linear algorithm, if you keep looking at larger problem sizes, you'll always find one where the linear algorithm wins and continues to win forever. So, we're talking about scalability, not absolute performance. Well, my scalability problem was fixed, but I really wanted to see how the standard library had improved on my backward deletion scheme. Crusty reminded me that Swift is open source, so I could pull it up on what he calls "the hipster web." But the rest of us know as GitHub. Now, the first thing I noticed was the dot comment, which was the source of all that Quick Help, describing both what the algorithm does and its complexity. Next, it turns out that removeAll(where) isn't just some method on a reg; it's a generic algorithm, which means that it works on a wide variety of different collections. It depends on a couple of things, the ability to rearrange elements, which comes from mutable collection, and the ability to change the length and structure, which comes from range-replaceable collection. And it's built from a couple of other order n algorithms. The first is a half stable partition, which moves all of the elements satisfying some predicate to the end and tells us where that suffix starts. The half stable in its name, that comes, that indicates that it preserves the order of the elements that it doesn't move, but it can scramble the elements that it moves to the end. Now, sometimes, that doesn't matter though; the second algorithm removes subrange. It's just going to delete them anyway. Have we all seen this partial range notation? It's just a really convenient way of writing a range that extends to the end of the collection. Okay. Now, remove subrange is part of the library's public API, so you can find its documentation online, but halfStablePartition is an implementation detail. Now, we're not going to step through all of it, but there are a few things worth noticing here. First, it starts by calling yet another algorithm, firstIndex(where), to find the position of the first element that belongs in the suffix. Next, it sets up a loop variable j, and there's a single loop, and the loop index j moves forward one on each iteration. So, it's a sure bet that j makes just one pass over the elements. You can almost see the order and complexity just from that. Lastly, because this method needs to rearrange elements but not change the length or structure of the collection, it only depends on mutable collection conformance. So, this is the first lesson I learned. Become familiar with what's in the Swift Standard Library. It contains a suite of algorithms with documented meanings and performance characteristics. And although we happened to peek at the implementation, and you can learn a lot that way, it's designed so that you do not have to. The official documentation should tell you everything you need to know in order to use the library effectively. You'll even find a playground tutorial there. Now, I realize there's a lot in Swift, so it can look daunting, but you don't need to remember everything. Having an idea of what's there and how to find it will take you a long way. Now, before we move on, I want to point out something else that happened in my code when Crusty made this change. Which one of these most directly describes its meaning? Now, I actually have to read and think through the first one to know what it's doing. Hmm. Maybe I'd better add a comment. Okay. How does that look? Oh, even with that comment, the reverse iteration is kind of tricky, and I don't want somebody breaking this code because they don't understand it. So, I'd better explain that. Okay. While we're clarifying things, the After code actually reads better with a trailing closure syntax. Now, take a breath and look again. Which one is more obviously correct? Even with all these comments, I still need to read through the first one just to see that it does inefficiently the same thing as the second one. Using the algorithm just made the code better in every way. So, this is a guideline, an aspiration for your code first put forward by Shawn Perin. Every time you write a loop, replace it with a call to an algorithm. If you can't find one, make that algorithm yourself and move the loop into its implementation. Actually following this might seem unrealistic to you now, but by the end of the talk, I hope it won't. For a little motivation though, think back to the last time you were looking at spaghetti code. Was it full of loops? I bet it was. All right. Done and done. I have just made the code shorter, faster, and better in every way. I was ready to call it a day. "Thanks for your help, Crusty," I said, fastening the titanium carabiners on my bespoke leather messenger bag, but he looked at me suspiciously, and said, "You suppose you might have made that mistake anywhere else?" I sighed, put my bag down and started hunting down the loops in my code. There were lots in the file [inaudible]. Bring to front, send it back, bring forward, which sort of hops the selected shape over the one in front of it. Let's do that a couple of more times. Send backward with hops, under the shape, below the selected shape, and last of all, dragging in the shape list at the left. Now, these sound really simple until you realize that they all need to operate on multiple selected shapes that might not even be contiguous in the list. So, it turns out that the behavior that makes sense is to leave all of the selected elements next to each other after the operation is finished. So, when you're bringing shapes forward, you hop the front most selected shape in front of its neighbor, and then you group all the other ones behind it. And when you send shapes backward, you hop the bottom most selected shape backward behind its neighbor and group the other ones in front of it. So, if you didn't follow that completely, don't worry. We'll come back to it. But suffice it to say that I had some pretty carefully worked out code to get all of these details right. For example, this was bringToFront, and when I looked at it, sure enough, there was an order n loop over the shapes containing two order n operations, remove(at:) and insert(at:), which makes this, you guessed it, n squared. In fact, that same problem showed up in every one of my other four commands. All of the implementations here loop over an array, performing insertions and removals, which means they're all quadratic. Now, I was kind of discouraged at this point, so I asked Crusty if he'd look at them with me. He said, "Can't stay too late," he said, "I got my ballroom dancing tonight, but I guess we better get a move on." So, I pulled up bringToFront, and Crusty's first question was, "What does it actually do?" "Well," I said, "there's a while loop and j tracks the insertion point and i tracks the element we're looking at." "In words, not in codes," said Crusty. "Describe it." "Okay. Let's see. It moves the selected shapes to the front, maintaining their relative order." "Write that down in a dot comment and read it back to me." I'm a superfast typist. "Moves the selected shapes to the front, maintaining their relative order." "Sound familiar?" said Crusty. That's when I realized that this was a lot like half stable partition, except it's fully stable. I was starting to get excited. And what do you think this one is called? So, I had to guess, "Stable partition?" "That's right. One of my old favorites, and you can find an implementation in this here file from the Swift Open Source project." So, I pulled the file into my project, while Crusty mumbled something about how that comment we added should have been there all along, and I started coding, getting about this far before I had a problem. You see, stable partition takes a predicate that says whether to move an element into the suffix of the collection. So, I had to express bring to front in terms of moving things to the back. I looked at Crusty. "Visualize it," he said. So, I closed my eyes and watched as the unselected shapes gathered at the back, which gave me my answer. Now, I guess sendToBack was even easier because we just need to invert the predicate. We're sending the selected ones to the back. Now, I was just about to attack the bring forward command, and I figured that Crusty would be as eager to move on as I was, given his plans for the evening, but he stopped me. "Hold your horses, Snuffy. I don't want to miss the opening tango, but aren't you going to check whether it'll scale?" He had a point, so I popped up the Quick Help for stable partition, and I saw that it had (n log n) complexity. So, for a way to think about (n log n) take a look at log n. It starts to level off really quickly, and the bigger it gets, the slower it grows, and the closer it comes to being a constant. So, when you multiply that by n, you get something that doesn't scale quite as well as order n but that gets closer and closer to linear as it grows. So, order n login is rightly often treated as being almost the same as order n. I was pretty happy with that. So, we moved on to bring forward. Now, as we said earlier, bring forward bumps the front most selected shape forward by one position and gathers the other selected shapes behind it. But Crusty didn't like that way of thinking about it at all. "That little do-si-do at the beginning is making a line dance look like a Fox Trot." he said, "You don't need it." When I looked at him blankly, he broke out the lozenges again, and with five of his free digits, he executed the bringForward command. "See it again?" he asked. I felt a little like a mark in a Three-card Monte, but I played along. "Look familiar?" "No." He threw a hanky over the first few. "How about now?" That's when I realized it was just another stable partition. Okay. I got this, I thought. If we find the front most selected shape, then move to its predecessor, and isolate the section of the array that starts there, we could just partition that. "But how do you modify just part of a collection?" I asked Crusty. "Ain't you never heard of a slice?" he said, taking over the keyboard. "Shapes, starting with predecessor. There. Stick that in your algorithm and mutate it." Which I promptly did. So, human knowledge about how to compute things correctly and efficiently predates computers by thousands of years, going back at least to ancient Egypt, and since the invention of computers, there's been an explosion of work in this area. If the standard library doesn't have what you need, what you do need is probably out there with tests, with documentation, often with a proof of correctness. Learn how to search the web for research that applies to your problem domain. Okay. Back to the code. I was intrigued by this slice thing, and when I checked out its type, I saw that it wasn't an array. Since we had used stable partition on an array and bringToFront and sendToBack, and now we were using it on an array slice, I guessed out loud that it must be generic. "Of course, it is. What's stable partition got to do with the specifics of array?" "That's right, nothing. Speaking of which, Blonkey, just what does bringForward have to do with shapes and selection?" "Well," I said, "it works on shapes, and it brings forward the selected ones." "That's right, nothing," he went on without listening. "Can you run bring forward on a row of lozenges? Of course, you can. So, it's got nothing to do with shapes." Hmm. "Are you suggesting we make it generic?" I asked. "Isn't that premature generalization?" Answering a question with a question, Crusty replied, "Well, just how do you plan to test this method?" "Okay," I said, "I'll create a canvas. I'll add a bunch of random shapes. I'll select some of them, and then finally..." But I didn't even finish the sentence because I knew it was a bad idea. If I did all of that, would I really be testing my function, or would I be testing the initializers of Canvas and various shapes, the addShape method, the isSelected property of the various shapes if it's computed? I have to build test cases for sure, but ideally, that code shouldn't depend on other code that I also have to test. If I can bring the lozenges forward, it should be possible to do something casual with [inaudible] in a playground, like this, which brings forward the number divisible by 3. Now, at the other end of the spectrum, I should be able to throw vast quantities of randomly generated test data at it and make sure that the algorithm scales. Neither of those was going to be easy as long as the code was tied to Canvas and shapes. And so, I admitted to Crusty that he was right and started making this non-generic bringForward into a generic one. The first step was to decouple it from Canvas and move it to arrays of shapes. Of course, this array is the shape, so I had to replace shapes with self, and then I decoupled it from selection by passing a predicate indicating whether a given shape should be brought forward. And everything kept compiling. Awesome! At that point, I was pleased to find there were no dependencies left on shape, and I could just delete the where clause. Pretty cool, I thought. Now, I can bring forward on any array. I looked over at Crusty, who had been quietly practicing the cha-cha-cha in the corner, but he didn't seem to think I was finished. "What does bring forward have to do with arrays?" he asked. "Well, nothing," I sighed and started thinking about how to remove this dependency. Let's see, there's a stable partition here, which requires mutable collection conformance. So, maybe I'll just move it to mutable collection. Hmm. I thought, clearly the index type doesn't match Int. Okay. So, there's a simple fix for this, right? Have you done this? Don't do this. It compiled, but Crusty had suddenly stopped dancing, and I knew something was wrong. "What?" I said. "Rookies always do that," said Crusty, shaking his head. "First, you got that comparison with 0, which is going to be wrong for array slice. So, did you know that array slices, their indices don't start at 0? The indices of all slices start with the corresponding index in the underlying collection that they were sliced from. That relationship is critical if you want to be able to compose generic algorithms using slices, so it's really important." Well, this problem, I knew to fix. Just compare it with start index. But the real problem is "What does bringForward have to do with having integer indices?" I interrupted. "Yeah, I know." "Well, I do have to get the index before i, which I can do with subtraction." Crusty just sighed and broke out the lozenges again. And then, he laid two fingers down on the desk and walked them across until his right hand was pointing at the first green lozenge. "Not a backwards step in the whole dance," he said. And I realized that Crusty had just shown me a new algorithm. Let's call that one indexBeforeFirst. "Now, the trick here is to keep your focus by assuming somebody already wrote it for you." And he proceeded to hack away all the code that we didn't need, like that and that. "Predecessor is the index before the first one where the predicate is satisfied. Now, just look how pretty that reads." Now, if you look at it, you can see that he's right. Taking away all the irrelevant detail about shapes, selections, arrays, and integers had left me with clearer code because it only traffics in the essentials of the problem. "I already showed you how indexBeforeFirst works. See if you could write it," he said. So, I think I was starting to catch on now because I got it right the first time. I told you, I'm a superfast typist. All right. So, return the first index whose successor matches the predicate. I was pretty excited about how well this was going. "All right, Crusty," I said, "let's do the next one." "Ain't you forgetting something, Bonkey?" he asked. I didn't know what he was talking about. The code was clean, and it worked. "Semantics, son. How am I supposed to use these from other code if I don't know what they mean?" And that's when I realized every time we'd use the new algorithm, we had leaned on his documentation to draw conclusions about the meaning and efficiency of our own code. And because most algorithms are built out of other algorithms, they lean on the same thing. Recently, I was interviewing a perspective intern and asked him about the role of documentation, and he began with a phrase I won't forget. "Oh, it's incredibly important," he said. "We're building these towers of abstraction," and now I'm paraphrasing, "the reason we can build without constantly inspecting the layers below us is that the parts that we build on are documented." Now, as an app developer, you are working at the very top of a tower that stretches through your system frameworks, DOS, and into the hardware, which rests on the laws of physics. But as soon as you call one of your own methods, it becomes part of your foundation; so, document your code. The intern was hired, by the way, and he's sitting right there. So, I took the hint and documented Crusty's new algorithm, which meant that we could forget about the implementation and just use it, knowing that Quick Help had everything that we needed. Now, I also documented bringForward. Cool! Now, because it seemed to be solving all of my problems, at this point, I was really curious to see what was going on inside stablePartition. It turns out I was well rewarded because it's a really beautiful and instructive algorithm. The public method gets the collections count and passes it on to this helper, which uses a divide and conquer strategy. So, first, it takes care of the base cases when the count is less than 2, we're done. We just need to figure out whether the partition point is at the beginning or at the end of the collection. Next, we divide the collection in two. Now, at this point, you have to temporarily take it on faith that the algorithm works because we're going to stable partition the left half and the right half. Now, if you take a look at these two ends, you can see that everything is exactly in the right place, but this center section has two parts that need to be exchanged. Now, they won't always have the same length as they do in this example, and fortunately there's an algorithm for that. We call it rotate. Okay. I'm not going to go into rotate here, but it's actually quite beautiful, and if you're interested, you can find its implementation in the same file as stable partitions. Okay. Back to shapes. Now, this mess implemented the dragging in the shapes list, and it had always been one of my most complicated and buggy operations. The strategy had been to allocate the temporary buffer; then, loop over the shapes before the insertion point, extracting the selected ones and adjusting the insertion point, and then separately loop over the rest of the shapes without adjusting the insertion point, extracting the selected ones. And then, finally, reinsert them. Honestly, I was a little scared to touch the code because I thought I finally had it right. It had been almost a week since the last time I discovered a new bug. But I had gotten pretty good at this process now, so I tried visualizing the operation happening all at once. Hey, that looks familiar. Let's see it again. Hmm. Suppose we do this first, and then take care of the parts separately. That's right. This is just two stable partitions with inverted predicates. So, the generic algorithm collapsed down to this two-liner, and here's what's left in canvas. Now, let's just see that next to the old code. That's not bad, but we got a reusable efficient documented general purpose algorithm out of it, which is spectacular. Okay. So, this is a learned skill to look past the details that are specific to your application domain, see what your code is doing fundamentally, and capture that in reusable generic code. It takes practice. So, why bother? Well, the practical answer is that because they're decoupled from those irrelevant details, generic algorithms are more reusable, testable, and even clearer than their non-generic counterparts. But I also think it's just deeply rewarding for anyone that really loves programming. It's a search for truth and beauty but not some abstract untouchable truth because the constraints of actual hardware keep you honest. As Crusty likes to say, "Programming reveals the real." So, treat your computation as a first class citizen with all the rights and obligations that you take seriously for types and application architecture. Identify it, give it a name, unit test it, and document its semantics and performance. Now, I want to close by putting this advice you saw from Shawn Perin in its full context. "If you want to improve the code quality in your organization, replace all of your coding standards with one goal, No Raw Loops." Thank you.  Welcome to designing web content for watchOS. My name is Wenson and I'm on the WebKit team. In watchOS 5, we're introducing the ability to render rich HTML content on Apple Watch using WebKit. Before we explore how it works, let's go over a couple of the ways WebKit enriches the user experience on watchOS. Previously in mail, rich HTML mail messages would be rendered in a text-only format requiring the user to go to another device for the full experience. New in watchOS 5, the full HTML mail message can now render on Apple Watch in cases where text-only formatting cannot express the full content of the message. When tapping links in mail or messages, we'd previously direct the user to iPhone in order to view the web page. Also new in watchOS 5, the user can now tap links to view and interact with the web page directly on Apple Watch. In this session, we'll explore the techniques WebKit uses to adapt existing content to watchOS and introduce a new mechanism you can use to optimize your content for display on Apple Watch. We'll also cover some other important details you need to know to ensure that users viewing and interacting with your content will have a great experience. I'd like to start by discussing how WebKit works on Apple Watch. Out of the box, WebKit supports the gestures you're already familiar with. Turning the digital crown scrolls vertically and you can also drag your finger on the screen to scroll the page. Double tapping zooms into the page and a subsequent double tab zooms back out. Lastly, a firm press reveals back and forward buttons, but you can also swipe forwards or backwards to navigate through your history. It's important to note that we optimized WebKit for quickly consuming content. Therefore, a limited set of features are not supported at this time. These include video playback, service workers, and web fonts. Now let's see how WebKit lays out web pages on Apple Watch. Most responsive content is already well proportioned when laid out at 320 CSS pixels, the width of iPhone SE. We ensure that it's also well-proportioned on Apple Watch by laying out at this width and then computing the initial scale of the page such that the content width fits within the viewport. This means that text and images may appear smaller but the overall layout of the page is preserved. When this technique is used, the viewport meta-tags initial scale is ignored in favor of an initial scale that contains the entire content of the page. Additionally, inner width is inflated to a minimum of 320 CSS pixels. And when using media queries, the device width will also appear to be 320 CSS pixels rather than the true width of Apple Watch. By shrinking to fit, we also avoid horizontal scrolling on pages where content is wider than the viewport. These heuristics adapt existing content to Apple Watch, but you can override them when designing content tailored for this form factor. Let's look at an example. I've written a web page that contains a gallery of photos I took from a recent trip to Vietnam, and I'd like to share it with my friends. If I send them a link to this page using messages, they can view it on Apple Watch. Right now it's laying out at a width of 320 CSS pixels and shrinking down to fit. However, I've also written this page to be responsive on Apple Watch by using a media query to limit my two-column layout only to viewports larger than 320 CSS pixels. Now all I need to do is tell WebKit that this page is already optimized for Apple Watch and doesn't need the default adaptations. To do this, I simply include a new meta-tag in the head of my document with name equal to disabled-adaptations and content equal to watch. I'm using this new disabled adaptations meta-tag along side the existing viewport meta-tag I'm already using to ensure responsive layout on iPhone and iPad. This allows WebKit to treat the viewport's device width as the real width of Apple Watch. Let's now switch gears and talk about some best practices for form controls. WebKit supports interaction with form controls out-of-the-box. To ensure a great experience for users when designing form controls and inputs, it's important to keep a couple of things in mind. First, choose the appropriate type attribute and element tag for your form controls. WebKit supports a variety of form control types including passwords, numeric and telephone fields, date, time, and select menus. Choosing the most relevant type attribute allows WebKit to present the most appropriate interface to handle user input. Secondly, it's important to note that unlike iOS and macOS, input methods on watchOS require full-screen interaction. Label your form controls or specify aria label or placeholder attributes to provide additional context in the status bar when a full-screen input view is presented. Lastly, let's dive into Safari Reader. You may already be familiar with Safari Reader, a feature on iOS and macOS that reformats text heavy web pages to make them easier to read. We've brought Reader to watchOS 5 where it automatically activates when following links to text heavy web pages. It's important to ensure that Reader draws out the key parts of your web page by using semantic markup to reinforce the meaning and purpose of elements in the document. Let's walk through an example. First, we indicate which parts of the page are the most important by wrapping it in an article tag. Specifically, enclosing these header elements inside the article ensure that they all appear in Reader. Reader also styles each header element differently depending on the value of its itemprop attribute. Using itemprop, we're able to ensure that the author, publication date, title, and subheading are prominently featured. This paragraph contains content that is of strong importance and other content that is emphasized, so we put this text under strong and em elements. Reader recognizes these tags and preserves their semantic styles. For this image, we use figure and figcaption elements to let the Reader know that the image is associated with the below caption. Reader then positions the image alongside its caption. For this quoted paragraph, we use a blockquote element rather than a stylediv. Reader automatically styles these blocksquote elements as appropriate. Lastly, adding open graph meta-tags gives more context to Reader and also ensures that links to the article look great when shared. For more information about open graph meta-tags and rich links, check out our video on Ensuring Beautiful Rich Links. To recap this session, we've seen how WebKit adapts existing content. And we've also seen what you can do to optimize content for layout in Apple Watch. Finally, we covered best practices for form controls and also learned how to ensure that Reader brings out the key parts of your web page. We're so excited to bring the power of the web to watchOS and can't wait to see what content you create for Apple Watch. Good afternoon, everybody, and welcome to Object Tracking in Vision session. Do you ever need to face and solve various computer vision problems? If you do, and whether your Mac and iOS, or each of you as developer, you're in the right place. My name is Sergey Kamensky. I'm excited to share with you how Vision Framework can help. Our agenda today consists of four items. First, we're going to talk about Why Vision? Second, we're going to look about what's new that we're introducing this year. Third, we're going to have a deeper dive into helping throughout this Vision API. And then, finally, we're going to get to the main topic of our presentation, which is Tracking in Vision. So, why Vision? When we designed our framework, we wanted it to be one central stop to solve all of your computer vision problems while first in simple and consistent interface, one multi-platform; our framework runs on iOS, macOS, and tvOS. We're privacy oriented. What that means is that your data never leaves the device. All the [inaudible] sync is local, and we're continuously working. We're enhancing our existing algorithms, and we're developing new ones. Let's look at the Vision basics. When you think about how to interact with Vision API, I want you to think about in these terms, what to process, how to process, and where to look for results. What to process is about a family of requests. That is how you tell us what you want to do. How is about our request handlers or engines-- our request handlers, they're used to process our requests. And finally, the results; the results in Vision come in forms of observations. Please take a look at this slide. If you were to remember anything from this presentation, this slide is probably one of the more important ones. This slide presents a philosophy, how to interact with Vision, requests, request handlers, and observations. Let's look at the requests first. This is a collection of requests that we offer today. As you can see, we have various detectors. We have Image Registration Request. We have two trackers, and we have a CoreML request. If you're interested to learn more about integration of Vision and CoreML, I invite you to come to the next session in this room where Fran, my colleague, will cover details of that integration. Let's take a look at the request handlers. In Vision, we have two. We have Image Request Handler, and we have Sequence Request Handler. Let's compare the two using this criteria. First, we'll look at the Image Request Handler. Image request handler is used to process one or more requests on the same image. What it doesn't say, it caches certain information like image derivatives and results of posting requests. So, other requests in the pipeline can use that information. I'm going to give you an example. If a request depends on running neural network, as you know, neural network expects images in certain sizes and certain colors schemes. Let's say your neural network expects 500 by 500 black and white. It's very rare that you'll get user input just in that format. So, what we'll do inside, we'll convert the image. We'll feed it into that neural network to get results for the current request, but we will also cache that information on the request handler object. So, the next request, when it comes, if it needs to use the same format, it's already there, and it doesn't need to be recomputed. We'll also cache results that we get from the requests so other requests can use it in pipelines, and we're going to look at pipelines going forward in this presentation. Let's take a look at the Sequence Request Handler. Sequence request handler is used to process a particular operation like tracking in a sequence of frames. What it does inside, it caches the state of that operation from frame to frame to frame for the entire sequence. In Vision, it's used to process tracking and image registration requests. All other requests are processed with our Image Request Handler. Let's look at the results. Results in Vision coming from observations. Observations is a collection of classes derived from VNObservation class. How do we get an observation? Well, first, the most natural way is when the request is processed, you're going to look at the results property of that request, and that results property is a collection of observations. That's how we tell you results of processing. The second way is you create it manually. We're going to look at examples for other presentations how to do both. Let's now look at what's new coming this year. First, we have our new face detector. Now, we can detect more faces, and we're now orientation-agnostic. Let's look at example. On the left-hand side, you can see an image with seven faces in it, and we could process that image with the detector from last year, we can detect only three faces, and those faces are the ones that are close to the upright position. If you process the same image with the face detector that's coming this year, as you can see, all faces can be detected, and orientation is not a problem anymore. Let's look a little bit more into details. Thank you. So, first, our new face detector uses the same API as from last year. The only difference is if you want to specify revision, you need to overwrite a revision property of that request and set it explicitly to User Vision #2. Again, I'll talk about the reasons right on the next slide. We're also introducing two new properties. One is roll, which is when the head rotates like this, and the other one is yaw, which when the head rotates around the neck. Revisions. What happens with Vision where when we need to introducing a new algorithm, we don't deprecate the old one right away. Instead, we're going to keep for some time both revisions or, maybe going forward, even more, simultaneously. You tell us which one you want to work with by specifying revision property of the request. So, that's the explicit behavior. But we also have a default behavior. If you create a request object, and you don't tell us anything at all, and you start processing that request, here's what's going to happen. By default, you're getting the latest revision of the request that your app is linked against-- of the SDK that your app is linked against. This is important to understand, and I'll give you an example. Let's say your app is linked against there's the SDK from the last year. Last year, we had only single detector. So, this is the detector you're going to get, even if you take that app and without a compilation, run it on the current OS. If, on the other hand, you recompile your app with the current SDK without changing a single line or coordinate, and you run it on the current OS, you're going to get by default revision #2 because this is the way this from the current SDK. We highly recommend you to future-proof your apps, but again it's exclusive to Vision. What you get is, first, you get the deterministic behavior. You know performance of the algorithm that you're quoting against. You know what to expect. We also can be-- you can get your app to be future-proof from the errors like, for example, if we deprecate certain revision going forward, like a couple of years from now, then you can [inaudible]against that [inaudible] today. Let's have a deeper dive into how to interact with Vision API. First, we're going to look at the example with Image Request Handler. So, as you remember, image request handler is used to process one or more requests on the same image. It's optimized by caching some information like image derivatives and request results. So, the consecutive requests that are coming to be processed can use this information. Let's look at code sample. Before we dive into the code sample, I just want to emphasize a couple of points about the code samples in this presentation. The error handling is not a good example how errors should be handled. I use a short version of try, and I use [inaudible]. This is just to simplify the examples. When you code your apps, you probably should use guards to protect against unwanted behavior. I also use Image URL when I create my image request handler objects, and that is just a place on the SSD where the file is located. Now, let's look at example. First, I'm going to create my detect faces request object. Then, I'm going to create my image request handler passing the image URL with the file of the image where the faces should be located. Then, I'm going to ask my request handler to process my request. And finally, I'm going to look at the results. Very simple. If I had an image with a single face in it, my results would look something like this. So, what I get back? I get back a face observation object, and the one of the more important fields in that object is the bounding box where the face is located. Let's take a look at this slide again. The first three lines is pretty much all you need to find all the faces in the image. Isn't that cool? Let's now take a look at the Sequence Request Handler. Well, sequence request handler, as you remember, is used to process a particular operation like tracking on a sequence of frames. Let's look at code sample, and this code sample is pretty much the simplest tracking sequence we can imagine with Vision API. First, I'm going to create my Sequence Request Handler. Then, I need to specify which object I want to track, and I'm going to do that by creating a detected object observation, which gets us a parameter location, a bounding box. Then, I'm going to start my tracking signals. In this example, I'm going to track my object for five consecutive frames. Let's look how the sequence works. First, I have a frame feeder object, which in your case could be like camera feed, for example. This is where I get my frames. I get my frame, I create my request object, passing the detected object observation as a parameter with its initializer. And that's something that I just created before the loop started. Then, I'm going to ask my request handler to process request. I'm going to look at the results, and this is the place where I should be analyzing results and doing something with them. And the last step, which is very important, what I'm doing here, I'm taking the results from the current iteration, and I'm passing it to the next iteration. So, when the next iteration request is created, I want to see those results inside. If I were to want it in a sequence of five frames, my results would look something like this. How to create a request object. Well, first, it's important to understand that our requests have two types of properties. There are mandatory properties and there are optional properties. And mandatory properties, as the name suggests, they need to be provided via initializer in order to be able to create a request object. Let's look at the example. There is something that would just go in the previous slide. I detected object observation that is passed into the initializer of the [inaudible] object request is an example of a mandatory property. We also have optional properties. By the way, both types of properties are declared. You can find them in the place where the request object is declared. Optional properties is separate properties where we have meaningful defaults for them. So, we'll initialize them for you, but you can override them later if you need to. Let's look at the example. What I'm doing here, I'm breaking my detect barcodes request object. If I were to do nothing else and just took my request object and fed it into request handler, I would be working on the entire image, looking for my barcodes. What I'm going to do here instead, I'm going to specify a small portion like a central property image. There's this where I want to focus to look for my barcodes, and I'm going to overwrite my region of interest property with that. If I take my request object now and feed it into request handler, I'm going to be focusing on the smaller portion of the image only. A region of interest property here is an example of the optional property that we have. What's important to understand here also that once you get a request object in your hands, it's a fully constructed object. It's the object that you can start working with whenever you have constructed objects. If you decide to overwrite certain properties later on, you're welcome to do so. But whenever you have the object, it's the object that you can work with. One thing that's not maybe on this slide is, which will be covered in more details in the next session, is to look at the bounding boxes. As you can see, the coordinates that we receive are normalized. They are from 0 to 1 and they're always relative to the lower left corner. The next session, which is CoreML integration with Vision, will cover this aspect in more details. Let's look at how to understand results. As we said, results in Vision come in forms of observations. Observations are populated through results property of the request object. How many observations can you get? All the collection be 0 to N. There's one more aspect here. If you get results set to nil, that means that the [inaudible] single request failed. This is different than getting 0 observations met. Getting 0 observations met means that whatever you were looking for is just not there. As an example, let's say you run a face detector. If you feed an image with no faces in it, naturally, you will get 0 observations met. On the other hand, if you feed the image with one or more faces in it, you'll get appropriate number of observations met. Another important property of observations is that observations are immutable. We will look at the example where it's used. There are two more properties that I want to pay your attention to, and they are both declared in the base [inaudible] for all observations. One is the unique ID. This unique ID identifies the processing step for this particular-- where this particular result was created. Another one is the confidence level. Confidence level tells you how confident the algorithm was producing the results. The confidence is in the range from 0 to 1. Again, this topic also will be covered in the next session in more details. Let's look at the request pipelines. So, what is a pipeline? Let's say I have three requests, and it happens to be that request #1 depends on execution of request #2, which in turn depends on execution of request #3. How do we process the sequence while the processing is done in the opposite order? What I'm going to do here, first, I'm going to process my request #3. I'm going to get the results from that request and feed it into request #2. I'm going to do exactly the same with request #2. And finally, I will process my request #1. Let's look at the examples of how to run request pipeline in implicit and explicit order. We're going to look at the next two slides in these two use cases, and we're going to run our face landmarks detector. As you probably know, landmarks, face landmarks are the features on the face. It's both your eyes, eyebrows, nose, and mouth location on your face. Let's first look at how to do it implicitly. I have a simple [inaudible] a bit similar to what we have seen already. First, I'm going to create my face landmarks request. Then, I'm going to create my image request handler. Then, I'm going to process my request. And finally, I'm going to look at the results. If I had an image with a single face in it, my results would look something like this. Sorry. N is for the sequence, and the results. So, I get the bounding box of the face. That's the location for the face, and I get the landmarks for the face. What's important to understand here is that when the processing of the face landmark request starts, face landmarks request figures out that faces have not been detected yet, and it runs on our behalf inside face detector. It gets results from that face detector, and that's where landmarks are being searched for. On the right-hand side, you can see a snippet of what face observation object would look like. These are just a couple of fields from that object. One is a unique ID that we discussed that is set to some unique number. Then, the bounding box, that's where the face is located, and then, finally, the landmarks field, which points to some object where the landmarks are described. Now, let's take a look at the same use case but now done explicitly. What I'm going to do here first-- first, I'm going to explicitly run my face detector. You've seen these four lines of code already several times in the presentation. When I run it, I get my bounding box back. As you can see, the results are returned in the same type, face observation. The fields that we saw on the previous slide may look like this, so you get some unique number to identify this particular processing step. Then, you get the bounding box location, which is the main outcome of processing this request. And the landmarks field is set to nil because face detector doesn't know anything about landmarks. What I'm going to do next is I'm going to create my landmarks request, and then I'm going to take the results from the previous step and feed it into the input object observation property of that request. Then, I'm going to ask my request handler to process it. And finally, I'm going to look at the results. If I run it in the same image, I get exactly the same results as I would on the previous slide. But let's see what happens with observations. Remember, we said that observations are immutable even though both face detector and face landmarks detector return the same type, but we don't override the observation that was fed in. What we do instead, we take the first two fields and copy it into a new object, and then we calculate landmarks and populate the landmarks field. Now, if you look now, you will notice that the UID in most cases is the same. Why is that? Because we're talking about the same face. It's the same processing step, if you will. Where would you use implicit versus explicit? Well, if your application is very simple, you would probably want to opt for implicit way. It's very simple. You create a single request; everything else is done on your behalf. If, on the other hand, your application is more complex, for example, you want to process faces first, detect them, then do some filtering. Let's say you don't care about faces on the periphery, or you want to just focus on the ones that are in the center, you can do that step, and then you can do landmarks on the remaining set of faces. In this case, you probably want to use the explicit version because, in this case, landmarks detector is not going to rerun face detector inside. We want your apps to have optimal performance both in terms of memory usage and execution speed. That's why it's important to look at the next two slides. How long should you keep your objects in memory? Well, for the image request handler, you should keep it as long as the image needs processing. This may sound like a very innocent and simple statement, but it is very important that you do just that. If you release the object early, and you still have outstanding requests to be processed, you will have to recreate your image to request handler. But now you have lost all the cache that was associated with the previous object, and you'll have to pay this performance to recalculate these derivatives. If you release it too late, on the other hand, then, first you're going to start causing memory fragmentation, and then the memory is not going to be reclaimed by your app for other meaningful things that you want to do. So, it's important to release it, use it as long as you need and release it right after. Remember, it caches the image and multiple image derivatives inside. The situation with sequence request handler is very similar with the only difference is if you release it too early, you pretty much kill the entire sequence because the entire cache is gone by now. What about requests and observations? Well, requests and observations are very lightweight objects. You can create them and release them as needed. There's no need to cache them. Where should we process your requests? Well many requests in Vision rely on running neural networks on the device. And, as we know, running neural networks is usually faster on GPU versus the CPU. So, the natural question is where should we run it? Here's what we do in Vision. If request is runnable in GPU, we will try to do that first. If GPU is not available for whatever reason at that point in time, we will switch to CPU because that's our default behavior. But let's say your application is dependent on displaying a lot of graphics on the screen, so you may want to save the GPU for that particular job. In this case, you can override user CPU on the property and set it to true on the request object. This will tell us to process your request directly on the CPU. Now that we've covered basic how to interact with Vision, Vision API, in particular, we've seen a couple of examples, let's switch to the main topic of our presentation, which is tracking in Vision. So, what is tracking? Tracking is defined as a problem of finding an object of interest in a sequence of frames. Usually, you find that object in the first frame, and you try to look for it in the sequence of frames. What are the examples of such application? You will probably see many of them. It's live annotational sports events, focus tracking with camera, many, many others. You may say why should they use tracking if I can do detection on every frame in the sequence? Well, there are multiple reasons for that. First, you probably don't have a specific tracker for every single type of object that you want to track. Let's say if you're tracking faces, you're lucky. You have a face detector for that purpose. But if you need, for example, to track a specific type of bird, you probably don't have that detector, and now you're in the business to create that particular detector, which you may not want to do because of the other things that you want to have done with your application. But let's say you are lucky, and you're tracking faces, should you use detector then? Well, probably not in this case either. So, let's look an example. You start your tracking sequence, and you run your face detector on the first frame. You get five faces back. Then, you run it in the second frame; you get another five faces back. How do you know that the faces from the second frame are exactly the same faces as from the first frame? One person could have stepped out; another one showed up. So, now, you're in the business of matching objects that you found, which is a completely different task that you may not want to deal with. Trackers, on the other hand, use [inaudible] information to match objects. They know the trajectory how the objects move, and they can slightly predict where they would be moving in the next frame. But let's say you're lucky again. You're tracking faces, and your use case is limited to a single face in the frame. Should you use detectors then? Well, maybe not even in this case. Now, speed is a problem. Trackers are usually lightweight algorithms, while detectors usually run your [inaudible], which is much longer. In addition, if you need to display your tracking information on a graphical user interface, you may find that trackers are smoother and not as jittery. Remember, in one of the first slides, I asked you to remember these three terms, what, how, and results. Let's see how this maps into the track and use case. First, request. So, in Vision, we have two types of requests for tracking. There is a general purpose object tracker, and there is a rectangular object tracker. How? As you should have guessed by now, we're going to use our sequence request handler. Results. There are two types that are important here. There is a detected object observation, which has an important property in it, bounding box, which tells you where the object is located, and there is a rectangular observation, which has four additional properties telling you where the vertices of the rectangle are. Now, you say if I had my bounding box, why do I need the vertices of the rectangle? Well, when you draw up rectangles, they are rectangular objects in the real life. The way they are projected in the frame, they may look differently. They may look like trapezoid, for example. So, the bounding box in this case is not the rectangle itself. It [inaudible] the minimal box that includes all the vertices of the rectangle. Let's look at the demo now. So, what I have here, I have a sample app that you, by the way, can download from WWDC website and the link is right next to this session. What the app does is it takes the movie; it parses that movie into frames. In the first frame, you select an object. You want to track a multiple objects or you want to track, and it does the tracking. So, let's first use this movie. The user interface is simple. First, you can choose between objects or rectangles, and second, you can choose which algorithm you want to use, fast or accurate. What happens in Vision that we support two types, fast and accurate, and this is a trade-off between the speed and the accuracy. I'm going to show objects in this case, and I'm going to use my fast algorithm. Let's select objects. So, I'm going to track this person under the red umbrella, and I'm going to try to track this group of people here. Let's run it. As you can see, we can successfully track the object that we selected. Let's look at the more complex example. What I want to track here, I want to track this wakeboarder guy, and in this case, I'm going to use my accurate algorithm. So, I'm going to select my object, and I'm going to run it. As you can see, this object changes pretty much everything about itself, its shape, the location, the colors, everything comes. We're still able to track it. I think this is pretty cool. Now, we're going to switch to my demo machine and see how the actual tracking sequence is implemented in this app. So, I have the Xcode running, and I have my headphones connected to it, which is running the same app as we just saw. I'm going to run it in the debugger, going to select my objects, and it's not important what I select because we just want to look at the sequence. And I'm going to run it. So, I have a breakpoint set up here and it breaks in the perform tracking function, which is the most important function of this app. That's the function that implements the actual sequence. Let's see what we do here. First, we're creating our video reader. Then, we are reading first frame, and we're discarding that frame because that frame was used to select the objects. There's the cancellation flag here. Then, I'm going to initialize the collection of my input observations. Remember, as we saw an example in the slides. Then, I have my bookkeeping to be able to display results in a graphical user interface, which are kept in the trackedPolyRect type. Then, I'm going to run the switch on the type, and the type is something that comes from the user interface, and in this case, we're working with objects. Now, we selected two objects. So, this is the information coming from the user interface. We should see these two objects here. Okay, there are two of them. So, this loop will run two times. It will initialize input observations. It'll create detected object observation as also shown in the slides by passing bounding box in. And we initialize our bookkeeping structures. Let's run it. Let's look at the observation object. There are a couple of fields that are important here. This is the unique ID that we discussed. And then it's our bounding box in normalize organize. Now, if I run through, I'm going to hit this breakpoint because this case we're not using [inaudible] rectangles. This is where I create my sequence request handler. Now, I have my frame counter, I have my flag if something has failed, and I'm finally going to start my tracking sequence. As you can see, this is an infinite loop, and the conditions to get out of that loop is if the cancellation was requested, or if the movie has ended. I'm going to initialize my rect structure to keep the information for the graphical user in this interface to be displayed later, and I'm going to start iterating over my input observations, which we have to. For each one, I'm going to create a track object request. I'm going to advance my request to the collection of all requests, and we have to in this case. Going to break off the loop, and finally, I'm ready to process my requests. Now, if you can see the performed request, the perform function accepts a collection of requests. In the slides, we only used a single request to be passed into that collection, but here we're going to track two requests at the same time. I'm going to perform it. Now, since the requests are performed, I'm going to start looking at the results, and I'm going to do that by looking at the results property of each one. So, I'm going to get results property. I'm going to get the first object in that property because we expect them, the single one in there as an observation. What I'm going to do here, I'm going to look at the confidence property of my observation, and I set an arbitrary threshold to 0.5. So, if it's above the threshold, I'm going to paint the bounding box with solid line, and if it's below the threshold, I'm going to paint it with a dashed line. So, I have, so I can have an indication if something is going wrong. The rest is simple bookkeeping. I'm just going to populate my rect structure, and this is the last step, which is very important where I take the observation from the current iteration, and I assign it for the next iteration. I'm going to do it a second time. I'm going to get to this breakpoint. Going to display my frame. I'm going to sleep for the frame rate in seconds time to simulate the actual movie. And then, before you know it, you're in the second iteration of your tracking sequence. So, let's get back to the slides now. Thank you. So, let's look at what's important to remember from what we have just seen. First, how to initialize initial object for tracking, and we saw two ways. There's automatic way, which is usually done by running certain detectors and getting bounding boxes out. And the second is manual, which usually comes from the user input. We also saw that we used a single tracking request per tracked object. The relationship here is one-to-one. We also saw that there are two types of trackers. One is the general purpose tracker, and another one is rectangular object tracker. We also learned that there are two algorithms for each tracker type. There is fast and accurate, and this represents the tradeoff between speed and accuracy. And last but not least, we looked at how to use a confidence level property to judge whether we should or should not trust our results. What are the limits of implementing tracking sequence in Vision? First, let's talk about number of trackers. How many objects can you track simultaneously? Well, in Vision we have a limit that is set to 16 trackers for each type. So, you can have 16 general purpose object trackers and 16 rectangular object trackers. If you try to allocate more, you'll get an error back. So, if it happens, you probably need to release some of the trackers that you're already using. How to do that? First way is you can set a last frame property under request and feed that request into the request handler for processing. That way, the request handler will know that the tracker associated with this request object should be released. Another way is to release the entire sequence request handler; in this case, all the trackers associated with that request handler will be released. Now, let's say you've implemented the tracking signals. What are the potential challenges that you may face? Well, as you've seen, objects in tracking sequence can change pretty much everything about themselves. They can change their shape, appearance, color, location, and that represents a great challenge for the algorithm. So, what can you do here? Well, one unfortunate answer is that there's no one size that fits all solution here, but you can try a couple of things. First, you can play with fast or accurate, and you can figure out that your particular use case works better with a particular algorithm. If you're in charge of selecting bounding box, try to find a select salient object in the same. Which confidence threshold to use? Again, there is no single answer here. You will find that some use cases work with certain thresholds while other use cases work with other thresholds. There's one more technique that I could recommend. Let's say you have a long tracking sequence, and for the sake of this example, 1,000 frames. If you start that tracking sequence, your object that you selected in the first frame will start deviating, and it'll change everything about itself the more you go off of that initial frame. What you can do instead, you can break that sequence into smaller subsequences, let's say 50 frames each. You run your detector, you track that object for 50 frames. You rerun the detector; you run it again for 50 frames, and you keep doing just like that. From the end user point of view, it'll look like you're tracking a single object. But what you do instead, what you do inside instead, you're tracking smaller sequences, and that's a smarter way of running and tracking sequence. Let's summarize what we have seen today. First, we talked about why you'd use Vision, and we talked about a multi-platform framework, privacy-oriented, which offers simple and consistent interface. Second, we talked about what's new, and we introduced a new orientation-agnostic face detector. We talked also about revisions. Then, we talked about how to interact with Vision API, and we discussed requests, request handlers, and observations. And finally, we looked at how to implement tracking sequence in Vision. For more information, I recommend you to refer to this link on the slide. I can also recommend you to stay for the next session, which will be at 3 o'clock in this room where Frank will cover details of integration of Vision and CoreML. This is especially important if you want to deploy your own models. That session will also cover some details about Vision Framework that were not covered by this session. And we'll also have Vison Lab tomorrow, which is 3 to 5. Thank you and have a great rest of your WWDC.  Privacy is about people. My name is Joey Tyson. I'm a privacy engineer at Apple, and I know you've heard a lot of great information this week on exciting new features and you're ready to get out there and build some new apps, but I know you also care deeply about your user's privacy. And this is the first of three big ideas I want to explore with you about privacy today before we get into this year's updates. Because we're going to talk a lot about data privacy, but we can never forget that data is about people. It belongs to them. So when I say that privacy is about people, I mean it's about building a relationship of trust with your users. This lays a foundation for better engagement, which leads to better apps. Think about other relationships. It's the people that you trust that you're more likely to work with and spend time with. And as your users understand why you're collecting data, how it's being used, as you handle that data respectfully and thoughtfully, you're going to get better data, because they're going to be more comfortable using your apps and sharing information, and this builds loyalty over time. And I start here because I want you to apply this context to your development process. None of us do engineering in isolation, so whether you're working with health records or just building a simple puzzle game, the information you gather and the ways that you use it could have a very real impact on people's lives. So it's critical for each of us to think carefully about the technologies that we're building. In a recent commencement address at Duke University, Tim Cook talked about Apple's approach to privacy, and he said that "In every way, at every turn, the question we ask ourselves is not what can we do, but what should we do." And that leads me to the second big idea, ask the "should" questions. No matter what your role, whether you're a solo app developer or part of a large organization, you can be the one to stand up for your users. Remember your responsibility towards them, and ask questions about the data flows in your app. For example, why do we actually need this data? This isn't merely an accusation. It's to think about is this necessary for our use case? Should we collect it? Would this surprise our users? If people understood this and it scared them, why should we be doing it at all? Could we use less granular data, less precise? Are there other approaches we should consider? And should we delete or aggregate this data after a period of time? Part of the reason for asking these kinds of questions is that we can all fall prey to assumptions about our data. So just thinking, we should just log this for everyone. Maybe that's the way we've always done it in the past or the way others do it, but again, is it really necessary for what we're trying to accomplish? Or you might think that data couldn't possibly be sensitive in the context you're working, but maybe that data is very sensitive in a different context or for users in a vulnerable population. If you're taking data that was gathered for one purpose and apply it in a new way, would users understand or expect that? You also hear people talk about personally identifiable information, or PII, but even data that falls outside that definition can still have a privacy impact. Just like if you're protecting data with encryption and good security, that's wonderful, but privacy is so much more than that. Because this again doesn't get back to the should questions of should we even have this data at all. Now as you're asking questions about the data flows in your app, if you want to even go one step further, you can create privacy guarantees. These are high-level statements about the privacy expectations in your app that you want to be able to make. And by establishing these early on in the development process, it provides a framework to guide you as you're building your features and something to test against once you're done. There's some examples on the screen here of these kinds of statements that are similar to statements Apple has made about some of our features, and there's many options for implementing each of these, which brings me to the third big idea. Align your data practices with your use cases. To illustrate this, let's think of data collection, the amount and types of data that you gather. We mentioned earlier those assumptions. You know, you may think sometimes well shouldn't I just gather as much data as possible? Well, you know, in the past you may have hear people call data the fuel of the new economy. Like actual fuel, data should be handled with caution. Because data is very powerful, and that unlocks a lot of great use cases, but because it's so powerful, it can also be dangerous if not handled carefully. Gathering data creates overhead for you as an engineer. You're going to need to spend time and resources managing that data, keeping up with it, filtering it. It's time you could be spending working on new features for your users. It also creates liabilities. We've all heard about companies suffering data breaches, which is a bad situation. But if the data that gets leaked includes information that's not relevant to the use case, that's an even worse situation. Unexpected data collection creates all sorts of risks. And it destroys that foundation of trust with your users. The next time you think about gathering as much data as possible, I want you to picture these tanks of chemicals and remember your responsibility to your users to handle their data carefully and thoughtfully. Instead, you want to practice what we call proportional data collection. This is the idea of collecting only what's necessary to achieve your goal, and again sometimes you might start off thinking that you need a lot of information when a different dataset may suffice. You can even start with the assumption of no data and figure out what's actually necessary for what you're trying to solve. This gets back to user expectations. People should understand why you're collecting data and how you're using it. It should be in line with what they expect. You should always be able to provide a clear rationale for the use cases that you're building. But of course, this is about data collection, but when we talk about aligning data practices with use cases, that extends to the entire data life cycle and being good stewards of the information that's been entrusted to you. So even beyond just proportional data collection, you want to develop and use privacy techniques throughout your app's workflow. You could develop a whole toolbox or repertoire of techniques that will help you build privacy into your app. Things like aggregation, providing transparency to users, using a scoped identifier instead of a real identity, automatically rotating those with time. Even more advanced techniques like differential privacy. I don't have time today to go into the entire list of techniques available, but what I want to focus on now is the idea of adjusting these to match your use case. You can think of a mixing board for music. If you have one track that's particularly loud or soft, you may need to adjust others to balance things out and achieve a good mix. And again, there will be times when you do need to collect a lot of data for a particular feature, but in those cases, you want to make sure you adjust those privacy techniques to create a great experience for your users. Ideally, these apply across all systems where the data lives so those privacy guarantees stay consistent and are billed as technical enforcement rather than just policy statements about what you plan to do. But I know this can all be a little abstract, so to illustrate further, I want to share some features that Apple has built where we've this kind of thinking. First is activity sharing where you can share fitness data with your friends. Now for me as a privacy engineer, I like to turn all of these sliders up to 100 percent as much as possible, but that's not always feasible for a given feature. In this case, you're sharing data with friends, so they know your name. They know whose data it is. So you can't just make this data de-identified. It's already very identifiable as part of the use case. Consequently, we turn up other privacy techniques like only showing a summary of the data, not minute-by-minute statistics or the exact location of your run. We also provide a lot of control over who you share with and when. Now, in the Apple News app, we collect analytics data using a scoped identifier that's not connected to your Apple ID. That gives us more flexibility around the precision of data we collect, but since it's still sensitive information, we still provide control through things like being able to reset that identifier at any time. Finally, there's photo memories. You may have seen these on your device. These use facial recognition data to identify people in pictures. It also uses precise location information to connect similar photos together. Now that's very sensitive data. So consequently, there's another privacy technique we turn way up. All the processing to build these memories happens on your device. And by the way, that's a great tool for your toolbox to do processing locally. So to recap, three big ideas. Privacy is about people, ask the "should" questions, and align your data practices with your use cases. In the time we have remaining, I want to talk through some features and tools that are available to you as developers to help you build privacy in your app. And these fall into two general categories of accessing user data and more broadly, data stewardship. So for data access, let's start by talking about iOS, and much of this guidance will apply to tvOS and watchOS as well. Let's imagine you're building a game for iOS where players can compete against each other, and you want them to be able to upload a photo to identify themselves. Now we've all seen those permission prompts for access to photos, but wouldn't it be great if we could just have the user click a button, select a picture, and have it appear immediately in their app? Well, you can already do this today. Because we have a feature available called out-of-process pickers for contacts, camera, and photos data, where the picker that appears runs outside your apps process so the only information that's shared back with the app is what the user selects. We talked about those privacy techniques. This is a case where because this doesn't involve ongoing access to the entire library, the user has control by picking what they share, so we don't need to show a permission prompt and ask them to make a decision about future access. This is the default method for accessing contacts, camera, and photos data. There are going to be times where you may need access to the broader library, but in most situations, you'll find that this works great for a whole lot of apps. This is a case where it does just work and only requires a few lines of code. You can see some snippets here for how to call these pickers in your app. Now as I said, there are going to be some times where you do need access to the broader data, and as you know, there's a variety of protected resources available on the device, but if you're using one of these APIs, you need to keep in mind three things before you start requesting access. You should only request access to the data that's necessary for your use case in your app. If you don't actually need the entire library of information, rather than requesting it, you should look for an alternative solution, like those out-of-process pickers. You should only make these requests when it's needed. You want it to be in the moment when a user makes a decision not when they first open the app and they're bombarded with questions that they don't even understand yet. You want the prompt to be in context. But also you want to rely only on the API for status. Remember, a user can revoke their decision at any time. You just want make sure your app still functions regardless of what the user had decided. Now when you request access, as you know, you need to include a purpose string or a usage description. And when I say you need to, these are required. You'll find your app rejected in app review without these, and in fact, you'll find your apps start crashing if you try to access this data without a purpose string. Now this is one way of providing transparency to users. It's certainly not the only way. By that, I don't mean showing a fig prompt before the real one to get them to click. I mean this should be part of the overall program of informing your users about data flows in your app. The goal here is to explain the reason for a request so that a user can make an informed, effective decision based on their priorities. When I say explain the reason, this is not what I'm talking about. And we've seen purpose strings like this in the past, but again, this is going to lead to rejections in app review. We're increasingly enforcing quality purpose strings both through automated validation and manual review. Placeholders or blank strings are not going to be sufficient. Just saying advertising doesn't tell a user much. Requires location doesn't explain the why. Even this last one about more relevant content, that's nice, but it's pretty vague. And when you look at our own maps app, when it requests location, this is what you see. It will be displayed on the map and used for directions, nearby search results, and travel times. So this is explaining the reason the use case for this request. It's specific, includes examples of how the data will be used. The TV app also, this is another one that Apple wrote using your location to determine what's available to you and show you live games, events, and news from your area. Remember, if users understand why they're being asked for this data, they're going to be more likely to allow it. If you were building a transit app for a subway system, you might write something like this. This app uses your location to show nearby stops and stations and allows you to plan trips from your current location. Again, explain the use case, be specific, provide an example. Some additional guidelines to remember when you're working with protected resources on a device. Access should not be required for your app to function. Again, this can lead to rejections in the app review process. Your app should have graceful fallback mechanisms so that even if a user declines access, it still functions. For example, with that transit app, if the user denies location access, you can have a field where they can enter location manually and use that instead. Again, you want to verify the authorization status of your app whenever it needs this data to make sure that the user still is getting access, and stay aware of third-part STKs. Again, requesting access should only be for the use cases of your app, and if you're including libraries that change that or tell you to set purpose strings, you should probably look for a different solution or update your code. Finally, you want to provide ongoing transparency. Again, this should not be the only time that a user gets to understand how their data is being used. You can do this in a privacy policy, which is required for apps in the App Store, or throughout your app you can provide links and documentation to help users understand how their data is being used. Now this is all broader guidance around these kinds of APIs, but there's a few specific ones I want to highlight with changes this year. The first is for WiFi network information. If your app uses see and copy current network info, you're going to need to add an entitlement, AccessWiFiInformation in Xcode. This is a capability that you can enable for your app. For example, if your app is communicating with a hardware accessory and needs to verify if they're on the same network, you would need this. If you're not doing that use case, you don't need to worry about this. This is only if it's necessary for the functionality of your app. You may have also heard about our new Health Records API. Because we know that developers have a lot of great ideas around building apps using health data, but we also recognize that's very sensitive information. So again, adjusting those privacy techniques rather than just a simple permission prompt, we provide a greater transparency and control for the user. You can see there's a purpose string, a link to a privacy policy, which should be a website or if it's in your app available while signed out. There's also controls over what categories of data to share, whether it's current or current and future data, and there's a session from Tuesday you can refer to that provides a lot more information about how to use this API in your app. Finally, for iOS, I wanted to share a quick update on how Apple's been using the technique of differential privacy to improve our apps and devices. The first new use case this year is for commonly misspelled words. If you're typing and correct yourself, devices that are opted in to device analytics will now donate data on those words to help us improve our keyboard algorithms even further while protecting user privacy. Also, for Safari, since last year, we've added the ability for those devices to donate data around websites that typically cause crashes to improve the stability of the browser. So that's iOS. Let's talk for a moment about macOS as well. Because as you may have heard, we've made some changes to how protected resources are handled on macOS. It's an expanded list of categories of data where there are protections in place, and these can now trigger a permission prompt or for some of these an opt-in through system preferences. I just want to highlight this so that if you're developing for the Mac, I want you to be aware of some of these changes because you need to know how they're going to affect your app if you're accessing these resources. Again, since these can trigger a permission prompt, you want to know when that's going to happen so your users aren't surprised, and please note, this applies to all third-party app processes including those outside of the app store. Just like with iOS, you'll need to set a purpose string for these permission prompts as well, and again there's a session from Tuesday that goes into a lot more detail on how this works for your app. Now to talk about accessing data on the web, I'm going to turn it over to my fellow privacy engineer, Brandon Van Ryswyk. Thanks Joey. The web is one of the largest venues for data access today. If your business depends on providing content on third-party websites, this section is for you. This year we introduced the Storage Access API. The Storage Access API allows users to engage with logged-in content from embedded third parties across the web including from domains that have been classified as a tracker by intelligent tracking prevention. Now the Storage Access API does this only with the user's explicit consent. Let's go through an example. Here the user is browsing a news site, news.example, and the news site has an embedded video player from video.example. Now the user has a paid account on the video site and would like to grant the embedded video access to its cookies so that they can enjoy the benefits of their subscription while reading the news. To accomplish this, video.example needs to implement the Storage Access API. Video.example should add a call to the Storage Access API when the user clicks the play button in their app. Now this is an asynchronous API that will return a promise, so you should be prepared to handle successes as well as failures. So when a user clicks the play button, this will kick off a request, which will result in a prompt asking the user if they would like to grant video.example access to its cookies while embedded in news.example. If the user clicks allow, this choice will be sticky, and the user won't be prompted again on this combination of domains. But if the user clicks deny, the site can always reprompt. Let's assume the user clicked allow. The request will go through, and cookies will be returned to the embedded site. Now, this could create a tracking risk as now video.example has their users logged in identity associated with their presence on this news site. Now this is especially important given changes to intelligent tracking prevention this year. Now outside of the user consent provided via the Storage Access API, cookies from domains that are classified as trackers will be partitioned immediately and can never be used in a third-party context. Additionally, after 30 days without user involvement, these cookies will be purged entirely. Now importantly here, access via the Storage Access API will count towards this 30-day interaction timer. That means that users who interact frequently with your site in a third-party context will stay logged in. So in this sequence, the user will visit video.example both in a first-party context, logged into the home page, and in a third-party context, where it's embedded across the web. So first, the user visits the site in a first-party context. Now notice that the days since interaction timer will read zero as the user is currently interacting with the site. But as the user interacts with the embedded content throughout their web browsing, the timer will update. This means that when the user returns in a first-party context, the days since interaction timer will read 5 days despite there being 45 days since the user was last on video.example in a first-party context. Adopting the Storage Access API will allow your users to stay logged in and prevent unwanted tracking. Now privacy does not end with gaining access to a user's data. Privacy is a continued obligation to your users to maintain their trust throughout the data's lifetime. This is where data stewardship begins. Now I want to give you examples from four areas of data stewardship for you to think about when developing your apps. First is deletion. Part of being a good data steward is respecting your user's intent to delete something from your app. So you should recognize that there are data flows that go outside your app, and you should ensure consistency between these systems when the user deletes something in your app. Now the operating system doesn't know what's happening inside your app, so if you've donated information to Siri Shortcuts or posted a notification, you should make sure that you delete that content when the user removes it from your app. For example, if a user deletes someone in your app's contact list, Siri Suggestions should not suggest them to message using your app. Or if a user deletes a thread in your messaging application, you should delete the notifications for that content as well as it would be unexpected for a user to see notifications still on their device from a thread they thought they'd completely deleted. And finally, if you're a passwords manager, and you've donated passwords to the New System Passwords API, you should make sure to delete this information if the user removes the site from your password manager. Now data stewardship continues through to device tracking, which means something very specific in the context I'll talk about today. You might have questions about the devices that use your apps. For example, did this device already consume a free trial, or was this device previously used by an abusive user or for fraudulent activities. We offer an API called DeviceCheck that allows you to answer these questions. DeviceCheck lets you set two bits of data per device, which are stored by Apple and can be returned to you with a signature. These bits persist across a device reset or a device erase install. These bits provide high-integrity answers to your questions about a device's history without exposing unique device identifiers. Now, you should adopt DeviceCheck and not rely on unsupported device tracking mechanisms like finger printing. As Craig said in the keynote, we continue to remove entropy from our platform and to remove functionality that is being abused to uniquely identify users. So you should adopt DeviceCheck to answer your questions while being a good data steward. Now in addition to being a good data steward in your own apps, you should consider your third-party partners as well. Now, you as developers are responsible for all of the code that chips in your app. This includes code that you've written but also code that you've imported in the form of a library. So you should understand how these libraries that you import access or transfer your user's data off the device. This way you can be complete when giving transparency. Don't just talk about the code that you wrote. You should describe the full impact on a user's privacy. And as Joey mentioned earlier, you should avoid unnecessary requests for resources. So, for example, if a library that you want to use requires an entitlement or access to some other sensitive resource that isn't required from the functionality you're trying to get out of this library, you should either find a different library or reach out to the developer of that library and ask them to remove that sensitive resource request. Now thinking about third-parties extends to your server side as well. You should understand how data flows to all third parties your server's touch. Now this includes the full breadth of systems that support your app, not just analytics or advertising networks but the network security systems, the email provider that sends customers password reset emails or third-party customer support integrations. Being a good data steward means taking responsibility for the full picture and considering privacy when considering partners to work with. Now for a topic you may have heard a little bit about, machine learning. So across the industry, much of the talk about machine learning centers around the performance characteristics of a new algorithm or the power of a cloud-based solution. And while these are important technical developments, as Tim said in his commencement address, the question that we ask is not what can we do but what should we do. Now this applies particularly to machine learning, and we've been working on it for years. Face ID was built with privacy-friendly machine learning at its core. And we've made it easy for you to add Face ID authentication to your apps using the LocalAuthentication API. You can take advantage of the work Apple has done to build strong biometric authentication using privacy friendly machine learning techniques. Similarly, ARKit uses machine learning to model the environment around a user's device. And new in ARKit 2, you can create, persist, and store this map of the environment in your own apps, but you should collect this map only if it's needed for your feature as this data might be quite sensitive. It comprises a representation of what's around a user. So if you send it off the device, it should be expected. Like if you're playing a game collaboratively with a shared object. And if you use Game Center, you can take advantage of the MultipeerConnectivity API, which supports end-to-end encryption to transfer these models between devices. Now Face ID and ARKit are good examples of features that Apple has developed that depend on privacy-friendly machine learning that you can use in your apps. But many of you want more flexibility. Create ML and Core ML allow you to build your own features on top of machine learning. With Create ML and Core ML, it is easier than ever to add on-device machine learning to your apps. Create ML allows you to train a machine learning model directly on your Mac, and Core ML lets you then take this model and evaluate it directly on a user's device. This avoids collecting sensitive user data to evaluate the model, and protecting sensitive data on your servers requires a lot of engineering work. Evaluating a model on a user's device can lower your server's security requirements and will lower your breach risk as you don't hold this sensitive information in the first place. Now these two APIs make adoption of on-device machine learning easy. And you should already be asking privacy questions when developing features like the ones Joey went over earlier, and these questions are great, and Joey and I use them every day doing feature reviews at Apple. However, machine learning requires you to ask a new set of questions to address the same underlying privacy goals. For example, you should ask does my model reveal anything about the data it was trained on? It's actually possible to invert a machine learned model and recover much of the data it was trained on. This could result in unexpected disclosure if you ship a model with your app and it's inverted to expose information about people it was trained on. Now this is an area of active academic research that you can learn more about in the paper that I've put on this slide. Similarly, you should ask, could I infer more about my users than they expected? So users might expect that you'd classify activity type via sensor data. But you should ask, did I accidentally encode the fact that this specific user uses a wheelchair? It could be great to offer a feature for wheelchair users, but this should be clear and sold as a feature. As with general data collection, you should obtain new consent if you have a new use case enabled by machine learning. Now it turns out that two small modifications can help mitigate both of these issues. The first is to ensure that you train on the right data. This means training on a sufficient quantity of diverse inputs that were collected with the proper consent. The second is to keep your model complexity proportional to the goal that you are trying to solve. Both of these techniques can prevent model overfitting, which makes a model inversion or unexpected inference more likely. Now at Apple, we believe that considering questions like these are an important part of building products that users can trust. Because fundamentally privacy is about people. It's about building trust with your users and respecting your users in handling their data. By applying the techniques that we've gone over in this presentation, you too can build products with great features and great privacy. Now in summary, I hope you take away three big ideas about privacy. That privacy is about people, that you have to ask the should questions, and that you should align your data practices with the use cases you're trying to solve. For more information, please check out these sessions, and I look forward to seeing you at our lab after this session so that we can help you build better apps through better privacy. Thank you.  Good morning. Welcome to Session 403, What's New in Testing. My name is Honza Dvorsky and I will share the stage today with my colleague, Ethan Vaughan. Today, we'll start by talking about the code's coverage improvements we made in Xcode 9.3. Then we'll look at the test selection and ordering features new in Xcode 10. And finally, Ethan will come up on stage to tell you about how you can speed up your tests by running them in parallel. Let's kick off with code coverage. We completely reworked code coverage in Xcode 9.3 and this resulted in many performance and accuracy improvements and it allowed us to add a new feature so that you have the granular control over which targets contribute to code coverage. We created a new command line tool called xccov and last but not least, we gave code coverage in the source editor a visual refresh. Let's look at all of these in detail. First, to give you a sense of how much better code coverage is now, we measured it on a large internal project at Apple. To understand the speed, we measured how long it took Xcode to load and show code coverage in the source editor. In Xcode 9, it took about six and a half seconds to do that. In Xcode 9.3, however, we got it down to less than a half a second, which is more than 95% faster. But we also wanted to make the coverage file smaller, as Xcode can potentially write out many of them, if you run your test often, as you should, so I'm happy to say that here, the improvements are just as dramatic. Xcode 9's coverage files were over 200 megabytes in size. And this is a lot but remember that we're talking about a large project with thousands of source files. But files written by Xcode 9.3 were less than a tenth of that. I'm sure you appreciate this if you maintain a continuous integration machine or if you're just running low on disk space. But the best part is that not only are the coverage files smaller and faster to read and write, they're also more accurate than ever before. One example of this is header files. Xcode 9 didn't correctly collect and show coverage in header files and this was a problem for a code basis that used C++ as in that language, you can have a good portion of your executable code in headers. So, if you're one of the people affected, you'll be happy to hear that Xcode now correctly gathers and presents code coverage for both implementation and header files. Now let's talk about code coverage features. The first one is target selection. This is a new option to control not only whether code coverage is enabled or disabled but when it's enabled, you actually control which targets are included. This can be important if your project builds third-party dependencies that you're not expected to cover by your tests or if you work at a company where another team supplies you with a framework that they already tested. You can customize the included targets in this scheme's test action under options. This means that it can continue to include all targets in code coverage or only hand pick some of them. Now in the name of more powerful workflows, we created a new command-line tool called xccov. It can be integrated into automation scripts easily, as it produces both human readable and machine parseable outputs. And at the high level, it gives you detailed view of your coverage data. So, I've mentioned the code coverage data a couple of times, let's look at how it looks under the hood. When tests are run with code coverage enabled, Xcode generates two files. First is the coverage report, or the xccovreport file extension, and that contains the line -- line coverage percentages for each target, source file, and function. The second one is the coverage archive and that contains the raw execution counts for each of the files in the report. And these coverage files live in your project's derived data directory and additionally, if you pass the result bundle path flag to Xcode build, these files will be placed into the result bundle as well. Let's look at an example. Here, we use xccov to view the coverage data of our Hello World app. We can see the overall coverage for each target but also detailed coverage for each file and even for each method. And of course, by passing the right flag identification, you can get the same exact information in JSON. This can make it much easier to integrate with other tools. So, as you can see, xccov is very flexible, so I would encourage you to explore its documentation. Now we talked about use -- viewing code coverage on the command line but the most convenient way is still going to be to see it right next to your source code. You control whether code coverage is shown by selecting editor, show, or hide code coverage. When you do enable it, you'll see the refreshed look of code coverage in the source editor. The whole line highlights when you hover over the execution count on the right-hand side. And this is just the best way to keep an eye on which of your code is already covered by tests and which still needs more work. Now let me show you all of these improvements in action. So, here I have a project called Dev Cast. It's a simple messaging app and I created the iOS version for this talk last year. So, this year, I wanted to create a Mac version. But I wanted to share the business logic between the iOS and Mac versions, so I put it all into a framework called DevCastkit and today, my goal is to explore and maybe improve the code coverage of this framework. So, right now, I'm not collecting code coverage, so I just need to turn it on. I'll do that by selecting the scheme, selecting edit scheme, go into the test action, go into options. And down here, I will just enable code coverage. Now I will run my test by going to product test. Right now, my framework, my test bundle and my Mac app are getting built, and my tests are being run. Now the test finished so let's look at the results. We'll go to the report navigator and to the latest coverage reports. Here, I can see that I have the -- both targets, the Mac app and also the framework. Now the Mac app only serves as a container for running tests, so I'm not really concerned about its low code coverage. In fact, I would actually not want to see it in this report at all. So, I can do that by editing the scheme again. And in the test action, here -- instead of gathering coverage for all targets, I'll change to just some targets. This gives me a hard cover list and I will add just the target that I'm interested in, which is the framework. Now I will rerun my tests and look at the updated coverage reports. So, now you can see that I only have the framework in the coverage report. This is what I wanted. So, now I can just focus on this one target. So, looking at the coverage percentage, is at 84%, which is not bad, but I know I can do better. So, to understand which of the files need more attention, I will disclose the target and see that the first file is only covered at about 66%. So, I'll jump to the file by clicking on the arrow here. And here, I'll look at my local server class. On the right-hand side, I can see the execution counts for each region of the code so I can see that all of the executable parts of my class are covered by tests. This is good. Unfortunately, my convenience function, get recent messages, hasn't been called for my tests at all, so I don't know if it's working properly. To fix that, I'll go to the corresponding test file and I will add a new test. This test just puts a couple of messages into a local server and then I call, get recent messages, on it to verify that it returns what I'm expecting. So, with the new test added, I will rerun my tests one more time. So, great, the test finished. I'll go to the coverage report again and now when I focus on my framework, I can see that it's covered at 100%. This is exactly what I wanted. So, we saw how I used the target selection feature to only focus on some of the targets. Then I used the coverage report to know exactly which file to focus on. And finally, I used the code coverage integration in the source editor to know exactly which part of my code still needs covering. So, this is the improved code coverage in Xcode. So, moving on from code coverage, let's talk about some new features in Xcode 10. First, we'll see how we can now better select and order our tests. Now why is this important? Well, not all the tests in your suite serve the same purpose. You might want to run all 1000 of your quick running unit tests before every single commit but only run your 10 long-running UI tests at night. And you can achieve this today by disabling specific tests in your scheme. The scheme encodes the list of disabled tests so that XE test knows which tests to skip. And this has an interesting side effect. Whenever you write a new test, it's automatically added to all the schemes that contain the corresponding test targets. But if that's not what you wanted, you have to go through all those schemes and disable the test there manually. So, in Xcode 10, we're introducing a new mode for schemes to instead, encode the tests to run. If you switch your scheme to that mode, only the test that you hand pick will run in that scheme. You control this mode in the scheme editors test action, where in the list of test targets, there's a new options pop-up, where you can declare whether the scheme should automatically include new tests or not. This way, some of your schemes can continue to run any new tests you write, while other schemes will only run a list of hand-picked tests. So, we've discussed how we can better control which of our tests run and when, but the order of our tests can be significant too. By default, tests in Xcode are sorted by their name. This means that unless you rename your tests, they will always run in the same order. But determinism can be a double-edged sword. It can make it easy to miss bugs, where one of your tests implicitly depends on another one running before it. Let's look at an example of when this can happen. Imagine you have tests A, B, and C. They always run in this order and they always pass. But when you look at your tests in detail, you realize that test A creates a database. Then test B goes and writes some data into it. And then finally, test C goes and deletes it. Now these tests only pass because they run in this specific order. But if you tried to shuffle them around, for example, by renaming them, then you try to run them again, you might have test B writing into a database that doesn't exist and your tests would just fail. So, to prevent issues like this, your tests should always correctly set up and tear down their own state. Not only will they be more reliable, but it will also make it possible for your tests to run independently of all the other ones and this can be really beneficial during development and debugging. So, to help you ensure that there are no unintentional dependencies between your tests, in Xcode 10, we're introducing the new test randomization mode. If you turn it on, your tests will be randomly shuffled before every time they're run. And if your tests still pass with this mode on, you can be more confident that they are really robust and self-contained. Randomization mode can be enabled in the scheme editor, just like the other features we've seen today. So, these are the new test selection and ordering features in Xcode 10. Now I'm really excited about what comes next. To tell you all about the new parallel testing features in Xcode, I would like to welcome Ethan Vaughan to the stage. Ethan. Thanks, Honza. So, many of you have a development cycle that looks like this. You write some code, you debug it, and then you run your tests before you commit and push your changes to the repository. By running your tests before you push, you can catch regressions before they ever make it into a build. However, one of the bottlenecks in this process can be just how long it takes to run your tests. Some of you have test suites that take on the order of 30 minutes to hours to run. And if you have to wait that long before you can confidently land your work, that represents a serious bottleneck in your workflow. We want to make sure that your tests run as fast as possible in order to shorten this critical part of the development cycle. So, last year, we introduced a feature in Xcode 9 to help you run tests faster, it's called Parallel Destination Testing. This is the ability to run all of your tests on multiple destinations at the same time. And you do this from the command line by passing multiple destinations specifiers to xcodebuild. Previously, if you were to run your tests, let's say on an iPhone X and an iPad, xcodebuild would run all of the tests on the iPhone X and then all of the tests on the iPad. Neither device would be running tests at the same time. But in Xcode 9, we changed this behavior so that by default, tests run concurrently on the devices and this can dramatically shorten the overall execution time, which is great. However, there are some limitations to this approach. First, it's only beneficial if you test on multiple destinations. If you just want to run unit tests for your Mac app, for example, this doesn't help. Also, it's only available from xcodebuild, so it's primarily useful in the context of a continuous integration environment, like Xcode Server or Jenkins. I'm excited to tell you about a new way to run tests faster than ever called Parallel Distributed Testing. With Parallel Distributed Testing, you can execute tests in parallel on a single destination. Previously, testing on a single destination looks like this, a continuous straight line with one test executing after the other. Parallel Distributed Testing allows you to run tests simultaneously so that testing now looks like this. In addition, it's supported both from Xcode, as well as xcodebuild, so no matter where you run your tests, you'll get the best performance. Now in order to tell you about how Xcode runs your tests in parallel, we first have to talk about how your tests execute at all, what happens at runtime. Let's start with unit tests. Your unit tests get compiled into a test bundle. At runtime, Xcode launches an instance of your app which serves as a test runner. The runner loads the test bundle and executes all of its tests, so that's how unit tests execute. What about UI tests? For UI, tests the story is similar. Your tests still get compiled into a bundle, but the bundle is loaded by a custom app that Xcode creates. Your app no longer runs the tests. Instead, the tests automate your app by launching it and interacting with different parts of its UI. If you'd like to learn more about this process, I'd encourage you to check out our session from 2016, where we go into even more detail. So, now that we understand how our tests execute, we can finally talk about how Xcode runs them in parallel. Just like before, Xcode will launch a test runner to execute our tests but instead of just launching a single runner, Xcode will launch multiple runners, each of which executes a subset of the tests. In fact, Xcode will dynamically distribute tests to the runners in order to get the best utilization of the course on your machine. Let's go into more detail. When Xcode distributes tests to the runners, it does so by class. Each runner receives a test class to execute and it'll execute that test class before going on to execute another one. And then testing finishes once all the classes have executed. Now you might be wondering why Xcode distributes tests by class instead of distributing individual test methods to the runners. There are a couple reasons for this. First, there may be hidden dependencies between the tests in a class, like Honza talked about earlier. If Xcode were to take the tests in a class and distribute them to different runners, it could lead to hard to diagnose test failures. Second, each test class has a class level set up and tear down method, which may perform expensive computation. By limiting the tests in a class to a single runner, XE tests only has to invoke these methods once, which can save precious time. Now I'd like to talk about some specifics to parallel testing on the simulator. When you run tests in parallel on the simulator, Xcode starts by taking the simulator that you've selected and creating multiple distinct copies or clones of it. These clones are identical to the original simulator at the time that they're created. And Xcode will automatically create and delete these clones, as necessary. After cloning the simulator several times, Xcode then launches a test runner on each clone and that runner then begins executing a test class, like we talked about earlier. Now the fact that your tests execute on different clones of the simulator has some implications that you should be aware of. First, the original simulator is not used during testing. Instead, it serves as a sort of template simulator. You can configure it with the settings and the content that you want and then that content gets copied over to the clones when they're created. Next, there will be multiple copies of your app, one per clone, and each copy has its own data container. This means that if you have a test class that modifies files on disk, you can't expect those file modifications to be visible to another test class because it could have access to a completely separate data container. In practice, the fact that class is executed on different clones will likely be invisible to your tests, but it's something to be aware of. So, where can you run tests in parallel? You can run unit tests in parallel on macOS, as well as unit and UI tests in parallel on the iOS and tvOS simulators. And with that, I'd like to give you a demo of Parallel Distributed Testing in action. All right, so this is the Solar System app, which you may have seen at some of the other sessions here at Dub-Dub. As one of the developers of this app, I want to run all of my tests before committing and pushing my changes to the repository. However, as I add more and more tests, this is starting to take longer and longer, and it's becoming a bottleneck in my workflow. So, let's see how parallel testing can help. I'll go ahead and switch over to the Xcode project and I already ran my tests and I want to see how long they took to run. All right, let's go back to the test report. So, here, you can see next to each method, we show exactly how long that method took to execute. Next, for each test class, we show the percentage of passing and failing tests, as well as how long the test class took to execute. And finally, in the top right corner, you can see exactly how long all of your tests took to run. So, here, we can see that our tests took 14 seconds. Now I'd like to enable parallelization, which I can do by going to the scheme. So, I'll select the scheme and choose edit scheme. Next, I'll click the test action and I'll click the options button next to my test target. Finally, I'll select the execute in parallel checkbox and that's all I need to do to enable parallel testing. So, let's go ahead and run our tests now by choosing product tests. So, I want you to look at the doc. Xcode launches multiple copies of our Mac app to run our unit tests in parallel. So, great, it looks like that finished. So, now, let's go back to the report and select the most recent report. So, whereas previously, our test took 14 seconds to run, now they only take 5 seconds. So, just by enabling parallelization, we've seen the greater than 50% improvement in test execution time. So, the Solar System app isn't just available for the Mac, it also has an iOS companion. And one of my responsibilities was to write a UI test suite to exercise the various screens of my iOS app. Now I already enabled parallelization for the iOS scheme, so I'll go ahead and just switch to that now. And then I'll run tests by choosing product, test. Now we'll go ahead and switch over to the simulator. So, here, you can see that Xcode has created multiple clones of the simulator that I selected, and these clones are named after the original simulator so that they're easy to identify. And on each simulator, Xcode has launched a test runner and each runner is executing a different test class in my suite. Now while my tests are executing, I'm going to switch back to Xcode and show you the test log. You can find the test log associated with the test report. The log is a great place to see how your classes are being distributed to the different runners. You can see an entry in the log for each runner and beneath a given runner, you can see the exact test class that it is currently executing. So, when the tests are completely finished, this is a great place to see how your classes were distributed, which gives you a picture of the overall parallelization. And with that, let's switch back to the slides. So, let's briefly recap what we learned in the demo. First, we saw how to enable parallelization in the scheme editor. Next, we saw how to view results in the test report, as well as how our classes are distributed in the test log. Then we saw Xcode launch multiple instances of our Mac app to run unit tests in parallel. And finally, we saw multiple clones of the simulator running our UI tests in parallel. Like I mentioned earlier, xcodebuild has great support for parallel testing as well. We've added some new command line options that allow you to control this behavior and I'd like to point out two of those now. First, we have parallel-testing-worker-count, which allows you to control the exact number of workers or runners that Xcode should launch during parallel testing. Normally, Xcode tries to determine an optimal number of runners based off of the resources of your machine, as well as the workload. This means that on a higher core machine, you will likely experience more runners. But if you find that the default number isn't working well for you, you can override it using this command line option. Next, we have parallel-testing-enabled, which allows you to override the setting in the scheme to explicitly turn parallel testing on or off. Now for the most part, all you need to do to get the benefits of parallel testing is just to turn it on. But here are few tips and tricks to help you get the most out of this feature. First, consider splitting a long-running test class into two classes. Because the test classes execute in parallel, testing will never run faster than the longest running class. When you run tests in parallel, you may notice a situation like this, where one test class dominates the overall execution time. If you take that class and divide it up, Xcode can more evenly distribute the work of test execution between the different runners, which can shorten the overall execution time. Now don't feel like you need to go and take all of your classes and divide them up. That shouldn't be necessary but if you notice a bottleneck like this, this is something to try. Next, put performance tests into their own bundle with parallelization disabled. This may seem counterintuitive, but performance tests are very sensitive to system activity so if you run them in parallel with each other, they'll likely fail to meet their baselines. And finally, understand which tests are not safe for parallelization. Most tests will be fine when you run them in parallel, but if your tests access a shared system resource, like a file or a database, you may need to introduce explicit synchronization to allow them to run concurrently. Speaking of testing tips and tricks, if you'd like to learn more about how to test your code, I'd encourage you to check out our session on Friday hosted by my colleague Stuart and Brian [assumed spellings]. You won't want to miss it. To wrap up, we started today's talk with code coverage and the new improvements to performance and accuracy. Next, we talked about new test selection and ordering features, which allow you to control exactly which tests run, as well as the order in which they're run. And finally, we talked about Parallel Distributed Testing, which allows you to distribute test classes between different runners to execute them in parallel. For more information, I'd encourage you to download our slides from developer.apple.com and come see us in the labs later this afternoon. Have a great WWDC, everyone.  Hi, everybody, I'm Ben. I work on the Swift standard library. And together with my colleague Doug, from the compiler team. We're going to talk to you about Swift generics. So the recent releases of Swift have added some important new features. Including conditional conformance and recursive protocol constraints. And, in fact, with every release of Swift, we've been refining the generic system, making it more expressive. And we feel that the 4.2 release marks an important point. It's the point where we can finally fully implement a number of designs that have always been envisioned for the standard library. Something that's critical for us in achieving our goal of API stability for Swift. So, we've given a lot of talks about generics in the past, but we haven't taken a step back. And talked about generics as a whole for a while. So today, we're going to take you through a few different features of the generics system. Both new and old, to help understand how they fit together. I'm going to briefly recap the motivation for generics. We're going to talk about designing protocols, given a number of concrete types, using examples taken from the standard library. We're going to review protocol inheritance, and talk about the new feature of conditional conformance. And how it interacts with protocol inheritance. And finally, we're going to wrap up with a discussion of classes and generics. So why are generics such an important part of Swift? Well one way of seeing their impact is by designing a simple collection, like type. We'll call it buffer, and it's going to be similar to the standard library's array type. Now, the simplest possible API for the reading part of a buffer might include a count of the number of elements. And a way to fetch each element to the given position in the index. But, what do we make of that return type? Now, if we didn't have generics, we'd have to make it some kind of type that could represent anything that we'd want to put inside the buffer. You can call that type ID or object or void star. In Swift we call it Any, which is a type that can stand in for any different kind of type in Swift. So if you wanted to handle anything in the buffer, you could have subscript return an Any. But, of course, you probably know that that leads to a really unpleasant user experience. At some point, you've got to get out that type from inside the box. In order to actually use it. And this isn't just annoying, it's also error-prone. What if somewhere in your code, maybe by accident, you put an integer into what was supposed to be a buffer of strings? But it's not just about ease of use. We also want to solve some problems relating to how these values are represented in memory. Now, the ideal representation for a buffer of strings, would be a contiguous block of memory. With every element held in line next to each other. But with an untyped approach, this doesn't work out quite so well. Because the buffer doesn't know in advance what kind of type it's going to contain. And so it has to use a type like Any that can account for any of the possibilities. And, there's a lot of overhead in tracking, boxing, and unboxing the types in that Any. Here, I might have just wanted a buffer of integers, but I have no way of expressing that to the compiler. And so, I'm paying for flexibility, even though I'm not interested in it. What's more, because Any has to account for any different kind of type. Including types that are too large to fit inside its own internal storage, it has to sometimes use indirection. It has to hold a pointer to the values, and that value could be located all over memory. And so we really want to solve these problems, not just for ease of use and correctness, but also for performance reasons. And, we do it using a technique called parametric polymorphism. Which is just another term for what we in Swift refer to as generics. With a generic approach, we put more information on the buffer, to represent the type that the buffer is going to contain. We'll call that type Element. Element is a generic parameter of the type, hence the term of parametric polymorphism. You can think of it kind of like a compile-time argument that tells the buffer what it's going to contain. Now it has a way of referring to that element type. It can use it wherever it was previously using Any. And, that means that there's no need to do conversions when you're getting a type out of the buffer. And if you make an accidental assignment of the wrong kind of type, or some issue similar to that. The compiler will catch you. Now, now there's no such type as buffer without an associated element type. If you try to declare a type like that, you'll get a compilation error. You might find that slightly surprising. Because sometimes you'll see that you can declare types like buffer without any element type. But, that's just because the compiler is able to infer what the element type ought to be from the context. In this case, from the literals on the right-hand side here. The element is still there, it's just implicit. This knowledge of exactly what type a buff-- a type like buffer contains is carried all the way through both compile and runtime. And this means that we can achieve our goal of holding all of the elements in a contiguous block of memory, with no overhead. Even if those types are arbitrarily large. And because the compiler has direct knowledge at all times of exactly what element type the buffer contains. It has optimization opportunities available to it that it wouldn't otherwise have. So, in the case here, where I've declared a buffer of integers. A loop like this ought to be compiled down to just a handful of very efficient CPU instructions. Now, if you were writing a loop like this, on a regular basis. To sum up a buffer of integers, it might make sense to extract it out into a method. An extension on buffer that's more unit-testable, and more readable when you actually call it. But, you probably know that if you've written code like this, you'll get a compilation issue, because not all element types can be summed up like this. We need to tell the compiler more about the capabilities the element needs to have, in order to make this method available on a buffer. Now, the easiest way to do that is by constraining the element type to be a specific type like the Int from our original loop. If you take this easy approach to get up and running with your extension, it's easy to generalize it later. When you find that you need to do something different. Like sum up a buffer of doubles, or floats. Just look at the type that you've constrained to. Look at the protocols it conforms to. And follow them up until you get the most general protocol that gives you everything that you need to do your work. In this case, the numeric protocol, which gives us the two things we're relying on here. The ability to create a new element with a value of zero, and the ability to add elements to it. Which come as part of the numeric protocol. Now, let's talk about that process of factoring out protocols from various types. So we've been talking about this buffer type, and we can make it generic across different elements. But what about writing generic code that's generic in a different direction? Or writing code that works on any different kind of collection? Such as an array that's very similar to our buffer type. But also more varied types, like a dictionary that's a collection of key value pairs. Or maybe types that aren't generic or are the different element types, like data or string that returns specific element types. We want to create a protocol that captures all of their common capabilities. We're going to create a, a cut down, simplified version of the standard library's own collection protocol. So notice that we considered a varied number of concrete types first. And now, we're thinking about a kind of protocol that could join them all together. And, it's important to think of things as this way around. To start with some concrete types, and then try and unify them with a protocol. What do those types have in common? What don't they have in common? When you're designing a protocol like this, you can think of it kind of like a contract negotiation. There's a natural push and pull here, between conforming types on the one hand. That want as much flexibility as possible in fulfilling that contract. And users of the protocol, that want a really nice, tight, simple protocol in order to do their extensions. That's why it's really important to have both a variety of different possible conforming types. And a number of different use cases in mind when you're designing your protocol. Because it's a balancing act. So, let's start to flesh out the collection protocol. So, first we need to represent the element type. Now, in protocols, we use an associated type for that. Each conforming type needs to set element to be something appropriate. In the case of buffer, or array, as of Swift 4.2, this happens automatically. Because we also named their generic parameters to be element as well. This is a nice side benefit of giving your generic arguments meaningful names that follow common conventions like the word element. Rather than giving them something arbitrary like T that you'd have to separately state was the element type. For other data types, you might need to do something slightly more specific. For example, a dictionary needs to set the element type to be the pair of its key and value type. Next, let's talk about adding the subscript operation. Now, if we were talking about just a protocol for types like array, we might be tempted to have subscripts take an Int as its argument. But making subscript take an Int would imply a very strong contract. Every conforming type would have to supply the ability to fetch an element's given position that was represented by an integer. And, that works great for types like array. It's also definitely easy for users of the protocol to understand. But is it flexible enough for a slightly more complicated type, like a dictionary? Now no matter how you model it, a dictionary's probably going to be backed by some fairly complicated internal data structure. That has specific logic for moving from one element to the next. For example, it could be backed by an internal buffer of some kind, and it could use an index type that stored an offset into that buffer. That it could then take as the argument to subscript in order to fetch an element to the position, using that offset. But it would be critical that the dictionary's index type be an opaque type that only the dictionary can control. You wouldn't want somebody necessarily just adding one to your offset. That wouldn't necessarily move to the next element in the dictionary. It could move some arbitrary, maybe uninitialized part of the dictionary's internal storage. So instead we want the dictionary to control moving forward through the collection by advancing the index. And so to do that, we add another method. That given an index, gives you the index that marks the position after it. Once you take this step, you need a couple more things. You need a start index property, and an end index property. Because a simple count isn't going to work anymore in order to tell us that we've reached the end. Now that we're not using Ints as our index type. So let's bring those back to the collection protocol. So we've got a subscript that takes some index type to represent a position, and gives you an element there. And, we've got a way of moving that position forward. But we also need types to supply what kind of type they're going to use for their index. We do that with another associated type. Conforming types would supply the appropriate types. So an array or a data would give an Int as their index type. Whereas a dictionary would give its own custom implementation that handles its own internal logic. So let's go back to count that we dropped a minute ago in order to generalize our indexing model. It's still a really useful property to have. So we probably want to add it back as an extension on collection. Something that walks over the collection, moving the index forward, incrementing a counter that it then returns. Now, if we try and implement this, we hit another missing requirement. Since we moved off of Int to a general index type, we can no longer assume that the index type was equatable. Ints are, but arbitrary index types aren't necessarily. And, we need that in order to know that we've reached the end. Now, we could solve this in the same way that we did earlier, of constraining our extension. Say that it only works when the index type is equatable. But, that doesn't feel right. We want a protocol to be easy to use. And it's going to get really irritating, if we have to always, on every extension we write, put this constraint on there. Because we're nearly always going to need to be able to compare two indexes. Instead, it's probably better expressed as a requirement of the protocol. As a constraint on our index-associated type. Putting this constraint on the protocol means that all types that conform to the protocol need to supply an equatable type for their index. That way you don't have to specify it every time you write the extension. This is another example of negotiating the protocol contract. Users of the protocol had a requirement that they really needed to be able to compare indexes. And, conforming types, they did a check that they can reasonably accommodate that without giving up too much flexibility. In this case, they definitely can. Ints, the data, and array are using are already equatable. And, with Swift 4.2's new automatic synthesis of equatable conformance. It's easy for dictionary to make its index type equatable as well. Next, let's talk about optimizing this count operation with a customization point. So, we've written a version of count, that calculates the number of elements in the collection by walking over the entire collection. But, obviously a lot of collections can probably do that a lot faster. For example, supposing a dictionary kept internally a count of the number of elements it held, for its own purposes. If it has this information, it can just serve it up in its own implementation of count. That means that when people call count on a dictionary, they're getting fast constant time. Instead of the linear time that our original version that works with any collection takes. But, when adding optimizations like this, there's something you need to be aware of. Which is the difference between fulfilling protocol requirements, and just adding lots of overloads onto specific types. Up until now, this new version of count on dictionary is just an overload. That means that when you have a dictionary, and you know it's a dictionary. You'll get the newer, better version of count. But, what about calling it inside a generic algorithm? So supposing we wanted, for example, to write a version of the standard library's map? If you're not already familiar with it, it's a really useful operation that transforms each element in the collection. And gives it back to you as a new array. The implementation's pretty simple. It just creates a new array, moves over the collection, transforms each element. And then appends it to the array. Now, as you append elements to an array like this, the array automatically grows. And, as it grows, it needs sometimes to re-allocate its internal storage. In order to make more room to accommodate the new elements. In a loop like this, it might have to do that multiple times over, depending on how big it gets. And, doing that takes time. Allocating memory can be fairly expensive. There is a nice optimization trick we can do with this implementation. We already know exactly how big the final array is going to be. It's going to be exactly the same size as our original collection. So we could reserve exactly the right amount of space in the array up front, before we start appending to it, which is a nice speed-up. And to do this, we're calling count. But, we're calling count here, in what's referred to as a generic context. That is, a context where the collection type is completely generic, not specific. It could be an array, or a dictionary, or a link list, or anything. So, we can't know that it necessarily has a better implementation of count available to it. When the compiler compiles this code. And so, in this case, the version of count that's going to be called is actually the general version of count. That works on any collection and iterates over the entire collection. If you called map on a dictionary, it wouldn't call the better version of count that we've just written yet. In order for customized method or property like this to be called in a gen-- in a generic context. It needs to be declared as a requirement on the protocol itself. We've established that there's definitely a way in which certain collections could provide an optimized version of count, so it makes sense to add it as a requirement on the protocol. Now, even though we've made it a requirement to implement it, all collections don't have to provide their own implementation. Because we've already provided one via our extension that will work on any collection. Adding a requirement to the protocol, and alongside it adding a default implementation via an extension is what we refer to as a customization point. With a customization point, the compiler can know that there's potentially a better implementation of a method or property available to it. And so, in a generic context, it dynamically dispatches to that implementation through the protocol. So now, if you call map on a dictionary, even though it's a completely generic function. You will get the better implementation of count. Adding customization points like this, alongside default implementations through extensions. Is a really powerful way of getting the same kind of benefit that you can also get with classes, implementation inheritance, and method overwriting. But, this technique works on structs and enums, as well as classes. Now, not every method can be optimized like this. And, customization points have a small but non-zero impact on your binary size, your compiler runtime performance. So, it only makes sense to add customization points when there's definitely an opportunity for customization. For example, in the map operation that we just wrote. There's no reasonable way in which any different kind of collection could actually provide a better implementation. And so, it doesn't make sense to add it as a customization point. It can just stay as an extension. So, we've created this collection type, and it's actually pretty fully-featured now. It has lots of different conforming types possible. And various different useful algorithms you can write for it. But, sometimes you need more than just a single protocol in order to categorize your family of types. You need protocol inheritance. And, to talk to you more about that, here's Doug. Thank you, Ben. So, protocol inheritance has been around since the beginning of Swift. And, to think about where we need protocol inheritance. Let's go look at this collection protocol that we've been building. It's a nice protocol. It's well-designed. It describes a set of conforming types, and gives you the ability to write interesting generic algorithms on them. But, we don't have to reach very far to find other collection-like algorithms that we cannot implement in terms of the collection protocol thus far. For example, if we want to find the index of the last element in a collection, that matches some predicate. The best way to do that would be to start at the end, and walk backwards. Collection protocol doesn't let us do that. Or say we want to build a shuffle operation to randomly shuffle around the elements in a collection. Well, that requires mutation, and collection doesn't do that. Now it's not that the collection protocol is wrong. But it's that we need something more to describe these additional generic algorithms, and that is the point of protocol inheritance. So, here the bidirectionalCollection protocol inherits from, or is a collection. What that means is that any type that conforms to the bidirectionalCollection protocol also conforms to collection, and you can use those collection algorithms. But bidirectionalCollection adds this additional requirement, of being able to step backwards in the collection. An important thing to note is not every collection can actually implement this particular requirement. Think of a singlyLinkedList, where you only have these pointers hopping from one location to the next. There's no efficient way to walk backward through this sequence, so it cannot be a bidirectionalCollection. So, once we've introduced inheritance, you've restricted the set of conforming types. But you've allowed yourself to implement more interesting algorithms. So, here's the code behind this last index where operation. It's fairly simple. We're just walking backwards through the collection, using this new requirement from the bidirectionalCollection protocol. Let's look at a more interesting algorithm. So here's a shuffle operation. So, it was introduced for, for collections in Swift 4.2. You don't have to implement it yourself, but we're going to look at the algorithm itself to see what kinds of requirements it introduces to figure out how to categorize those into protocols meaningfully. So the Fisher-Yates shuffle algorithm's a pretty old algorithm. It's also fairly simple. You start with an index to the first element in the collection. And then, you select randomly some other element in the collection, and swap those two. In the next iteration, you move the left index forward one. Randomly select between there and the end, swap those elements. And so, the algorithm is pretty simple. It's just this linear march through the collection, randomly selecting another element to swap with. And, at the end of this, you end up with a nicely shuffled collection. So, we can actually look at the code here. It's a little bit involved. Don't worry about that. And, we're going to implement it on some kind of collection. So, we'll look at the core operations in here. So, first we need to be able to grab a random number between where we are in the collection and the end of the collection, using this, this random facility. But, that's an integer. And what we need is an index into the collection. We know those are different. So we need some operation. Let's call it index offsetBy. To jump from the start index quickly over to whatever position we've selected. The other operation we need is the ability to swap two elements. Great. We have two operations that we need to add to the notion of a collection to be able to implement shuffle. Therefore, we have a new shuffleCollection protocol. Please don't do this. So this is an anti-pattern that we see. And the anti-pattern here is we had one algorithm. We found its requirements, and then we packaged it up into a protocol that is just that one-- just describes that one algorithm. If you do this, you have lots and lots and lots of protocols around that don't have any interesting meaning. You're not learning anything from those protocols. So what you should do is notice that we actually have distinct capabilities here. So shuffle is using random access, and it's using mutation. But, these are, these are separate, and we can categorize them in separate protocols. So, for example, the randomAccessCollection protocol is something where it allows us to jump around the collection, moving indices quickly. And there are types like unsafeBufferPointer that can give you random access. But, do not allow any mutation. That's a separate capability. So, we also have the mutableCollection protocol here. And, we can think of types here that allow mutation, but not random access, like the singlyLinkedList that we talked about earlier. Now, you notice that we've essentially split the inheritance hierarchy here. We've got the access side for random access, bidirectional, and so on. And then, we've got this mutation side. That's perfectly fine, because clients themselves can compose multiple protocols to implement whatever generic algorithm they're doing. So, we go back to our shuffle algorithm. And it can be written as an extension on randomAccessCollection, with a self-type. So this is the type that conforms to randomAccessCollection also conforms to the mutableCollection protocol. And now, we've pulled together the capabilities of both of these. Now, when you have a bunch of conforming types, and a bunch of generic algorithms, you tend to get protocol hierarchies forming. Now, these hierarchies, they shouldn't be too big. They should not be too fine-grained. Because you really want a small number of protocols that really describe the kinds of types that show up in the domain, right? And now, there's things, things that you notice when you do build these protocol hierarchies. So, as you go from the bottom of the hierarchy to the top, you're going to protocols that have fewer requirements. And therefore, there are more conforming types that can implement those requirements. Now, on the other hand, as you're moving down the hierarchy, and combining different protocols from the hierarchy. You get to implement more intricate, more specialized algorithms that require more advanced capabilities. But naturally work with fewer conforming types. Okay. So let's talk about conditional conformance. This is, of course, a newer feature in, in Swift. And, let's start by looking at slices again. So for any collection that you have, you can form a slice of that collection by subscripting with a particular range of indices. And, that slice is essentially a view into some part of the collection. Now these are default type that you get from slicing a collection, is called slice. And slice is a generic adaptor type. So it is parameterized on a base collection type, and it is itself a collection. So our expectation on a slice is that you can do anything to a slice that you can do to the underlying collection. It's a reasonable thing to want. And so, certainly we can go and use the forward search operations like indexwhere, to go find something matching a predicate. And that works on the collection and any slice of that collection. So, we'd like to do the same thing with backwards search, but here we're going to run into a problem. So even if the buffer is a bidirectionalCollection, nothing has said that the slice is a bidirectionalCollection. We can fix that. Let's extend slice to make it conform to the bidirectionalCollection protocol. We need to implement this index before operation, which we can implement in terms of the underlying base collection. Except the compiler's going to complain here. The only thing we knew about that base collection is that it's a collection. It doesn't have an index before operation on it. We know how to fix this. All we need to do is introduce a requirement into this extension to say that well, base needs to be a bidirectionalCollection. This is conditional conformance. All it is, is extensions that declare conformance to a protocol. And then the constraints under which that conformance actually makes sense. And the wonderful thing about conditional conformance, is it stacks nicely when you have these protocol hierarchies. So we can also state that slice is a randomAccessCollection. When its underlying base type is a randomAccessCollection. Now, notice that I've written two different extensions here. Now, it's generally good Swift style. Write an extension, have it conform to one protocol, so you know what that extension is for, you know its meaning. It's particularly important with conditional requirements, conformances, because you have different requirements on these extensions. And, this allows for composability. Whatever the underlying base collection can do, the slice type can also do. So let's look at another application of conditional conformance, also in the standard library, and these are ranges. So, ranges have been around forever in Swift. And, you can form a range with, for example, these dot-dot less than operations. And so you can form ranges of doubles, you can form ranges of integers. But some ranges are more powerful than others. So, you can iterate over the elements in a range of integers. Well, why can you do that? It was because an intRange conforms to collection. Now, if you're actually look at the type, it's reduced by that dot-dot less-than operator. It is aptly named the range type. Again, it's generic over the underlying bound type. So in this case, we have a range of doubles, and it merely stores the lower and upper bounds. That's fairly simple. But, prior to Swift 4.2, you would get from an integer range, an actually different type. This is the countableRange type. Now, notice it's structurally the same as the range type. It has one type parameter. It has lower and upperBound. But it adds a couple additional requirements onto that bound type. That the bound be stridable, right? Meaning you can walk through and enumerate all the elements. Now that's the ability you need so that you can make countableRange conform to randomAccessCollection. That enables the forEach, the forEach iteration loop, and other things. But with conditional conformance, of course, we can do better. So let's turn the basic range type into a collection, when the bound type conforms this-- has these extra stridable requirements on it. It's a simple application of conditional conformance. But it makes the range type more powerful when used with better type parameters. Now, notice that I'm just conforming to randomAccessCollection. I have not actually mentioned collection or bidirectionalCollection. With unconditional performances, this is okay. Declaring conformance to randomAccessCollection implies conformances to any protocols that it inherits. In this case, bidirectionalCollection and collection. However, with conditional conformance, this is actually an error. Now, if you think back to the slice example, we needed to have different constraints for those, for those different levels of the hierarchy for collection. Versus bidirectionalCollection versus randomAccessCollection. And so, compiler's enforcing that you've thought about this, and made sure that you have the right set of constraints for conditional conformance. In this case, the constraints across the entire hierarchy are the same. So, we can just write out explicitly collection and bidirectionalCollection. To assert that this is where all these conformances are. Or we can do the stylistically better thing, and split out the different conformances. Now at this point, our range type is pretty powerful. It does everything the countableRange does. So what should we do with countableRange? We could throw it away. In this case we're talking about the standard library, and there's a lot of code that actually uses countableRange. So we can keep it around as a generic type alias. This is a really nice solution. So the generic type alias adds all of those extra requirements you need to make the range countable. The requirements you need to turn it into a collection, but it's just an alternate name for the underlying range type. Again, this is great for source compatibility, because code can still use countableRange. On the other hand, it's also really nice to give a name to those ranges that have additional capabilities of being a randomAccessCollection. In fact, we can use this to clean up other code. To say, well, we know what a countableRange is. It's a range with this extra striding capability, so we can go extend countableRanges. And that is a case in which we have randomAccessCollection conformance. So, we've introduced this in Swift 4.2 to help simplify the set of types that we're dealing with. And make the existing core types like range more composable and more flexible. Recursive constraints describe relationships among protocols and their associated types. This is a topic that we didn't cover in the WWDC version of this talk. But it's an important part of the standard library's use of Swift's generic system. Let's jump right in. A recursive constraint is nothing more than a constraint within a protocol that mentions that same protocol. Here, collection has an associated type named subsequence. That is itself a collection. Why would you need this? Well, let's look at a generic algorithm that relies on it. So here, given an already sorted collection. We want to find the index at which we should insert a new value. To maintain that sort order. We're going to compute the sorted insertion point for the value 11. When we go ahead and insert 11 at that index, the result is still a sorted array. The sorted insertion point of function is implemented in terms of a binary search. Binary search is a classic divide-and-conquer algorithm. Meaning that at each step, it makes a decision that allows it to significantly reduce the problem size. For the next step to consider. For binary search, we first look at the middle element, 8. And compare it against the value that we want to insert. That's 11. And because 11 is greater than 8, we know that 11 needs to be inserted somewhere after the 8. In the latter half of the collection. So we restrict our search space by half. In our next step, we find the new middle, 14, and compare it against the value we want to insert. Eleven is less than 14, so the insertion point has to come before the middle. Divide the remaining collection in half again. Continue dividing collection we're looking at in half. Until we're pointing at the proper insertion point. That's our solution. Divide-and-conquer algorithms like this are fantastic. Because they're extremely fast. Binary search takes logarithmic time. Which means that doubling your input size doesn't make the algorithm run twice as slowly like it would for a linear algorithm. With a logarithmic algorithm like binary search, it only has to perform one additional step to cut the problem size in half again. So let's turn that into code. The first thing we need to do is find the index of the middle element. Which we can do using randomAccessCollections index offset by a function. Next, we check whether our value comes before the middle element. So we know which half of the collection contains our insertion point. Now in our example, the value to insert is greater than the middle element. So we take a slice of the collection from the index after the middle. All the way until the end. And recursively call sort and insertion point of on that slice. This is common of divide-and-conquer algorithms. Where you reduce the problem space and then recurse. Now to make this work, we need that slicing syntax. Provide a suitable slice of a collection. We can do this for all collections by introducing a general operation that takes a range of indices and produces a slice. Like this. Now remember that the slice adapter we discussed earlier works on any collection. Providing a view of the elements of the underlying collection that is itself a collection. This makes our divide-and-conquer algorithm work for any collection. As well as providing slicing syntax to all collections. That's great, but there's just one problem. Some collections don't want this particular slice type. They really want to provide their own slicing operations that produce a different type. String is the most common example. When you slice a string, you get back a substring. And so if you apply our divide-and-conquer algorithms to the string collection. You really want those to be in terms of substring. Rather than some other type like the slice of a string. Range is another interesting example. Because its slicing operation returns an instance of the exact same range type just with the new bounds. So to capture this variation among different types that conform to collection, we can introduce new requirements into the collection protocol. Specifically for slicing. So here we've pulled the slicing subscript into the collection protocol itself as a requirement. Now note that the result type of this subscript is described by a new associated type: subsequence. Now both string and range meet these new requirements of collection. With string, the subsequence type is substring. For range, the subsequence type is going to be the range itself. Now this works well for, for string and range. But for all the other collection types that don't want to customize the actual subsequence type. We can provide default limitations of slicing. So the authors of these collection type don't actually have to do any extra work to conform to collection. They get all the slicing behavior for free. So we're going to start with subsequence. Associated types themselves can have default values. Written after the equals sign. For subsequence, the slice adaptor type is a perfect default because it works for all collections. So this default will be used for any conforming type that doesn't provide its own subsequence type. This pairs well with the implementation of the slicing subscript we started with earlier. Written in extension on the collection protocol. And it can act as a default implementation, providing the slicing subscript operation that returns a slice. We can even go one step further and limit the applicability of our default slicing subscript implementation to those cases where we picked the default subsequence type. So this pattern prevents the default implementation from showing up as an overload on collection types that have customized their subsequence. Like string and range. So this is all great for conforming types. They get slicing for free, or they can customize it if they want to. But remember our goal here. We're trying to write our divide-and-conquer algorithms against the collection protocol. So we have to answer one really important question. What does subsequence do? All we know about subsequence right now is that it's the result type of the slicing subscript operation. But we need more to actually use it. So to answer this question, we have to go back to the algorithms that we want to write in terms of subsequence. Our algorithm is recursive. It forms a slice which is now a value of the subsequence type. And then recursively calls sort insertion point of on that slice. Now this only makes sense if the subsequence type you get back is itself a collection. Now when it performs that call, we're going to pass a value of the collection's element type. But the recursive call itself is going to expect a value of this subsequence's element type. The only way this can possibly make sense is if those element types are identical. Now the same issue comes up when returning an index from the recursive call. Which is going to be computed in terms of the subsequence. But that index that's returned also needs to be a valid index for the current collection. So we can capture all of these requirements in the collection protocol itself. Now the first thing we want to do is say that the subsequence of a collection is itself a collection. This is a so-called recursive constraint. Because the associated type conforms to its own enclosing protocol. We can then use associated type where clauses to further constrain our subsequence. As we talked about earlier, it has an element type. And that element type needs to be the same as that of the original collection. So we can express that here with the same type constraint. Subsequent element is the same as element. We can do exactly the same thing for the index type. Now, these cover all the properties that we discovered by looking at the implementation of the sorted insertion point of algorithm. This leads us to an interesting question. Can you slice a subsequence? Well, every subsequence is a collection and every collection has a slice operation. So of course you can slice a subsequence. And the result is going to be a subsequence of the subsequence. Now you can do this again and get a subsequence of a subsequence of a subsequence. And keep on going on and on and on. Now interestingly, at each point we could have a brand-new type. And so we have this potentially infinite tower of types. That's actually okay. Each recursive step in our generic algorithm could conceivably create a new type. Based on the current collection type. So long as the recursion eventually terminates at runtime, there's no problem with this. However, it's often the case divide-and-conquer algorithms can be implemented more efficiently by making them nonrecursive. So here is the nonrecursive implementation of the sort and insertion point of algorithm. We're going to walk through it. Because the core algorithm is the same. But it's expressed iteratively with this while loop rather than recursively. So the first thing we're going to do is take a slice of the whole collection. This slice variable is going to represent the part of the collection that we're looking at in each iteration. And now we see the familiar divide-and-conquer pattern. Find the middle of the slice. And then compare the value to insert against the middle element in the slice. We then narrow the search base by slicing the slice before we go in loop again. However, here we have a problem. We're performing an assignment to the slice variable. Which is of the subsequence type. On the other hand, the right-hand side is a slice of a slice. And as we talked about before, this is a subsequence of the subsequence and could be a completely different type. So we're going to get a compiler error telling us that these two types are not necessarily the same. That's really inconvenient here because it prevents us from writing this nonrecursive algorithm. And it doesn't really reflect how specific collection types behave. Think about string. If you slice a string, you get a substring. If you slice a substring, you don't get a sub-substring. You just get another instance of the substring type. So let's go back to how this slice adapter works to generalize this notion. We have a collection. We're going to call it Self, that we've sliced from I to J. Now that's going to build something of the type slice of Self. Which is just a view on the underlying Self collection. If we then slice the slice, we get a slice of slice of self. Which is a view of a view on that same underlying Self collection. So this is our infinite tower of types in practice. However, it doesn't have to be this way. Remember that the slice types use the same indices as their underlying collection. And they also know about their underlying basic collection. So when we slice the slice, we can take those new indices, I2 and J2. Bring them back to the original base collection and form the new slice from there. And what this does is it means that when you slice a slice, you get something back of the same slice type. Effectively tying off the recursion. This is exactly the same behavior we saw with substring. And it's completely reasonable to expect that all subsequence types behave in this way. So let's model it as an explicit part of the collection's protocol requirements. So here we're saying that the subsequence of a subsequence is the same type as the subsequence. In other words, when you slice a slice, you get back the same slice. This makes our nonrecursive divide-and-conquer algorithm work. And simplifies the use of the collection protocol. There's no more need to reason about infinite tower of types. Now there's one last issue here involving subsequence. We've said it's required to be a collection. But we need the subsequence type to be a random access collection to perform this index offset by operation. To describe this, we can use protocol where clauses. So when bidirectionalCollection inherits from collection. It can add a new constraint on subsequence, requiring it to conform to that bidirectionalCollection protocol. This again is a recursive constraint but now it's expressed on the bidirectionalCollection protocol. We can do the exact same thing for randomAccessCollections. Such as the subsequence of a random access collection, itself conforms to randomAccessCollection. Note how the constraints on subsequence follow the enclosing protocol. This might sound a little bit familiar. Both recursive constraints and conditional conformance tend to track the protocol hierarchy like this. And the features support each other. This is particularly important because we want the default associated type for subsequence, the slice of Self, to work at every level of the collection hierarchy. Slice is always a collection. When we go ahead and create the bidirectionalCollection protocol. It now requires that the subsequence type also conform to bidirectionalCollection. The slice adapter's conditional conformance to bidirectionalCollection which kicks in anytime itself is known to be a bidirectionalCollection. Satisfies that requirement. RandomAccessCollection works the same way. Subsequence gains a randomAccessCollection requirement. And slices conditional conformance to randomAccessCollection satisfies that requirement. Now itself is known to be a randomAccessCollection. This behavior where an associated type default works for every protocol within the hierarchy is a good indicator of a cohesive design. If you find yourself needing different associated type defaults at different points in the collection hierarchy. You might have a problem with your design. Recursive restraints are a powerful tool. Used with associated type and protocol where clauses, they help us write the protocol requirements we need to express divide-and-conquer algorithms naturally in generic code. And now we return to the final portion of the WWDC talk. So, Swift is a multi-paradigm language. We've been talking exclusively about generics right now. But of course, Swift also supports object-oriented programming. And so, I'd like to take a few moments to talk about the interaction between those two features. How they work together in the Swift language. So with class inheritance, we know how class inheritance works. It's fairly simple. You can declare a superclass, like Vehicle. You can declare some subclasses, like Taxi and PoliceCar that both inherit from Vehicle. And, once you do this, you have this object-oriented hierarchy. You have some expectations about where you can use those subclasses. So if I were to extend Vehicle with a new method, to go let it drive, I fully expect that I can call that method on one of my subclasses, Taxi. So, this is a fundamental aspect of object-oriented programming. And, Barbara Liskov, actually described this really well in a lecture back in the '80s. Since then, we've referred to this as the Liskov substitution principle. And, the idea's actually fairly simple. So, if you have someplace in your program that refers to a supertype, or superclass, like Vehicle. You should be able to take an instance of any of its subtypes, or subclasses, like Taxi or PoliceCar, and use that instead. And the program should still continue to type check and run correctly. So, the substitution here is an instance of a subclass should be able to go in anywhere that the superclass was expected and tested. And, this is a really simple principle. We've all internalized it, but it's also really powerful. If you think about it. And at any point in your program think well, what happens if I get a different subclass, maybe a subclass I haven't thought about here. So, getting back to generics, what are our expectations when applying Liskov substitution principle to the generic system? Well, maybe we add a new protocol, Drivable. Whatever. And extend Vehicle to make it Drivable. What do we expect to happen? Well, we expect that you can use that protocol, conformance of Vehicle to Drivable, for some of its subclasses as well. Say, you add a simple generic algorithm to the Drivable protocol to go for a sundayDrive. Well, now you should be able to use that API on a PoliceCar, even if that might not be the best idea. So, the protocol conformance here is effectively being inherited by subclasses. And this puts a constraint on the conformance. The one conformance that you write, the thing that makes Vehicle Drivable. Has to work for all of the subclasses of Vehicle now and anyone that comes up with it later. Most of the time, that just works. However, there are some cases where this actually adds new requirements on the subclasses. The most common one is when dealing with initializer requirements. So, if you've looked at the decodable protocol, it has one interesting requirement. Which is the initializer requirement to create a new instance of the conforming type from a decoder. How do we use this? Well, let's go add a convenience method to the decodable protocol. It's a static method decode that creates a new instance from a decoder, essentially a wrapper for the initializer, making it easier to use. And, there's two interesting things to notice about this particular method. First, is it returns Self with a capital S. Remember this is the conforming type. It's the same type that you're calling the static method on. Now, the second interesting thing is, how are we implementing this? Well, we're calling to that initializer above to create a brand-new instance of whatever decodable type we have, and then return it. Fair enough. We can go ahead and make our Vehicle type Decodable. And then, what we expect, when applying the Liskov substitution principle, is we can use any subclass of Vehicle. With these new API's that we've built through the protocol conformance. So, we can call Decode on a Taxi. And what we get back is not a Vehicle not some arbitrary Vehicle instance, but the Taxi, an instance of Taxi. This is great, but how does it work? So let's take a look at what Taxi might have. Maybe there's an hourly rate here, and when we call Taxi.decode from, we're going through the protocol, going through the protocol initializer requirement. There's only one initializer this can actually call, and that's the initializer that's declared inside the Vehicle class, in the superclass here. So that initializer, it knows how to decode all of the state of a Vehicle. But it knows nothing about the Taxi subclass. And so, if we were to use this initializer directly, we would actually have a problem that the hourly rate would be completely uninitialized, which could lead to some rather unfortunate misunderstandings when you get your bill at the end. So, how do we address this? Well, it turns out Swift doesn't let you get into this problem. It's going to diagnose at the point where you try to make Vehicle conform to the decodable protocol that there's actually a problem with this initializer. It needs to be marked required. Now, a required initializer has to be implemented in all subclasses. Not just the direct subclasses, but any subclasses of those, any future subclasses you don't know about now. Now by adding that requirement, it means that when Taxi inherits from Vehicle, it also needs to introduce an initializer with the same name. Now, this is important because this initializer's responsible for decoding the hourly rate. And then chaining up to the superclass initializer to decode the rest of the Vehicle type. Okay. Now, if you're reading those red boxes really quickly, you may have noticed the subphrase non-final. So, by definition, final classes have no subclasses. So, it essentially exempts them from being substituted later on. That means that there's no sense in having a required initializer because you know there are no subclasses. And so final classes are in a sense a little easier to work with when dealing with things like decodable or other initializer requirements. Because they're exempt from these rules of having required initializers. So when you're using classes, for reference semantics, consider using final when you no longer need to customize your class through the inheritance mechanism. Now, this doesn't mean that you can't customize your class later. You can still write an extension on it. The same way you can extend a struct or an enum. You can also add conformances to it, to get more dynamic dispatch. But final can simplify the interaction with the generic system, and also unlock optimization opportunities for the compiler in runtime. So we've talked a bit about Swift generics today. The idea behind Swift generics is to provide the ability to reuse code while maintaining static type information. To make it easier to write correct programs, and compile those down into efficient, efficiently executing programs. When you're designing protocols, let this push and pull between the generic algorithms you want to write against a protocol. And the conforming types that need to implement that protocol guide your design to meaningful extractions. Introduce protocol inheritance when you need some more specialized capabilities to implement new generic algorithms that are only supportable on a subset of the conforming types. And, conditional conformance when you're writing generic types, so that they can compose nicely, especially when working with protocol hierarchies. And finally, when you're reasoning about the tricky interaction between class inheritance and the generic system. Go back to the Liskov substitution principle, and think about what happens here if I introduce a subclass rather than a superclass at which I wrote the conformance. Well, thank you very much. There's a couple of related sessions on embracing algorithms and understanding how they can help you build better code. As well as using Swift collections effectively in your everyday programming. Thank you.  Good morning. Welcome. I'm Michael, and this session is about what's new in Core ML. So introduced a year ago, Core ML is all about making it unbelievably simple for you to integrate machine learning models into your app. It's been wonderful to see the adoption over the past year. We hope it has all of you thinking about what great new experiences you can enable if your app had the ability to do things like understand the content of images, or perhaps, analyze some text. What could you do if your app could reason about audio or music, or interpret your users' actions based on their motion activity, or even transform or generate new content for them? All of this, and much, much more, is easily within reach. And that's because this type of functionality can be encoded in a Core ML model. Now if we take a peek inside one of these, we may find a neural network, tree ensemble, or some other model architecture. They may have millions of parameters, the values of which have been learned from large amounts of data. But for you, you could focus on a single file. You can focus on the functionality it provides and the experience it enables rather than those implementation details. Adding a Core ML model to your app is simple as adding that file to your Xcode project. Xcode will give you a simple view, describe what it does in terms of the inputs it requires and the outputs it provides. Xcode will take this one step further and generate an interface for you, so that interacting with this model is just a few lines of code, one to load the model, one to make a prediction, and sometimes one to pull out the specific output you are interested in. Note that in some cases you don't even have to write back code because Core ML integrates with some of our higher-level APIs and allows you to customize their behavior if you give them a Core ML model. So with Vision, this is done through the VNCoreML Request object. And in the new Natural Language framework, you can instantiate an MLModel from a CoreML model. So that's Core ML in a nutshell. But we are here to talk about what's new. We took all the great feedback we received from you over the past year and focused on some key enhancements to CoreML 2. And we are going to talk about these in two sessions. In the first session, the one you are all sitting in right now, we are going to talk about what's new from the perspective of your app. In the second session, which starts immediately after this at 10 a.m. after a short break, we are going to talk about tools and how you can update and convert models to take advantage of the new features in Core ML 2. When it comes to your app, we are going to focus on three key areas. The first is how you can reduce the size and number of models using your app while still getting the same functionality. Then we'll look about how you can get more performance out of a single model. And then we'll conclude about how using Core ML will allow you to keep pace with the state-of-the-art and rapidly moving field of machine learning. So to kick it off, let's talk about model size. I'm going to hand it off to Francesco. Thank you Michael. Hello. Every ways to reduce the size of your Core ML app is very important. My name is Francesco, and I am going to introduce quantization and flexible shapes, two new features in Core ML 2 that can help reduce your app size. So Core ML, [inaudible] why should you learn in models and device. This gives your app four key advantages compared to running them in the cloud. First of all, user privacy is fully respected. There are many machine-learning models on device. We guarantee that the data never leaves the device of the user. Second, it can help you achieve real-time performance. And for silicon [phonetic] and devices are super- efficient for machine-learning workloads. Furthermore, you don't have to maintain and pay for Internet servers. And Core ML inference is available anywhere at any time despite natural connectivity issues. All these great benefits come with the fact that you now need to store your machine-learning models on device. And if the machine-learning models are big, then you might be concerned about the size of your app. For example, you are -- you have your [inaudible] map, and it's full of cool features. And your users are very happy about it. And now you want to take advantage of the new opportunities offered by machine learning on device, and you want to add new amazing capabilities to your app. So what you do, you train some Core ML models and you add them to your app. What this means is that your app has become more awesome and your users are even happier. But some of them might notice that your app has increased in size a little bit. It's not uncommon to see that apps grow up, either to tens or hundreds of megabytes after adding machine-learning capabilities to them. And as you keep adding more and more features to your app, your app size might simply become out of control. So that's the first thing that you can do about it. And if these machine-learning models are supporting other features to your app, you can keep them outside your initial bundle. And then as the user uses the other features, you can download them on demand, compile them on device. So this -- in this case, this user is happy in the beginning because the installation size is unchanged. But since the user downloads and uses all the current Core ML functionality in your app, at the end of the day the size of the -- of your app is still large. So wouldn't it be better if, instead, we could tackle this problem by reducing the size of the models itself? This would give us a smaller bundle in case we ship the models inside the app, faster and smaller downloads if instead of shipping the models in the app we download them. And in any case, your app will enjoy a lower memory footprint. Using less memory is better for the performance of your app and great for the system in general. So let's see how we can decompose the size of a Core ML app into factors to better tackle this problem. First, there is the number of models. This depends on how many machine-learning functionalities your app has. Then there is the number of weights. The number of weights depends on the architecture that you have chosen to solve your machine-learning problem. As Michael was mentioning, the number of weight -- the weights are the place in which the machine-learning model stores the information that it has been learning during training. So it is -- if it has been trained to do a complex task, it's not uncommon to see a model requiring tens of millions of weights. Finally, there is the size of the weight. How are we storing these parameters that we are learning during training? Let's focus on this factor first. For neural networks, we have several options to represent and store the weights. And the first, really, is of Core ML in iOS 11. Neural networks were stored using floating-point 32-bit weights. In iOS 11.2, we heard your feedback and we introduced half precision floating-point 16 weight. This gives your app half the storage required for the same accuracy. But this year we wanted to take several steps further, and we are introducing quantized weights. With quantized weights we are no longer restricted to use either Float 32 or Float 16 values. But neural networks can be encoded using 8 bits, 4 bits, any bits all the way down to 1 bit. So let's now see what quantization here is. Here we are representing a subset of the weights of our neural networks. As we can see, these weights can take any value in a continuous range. This means that in theory, a single weight can take an infinite number of possible values. So in practice, in neural networks we store weights using floater 32 point -- Float 32 numbers. This means that this weight can take billions of values to better represent the -- their continuous nature. But it turns out the neural networks also work with lower precision weights. Quantization is the process it takes to discontinue strings of values and constrains them to take a very small and discrete subset of possible values. For example, here quantization has turned this continuous spectrum of weights into only 256 possible values. So before quantization, the weights would take any possible values. After quantization, they only have 256 options. Now since its weight can be taken from this small set, Core ML now needs only 8 bits of stored information of a weight. But nothing can stop us here. We can go further. And for example, we can constrain the network to take, instead of one of 56 different values, for example, just 8. And since now we now have only 8 options, Core ML will need 3-bit values per weight to store your model. There are now some details about how we are going to choose these values to represent the weights. They can be uniformly distributed in this range, and in this case we have linear quantization instead in lookup table quantization, we can have these values scattered in this range in an arbitrary manner. So let's see practically how quantization can help us reduce the size of our model. In this example, you are focusing on Resnet50, which is a common architecture used by many applications for many different tasks. It includes 25 million trained parameters and this means that you have to use 32-bit floats to represent it. Then the total model size is more than 100 megabytes. If we quantize it to 8-bits, then the architecture hasn't changed; we still have 25 million parameters. But we are now using only 1 byte to store a single weight, and this means that the model size is reduced by a factor of 4x. It's only -- it now only takes 26 megabytes to store this model. And we can go further. We can use that quantized representation that only uses 4 bits per weight in this model and end up with a model that is even smaller. And again, Core ML supports all the quantization modes all the way down to 8 bits. Now quantization is a powerful technique to take an existing architecture and of a smaller version of it. But how can you obtain quantized model? If you have any neural networking in Core ML format, you can use Core ML Tools to obtain a quantized representation of it. So Core ML 2 should quantize for you automatically. Or you can train quantized models. You can either train quantized -- with a quantization constraint from scratch, or retrain existing models with quantization constraints. After you have obtained your quantized model with your training tools, you can then convert it to Core ML as usual. And nothing will change in the app in the way you use the model. Inside the model, the numbers are going to be stored in different precision, but the interface for using the model will not change at all. However, we always have to consider that quantized models have lower-precision approximations of the original reference floating-point models. And this means that quantized models come with an accuracy versus size-of-the-model tradeoff. This tradeoff is model dependent and use case dependent. And it's also a very active area of research. So it's always recommended to check the accuracy of the quantized model and compare it with the referenced floating-point version for relevant this data and form metrics that are valid for your app and use case. Now let's see a demo of how we can use -- adopt quantized models to reduce the size of an app. I would like to show you a style transfer app. In style transfer, a neural network has been trained to render user images using styles that have been learned by watching paintings or other images. So let me load my app. As we can see, I am shipping this app with four styles; City, Glass, Oils and Waves. And then I can pick images from the photo library of the users and then process them blending them in different styles right on device. So this is the original image, and I am going to render the City style, Glass, Oils, and Waves. Let's see how this app has been built in Xcode. This app uses Core ML and Vision API to perform this stylization. And as we can see, we have four Core ML models here bundled in Xcode; City, Glass, Oils, and Waves, the same ones we are seeing in the app. And we can see -- we can inspect this model. These are seen as quantized model, so each one of these models is 6.7 megabytes of this space on disk. We see that the models take an input image of a certain resolution and produce an image called Stylized of the same resolution. Now we want to investigate how much storage space and memory space we can use by -- we can save by switching to quantized, models. So I have been playing with Core ML Tools and obtained quantizer presentation for these models. And for a tutorial about how to obtain these models, stay for Part 2 that is going to cover quantization with Core ML Tools in detail. So I want to focus first on the Glass style and see how the different quantization versions work for these styles. So all I have to do is drag these new models inside the Xcode project, and rerun the app. And then we are going to see how these models behave. First we can see that the size has been greatly reduced. For example, the 8-bit version already from 6 or 7 megabytes went down to just 1.7. In 4-bit, we can save even more, and now the model is less than 1 megabyte. In 3-bit, it is -- that's even smaller, at 49 kilobytes. And so on. Now let's go back to the app. Let's make this same image for reference and apply the Glass style in the original version. Still looks as before. Now we can compare it with the 8-bit version. And you can see nothing has changed. This is because 8-bit quantization methods are very solid. We can also venture further and try the 4-bit version of this model. Wow. The results are still great. And now let's try the 3-bit version. We see that there are -- we see the first color shift. So it probably is good if we go and check with the designers if this effect is still acceptable. And now, as we see the 2-bit version, this is not really what we were looking for. Maybe we will save it for a horror app, but I am not going to show this to the designer. Let's go back to the 4-bit version and hide this one. This was just a reminder that quantized models are approximation of the original models. So it's always recommended to check them and compare with the original versions. Now for every model and quantization technique, there is always a point in which things start to mismatch. Now we -- after some discussion with the designer, extensive evaluation of many images, we decided to ship the 4-bit version of this model, which is the smallest size for the best quality. So let's remove all the floating-point version of the models that were taking a lot of space in our app and replace them with the 4-bit version. And now let's run the app one last time. OK. Let's pick the same image again and show all the styles. This was the City, Glass, Oils, and big Wave. So in this demo we saw how we started with four models and they were huge, in 32-bit -- or total app size was 27 megabytes. Then we evaluated the quality and switched to 4-bit models, and the total size of our app went down to just 3.4 megabytes. Now -- This doesn't cost us anything in terms of quality because all these versions -- these quantized versions look the same, and the quality is still amazing. We showed how quantization can help us reduce the size of an app by reducing the size of the weight at the very microscopic level. Now let's see how we can reduce the number of models that your app needs. In the most straightforward case, if your app has three machine-learning functionalities then you need three different machine-learning models. But in some cases, it is possible to have the same model to support two different functions. For example, you can train a multi-task model. And multi-task models has been trained to perform multiple things at once. There is an example about style transferring, the Turi Create session about multi-task models. Or in some cases, you can use a yet new feature in Core ML called Flexible Shapes and Sizes. Let's go back to our Style Transfer demo. In Xcode we saw that the size of the input image and the output image was encoded in part of the definition of the model. But what if we want to run the same style on different image resolution? What if we want to run the same network on different image sizes? For example, the user might want to see a high-definition style transfer. So they use -- they give us a high-definition image. Now if I were Core ML model all it takes is a lower resolution as an input, all we can do as developers is size -- or resize the image down, process it, and then scale it back up. This is not really going to amaze the user. Even in the past, we could reship this model with Corel ML Tools and make it accept any resolution, in particular, a higher-resolution image. So even in the past we could do this feature and feed directly the high-resolution image to a Corel ML model, producing a high-definition result. This is because we wanted to introduce a finer detail in the stylization and the way finer strokes that are amazing when you zoom in, because they have -- they add a lot of work into the final image. So in the past we could do it, but we could do it by duplicating the model and creating two different versions: one for the standard definition and one for the high definitions. And this, of course, means that our app is twice as much the size -- besides the fact that the network has been trained to support any resolution. Not anymore. We are introducing flexible shapes. And with flexible shapes, you have -- if you have -- you can have the single model to process more resolutions and many more resolutions. So now in Xcode -- -- in Xcode you are going to see that this -- the input is still an image, but the size of the full resolution, the model also accepts flexible resolutions. In this simple example, SD and HD. This means that now you have to ship a single model. You don't have to have any redundant code. And if you need to switch between standard definition and high definition, you can do it much faster because we don't need to reload the model from scratch; we just need to resize it. You have two options to specify the flexibility of the model. You can define a range for its dimension, so you can define a minimal width and height and the maximum width and height. And then at inference pick any value in between. But there is also another way. You can enumerate all the shapes that you are going to use. For example, all different aspect ratios, all different resolutions, and this is better for performance. Core ML knows more about your use case earlier, so it can -- it has the opportunities of performing more optimizations. And it also gives your app a smaller tested surface. Now which models are flexible? Which models can be trained to support multiple resolutions? Fully convolutional neural networks, commonly used for MS processing tasks such as style transfer, image enhancement, super resolution, and so on -- and some of the architecture. Core ML Tools can check if a model has this capability for you. So we still have the number of models Core ML uses in flexible sizes, and the size of the weights can be reduced by quantization. But what about the number of weights? Core ML, given the fact that it supports many, many different architecture at any framework, has always helped you choose the right -- the model of the right size for your machine-learning problem. So Core ML can help you tackle the size of your app using this -- all these three factors. In any case, the inference is going to be super performant. And to introduce new features in performance and customization, let's welcome Bill March. Thank you. Thank you. One of the fundamental design principles of Core ML from the very beginning has been that it should give your app the best possible performance. And in keeping with that goal, I'd like to highlight a new feature of Core ML to help ensure that your app will shine on any Apple device. Let's take a look at the style transfer example that Francesco showed us. From the perspective of your app, it takes an image of an input and simply returns the stylized image. And there are two key components that go into making this happen: first, the MLModel file, which stores the particular parameters needed to apply this style; and second, the inference engine, which takes in the MLModel and the image and performs the calculations necessary to produce the result. So let's peek under the hood of this inference engine and see how we leverage Apple's technology to perform this style transfer efficiently. This model is an example of a neural network, which consists of a series of mathematical operations called layers. Each layer applies some transformation to the image, finally resulting in the stylized output. The model stores weights for each layer which determine the particular transformation and the style that we are going to apply. The Core ML neural network inference engine has highly optimized implementations for each of these layers. On the GPU, we use MTL shaders. On the CPU we can use Accelerate, the proficient calculation. And we can dispatch different parts of the computation to different pieces of hardware dynamically depending on the model, the device state, and other factors. We can also find opportunities to fuse layers in the network, resulting in fewer overall computations being needed. We are able to optimize here because we know what's going on. We know the details of the model; they are contained in the MLModel file that you provided to us. And we know the details of the inference engine and the device because we designed them. We can take care of all of these optimizations for you, and you can focus on delivering the best user experience in your app. But what about your workload? What about, in particular, if you need to make multiple predictions? If Core ML doesn't know about it, then Core ML can't optimize for it. So in the past, if you had a workload like this, you needed to do something like this: a simple for loop wrapped around a call to the existing Core ML prediction API. So you'd loop over some array of inputs and produce an array of outputs. Let's take a closer look at what happens under the hood when this -- when we are doing this. For each image, we will need to do some kind of preprocessing work. If nothing else, we need to send the data down to the GPU. Once we have done that, we can do the calculation and produce the output image. But then there is a postprocessing step in which we need to retrieve the data from the GPU and return it to your app. The key to improving this picture is to eliminate the bubbles in the GPU pipeline. This results in greater performance for two major reasons. First, since there is no time when the GPU is idle the overall compute time is reduced. And second, because the GPU is kept working continuously, it's able to operate in a higher performance state and reduce the time necessary to compute each particular output. But so much of the appeal in Core ML is that you don't have to worry about any details like this at all. In fact, for your app all you are really concerned with for your users is going from a long time to get results to a short time. So this year we are introducing a new batch API that will allow you to do exactly this. Where before you needed to loop over your inputs and call separate predictions, the new API is very simple. One-line predictions, it consumes an input -- an array of inputs and produces an array of outputs. Core ML will take care of the rest. So let's see it in action. So in keeping with our style transfer example, let's look at the case where we wanted to apply a style to our entire photo library. So here I have a simple app that's going to do just that. I am going to apply a style to 200 images. On the left, as in your left, there is an implementation using last year's API in a for loop. And on the right we have the new batch API. So let's get started. We are off. And we can see the new is already done. We'll wait a moment for last year's technology, and there we go. In this example we see a noticeable improvement with the new batch API. And in general, the improvement you'll see in your app depends on the model and the device and the workload. But if you have a large number of predictions to call, use the new API and give Core ML every opportunity to accelerate your computation. Of course, the most high-performance app in the world isn't terribly exciting if it's not delivering an experience that you want for your users. We want to ensure that no matter what that experience is, or what it could be in the future, Core ML will be just as performant and simple to use as ever. But the field of machine learning is growing rapidly. How will we keep up? And just how rapidly? Well let me tell you a little bit of a personal story about that. Let's take a look at a deceptively simple question that we can answer with machine learning. Given an image, what I want to know: Are there any horses in it? So I think I heard a chuckle or two. Maybe this seems like kind of a silly challenge problem. Small children love this, by the way. But -- so way, way back in the past, when I was first starting graduate school and I was thinking about this problem and first learning about machine learning, my insights on the top came down to something like this: I don't know -- seems hard. I don't really have any good idea for you. So a few years pass. I get older, hopefully a little bit wiser. But certainly the field is moving very, very quickly, because there started to be a lot of exciting new results using deep neural networks. And so then my view on this problem changed. And suddenly, wow, this cutting-edge research can really answer these kind of questions, and computers can catch up with small children and horse recognition technology. What an exciting development. So a few more years pass. Now I work at Apple, and my perspective on this problem has changed again. Now, just grab Create ML. The UI is lovely. You'll have a horse classifier in just a few minutes. So, you know, if you are a machine learning expert, maybe you are looking at this and you are thinking, "Oh, this guy doesn't know what he is talking about. You know, in 2007 I knew how to solve that problem. In 2012 I'd solved it a hundred times." Not my point. If you are someone who cares about long-lasting, high-quality software, this should make you nervous, because in 11 years we have seen the entire picture of this problem turn over. So let's take a look at a few more features in Core ML that can help set your mind at ease. To do that, let's open the hood once again and peek at one of these new horse finder models, which is, once again, a neural network. As we have illustrated before, the neural network consists of a series of highly optimized layers. It is a series of layers, and we have highly optimized implementations for each of them in our inference engine. Our list of supported operations is large and always growing, trying to keep up with new developments in the field. But what if there is a layer that just isn't supported in Core ML? In the past, you either needed to wait or you needed a different model. But what if this layer is the key horse-finding layer? This is the breakthrough that your horse app was waiting for. Can you afford to wait? Given the speed of machine learning, this could be a serious obstacle. So we introduced custom layers for neural network models. Now if a neural network layer is missing, you can provide an implementation with -- will mesh seamlessly with the rest of the Core ML model. Inside the model, the custom layer stores the name of an implementing class -- the AAPLCustomHorseLayer in this case. The implementation class fills the role of the missing implementation in the inference engine. Just like the layer is built into Core ML, the implementation provided here should be general and applicable to any instance of the new layer. It simply needs to be included in your app at runtime. Then the parameters for this particular layer are encapsulated in the ML model with the rest of the information about the model. Implementing a custom layer is simple. We expose an MLCustomLayer protocol. You simply provide methods to initialize the layer based on the data stored in the ML model. You'll need to provide a method that tells us how much space to allocate for the outputs of the layer, and then a method that does the computation. Plus, you can add this flexibility without sacrificing the performance of your model as a whole. The protocol includes an optional method, which allows you to provide us with a MTL shader implementation of your model -- of the layer, excuse me. If you give us this, then it can be encoded in the same command buffer as the rest of the Core ML computation. So there is no extra overhead from additional encodings or multiple trips to and from the GPU. If you don't provide this, then we'll simply evaluate the layer on the CPU with no other work on your part. So no matter how quickly advancements in neural network models may happen, you have a way to keep up with Core ML. But there are limitations. Custom layers only work for neural network models, and they only take inputs and outputs which are ML MultiArrays. This is a natural way to interact with neural networks. But the machine learning field is hardly restricted to only advancing in this area. In fact, when I was first learning about image recognition, almost no one was talking about neural networks as a solution to that problem. And you can see today it's the absolute state of the art. And it's not hard to imagine machine-learning-enabled app experiences where custom layers simply wouldn't fit. For instance, a machine-learning app might use a neural network to embed an image in some similarity space, then look up similar images using a nearest-neighbor method or locality-sensitive hashing -- or even some other approach. A model might combine audio and motion data to provide a bit of needed encouragement to someone who doesn't always close his rings. Or even a completely new model type we haven't even imagined yet that enables novel experiences for your users. In all these cases, it would be great if we could have the simplicity and portability of Core ML without having to sacrifice the flexibility to keep up with the field. So we are introducing custom models. A Core ML custom model allows you to encapsulate the implementation of a part of a computation that's missing inside Core ML. Just like for custom layers, the model stores the name of an implementation class. The class fills the role of the general inference engine for this type of model. Then the parameters are stored in the ML Model just like before. This allows the model to be updated as an asset in your app without having to touch code. And implementing a custom model is simple as well. We expose a protocol, MLCustomModel. You provide methods to initialize based on the data stored in the ML Model. And you provide a method to compute the prediction on an input. There is an optional method to provide a batch implementation if there are opportunities in this particular model type to have optimizations there. And if not, we'll call the single prediction in a for loop. And using a customized model in your app is largely the same workflow as any other Core ML model. In Xcode, a model with customized components will have a dependency section listing the names of the implementations needed along with a short description. Just include these in your app, and you are ready to go. The prediction API is unchanged, whether for single predictions or batch. So custom layers and custom models allow you to use the power and simplicity of Core ML without sacrificing the flexibility needed to keep up with the fast-paced area of machine learning. For new neural network layers, custom layers allow you to make use of the many optimizations already present in the neural network inference engine in Core ML. Custom models are more flexible for types and functionality, but they do require more implementation work on your part. Both forms of customization allow you to encapsulate model parameters in an ML model, making the model portable and your code simpler. And we've only been able to touch on a few of the great new features in Core ML 2. Please download the beta, try them out for yourself. Core ML has many great new features to reduce your app size, improve performance, and ensure flexibility and compatibility with the latest developments in machine learning. We showed you how quantization can reduce model size, how the new batch API can enable more efficient processing, and how custom layers and custom models can help you bring cutting-edge machine learning to your app. Combined with our great new tool for training models in Create ML, there are more ways than ever to add ML-enabled features to your app and support great new experiences for your users. After just a short break, we'll be back right here to take a deeper look at some of these features. In particular, we'll show you how to use our Core ML Tools software to start reducing model sizes and customizing your user experiences with Core ML today. Thank you.  [ Applause and Cheering ] Northern California's bay area. Here is the stage for a truly remarkable natural occurrence. Every year, a great migration is made by one of the world's most mysterious species. To reach this utopian destination, these unique mammals will instinctively embark on a nomadic journey from all corners of the world. This is the developer's [triterapsus], or as it is more commonly known, the developer. Eleven and a half months of hibernation has taken its toll, and the sun is harsh as these nocturnal cave-dwelling creatures must greet daylight. For most, this momentous migration is their first; while for a select few the pilgrimage is a time-honored tradition. Originating in the remote garages of Silicon Valley, developers evolved at an unprecedented speed. To understand how, we must observe them here, in their sacred temple. And so, it begins. As developers congregate at the event, access to the herd can be hard to come by. For some, blending in is easier said than done. To distinguish themselves, developers identify each other through decorative tribal symbols. These extraordinary coolidges [phonetic] are designed to entice and attract. Here, a pack of rogue younglings at play. The safety of numbers allows them to-- oh, wait a moment. They have now entrapped an indigenous silver-crested king developer. Although tempting, one may look at but not touch the mane. The feeding frenzy at the great gathering. Moving in like a pack of famished piranhas, this frugal breed will spare no morsel. Those who can, pull rank on the prime cuts. The event culminates in what is known as the keynote. As the doors open, a behavior [anomaly] can be found. Developers running at full speed, battling to secure a coveted front-row seat. Following the ceremony, the latest beta software is unleashed, it's nature at its cruelest. Only the strongest apps will survive. The many languages of the developer are a mystery to modern science. Just look at these letters, symbols, formulas, and mumbled jargon. Only these enlightened shamans can decode their true meaning. The creative bonds formed during the great migration will enable these geniuses to unlock our future world. The developer is no doubt a species scientists will continue to study for [millennia]. [ Applause and Cheering ] Good morning! Good morning. [ Applause and Cheering ] I hear the student developers. Good morning and welcome to WWDC 2018. It is great to be back in San Jose with everyone. We have developers here from all over the world this morning, from 77 countries. That's more than ever before. And I couldn't be happier to announce that we now have over 20 million Apple developers around the world. [applause] That's more than ever before. We've got 6000 folks in the hall this morning. Welcome. And many millions more watching online. Now the way the developers get to share their amazing work is through the App Store. Your creativity and hard work have made the App Store the best place to get the very best apps. Next month, the App Store turns ten. And in these ten years, the App Store has fundamentally changed the way we all live. It's enabled countless new companies, created tens of millions of jobs, spawned entirely new industries, and it has forever changed our lives. The App Store is the world's largest app marketplace [applause], and we now welcome over 500 million weekly visitors. This is mind-blowing. It's the most incredible app marketplace that the world has ever seen. We're also happy to announce that this week we're going to achieve another huge milestone. The money that developers have earned through the App Store will top $100 billion. This is just, this is beyond remarkable. The App Store is clearly the best place for you to be rewarded for your hard work and creativity. Now at Apple, we know that developers are a powerful group of creators, that they can achieve and they can develop anything that they can imagine with their code. We want more people to learn the power of code, and it all starts with Swift and Swift Playgrounds. We created Swift to make it easy, to make it so easy to learn to code that it was as easy as our products are to use. Swift is extremely popular. In fact, it's the fastest growing programming language out there. Now Apple developers are already using it in huge numbers. In fact, over 350,000 apps have been written in Swift on the App Store. Now [applause] we believe that coding is an essential skill and believe it should be offered by every school in the world. Learning to code has so many benefits. It develops problem solving and critical thinking skills. That's why created Everyone Can Code with free teaching and learning resources so that everyone could learn to code. It's been so successful and is now available to tens of millions of students around the world. Just imagine what this new generation of coders will create. Whatever it is, I'm sure that it's going to change the world. At Apple, changing the world and making it a better place is what it's all about for us. We aim to put the customer at the center of everything that we do. That's why, together with you, the developer community, we're working hard to provide new and better experiences for our customers to help them live a better day. And these experiences of course are expressed through our four amazing platforms. Today is all about software, and we've got some very exciting updates across all four platforms. We're going to get started with iOS. IOS embodies our philosophy of putting the customer at the center of everything that we design. Every year, we deliver a major iOS update that brings awesome new features that will impact the world. To tell you all about what we had planned for this year, I'd like to introduce Craig Frederique [phonetic]. Craig. Hey, good morning. All right. The next release of iOS is, you guessed it, iOS 12. Now, our customers, of course, are going to receive iOS 12 as a free software update. Now, it's easy to forget now, but iOS pioneered this approach of helping you get more out of the device you already own through free updates. And some of those updates have been really quite profound. Like the App Store, giving us a whole new way to discover and download apps. It itself was delivered via software update, as were the folders we used to organize those apps. And can you imagine living without Find My iPhone or iMessage with its end-to-end securely encrypted messaging. Or the revolution in iPad productivity with Slide Over, Split View, and Drag and Drop, or AR, changing the way we interact with the world around us, all delivered via software updates. And of course we want to get these improvements to as many of our customers as possible. IOS 11 supports devices that were introduced as far back as 2013, like the iPhone 5S. And we just love the way customers race to update to our newest releases. In fact, half of our customers upgraded to iOS 11 in just seven weeks. It's incredible. Now as we stand here today, 81 percent of our over a billion active iOS devices are running our latest release. And now, when you look at the competition, well it's hard to say they really have a software update model. So iOS has the fastest adoption of any operating system, but what's much more important to us is customer satisfaction, and we're thrilled to report that customer sat for iOS 11 is at 95 percent. Now, delivering all of these features across such a wide range of devices will maintaining high performance is a challenge we take really seriously, and so for iOS 12, we are doubling down on performance. We're working top to bottom making improvements, to make your device faster and more responsive, and because we want these changes to be available to the full range of our customers, iOS 12 will be available on all the same devices as iOS 11. This is the largest base ever supported by an Apple release, and we're focusing our efforts especially on the oldest devices. And while it's still early days, we're really pleased with some of the results we're seeing. And so I'd like to share some with you. And I'm going to use an example of a popular phone from a few years ago. This is the iPhone 6Plus. Now on that device, iOS 12 delivers a number of improvements across common operations. You'll see that apps launch up to 40 percent faster. The keyboard can come up up to 50 percent faster, and you can slide to take a photo at up to 70 percent faster. Now our deepest focus this year is optimizing the system when it's under load, and that's where you need performance the most and where iOS 12 really shines. Now, we've put iOS 12 through our stress test, and we saw in those conditions Share Sheet coming up twice as fast and apps launching twice as fast. These are big, big improvements. Now this took changes-- thank you very much. This took improvements in many, many places in the system, and I want to highlight just one. And it starts with the silicon. You know, our tight collaboration with our chip team has enabled us to optimize iOS across the full range of our A series silicon. Now CPUs traditionally respond to increased demand for performance by slowly ramping up their clock speed. Well now on IOS 12, we're much smarter. When we detect that you need a burst of performance, like when you begin scrolling or launching an app, we ramp up processor performance instantly to its highest states, delivering high performance and to ramp it down just as fast to preserve battery life. Now, these are just some of the improvements that are coming to not just our older devices, but the full range of devices, and that's a quick update on performance. Now if this is all we'd done in iOS 12, I think it would be a great release, but we've done more. Much more. And we have a lot to cover today, and it starts with augmented reality. Yeah. [applause and cheering] Now AR is transformational technology. By bringing experiences into the real world, it enables all kinds of new experiences, changing the way we have fun and the way we work, and in iOS 12, we wanted to make an easy way to experience AR across the system, and to do that, we got together with some of the greatest minds in 3D at Pixar, and together we created a new file format for AR. It's called USDZ, and it's a compact, single-file format that's optimized for sharing while retaining great 3D graphics and even animations. Now, you can use USDZ across the system from the files app, to Safari, even sharing them over messages and mail, and what's great is you can place these 3D objects into the real world. It's something that's like AR quick look. It's really awesome. Now, we want all kinds of creatives to be able to create content for AR, and so we're working with the leading companies for 3D tools and 3D libraries to bring their support for USDZ. Now one company that's been all in on USDZ and ARKit is Adobe, and to tell you about what they're up to, I'd like to invite Abhay Parasnis, their CTO, to the stage. Abhay. Thanks Craig. It's great to be here this morn. So at Adobe we believe augmented reality is an incredibly important technology. And with ARKit, Apple is by far the most powerful platform for AR. So earlier Craig talked about USDZ format. It's actually a pretty big deal. There is now a way to deliver AR experiences across the entire iOS experience. And so today we are actually announcing that we are going to bring native USDZ support to Adobe's Creative Cloud. With Creative Cloud, designers and developers will now be able to use familiar apps, apps that they know and love, like Photoshop or Dimension, to create amazing AR content and bring it easily via USDZ. And of course, we are not going to stop there. We are going to bring the power of immersive design to Creative Cloud with a new set of services and applications including a brand-new iOS application that's going to let all of you design amazing AR experiences quickly. So, you will be able to bring in images, videos, text, any object from Creative Cloud directly into a native AR environment. In fact, for the first time with Creative Cloud and iOS, you will have a what you see is what you get editing in AR. It's pretty cool. So, come join this afternoon State of the Union where we're able to give you a sneak peak of some of these new immersive design tools for the first time. Thanks. Back to you Craig. [applause] Thank you, Abhay. [applause] Now a critical part of enabling AR is accurate measurement, and fortunately the precise calibration of sensors in iOS devices and our tight hardware software integration mean that we do this really well, and we want to enable everyone to take advantage of this capability. So we're introducing a new app, and it's called Measure. It makes it really easy to measure objects, detect rectangles and get their dimensions, and measure lines along surfaces, and I'd like to show it to you now. [applause] Now, in looking for something to measure, I ended up digging through the attic and came upon my old suitcase from my traveling days in college. A lot of memories in here. Now I'm actually in the Measure app, and you see I can easily measure along this suitcase by just tapping and dragging out a line, like that, and check that out. It's a measurement. [applause] Now, what's really cool is I can extend these measurements so I can just tap, drag along another edge just like that, and even take it into full 3D by dragging down to the bottom like that. Isn't that cool? Now I mentioned that we could automatically detect rectangles and show you dimensions of objects. So I have a photo here actually. It's one my mom always wanted me to travel with of me as a baby, and so, you see, what I can do with Measure is it automatically detects the dimensions of that photo. I can just tap and get, oh, yeah, a cute little baby, wasn't I? And you can get measurements just like that. It's really fantastic. Now, I'd like to turn now to USDZ and its support throughout the system. You know, you can experience USDZ in so many places, and one of those places is news. Now here's an article. We're all accustomed to seeing images in news articles. But check this out. Here you see a USDZ asset. It can just tap on this little 3D control and jump right in and experience it. You can see the amazing animations that are possible, and of course it's fully interactive. So I can zoom in and pan. Isn't that amazing? [applause] Now USDZ is also great in the web. So here I am at the Fender website and actually let you configure your guitar with the kind of finish and pickguard that you want. So I can select a configuration option here, and then I can see the guitar I've configured. So I'm just going to tap in. Of course I can see the guitar here, but wouldn't it be cool if I could see it in the real world in its real size? Well, there it is. Check that out. [applause] I think I'm going to capture that for posterity. And that's a quick look at USDZ and Measure in iOS 12. Next-- thank you. [applause] Next, I'd like to talk about the key technology behind these augmented reality experiences, and that's ARKit. ARKit opens up the world of AR to hundreds of millions of users, making it the world's largest AR platform by far. And we're on a relentless pace of advancement with AR, and that continues today with ARKit 2. Yes. ARKit 2 delivers advances with improved face tracking, more realistic rendering, and support for 3D object detection and persistence, which enables launching into AR experiences associated with a particular object or a physical space, like starting a game built around a physical toy or having a physical classroom serve as the foundation for a lesson that was authored in AR. But probably best is the support for shared experiences. Now this delivers true, multiuser augmented reality. You and the people around you will be able to see your own perspective on a common virtual environment. And to help all you developers get started, we created this sample app written in Swift that you'll all be getting today. Now, check out how both players and even a third observer can all experience the same environment in real time. It's really fun. Now, we've also brought in a select few developers into our labs over the last couple of weeks to work with ARKit, and they love it. Now, one of them is Lego. And what they've done is so fun, you just need to see it. So I'm pleased to invite Martin Sanders, director of innovation of Lego, to the stage to give you a live demonstration. Martin. Craig, thank you very much. Creating and playing with physical Lego sets brings great joy to millions of children and Lego fans all over the world. And now, with ARKit 2, we get to expand those creative possibilities like never before and take things beyond the physical. What we try to do is combine physical and digital together to really open up those creative play possibilities, because our Lego sets are really the start point for all of those children's imaginations. And when we get a chance to really imbed ARKit 2, it takes it to the next level. Let me show you what I mean. Here we have Assembly Square. It's one of our Lego creator sets and already has so many great details it's awesome to play with. But wouldn't it be great if we could take things even further? Well now with 3D object detection, we get to recognize our models and bring them to life. And just look at all of those rich details we can now bring into our sets, because when we combine physical and digital together like this, it really opens up those creative play possibilities. And there's so much to do here. Did you see these icons of the people and objects? Well they represent missions and stories that we can explore. And with a world as rich and as immersive as this, who wouldn't want to play? So let's add a character. How about this little guy here? Welcome back, let's go on an adventure. Awesome. Let's go on an adventure. But going on an adventure with friends is often way more fun. Anders, why don't you pop in here. I'm always up for an adventure. Perfect. Because now with ARKit 2's multiuser support, we get to play with up to four friends in the same space. Let's go ahead and add a few things from our collection, Anders. Okay. I will add a bank over here. Very nice, I like it. How about taking this for a spin. Let's just look at all those rich details you can see on the outside of the building and even on the inside. Yeah, I think I'm going to add this guy. He's definitely going to keep the bank safe. Oh look, my character's hungry. Let's take him over to the bakery. Well let's see what they have. Because now with ARKit 2, we get to see inside our physical creations and check out all the details that were hidden before. We've got a ballerina, a little music session going on, and let's make a bathroom-- oops sorry. Moving on. Oh, another play trigger in the bakery. Let's see what happens when I click on this. Um, fresh pretzels. Oh, that doesn't look too good. Anders, I've got a situation. I'm going to need your help over here. Yeah, okay, okay. I've got a firetruck. Perfect. You put out those flames while-- oh, oh dear. Somebody's got themselves trapped on the roof, so I'm going to use this helicopter to go and pick up these clowns. [music] How are you getting on down there Anders? I'm almost done, almost done. Great. Those guys are safe. And the flames look like they're almost out. Perfect, we did it. Yeah. What? And we've even unlocked a new item for our collection. But the fun doesn't need to end there because with ARKit 2, we get to save our entire world back into our physical set and pick up where we left off. [applause] All right. That's awesome. So much fun. But what makes this truly amazing is how with just a single Lego creation and ARKit 2 it really opens up those creative play possibilities. So look out for more Lego AR experiences in the App Store later this year. Thank you very much everybody. Isn't that great? So that's our update on AR and iOS 12. Next, I'd like to turn to Photos. You know, over a trillion photos are captured on iPhone each year, and Photos is the best way to relive and share those moments. This year we're making photos even better, and it starts with Search. Search has powerful object and scene recognition. It lets you search for photos based on things like searching for cars or dogs or flowers, and that's a great way to explore your library, but in iOS 12, Search now starts working for you even before you start typing with Search suggestions. It'll highlight things for you like key moments and people that are important to you, places where you've taken some great photos and even categories of photos like hiking and water sports. And Search is much more powerful than ever. You can search for places by business name. So you could search for SFMOMA or even a broad category like museum. And, Photos indexes over four million events by time and place. Things like sporting events and concerts, and so you can search for them and find photos you took at those events. And search is super powerful. You can now search for multiple search terms like surfing and vacation and even get suggestions for additional search terms to help you find exactly what you're looking for. And now in iOS 12 we have an all new tab. It's called For You. And with For You, you have all of your memories, so those great memory movies, but more like featured photos highlighting a photo that you took on this day in past years. And Effects Suggestions, for instance, suggesting looping a live photo or applying a new portrait effect to one of your portrait photos. And we even highlight your shared album activity. Now when it comes to sharing, there are many ways you can share photos, but our focus is on sharing great photos with the people you care about most, and that's why this year in For You we've added Sharing Suggestions. So imagine you've gone out for a great dinner with some friends and you took some photos. Well afterwards in For You, you'll see a suggestion like this to share those photos. If you tap in, you'll see that photos is even recommending a set of photos from that set that you might want to share and suggest who you might want to share them with based in part by the people that appeared in the photos. And when you share them, they're shared at full resolution out of your iCloud photo library and when you're friend receives them, something really magical happens. Their phone searches their libraries for other photos they took at that event and suggest that they share them back to you, so you both can end up with a full set. Now this is built around iMessage so of course it's private, using end-to-end encryption, and all of those smarts are done with on-device machine learning. So that's your quick update on Photos in iOS 12, and next, let's turn to Siri. Now Siri is by far the world's most used digital assistant with over 10 billion requests processed per month, and because Siri works across all your devices, it's always there to help you through your day getting things done. Now we all know that Siri works with many third-party apps for things like messaging, ride sharing, and payments. But we wanted to make Siri able to do much more for you, and we're doing that by taking advantage of the power of apps with a new feature we call Shortcuts. Now with shortcuts any app can expose quick actions to Siri. Let's look at some examples. Now say you have the Tile app because you're always losing your keys. Well, the Tile app can expose the option to add a shortcut to Siri. And you can assign your own phrase, such as I lost my keys would be a good choice, and when you then say it, Siri will automatically activate Tile and show you right in the Siri UI start ringing your Tile just like that. It's really great. [applause] Of course there's so many uses for this kind of thing. You could say game time to get your team's schedule from TeamSnap or help me relax to kick off a meditation or order my groceries to order your usual. You know, with millions of apps, Shortcut enables incredible possibilities for how you use Siri. Now, as you know, Siri is more than just a voice. Siri is working all the time in the background to make proactive suggestions for you even before you ask, and now with Shortcut, Siri can do so much more. So, for instance, let's say you order a coffee every morning at Phil's before you go to work. Well now, Siri can suggest right on your lock screen that you do that. You tap on it, and you can place the order right from there. Or if when you get to the gym you use Active to track workouts, well that suggestion will appear right on your lock screen. And this even works when you pull down into Search. You'll get great suggestions. Like say you're running late for a meeting, well Siri will suggest you text the meeting organizer. Or when you go to the movie, suggest that you turn on Do Not Disturb. That's just being considerate. And remind you to call grandma on her birthday. Just tap, and it'll dial the call for you. Now, we think we're all going to really enjoy using Shortcuts, and so we went a step further. We wanted to let you create your own shortcuts as users, multiple steps across multiple applications, and we're doing it with a new Shortcuts app. So with the Shortcuts app, you could do something like create a shortcut for surf time, and it could go get you the surf report, look up the current weather, get you the ETA to the beach, and even create a reminder for you to put on sunscreen when you get there. Now, it's all done with simple drag and drop steps in the Shortcuts editor right here. It's really easy. Now to show you how Shortcuts can streamline your day, I'd like to invite one of our leaders from our Siri Shortcuts project, Kim Beverette [phonetic], to the stage to give you a live demo. Kim. Hey! I'm am so stoked to show you Siri Shortcuts. To do that, I'm going to walk you through my day. So imagine it's the morning. I'm headed to work, and I pick up my phone, and I see this suggestion from Phil's Coffee. Siri has learned that I do this most mornings, so now I can just tap on the suggestion and I see all the details I need to confirm my perfect Mint Monkey Dough right here on the lock screen without even going into the app. So let's get caffeinated. And I'm done. Fast forward a little bit, and I'm sitting at my desk at my office and I need to know when my next meeting is. I'll go to the Up Next widget, and it looks like I am running a little late for a rather important meeting, so I should probably let someone know. And it looks like Shortcuts is a few steps ahead of me. I could call into the meeting, or I could let the organizer know that I am running late. I should probably tell Arie what's up. That looks like just what I want to say, sorry Arie, let's send it. Perfect. I also want to show you how you can add a shortcut to Siri. So let's take a look at Kayak. I keep all of my travel details in Kayak. Most important is my post WWDC relaxation trip to Los Angeles. You can see I have got my flight, my hotel, all the details, everything I need, but what I really want is to be able to use this and get to this information with my voice while I'm on the go. So, let's head back, and I can just tap add to Siri, record my custom phrase, travel plans [beep], and I'm done. So now when I land at the airport and I'm about to get in a cab and I could really use that hotel address, I can just say, travel plans. Kayak says your hotel is at 929 South Broadway. You can check in after 3 p.m. Isn't that cool? [applause] It's pretty cool. So I would love to be on that vacation, but I should, I don't know, probably finish this demo, so let's head back to work, and I can show you how the Shortcut app can help me with my commute home. We start in the Gallery where there's hundreds of premade shortcuts that you can download, or we can hop over to the library, and I've got a bunch of shortcuts here, but I want to show you my heading home shortcut. You can see that it's just really a series of steps. It grabs my location and travel time, and it sends my ETA to my roommate. It sets my HomeKit thermostat to 70 degrees, and it turns on my fan. And last, it gets directions home with Apple maps with the best route to avoid traffic. Now this is already pretty cool, but I happen to be an NPR News junkie, so I should probably just add that to my shortcut to save me some trouble on the ride home. Let's tap search, and there's a bunch I can add here, but I can just tap Siri suggestions, and there it is, play KQED Radio, we'll drag this in, drop it, and we're set. I've already added the shortcut to Siri with a custom phrase, heading home, so now, whenever I leave work, I can just say, heading home. You will get there in one hour. I sent a message to Cheryl. Your thermostat is set to 70 degrees, and I turned on the fan. Playing KQED Radio. [music] Right? That's Siri Shortcuts in iOS 12. Thank you so much. Thank you, Kim. And that's Siri Shortcuts. It works on iPhone and iPad, and of course you can run your shortcuts from your home pod and your Apple Watch. And that's your quick update on Siri. Next, let's talk about apps, and to tell you about the latest, I'm going to hand it off to Susan. Susan. [ Applause and Cheering ] Hello. Thanks Craig and it is so great to be here. I'm excited to tell you about some great updates in some of our most popular apps, starting with one of my favorites, News. So News is a personalized feed where you can see all the stories you want to read pulled together from trusted sources. And, our top stories are handpicked by the Apple News editorial team to make a great collection of curated content. With our new browse tab, you can discover new channels and topics, and we've made it even easier to jump to your favorites because that's why they're your favorites, right? News shines on the iPad. We've added a new sidebar, and it's a great way to navigate. It makes it easy and I fun to dig into the areas you're most interested in. So that's news. Now, we've completely rebuilt the Stocks app, and it's got a beautiful new design. Of course, you can still see the stock prices and the changes at a glance, but we've added spark lines, those little charts, that show the stock performance throughout the day. And that's cool, but are you ready? We are so excited to announce we're bringing Apple News to Stocks. I'm really excited about that, and the top stories in Stocks features business news, right, curated by the Apple News editors. It's pretty terrific. You can tap on any stock to get a more detailed view, so you can see an interactive chart that now includes after hours pricing, and you see relevant headlines from Apple News curated by our Apple News editor. So it looks great. And if you tap on one of those headlines, you'll see the full article without leaving the app, and of course it's formatted to look gorgeous on the iPhone. Now, with iOS 12, we're bringing Stocks to iPad. It's pretty great, and we take advantage of the larger display, so you can keep your eye on your stocks on the left while you browse through your financial news. It's a pretty great experience. Next up, Voice Memos. We've also completely rebuilt Voice Memos to make it even easier to use, and we're bringing Voice Memos to the iPad for the first time. Importantly, we've also added iCloud support so your recordings stay in sync across all your devices. We think iPad users are just going to love this. And we think that iBooks is the best way to discover and experience eBooks as well as audio books, and with iOS 12, we're introducing an all new design, and we think the update is so great, we're calling it Apple Books, a new name. Very dramatic. We dropped the i. [applause] Apple Books has some great new features. For example, Reading Now, with a preview that makes it really easy for you to pick reading right where you left off. And there's so much more, including a stunning new store that makes browsing through your eBooks and audio books better than ever. We love these updates, and we think you will too, but we also have a smart and safe way to use your apps in the car. I think you know I'm talking about Car Play. Car Play already supports third-party audio and voice messaging, you know, voice calling and messaging apps, you problem know that. But what you might not know is with IOS 12, Car Play will also support third-party navigation apps. So now you have even more choices when you use Car Play. That is a really quick look at some of our app dates, and Craig, back to you. [applause] Thank you, Susan. [applause] Well, now I'd like to take a moment to talk about something that's on a lot of people's minds lately. You know, iPhone and iPad are some of the most powerful tools ever created for learning, exploring, and keeping in touch, but some apps demand more of our attention than we might even realize. They beg us to use our phone when we really should be occupying ourselves with something else. They send us flurries of notifications, trying to draw us in for fear of missing out. And some of us, it's become such a habit that, you know, we might not even recognize just how distracted we've become. Well, we've thought deeply about this, and today we're announcing a comprehensive set of built-in features to help you limit distraction, focus, and understand how you're spending your time and balance the many things that are important to you. Now it starts with Do Not Disturb. There are times of the day, or times when you just don't want to be disturbed, and one of those, of course, is at night. Sometimes you wake up in the middle of the night, and you look at your phone, maybe just to check the time, and you're confronted with something like this, a barrage of notifications that spin you up and keep you from falling back asleep. And so we're introducing Do Not Disturb during bedtime where all you'll see is this. Nothing to get you spun up. And in the morning, yeah. In the morning when you wake up, you're gently eased into your day. You can tap when you want to start confronting those notifications. Now we've all found ourselves in situations like this. [laughter] Now, rest assured, he stuck the landing on this one. But now Do Not Disturb can help, and we made it easier than ever to use Do Not Disturb because now we have a great new mode where when you press in to Do Not Disturb and Control Center, you can set an ending time for Do Not Disturb for when you leave a particular location or when an event ends on your calendar. So I think we're all going to be using Do Not Disturb a bunch more. Now next I want to talk about Notifications. Now, Notifications help keep us informed and connected to important things that happen throughout the day, and we'd like to give you more control over how many notifications you receive, and so we're enabling what we call Instant Tuning for Notifications right from the lock screen. You can press in to a notification, and from there, you can decide to send future notifications from that app directly to Notifications Center, bypassing your lock screen. Or turn them off altogether, and Siri will even help by suggesting that you turn off Notifications for apps that you're no longer using. Now, we also wanted to give you help managing large numbers of notifications. So I'm thrilled to announce that we're bringing to iOS support for grouped notifications. Notifications are grouped not just by app but also by topic and thread. It gives you a great overview of the notifications you've received. You can tap in and look at a particular group. But of course just as important with a single swipe, you can triage a whole group of notifications away. So that's Notifications. Now, in addition to these great features for helping you limit distractions, we wanted to go further, and it's with a feature we call screen time. Screen time empowers you with both insight and control over how you spend your time, and it starts with reports. Every week, you get a weekly activity summary that details how you used your iPhone or iPad. You tap in, and you get to view your full activity report. It's really detailed. You get deep insight on how much time you're spending, where you're spending it, and even how your use breaks down during the day or the night. You get a summary of the time you're spending in apps, how much time you're spending, how often per hour you're picking up your phone and what's drawing you in and what apps are sending you the most notifications. Now, equipped with this insight, you can make decisions about how much time you want to spend with your device each day. But we know there are people who would like a little extra help in managing their use of apps, and for them we've created App Limits. So if in your activity report you see an app where you might want to be spending a little bit less time, well you can set your own limit, and then during the day, when you're using the app, you receive a helpful notification letting you know time is almost up. And once you've reached your limit, instead of the app, you'll see this. It's time to move on. Now, we'll let you grant yourself and extension if you want, but we'll give you a reminder later to move along. Now, this is also in sync across your iPhone and iPad, so your limits apply to your total usage. And we think this is going to be helpful for many people, but especially for some kids. And we know this is something that can help families achieve the right balance for them. And of course it starts with providing your kids with great information so they get an activity report of their own, but as a parent, you get one as well on your device, and based on what you see, you have the option of creating allowances. Now you have many options. One of them is downtime, time when you want your kids to unplug altogether. For instance, at bedtime. And you can also limit your kids' time in apps by category or by individual app. Now, there's some apps you may want to always allow them to use. For instance, you may want them to be able to get at the phone at all times, so they can contact you. Or you may want to give them access to educational apps. And you can also limit access to only movies, apps, and websites that you deem age appropriate. Now, this works of course across their iPhone and iPad, and it's uses family sharing, so it's super easy to set up, and you can manage it all remotely from your parent parental device. And so that's are some-- Screen Time has great features to help you better manage your time. Now, next I'd like to talk about one of the most important uses of our devices, and that's communication. And we'll start with messages. Messages has given us fun ways to express ourselves with emoji and now Animoji, and one of the things that make Animoji so fun is how expressive they are, you know, from smiles to frowns, to nods of the head and blinking of the eye. Animoji does such an amazing job tracking our expressions. And this year, we're taking Animoji to a whole new level, the breakthrough new technology we call Tongue Detection. [applause] That's right. Now, you can make your favorite Animoji do this. We're all going to be sticking out our tongues to our phones in the near future. Now, we've also, we're also introducing some great new Animoji that I think you're all going to love, like ghost, koala, tiger, and T-Rex. But we wanted to take Animoji even further by making them even more personal. So I'm thrilled today to announce the arrival of the era of Memoji. [applause] That's right. With Memoji you can create your very own personalized Animoji. Now, these Animoji can look like you or the real you. And we've worked hard to build a deep set of customization options to let our customers create an incredibly diverse set of Memoji. It's really incredible what you can create. And we've designed a beautiful new experience to create these Memoji that makes the process fun and easy. Now, to tell you more about it, I'd like to invite one of the managers of our Messages and Animoji features, Kelsey Peterson, to give you a live demo. Kelsey. [ Applause and Cheering ] Good morning. I cannot wait to tell you what's new with Messages. Let's get started with Animoji. First, you need to meet the newest members of the team. We've got a new cat in town, our tiger. She's so cute. And now, my personal favorite, the koala. Just getting excited scrolling through here. They can't all be cute and cuddly though, so here's our T-Rex. And we've our very own friendly little ghost. So much fun. And if I swipe right, here's where I can create my very own Memoji. Let me show you just how easy it is. I recently chopped my hair, so I want one that matches the new me. So I've selected a skin color, and now I'm trying to figure out just the right amount of freckles. It's a real Goldilocks scenario. Yeah, these are just right. Okay. Onto the main event. There are so many hairstyles to choose from. First, I'm going to grab my color, and then like I said, I need to go a little bit shorter. All right. Nope, this is the one. Now that I'm all set, I can of course select my eye color, and what's really amazing is as I'm making changes, the character up above is coming to life. There are tons of options for me to customize. I could add earrings, but what I really want is a great pair of Sunnies, so I'm going to come over to eyewear and pick out some frames. Maybe not for today. I think I need two lenses. These are the ones. Now that I have frames, I'm going to tint my lenses to make a great pair of sunglasses. Perfect. Okay. I think I'm all set. I'll tap done, which saves my grandly created Memoji right here into the drawer alongside the rest of my team. And that's how simple it is to create your very own Memoji. Now I have a brand-new feature to introduce you to. This wasn't even in Craig's slides. We are bringing fun effects into the Messages camera. Let's take a look. I've a message here from my partner. It looks like he's informing me that he's bought our dog, Ferdinand, yet another tiny dog hat. This isn't even the first. I think this is the perfect opportunity for a response with this new fun camera. So I'll tap to pull up the camera, and then I can see this little star over here on the left. Tapping on that gives me a strip with all sorts of new effects. So I could add things like shapes or text. But let's check out these filters. Ooh. So this is comic book. It's really fun, but for a response about a tiny dog hat, I think I'm going to go in a different artistic direction. So what I really need to do is add a sticker from one of my favorite sticker packs. Ferdy looked really excited about that new dog hat, so I'm going to put him right here, and now we have an all new way for you to use Animoji. I can apply my favorite Animoji right here live. Here's the Memoji I just created, but I actually have just the one. It's of me in a very similar red hat, which is kind of perfect for twinning with my pup. So I'm going to set up my shot, snap, and send. And that's a demo of the fun new effects in messages. To you Craig. So that's Memoji and some fun new effects in the camera and messages. Next, let's talk about Facetime. Yeah. Facetime is the way that so many of us connect with the people in our lives and share some of our most important moments, and it's helped us deepen our connect with people important to us wherever they are, and of course it's a fun place just to hang out. Now this year Facetime is going to take a big leap forward because today we're introducing Group Facetime. Now, you'll be able to facetime with two people, three people, actually up to 32 simultaneous participants. Now, setting up a group call couldn't be easier. Just instead of typing one person's name, you can do many. You can ring them by tapping audio or video, but we also introduced a great new way because Facetime is now integrated into messages, so you can quickly go from a group chat you have going directly into a group Facetime, and members of the group can join in and drop out at any time. It's really great. Would you like to see it? [applause] Well let's do a demo. [applause] Well I think for our first live demo of group Facetime, I'm going to contact the folks back in Cupertino. I can just dive in to this conversation I have going with the members of the Facetime team, and it looks like actually they're already on a group Facetime call, so I'm just going to join right in. Hey everybody. Hey Craig. Check it out. So it's this beautiful Facetime UI with these big gorgeous tiles right up front where you see some of the leaders of the Facetime team, and down at the bottom there's and area we call the roster that contains everybody else. And of course I'm right there in the lower right-hand corner. Hey Craig. Wait, am I on the big screen? Yes, Lauren, this is not a test. You're in front of 6000 of your biggest new fans. Now, what you probably notice is when Lauren spoke, her tile automatically got larger to reflect her prominence in the conversation. This is totally automatic. Hey Roberto, how's it going back there in Cupertino? I'd say it's going pretty well, and Lauren, sorry for stealing your spotlight. So this works of course for people in the roster as well. When they speak, they come forward. Hey Christopher, you ready to make your big entrance? Finally, my moment has come. Hello world. Now, well done. Now you can control this too, so I want to bring Woody front and center, I just double tab. There he is. Woody, your baby is performing admirably here. Thanks Craig. It's exciting to finally be able to share Group Facetime with everyone. It sure is. Now, we have not only all of this, but we've also brought the fun effects to the Facetime camera. I can just tap in, and I have access to Animoji, filters, and all of my sticker packs, and everyone else on the call can apply them too. Now this is the future. Hey, Craig. Check this out, I'm a comic book koala, something I've always wanted to be. [laughter] I'm glad you've finally been able to express that side of yourself, Roberto. Hey, Tim, is that you? Yeah, it's me. I've signed up to help test Facetime. All right. [applause] Well, well thank you, Tim. Every little bit counts. Happy to help, and thanks to everyone on the Facetime team for making it a reality. I can't wait to start using it every Sunday night to follow the leadership team. Looking forward to that, Tim. Of course what I'm really looking forward to is getting Group Facetime to everyone on iOS 12. Thanks guys for a fantastic call. We'll see you back in Cupertino. So that's Group Facetime. It works on iPhone, iPad, and Mac, and you can even answer an audio on your wrist on your Apple Watch. So that's Facetime and Messages, and this is iOS 12. Improved performance, new AR experiences, Siri Suggestions, Screen Time, Memoji and fun effects in the Messages camera, and Group Facetime. I hope you like it. I'm going to hand it back to Tim. Thank you. Thank you, Craig. IOS 12 looks fantastic, and we can't wait for everyone to get their hands on it. Next up, we'd like to talk about the Apple Watch. Yeah. [applause] When we began development of the watch many years ago, we had a vision for just how impactful and essential it could become in our lives, so we worked very hard to create something that you would love and want to wear all the time, and customers do love it. In fact, Apple Watch is number one in customer satisfaction, [applause] and not just this year, but every single year since we launched in 2015. And growth has been off the charts. Apple Watch grew 60 percent last year. We're constantly hearing from customers about the many ways that the Apple Watch has changed their lives. And I'd like to share just one of them with you this morning. Mary Dovgen was boating with her husband, John, when due to a medical condition all of John's muscles went completely limp, and he fell into the ice cold water. With her arms wrapped around John to keep him from drowning, Mary could not reach her phone to call for help. But with her Apple Watch, she was able to call Siri, or to use Siri to call 911, rescuers soon arrived, and saved John's live. As Mary told us, if it wasn't for my Apple Watch, he really would not be here today. This is just one of the many stories that we've heard about about how Apple Watch is impacting people's lives. They range from getting people to be more active, to helping users live a healthier life, or even to alerting users to an elevated heart rate. Apple Watch brings such amazing capabilities right to the wrist, and of course at the heart of this is watchOS. We're excited to introduce watchOS 5 today, which brings even more ways for you to stay active and connected, and I'd like to hand it off to Kevin Lynch to tell you all about it. Kevin. Good morning. You know, stories like that, about how Apple Watch is supporting people around the world in even extreme situations like Mary and John's but also in their daily lives is really super motivating to us as a team. And there are things you do every day that shape your life. And Apple Watch has two [key areas] working to help you support those activities every day. First, staying active to increase your well-being and health and second being connected to the people and the information that you care most about, and we're moving watchOS in both of these areas. Let's start with health and fitness. Now much of the power in the health and fitness features that we've put in Watch are really empowered by the investment we do to make sure the data you see is accurate. The data from our custom-built heart rate sensor, Accelerometer, Gyro, GPS, are all thoroughly validated. In fact, from our fitness lab, we've studied over six terabytes of data where 12,000 study participants logged over 90,000 hours of sessions. And they actually burned 2.3 million calories doing this. We believe this is the largest biometric data collection of its kind. And we take this information and we work to integrate this seamlessly with the user experience so when you raise your wrist and you look at your activity rings, the information is not only accurate, but it's also meaningful to you. We really love hearing about your focus on closing these rings. And we really enable this through a number of ways in Apple Watch. First, of course, we do daily coaching so you can see what your goals are for the day. We also support celebrations when you achieve your goals, and we have special edition challenges like the most recent birthday challenge that we did. And on a monthly basis, we do monthly goals that are personalized to you. And of course the activity app also supports activity sharing, which has become one of the most popular features of the activity app. Many of you love the excitement of good old-fashioned competition though, so in watchOS 5, you can challenge any of your activity-sharing friends to a seven-day competition whenever you would like. And if they accept, you each try to win the week by closing your rings and earning points. You earn one point for each percent of a ring that you close. And while you're in the middle of a competition, your progress notifications are updated, do not only show you the progress your friends are making, but also where you stand in the competition. And when you win, you receive a new award. We're really excited about this edition, and if you're competition, it gives you a whole new way to enjoy the activity app. Now when you're doing [applause]-- now when you do want to go do a workout, there's a lot of workout types you can choose from, and they all have custom algorithms to measure things like calorie burn, pace, distance, elevation gain, and when you're swimming, it even counts laps and detects which swim strokes you're using. And with Gym Kit, your metrics are in sync with your favorite gym equipment. And we're really excited to bring more enhancements to workouts in watchOS 5, starting with a new workout type for yoga. Now this works primarily from your heart rate, and we calibrate this to your fitness through the rest of your day, so now you can more accurately track those yoga sessions including those intense Vinyasa sessions. New we've also added a new workout type for hiking. This takes into account pace and heart rate and elevation gain so you can more accurately get exercise credit while you're hiking on steep terrain or really long stages. And Apple watch has become a really great running companion especially now that we've added GPS and cellular and music streaming. And we're making this an even better experience now for training runs and races. In addition to current and average pace, you now have the option to keep track of your rolling mile pace, which is how fast you ran the immediately preceding mile. You can also now set a custom pace alert, so your Apple watch will tap you when you're above or below the pace that you've set. And finally, runners will now get cadence so you can see your current steps per minute. We're really excited for runners to try these out. Now-- Now there are sometimes when you forget to start a workout on watch, but you've started working out, and to solve this now, we're adding automatic workout detection. So your Apple Watch will now offer to start tracking your workout if it senses that you're beginning one. And even if you press start sometime after you began working out, you'll get retroactive credit for the workouts that you did. And [these start alerts] wills support all these great workouts on Apple Watch. Now when you reduce the intensity of your movement or your heart rate decreases but you forget to end your workout, of course Watch will also detect that and suggest that you stop. So, all these new features, activity competitions, the new yoga and hiking workout, new features for runners and automatic workout detection are all enabling you to more accurately track your workouts and stay motivated while you do. Now let's talk about being connected. Apple Watch enables you to remain in the moment while also easily connected to the people and information that you care about. And the introduction of cellular made this even better. You can stay connected even when you're going out for an evening, running some errands, or even going for a swim. Or, stay in touch when your phone might not be easily available to you. And staying connected with people you love is something that our customers love about Apple Watch. You can easily make or receive a phone call, and you can hear the emotion and tone at the other end of the voice as you talk in real time. Or you can use Messages to have impromptu, short conversations with loved ones in a Message threat. In watchOS 5, you'll have an entirely new way to communicate on your watch. That's real-time voice but with a spontaneity of short messaging. Did you steal my chips? Maybe. I cannot wait until you go to college. Introducing Walkie Talkie. This is a new app by Apple Watch. It's fun, easy way to talk with friends and family. Let's take a look at how it works. First you choose who you'd like to enable Walkie Talkie with, and it suggests some people that you often communicate with, so you could easily add them. Now the first time that you do this, your friend will receive a one-time request to allow a walkie talkie connection with you. If they accept, then you can speak to each other with walkie talkie whenever you like. And to do this, you just press to talk, and then your friend can hear your voice just like a walkie talkie. And they're going to feel a [haptic, hear a beep beep sound right before your voice comes out. And this new watch-to-watch connection works over cellular or Wi-Fi, and it has really high audio quality. And it's a lot of fun. We can't wait for you to try this out. That's walkie talkie. Now last year we introduced a Siri watch face, which presents the right information to you at the right time. And Siri does this using machine learning, so it's going to get better at predicting your actions over time by combining inputs like the time of day, your location, your daily routines, or which apps you use when. We're making some great enhancements to the Siri watch face now. First, we're adding new content. So now you can get live sports scores, you can get commute time home or to work, or you can see your heart rate, for example, after a workout or your resting heart rate. And we're also adding Siri Shortcuts. So the shortcuts you saw coming to iOS 12 are also going to be available in watchOS. So in addition to getting relevant information, you'll also receive predicted shortcuts right on the Siri watch face. So in a wrist raise, you'll be able to directly do things like turn on your leaving home scene or start an outdoor walk or play your favorite morning playlist. And these shortcuts app based on whether you typically do those actions at those times. So super easy now to just tap and do those actions. Also for the first time, you can now use third party apps on the Siri watch face. So now you can see both relevant content and shortcuts from your favorite apps. So if you always go running with Nike Plus Run Club at a certain time or you're logging your meals with LoseIt or you use City Mapware to find your commute home, you just raise your wrist and tap, it's that easy. So new content, support for Shortcuts, and third-party apps making for an even more powerful Siri watch phase. Now currently to talk to Siri you raise your wrist and you say, hey Siri. And when you think about this, it's quite a strong signal when you raise your wrist and talk to your watch. It's kind of like when someone's standing right in front of you, you don't need to say hey because you have their attention already, so we're bringing this same social cue to Siri on the watch so you no longer need to say, hey Siri. You just raise your wrist and talk to Siri. [applause] Now we're also bringing a number of improvements to Notifications, which are now more actionable and interactive. So, for example, with this notification from Quantis, you can check in and share your flight details right from the notification. After you finish a ride with Dee, you can scroll down. You can rate your ride and pay with Apple Pay right there in the notification. And with this one from Yelp that your table is ready, if you need a bit more time, you can extend the reservation out a bit just by tapping here. So super easy now to interact right in place inside Notifications. We've also improved our Message notification. Today if someone sends you a weblink, you aren't able to view it on your list. With watchOS 5, we've integrated WebKit, so now you have the ability to view web content and mail or messages. You can even tap on that link, yes, to easily view things like menus here. And while we think full browsing doesn't make sense on your wrist, there are times you get content. You'd like to see it right in the moment, and now you'll be able to do that and watch OS 5. And the contents can be formatted for the small screen, and the where reader made is available, it uses that on the watch. So WebKit on watchOS. Now with watchOS 4, we introduced an entirely new way to listen to music while you're on the go. You can stream 48 million songs on your wrist with Apple, or you can listen to automatically synced and created play lists. And now with watchOS 5, we're giving you even more to listen to. That's right. The Apple podcast app is coming to Apple Watch. Upcoming episodes from subscribed podcasts will be automatically synced to your watch, so you can get them right there, or you can just ask Siri to stream a podcast for you, and it'll start playing and playback resume across all your devices so you can continue just where you left off. So that's Podcast. Now those are the great new features coming in watchOS 5. So stay connected with people and information that you care most about, interactive notifications, web content, new content and shortcuts in the Siri face, podcasts and of course walkie talkie. Now we'd like to show you this stuff live in action, and we thought we'd increase the challenge a bit in our live demo here today to keep it interesting. So, Jules, who helps lead our fitness experiences, is actually going to do the demo while biking. So Jules has expertise in coaching, and she knows how to move and demonstrate and motivate all at the same time. Take it away, Jules. Thank you, Kevin. I've got some great stuff to show you that I think you're really going to love, and the bonus is, I get to work on closing my rings while I do it. So, first, I'm super excited to show you how easy it is to start a workout with GymKit. I just tap to connect and accept on the watch, and now I can control and start the workout right here from the console on the bike. All of my data between the watch and the bike is in sync, and all of my workout metrics are accurate. Now, I wouldn't typically be trying to demo things for you and sneak in a workout at the same time, but I want to show you activity competitions. And to do that, I need to keep moving. This morning I got this notification, and it says, your competition with Jay is down to the last day, and it's a close one. Have you got what it takes to win? Have I got what it takes? You bet I do. But if I scroll down, I can see that Jay has moved ahead of me by a couple of points, and that's not okay because this thing ends tonight. So, I think I'm going to add a little resistance to this workout here and work to earn as many points as possible and move ahead of Jay by the time this demo is done. I can simply tap here and send him a little smack talk to let him know I'm coming for him. Let me show you interactive notifications because I have a dinner reservation tonight that I made through Yelp. If all goes well here, I'm going to go out with my girlfriends from work to celebrate. And because we all know that Jay is going to lose this competition, maybe I should invite him along so it can be his treat. Now I can simply add to the number of guests and confirm right here in the app without ever having to open it. It's that simple. Now in watchOS 5, notifications from apps are smartly grouped together, and here are a couple messages from my friend Catherine. We're trying to get to a yoga retreat this summer, and it looks like she sent me a link. Now, I can tap it here and see if she's found somewhere great for us to go. Oh yeah, this looks beautiful. I'm already in, but I can continue to scroll through the site and maybe read a little bit more about the retreat, even check the dates to make sure that they work. And I can do all that right here from my wrist. Let me show you the enhancements that we've made to the new Siri watch face. At the top here, you see the workout that I'm in right now, and when I'm ready to get back to it, I can just tap. Next up is Glow Baby, because it's about this time each day that I check the app to see if my husband has logged our one-year-old's nap. Oh, good job, baby. You slept two hours, and you parents know that good naps equal good nights, so I'm looking forward to that. AR. As I scroll through my day, I can see my next meeting. I can see traffic and directions to places that I'm going later today. Here's that dinner that Siri found in my calendar, and waiting for me at the end of my day is a shortcut to my evening meditation or ten percent happier, nice. Now, I want to check back in on this competition, but I thought it might be a good idea to get a little moral support from my number one fan. And you heard Kevin say how walkie talkie is so great for staying in touch with close friends and family. So I set my daughter up with walkie talkie so that she can help demo what it's like to receive one and maybe give me a little bit of love up here. Oh, there she is. Mommy, I see you on TV. Isn't that fun? How am I doing? So good. I know you'll beat Uncle Jay, #mommyforthewin. Thanks Peach. I love you. And that's how walkie talkie works. AR. The time is now to check in on this competition and see if I have done enough to pull ahead. I'm going to tap into activity and swipe over to see the score. Oh, yes. Sorry Jay, but the day is still young. Thanks everybody, and back to you, Kevin. Way to go, Jules. So in watchOS 5 we've enabled even deeper integration on the watch for apps, enabling them to work right in the moment. And apps can include interactive controls within notifications so you can quickly do more without even opening an app like extend your parking with Pay by Phone. And with Shortcuts, there's also new opportunities for third-party apps to appear on Apple Watch right on the watch face. You can tap on a shortcut on the Siri face to order coffee, rent a bike, or pick up where you left off in a workout. You can even use your custom Siri commands that you created on your phone to speak on your watch. And for native apps building rich experiences, we've improved the workout API for greater performance, and we've added the ability for third-party apps to play background audio. This enables you to easily sync things, of course, like audio books, favorite playlists, guided meditations right to your watch and to be able to play continuously in the background. Now there are so many other features coming into watchOS including things like the ability to customize the button arrangement in your control center, or you can add an air quality complication to your watch face, but one really exciting one is student ID cards. You're going to have the ability to add your student ID card to wallet on your iPhone and Apple Watch. And this works by simply holding your watch near a meter anywhere you can use your student ID cards on and off campus. You can get access to places like your dorm or the library or events on campus. You can even pay for things like snacks or laundry or dinners, and this works with your watch or your phone, and it will be available this fall starting with these universities and will expand to more campuses over time. Now this is also Pride month, and we're really excited to introduce an all-new Pride education watch band. And we didn't stop there. We made a beautiful new face that matches perfectly with the band. And the face and the band are available today. And if you'd like to use the new watch face, you can choose to add it from the watch face gallery starting at noon today. That's just some of what's coming in watchOS 5. Thank you very much. Back to Tim. [applause] Thanks Kevin. Thank you, Kevin. WatchOS just keeps getting better and better, and we're excited for you to try all of these great new features to help you stay more active and connected. Next up is Apple TV. Last September, we introduced Apple TV 4K, and our customers love it. Since its introduction, the Apple TV business has grown an incredible 50 percent. Today, we've got some great new enhancements to Apple TV 4K and tvOS, and we think you're going to love them. So I'd like to invite Apple TV lead designer, Jen Fuls [phonetic] to the stage to tell you all about it. Jen. [applause] Thanks Tim. Apple TV is built on an incredible platform, tvOS. And tvOS is built specifically for the living room, to make it easy to enjoy your favorite TV shows, movies, apps, and games. Now as Tim mentioned, last September, we introduced Apple TV 4K with the goal of bringing the highest quality cinematic experience right to your home. It offers both 4K and high dynamic range, including support for HDR 10 and Dolby Vision. Time said, the new Apple TV is a high fidelity powerhouse. And to ensure you can enjoy this beautiful experience with as much content as possible, iTunes offers the largest collection of 4K HDR movies. And we've upgraded your previously purchased movies to 4K HDR for free on all available titles. There are also, thank you, there are also many 4K and HDR TV shows available from popular services like Amazon Prime video and Netflix. And we know what makes for an amazing cinematic experience is not just great picture quality. It's also incredible sound. So Apple TV 4K is bringing you the latest in audio technology, Dolby Atmos. With Atmos, you get room-filling sound. The perfect complement to the stunning visuals of Apple TV 4K. Now what makes Atmos so special is that unlike a traditional surround sound setup, where sound is assigned to channels like the left or the right, Dolby Atmos has the ability to completely immerse you. Atmos moves sound in three-dimensional space, creating an experience with powerful audio that flows all around you. It puts you right in the center of the action. All this with a home theater setup as simple as a Dolby Atmos enabled soundbar and an Apple TV 4K. In fact, Apple TV 4K is the only streaming player to be both Dolby Vision and Dolby Atmos certified. And this Fall, iTunes will be bringing you the largest collection of Atmos content anywhere. And just like with 4K HDR, your iTunes libraries will be upgraded to include Dolby Atmos on all supported titles for free. [applause] Now of course these movies and so much more are available in the Apple TV app. The Apple TV app is the center of your video experience, a single place to find and watch what you love across your favorite apps. It's on Apple TV, iPad and iPhone where you can find not only On Demand TV shows and movies but also live content including sports. The Apple TV app now offers a huge range of live sports. And we've added live news to make it even easier to stay current. The edition of live sports and news providers brings the coverage of the TV app to over 100 video channels, giving you a massive selection of content all in one place. With all this, Apple TV is the best box to connect to your TV. And that's now more true than ever as more and more cable companies fundamentally shift how video gets to your TV. This typical cable box is becoming a thing of the past as these companies embrace internet-based delivery. And many of them share our vision of Apple TV as the one device for live, On Demand, and Cloud DVR content. And we've already started working with partners around the world to make this a reality. In France, we're working with Canal Plus. Almost a third of French households subscribe to Canal services, and their subscribers can now choose Apple TV to access more than 100 live channels and some of the biggest sports including the French Open in 4K. In Switzerland, we've partnered with Salt. Salt just launched a new TV service with more than 300 live TV channels, and it's available exclusively on Apple TV. And I'm really excited to announce that here in the U.S., Charter Spectrum will be coming to Apple TV late this year. That means that up to 50 million homes will be able to choose Apple TV to access all their live channels and thousands of On Demand programs. And, they'll be able to use Siri and the TV app to get access to their TV service not only on Apple TV but on iPhone and iPad as well. Now with nearly any cable subscription, you can get access to dozens of video channels like ESPN, Showtime, NBC, and many more, but it used to be really painful to set up, having to authenticate each app individually. I'm sure all of you have seen that six-digit code you have to go online and type into a web browser. Well, last year we introduced single sign-on. Enter your cable credentials only once, and we unlock all of the apps your subscription supports. Well now we're making it even easier with zero sign-ons. So now if you're on your TV provider's broadband network, we'll securely and automatically unlock all the supported apps included with your TV service. No credentials needed. It just works. Charter Spectrum [applause]-- this is really awesome-- and Charter Spectrum will be the first to support zero sign-on and we'll be adding more providers over time. We're also giving you even more options for ways to control your Apple TV. For example, you can get quick access on your iPhone where Apple TV users will get the Apple TV remote automatically added to control center. And, for those that have installed home control systems, we're working with the leading providers so that you can use their remotes to control your Apple TV, even including support for Siri. At Apple, I love working with an amazing team designing for a product that I enjoy every day and on the biggest screen in my home. I get to watch all my favorite movies, TV shows and more. Over 40 million songs including the best metal on Apple Music. All of my photos, videos, and memories as well as thousands of apps and games that many of you have built. And, Apple TV brings some amazing views right to my living room with Aerials. Now, one of the questions I get asked the most about Aerials, especially by my family members is where is this? Well, this ball with just a tap of the Siri remote, you'll now be able to see every Arial location. And, you can even swipe between locations so you can see even more. And today, I have an incredible new location to share with you. For the past year, we've been collaborating closely with an amazing partner, filming some stunning footage from around the world with a very unique vantage point, and here it is. Earth. This is filmed by astronauts aboard the International Space Station. You can see Sicily just off the boot of Italy, and check out that Mediterranean Sea. The blue is just phenomenal. So here's a fun fact. The Space Station makes an orbit every 90 minutes, so that means they get 16 sunrises and sunsets every day. And with all those sunsets, nighttime can be particularly stunning the way those urban areas pop with light. Here we are flying over South Korea toward Japan. You can see Tokyo coming into view at the top, some distant stars, and even upper reaches of the Earth's atmosphere, which is that orange band enveloping the Earth. And here's one that might look a little more familiar. This is the Northern California coast heading southward. You can see Lake Tahoe surrounded by snowcaps and the San Francisco Bay. These are truly incredible and offer such a unique perspective on the world. I'd like to give a huge thanks to the International Space Station National Lab and CASIS for all their help in making this a possibility. These are going to look incredible at home in 4K HDR. So that's tvOS. Apple TV gives you amazing picture and sound with 4K HDR and now Dolby Atmos. And the Apple TV app is the center of your video experience. Available on Apple TV, iPad, and iPhone, it's the one place to find all your favorite TV shows, movies, sports, and news. And with that, I'll hand it back to Tim. Thank you. Thanks Jen. Apple TV 4K and tvOS look and sound awesome. You're really going to love it. Next up is the Mac. At Apple [applause]-- yeah. We love the Mac. The Mac was the first computer that made powerful technology so easy to use and put the customer at the center of the experience. And of course that remains at the core of all Apple products. For more than 30 years, the Mac has empowered people to create all kinds of amazing things, from the personal to the professional. Today, we're excited to take Mac a huge leap forward. The next version of macOS is chock full of new features inspired by pro-users but designed for everyone. I'd like to bring Craig back up to talk about it. Craig. Hello again. It's true that macOS is the heart of what makes a Mac a Mac. And we want as many Mac users as possible to have access to our latest software, and that's why six years ago with the introduction of OS 10 Mavericks, we began offering our macOS updates free for Mac users. And 2013 was also the year that we first introduced our California naming theme. Now after spending a year by the ocean, we not only modernized the look and feel of macOS, but we headed to the mountains with macOS Yosemite. Now, as you may be aware, our naming of Mac releases is handled by our crack marketing organization, and as you've probably noticed, they went on a four-year mountain-bound bender. [laughter] In El Capitan we added metal, our ground-breaking graphics technology. In Sierra, we brought Siri to the Mac and extended our capabilities and continuity. And last year with High Sierra, we focused on deep technology, preparing the Mac for future innovation. Well this year, we made some striking changes to macOS, and we've left the high country for a place entirely different but not less beautiful and here still in California, and I'd like to take you there now. Our next release of macOS is macOS Mojave. Now, Mojave is beautiful during the day, but what really captured our imagination was the beauty of the desert at night. And this inspired one of our most distinctive new features, and I'd like to show it to you now. So here we are live in macOS Mojave, and I'd like to show you a new side of Mojave. We call it dark mode. And as you see, dark mode is not just about the dock or the menu bar. It extends to your Windows Chrome, your sidebar, and even the content of the Windows, and it's so great for Pros. It makes photographic content absolutely pop off the screen. It's just gorgeous. It's so nice. And this is great not just for photography but when working on presentations or documents. It's also great doing ordinary things. Maybe you're working in a dark environment. Just look at calendar. Even mail in dark mode. It's so great. And I think some of us are going to want to run dark mode just because it's so cool. I mean your emoji look great. Your photos look great. I mean check out your album art and music or your For You feed in Apple Music. But I think one audience that's going to especially appreciate dark mode are some of you here in this room, our developers, because Expo looks fantastic in dark mode. Whether it's your source code or even interface builder and all of its inspectors, they just look fantastic in black. And that's a quick look at dark mode. Now, we were so inspired by this changing desktop wallpaper that we decided to add a new feature to Mojave that I think you'll enjoy. It's called dynamic desktop, and when you're using it, you're desktop actually subtly changes throughout the day from morning, to afternoon, to evening. It's really cool. Now, there's much more to Mojave that I'd like to share with you through demos, and it starts with the desktop. Now, the desktop is so crucial to how many of us use our Macs. When we have files that we're actively working on, we often put them on the desktop, but the result can be a desktop that looks something like this. And so now in Mojave we have a really great solution, and we call it Desktop Stacks. All of the contents of your desktop are automatically arranged into these stacks, and they can be arranged by kind, by date, or even by tag, and they're really easy to use. You just click on them. You can see all the contents in the stack. You can double click to open a document and put it away. And they stay organized. So for instance, if I bring forward mail, maybe I drag an image out, I want you to watch what happens because the image flies right into the right stack. Now you can also scrub your stacks. So, for instance, I'll just scrub across this stack. You see I can select between different photos, pick one up. Actually let me just hide mail here mid drag. I got a little excited with my, with all of my stack action. So I can just drag this out and drop it in just like that. And that's a quick look at Stacks. Now-- We've also brought some great new changes to the finder. I'd like to show them to you now. Now it starts with a new view. We all enjoy using icon view, list view. There's of course column view, but now we've added an all-new view called gallery view. It has a big preview up top, a set of thumbnails along the bottom, and it makes it easy to preview images, video, presentations, documents, spreadsheets, PDFs, and of course with images sometimes you want to know more detail about, for instance, how they were captured, and now the new sidebar in Mojave really helps because it now supports full metadata, so you can see around your photo, the camera you took the photo on, the kind of lens, the aperture settings, and so forth. It's really handy. And you'll notice also along the bottom there's this new area called Quick Actions, and Quick Actions that you act on the current photo. So, for instance, if I have a photo like this and I want to edit it, I don't have to open it and go through a new app. I can rotate it right here inside the finder. It's really powerful. Now this sidebar is available in other views of the finder. So for instance, I'm just going to bring up the preview pane here, and I'm going to do a multi selection of a PDF as well as several images, and you'll notice that the quick actions area is contextual, so it shows me create PDF as an option. I'm going to click create PDF, and it's going to assemble all of these photos into a PDF just like that. But what's really great is these actions are also customizable, so you can create automator actions and assign them to buttons here inside of Finder. So you'll notice that now that I have this PDF selected, I have an option to run a custom automator action that I've created called Watermark PDF. When I click it, my custom action runs, and my document is watermarked, just like that. And those are some quick enhancements to the finder. Now a tool that I think many of us love when working with files is Quick Look, and now in Mojave, we made Quick Look more powerful than ever by integrating Markup. Let me show you how it works. So you see down here I have a permission slip and it's a PDF document. I'm just going to hit tap spacebar to Quick Look it, and you notice now I have the option to invoke Markup. I click, and now I have access to my markup tools, including my ability to sign this document. I can just drag out my signature like this, and I'm done. Now this works for all kinds of files. So, for instance, with images I can rotate and crop, and with video I can even trim right here inside of Quick Look. And that's a quick look at Quick Look. Next, I want to talk about how we capture content on our Mac because one of the tools I think many of us use all the time is Screenshots, and we've made Screenshots more powerful than ever in Mohave. So let's take a look here at a webpage, and I'm just going to take a screenshot in the traditional way. I'm going to screen shot a selection of the page, and I want you to watch what happens in the lower right. I get a thumbnail instantly of that screenshot, and when I double click in, I get an accelerated workflow right into Markup where I have access to all of my tools. So, for instance, if I want to create a magnification here, I can just drag that out. Magnify. It's that easy. But now we've also made it easier to access a variety of tools, so when I bring up my screenshotting, you see I'm presented with this hud that tells me I can capture the entire screen, a selected window or selection, but we've also added screen capture for video right into screenshotting. Let me show you how that works. So I'm going to go to a webpage here that has an animation running. I'm going to bring up my screenshotting tools, and say record the selected area. I'll just make a nice selection here. Okay. And I'm recording just like that. Of course I can manipulate the app, so all will be reflected in my recording, and when I'm done, I can click the stop button right up here. And now you notice I have this thumbnail in the lower right. Well I can actually pick this up and drag it into a new space and incorporate it right into a document, just like that. Now, we've further enhanced the way you capture content, and that brings me to Continuity. So Mac users love Continuity for the way it lets us work across our devices with things like Air Drop or the ability to unlock your Mac using your Apple Watch. When it comes to capturing content, we'll walk around with one of the best content capture devices in the world in our pockets, our phones. And so we wanted to take advantage of continuity to bring that to the Mac with a feature we call Continuity Camera. Let me show you how it works. So here in my Keynote presentation, I have a space that's just waiting for a new photo, and on my phone right here. Well, when I select this object, I can choose to take a photo. And I want you to watch what happens when I select this to my phone. It automatically immediately lights up ready to take a photo. So I can take one like this, and when I do, I can select use photo, and it apps directly in my document. Isn't that cool? Now, this works as well for scanning documents. So here I have a place where I could use a scan. Once again, I'm going to select from the menu and this time scan document. Again, my camera lights up, this time right in my document scanner. I just scan like this. I can save it. And my scan goes immediately, and I think I forgot to push the save button. Sorry about that everybody. There you go, appears immediately in my document. Thank you. [applause] And so just like that I can take photos, stills, and even capture video, and this is a quick look at some great new features in Mojave. Next, I'd like to turn to apps. We are bringing News to the Mac. Now News has all of the stories you've come to expect from News on iOS, and it looks amazing on the Mac display. You get top stories picked by our editors, trending stories, you're personalized for you and that's not for me, and you also get this great new sidebar where you can drill in and jump right to the topics and channels you follow. News is going to be great. We also have Stocks coming to the Mac. Now you get your stock prices combined with high-quality business news delivered from Apple News. It shows you your watch list with your prices on the left, and you can drill in to this interactive chart to get more information and, of course, great news. We're also bringing Voice Memos to the Mac. Now Voice Memos is the most popular voice recorder on iOS, and so many of us use it to capture music recordings or lectures, and now that Voice Memos syncs via iCloud to your Mac, you can take those recordings that you make and for instance drag them right into Garage Band as the foundation for a song. Finally, thrilled to announce Home is coming to the Mac as well. You have all of your accessories here. You can run your scenes as well as monitor your video cameras, and of course with Siri you can command your home with your voice. So those are four great new apps coming to Mojave. Next, I'd like to talk about security and privacy. You know, one of the reasons that people choose Apple products is because of our commitment to security and privacy. And we believe that your private data should remain private, not because you've done something wrong or that you have something to hide but because there can be a lot of sensitive data on your devices, and we think you should be in control of who sees it. Now, to begin we protect your information on your devices using state of the art hardware and software, and this year we're adding greater protections about how apps can access that information. Today, Apple devices check in with you before granting an app access to information like your location. You could tell because you'll see an alert just like this one, and macOS already provides API-level protections for things like contacts, photos, calendar, and reminders, but now in Mojave, we're extending these protections to include your camera and your microphone as well as protecting sensitive parts of your file system, like your mail database, your message history, and your backups. And all of this is protected by default for any app that you run on the system, an important protection. Next, I want to turn to some great enhancements to Safari. Safari works really hard to protect your privacy, and this year it's working even harder. Last year, we introduced intelligent tracking prevention to dramatically reduce the ability for apps to track you across websites using cookies. This is the kind of thing where you look at a product on one site. Then you move to another site and another site and somehow this is just following you wherever you go. Well, we've all seen these, these like buttons and share buttons and these comment fields. Well, it turns out these can be used to track you, whether you click on them or not, and so this year we are shutting that down. Now if you do want to interact with one of these or one of these apps tries to access that information, you'll get this, and you can decide to keep your information private. Now next let's talk about Fingerprinting. You know, data companies are clever and relentless, and in addition to cookies, they use another method called Fingerprinting. And here's how it works. Just like you can be identified by a fingerprint, it turns out that when you browse the web, you're device can be identify by a unique set of characteristics, like its configuration, its fonts that you have installed and the plug-insulin that you might have on the device. And these data companies can use the set of characteristics to construct a unique fingerprint to track your device from site to site. With Mojave, we're making it much harder for trackers to create a unique fingerprint. We're presenting webpages with only a simplified system configuration. We show them only built-in fonts, and Legacy plug-insulin are no longer supported, so those can't contribute to a fingerprint, and as a result, your Mac will look more like everyone else's Mac, and it will be dramatically more difficult for data companies to uniquely identify your device and track you. Now, we're bringing all of these new protections to Safari on both Mojave and iOS 12. Next, let's talk about the mac App Store, and to do that, I'd like to invite to the stage Anne Tye, our product marketing manager for the App Store. Anne. Last year, we launched a completely redesigned App Store on iOS. Every day, we celebrate apps, games, and developers. We've written more than 4000 stories for the New Today tab, and hundreds of them have each been read by more than a million people. The response has been incredible, and we've learned a lot. We've got a bunch of great new features coming later this year that we'll cover in sessions. This year, we are turning our attention to the Mac App Store. [applause] Since it launched in 2011, it's changed the way we download and install software for Mac, making it easy with one click. It's the biggest catalogue of Mac apps in the world. It's also a trusted and safe place to download software. Entrusting where you get your apps from has become more important than ever. Developers can distribute their apps to 155 countries and get worldwide payment processing, and it offers seamless software updates from one place. This adds up to a great experience for our users. We've spent a lot of time thinking about what people do on their Macs and wanting to create a place organized around those themes. So we've redesigned an all new Mac App Store from the ground up, and we're thrilled to show it to you now. It's got a beautiful UI that should feel familiar but new and designed first and foremost to be a great Mac app. Starting with the new discover tab where each week you can find in-depth editorial about the best Mac apps through stories and collections and see what's most popular with top charts. Here's a story about musician and founder Karim Morsy. Learn about how he uses his app, Djay Pro 2, and get inspiration for your own set. Helpful videos auto play, so you can see what apps are capable of before downloading them. Visit the all-new create, work, play, and develop tabs where you'll find helpful recommendations and expertise around each theme. Here's the create tab where you can find apps that bring your artistic ideas to light. These tabs will also help you make the most of apps you might already have with tips and tutorials even the most expert users will find useful. The work, play, and develop tabs share the same beautiful design, and you can still browse by category on the categories tab. We've redesigned product pages too, bringing many features over from iOS based on our learnings there. It has more useful information like video previews available on the Mac App Store for the first time, an apps rank if it's charting and if it's been named editor's choice. Ratings and reviews are now front and center, and these are so important for app discovery. So we're introducing a ratings and review API for Mac apps. Now, it'll be easier than ever for people to leave feedback. We're really excited about the all-new Mac App Store. We've talked to some developers already, and they're really excited too. Like Microsoft, who will bring Office 365 to the Mac App Store later this year. And Adobe is bringing Lightroom CC. Panic is bringing Transmit. And Bare Bones is bringing BB Edit. [applause] And many more great names are coming to the all-new Mac App Store too. We can't wait for you to check out the new Mac App Store. Now, I'll hand it back to Craig. So we think the Mac App Store is going to inspire whole-new generations of apps, and so we want to talk about some of the technologies that will be behind some of that next generation. We spoke earlier about ARKit. I want to talk about two more. And let's start with Metal. Now Metal is the technology to get the highest performance graphics and computation from graphics processors. Metal was designed for modern GPUs. It's incredibly efficient, and that enables amazing console-level games like Fortnight from Epic to run great for the first time on mobile. But Metal also enables these games to scale to take full advantage of modern Macs. In fact, across iOS and the Mac, there are over one billion Metal-enabled devices, and we're constantly making Metal better. To bring the highest GPU performance in reach of all Macs, we've recently added support for external GPUs, and this is powered by Metal, and the results are truly mind boggling. For instance, running a filter in DaVinci Resolve, look at how it scales on the incredibly fast iMac Pro as you add up to four eGPUs. Now, the results are even more staggering when you take a 13-inch MacBook Pro and add eGPUs to it, achieving up to 8.5 times speedup. It's pretty awesome. Now, eGPUs also enable Macs to achieve all-new levels of performance and realism in 3D rendering and gaming. Now this beautiful forest you see here, this isn't a video capture. This is from Unity's new Book of the Dead interactive demo, and it's being rendered live right now on a MacBook with an eGPU powering this display. I mean doesn't this look amazing? Now, of course because this is rendered live, we can check what's up ahead, so let's start walking. Now Unity is using Metal's unified graphics and compute to generate real-time lighting and complex post-processing effects. So cool. And that's all rendered live on a MacBook running with an eGPU. It's pretty great. Now another place where we're doing incredible acceleration with Metal is in machine learning. And today ML specialists usually use one of these third-party libraries to train their models using servers, and now it turns out we can accelerate tools like this with our new Metal performance shaders. We're seeing speedups of up to 20 times and using the GPU with Metal instead of CPU-based training. And while speeding these tools up is great, we actually thing there's a better way for most developers, and that's training on the Mac you already have using a great new tool we call Create ML. Now, Create ML is designed to let you train without being a machine learning expert. You can train vision and natural language models. You can bring your own custom data, and it's really easy to use because it's all built in Swift. In fact, you can use Xcode Playgrounds to train your model. Just drag one in, you're training set, you can drag in your test set as well, and the training is all GPU accelerated, so it's incredibly fast. Now, as an example, we worked with Memrise. They're a developer who uses the camera to identify objects and speak them in multiple languages, and in the past, they would train their model with 20,000 images, and it would take them 24 hours to do so. Well now with Create ML, they can train that same model in 48 minutes on a MacBook Pro. And on iMac Pro, it's just 18 minutes. And what's even more incredible is that model in the past for them was 90 megabytes, and now it's just three megabytes. It's a huge difference. Now we also are making models run much faster on device using Core ML 2. Now Core ML is our technology for high performance on-device machine learning, and now it's better than ever. It's 30 percent faster in on-device processing using a technique called batch predictions, and you can reduce your model size by up to 75 percent using quantization. And so that's Core ML and Create ML. You no longer have to be an expert in machine learning to build those techniques into your app. Now, these technologies are redefining what's possible in apps, and they're common not just to macOS but also to iOS, and the fact that the mac and iOS share so much technology has led people almost every year to keep asking us the question, are you merging iOS and macOS? So I'd like to take a moment to briefly address this question. No. Of course not. We love the Mac because it's, and we love macOS because it's explicitly created that a unique characteristics of Mac hardware, like the ergonomics of the keyboard and the trackpad, the flexibility in displays and storage and because of the power it exposes. It makes the Mac able to accomplish almost anything. So we think that this question is actually coming from something else. You know, Mac users have access to a rich set of great native applications, apps that take full advantage of the power of Mac technologies. But Mac users also use apps based on other technologies. We routinely access web-based experiences like Netflix that build on WebKit, the standards-based web technology in Safari, and we also run sometimes cross-platform games built on technology like metal. And all of these platforms enrich the Mac user's experience. But we think there's room for one more, and so we'd like to give you a sneak peek of a multiyear project we have going on. Because we see a huge opportunity for the Mac to tap into the world's most vital app ecosystem. It's called iOS. I think you might be familiar with it. Now, there are millions of iOS apps out there, and we think some of them would be absolutely great on the Mac, and Mac users would love to have them there. And from a technical standpoint, it's actually a really good fit because from day one iOS and macOS have shared common foundations. But iOS devices and Mac devices of course are different, and the user interfaces are somewhat different and so the frameworks underneath are as well. And that makes today porting an app from one to the other some work. We wanted to make this much easier, and so we've taken some key frameworks from iOS and brought them to the Mac. And we've adapted them to specific Mac behaviors like use of trackpad and mouse, window resizing, integration of things like copy and paste and drag and drop into the system services on the mac. Now, phase one of this effort is to test it on ourselves. So this year in macOS, we've taken some of our own iOS apps, and we brought them to the Mac using those technologies to make sure it works well. You've actually heard about several of them earlier today, and it turns out they make fantastic Mac apps, and we're able to bring them to the Mac with very few code changes. Now, this is going to be coming to you developers next year. So you can easily bring your iOS apps to the Mac, and in the meantime, we hope you enjoy News, Stocks, Voice Memos, and Home in Mojave. Now we're really excited about Mojave. From desktop, Stacks, to Finder with gallery view to enhanced screenshots and markup, News and Home on the Mac for the first time. Stocks and of course the redesigned Mac App Store. And of course there's even more. Like APFS now supports Fusion and hard drives, and Safari tabs can now have favicons if you want them there. And, of course, Group Facetime. So that's macOS Mojave. I hope you like it. I'm going to hand it back to Tim. Thank you so much. Thank you, Craig. What a huge update to macOS and what an extraordinary morning. We got started with iOS 12 with all-new capabilities including taking AR further than ever before, bringing Siri to any app with Siri Shortcuts and ScreenTime and cool new communication features like Memoji and Group Facetime. And watchOS, now with Walkie Talkie, Activity, Competitions, New Workouts, New Siri Capabilities, and so much more. And Apple TV 4K now with Adobe Atmos, some great new partnerships, and the ease of zero sign-on. And you're going to really love those new Aerial screensavers. And macOS Mojave with all new dark mode, great updates to desktop and finder, enhanced privacy and security, and a completely redesigned Mac App Store. The updates will be available to our users this Fall, and there will be developer betas for each of them after the Keynote this morning. Thank you. Now, before we close, we wanted to celebrate you and the amazing work that you do, so we went out and talked to some of the most important people in your lives, the ones that know you the best, and we made a short video, and I'd love to run it for you now. I don't have any idea at all how to create an app. I don't understand any of it. Ask me another question. My son is a developer at Robinhood. My daughter, Jody, is a WWDC 2018 scholarship winner. My brother started and founded Yelp. All the apps. Timeless. One drop. Splitter critters. Homework. Homework, you talk to her. My brother Derek created the app Refugees and Immigrants the creative, the collective. Why did I say the creative. At Christmastime, when most kids maybe would want skis or something, he wanted computer books. He stayed in his bedroom, making games while the other kids were outside playing. And when she is really doing the coding almost always we have to tell my daughter to stop, but then my daughter will be like, just give me another ten minutes. I want to get this solved first. When I watch him code, his eyes move in a weird way, and he's like totally focused. Even when my friends are over, I'd say, we'll look at Christopher, look at him. And they'd go, wow. When he first came to us with the idea, we were like, okay, good luck. He started off in his small little one-bedroom apartment with next to nothing, and sacrificed eating a burger and just have ramen just so that he could, and I'm like just buy the burger. And he's like no, that's three bucks that I could put towards the app. Jeremy went out to build the first version of Yelp, and it was a total bust, a total failure. She tried really hard on a couple different titles, and you've never heard of them. And, you know, that's a really rough feeling. That was definitely a low point for Jeremy because it was like, hey we got to retool and try again. What Jeremy did notice in the data was that people loved writing comments, and that was really the genesis of the second version of Yelp, which worked. You have to be really okay with waking up to failure and then at the end of a whole bunch of failures is something that's great. Tessa didn't really think I want to make an app. Tessa found a problem, a food waste which you could use the app to help to solve. Emma's grandma was diagnosed with Alzheimer's. She even forgot my daughter's birthday. So Emma tried to search for an app that would help, but she couldn't find it. So she said, you know, why don't we just do it ourselves. My husband was a diabetic. That inspired Jeffrey to do something about diabetes, and it's an answer to a lot of people's prayers. Jessie won the iPhone game of the year. I couldn't believe that he made it. I mean to me it was remarkable. She pulled out her cell phone, and I saw his app on her phone, and I was like trying not to freak out, but oh my gosh. For my wife, it's not just creating, it's really wanting to be a part of something big. The one lesson that I learned from Jeremy every day is just that determination, tenacity, to focus. He foresaw the impact that the iPhone was going to have, and he bet everything on getting an app for it, and I think that it was one of the most important things that Yelp did. I think he's made a big difference, and so do thousands of other people. She wants to not just learn also teach others and it makes me really proud to see her blossom. As a parent, no better thing is letting your kids do what they want to do, because it isn't how much money you've made. It's really changes you've left behind. And she's hoping to leave a lot of change behind. I love that video, and I'm pleased that some of the developers are in the audience this morning. Their stories are great examples of all of your passion and your creativity. We love the work you do and the impact that it has on the world. It inspires all of us at Apple deeply every day. On behalf of everyone at Apple, thank you. And I'd also like to thank everyone at Apple who made today possible. Days like this only come from years of effort and hard work and great sacrifice, and so I thank them and their families. Let's have an incredible week together. Thank you.  Good afternoon, everyone. Welcome to Core Data Best Practices. My name is Scott Perry. I work on Core Data. And we'll be joined later by my teammate, Nick Gillett. The plan for today, first, we'll talk a bit about how the process of getting started with Core Data has evolved over time. Then, we will cover ways we can evolve our application more easily by taking advantage of extension points in the persistent container. We'll follow that with how we can evolve our model as our apps requirements change and our data grows. Then, Nick is going to talk about a few ways our app can maintain its performance, even as it scales beyond our wildest dreams, and we'll wrap up with some good stuff about transformers, debugging, and testing. But first, let's build an app. I like to take photos, so we're going to build something that allows me to share photos with friends and get comments from them, even if it's just Nick asking how my slides are going. Where should we keep our app's data? Well, we could keep it all online, but I usually take photos when I'm traveling, and the connection can be kind of spotty, so we should keep it locally, organized into some kind of store. So, we have posts and comments and their instances and the relationships between them form an object graph and we've decided we need to persist these things on disk, so that's what Core Data is for. So, we'll use that and we'll start by translating our mock here into something a store can understand, a managed object model. We'll need fields for everything, with attributes for things like the image's data as well as the time it was posted, and we'll need relationships for posts and comments. We've also defined a need for a store, but there's a lot involved in maintaining data on disk over time. Luckily, Core Data provides a persistent store coordinator to manage that. The coordinator can do things like compare the app's model with the store's version and automatically migrate it forward as our app evolves. Finally, managed object context provide safe, fast, and predictable access to our data, even when we're using many at the same time through features like query generations, connection pooling, and history tracking. Setting this all up requires finding the model and loading it and deciding where to keep the store, but a lot of these error paths can't actually fail once you've shipped your app, so Core Data provides a container type that dramatically reduces the amount of boilerplate required to set up your stack, just refer to the model by name, and the persistent container will load it out of the main bundle and keep a stored in a consistent location. This persistent container type encapsulates a whole stack and includes conveniences for a shared main queue view context as well as factory methods for generating background contexts as well as performing background work. It's also designed to be easy to work with as our app grows. For example, let's say we want to factor our model layer into its own framework. We can do that by creating a new framework target in Xcode and moving our code into it. It's all super easy, but when we move our model into the new target, in the built product, targets move from the app into the new framework, which is what's supposed to happen, but now NSPersistentContainer doesn't know where to find our model anymore. This is because it only checks the main bundle by default. Why stop there? Well, searching all of the app's bundles could get really slow for a complicated app and it's not a cost you want to pay every time you spin up a stack. How do we fix this? Well, we could resuscitate the model out of the framework bundle ourselves and use one of the container's other initializers, like one that takes an explicit managed object model, but NSPersistentContainer actually has a way for you to change which bundle it searches. See, NSPersistentContainer knows when it's been subclassed and will use the type of the subclass as a hint when it looks for the model. All we need to do to take advantage of this is to create a subclass. It doesn't even need to have anything in it. Then, any code setting up through the container that wants to use our model can just adopt that subclass and the persistent container will check in our frameworks bundle for our model instead. So, that's fun, but since we're going through the effort of factoring our app's resources, wouldn't it be nice if we also improved the organization of our data on disk? By default, new persistent containers come with a store description for an SQLite store with automatic migration that on iOS lives in our app's documents directory. That was great when our model code was part of the app, but we should try to keep our new frameworks files from mingling too much with the apps. Since we already subclassed NSPersistentContainer to make finding the model easier, let's build on that to improve this. The brute force way to change a store's location is to directly modify the URL in the persistentStoreDescription before loading the store. Sometimes that's what you want and we could use that pattern here, but we don't have to because NSPersistentContainer calls its own default directory URL method when creating persistent store descriptions. And it's made to be overridden. In this case, we can just append a path component, but this is also a good way to set up containers for caches or other kinds of stacks that need to keep their stores in different locations, like your tasks. So, now that we've got our Core Data stock all figured out, let's have a look at our app and some of the view controllers that we've written. It looks like we've got some pretty specialized view controllers here. Here's one that shows all of my posts as well as another that shows all posts by all authors. Even the detail views are duplicated. It feels a lot like we could have written half the code. All we should really need is one view controller for displaying a list of posts and another for displaying a single post. We can accomplish this by defining good boundaries in between our view controllers in the form of interfaces that take model objects. Each controller gets configured by its model parameters and then they can customize their views in cells based on whether they're showing my posts or someone else's. When drafting view controllers using Core Data, list views should get fetch requests and detail views should get managed objects. View controllers also need a managed object context, either the container's view context or some other main queue context. And this pattern for generalizing view controllers with Core Data isn't just for UIs; it works really well for utility types as well. Instead of passing Core Data types for presentation, we can pass things like URLs or serialized data into background work controllers and turn those into new and updated managed objects using a background context instead of a view context to do our work. Adopting this kind of interface and utility type is super easy since we own the initializer, so we can just require the parameters to create the controller. But how do we get our boundary variables into our view controllers? Well, if we're using segues, we can override the prepare method and get a reference to the destinationViewController and then configure it there. If we're using storyboards or nibs, then we already have code that has to cons up a destinationViewController, so all we need to do is set the properties before presentation. And, if we're driving stick, we can just write an initializer that explicitly defines the boundary conditions, just like we do with our utility types. OK. So, now we've got a fetch request and a context for our view controller, but before we smash them together to get our results, we should configure the fetch request a little bit more to make sure that our controller will have great performance. Sometimes it makes sense to set a fetch limit, but in the case of our list view, batching makes more sense because we want to show all the data and we know exactly how many cells our view controller can fit on the screen at once. In general, at least one of these options should always be set for fetch requests that might return an unbounded number of results. So, at this point, we could turn our fetch request into objects and populate a list view with the results, but what if we want to keep the UI up to date with changes as they happen? Core Data has us covered here as well with the fetched results controller. Available on all platforms since Sierra, adopting the fetched results controller just requires writing an adaptor between the delegate protocol and the view it's driving. And to create one, all we need is a fetch request and a context. The fetched results controller even supports driving more advanced list view concepts such as sections. If we wanted to group posts into sections by the day they were posted, we could do it by extending the post type that's generated by Xcode with the computed property and passing the name of it to the fetched results controller's initializer. This works well, but what if we have a view controller more complicated than just a list of our objects? What if we want to show something like a chart of the posts per day on our app? Well, the first thing we should do is not underestimate the power of fetched requests. I'm just one person, so in the last month, I haven't managed to post more than 40 pictures per day. Over the course of 30 days, that's still a fairly reasonable amount of data to pull out of the store at once. If the day property that we defined earlier was actually part of the entity in the model, then we could write a fetch request that counts up the number of posts grouped by the day they were posted. There's three parts to this request. The first one is just setting the range. We want the last 30 days of data. Next, we want to group together all results whose day attributes share the same value. Since we're now fetching aggregates instead of individual objects, we have to change the result type to something more sensible as well, in this case, a dictionary. Finally, we define an expression that represents the number of objects in each group and tell the fetch requests to return that count along with the day it represents. This fetch request returns 30 results, each of which is one point on our chart. If you're into databases, this is the SQLite query that Core Data generates from that fetch request. It's exactly what you do if you're writing the query yourself. Core Data understands how to convert many expression functions into optimal database queries. A group by query can use aggregate functions such as average and sum and scalar queries, like a normal fetch request, can use scalar math and date functions, like abs for the absolute value and now for the current time. If you want to know more about what you can do with NSExpression, check out the documentation for its list of functions. Many of them are supported by fetch requests in Core Data. OK. So, fetch requests can accomplish a lot through the use of expressions, but SQLite still reads every one of our posts through memory when computing the counts for our graph here. That works fine for charts showing the amount of posts generated by one human in a month, but what if we want to chart something bigger? What if we want to show a whole year or what if our little app starts handling orders of magnitude more data? Now the fetch request would be counting at least 50,000 posts one by one just to show 30 data points and that's not going to be fast enough. The mismatch between our views and our model has gotten to the point where we need to start doing some denormalization. Denormalization is when we add redundant copies of data or metadata to improve read performance at the expense of some additional bookkeeping. Database indexes are a good example of this. Adding count metadata to our store is exactly the kind of compromise we need to get our chart's performance again. So, let's look at how our model can group posts into counts by day. We'll need a new entity with two attributes, plus a bit of extra maintenance to keep them accurate. Grouping by day improves our fetch request so much that it guarantees good performance for charts covering years of data, so we only have to create this one level of denormalization and the fetch request that we passed to the chart view controller? It's super simple. It's really not that much different than the fetch request that we'd pass off to any other list view, which is actually kind of sort of what a chart view is if you squint hard enough. But what about that extra maintenance? We've got to increment the count whenever a post gets published and decrement it whenever a post is removed. We could do this in the methods that change the post object's relevant state, but a more foolproof solution is to update our counts in response to the context saving. We could just register for the managed object contextWillSave notification with a function that goes through all of the posts that have been inserted, incrementing the count for each day that's relevant, and another loop going through all of the deleted objects, decrementing the count for each day. And that affects the state of the context before it commits to the database, so it all winds up happening in one transaction, so this scales really well, which is useful because my teammate Nick Gillett is here to talk about how Core Data can help us as our little app scales beyond our wildest dreams. Nick. Thanks, Scott. So, as Scott said, as your applications grow, they get more and more complex and at Core Data it's really important to us that your applications do grow. In fact, we want this. That's the whole reason we exist is to help you manage this growth, make it efficient for you to work with, and help you give more value to your customers. But this happens in ways that are very specific to your application. It's also highly aligned with your customer experience or the way that you want them to experience your application. Unfortunately, like all complex systems, as it grows and becomes more complex, it also tends toward chaos. So, today we're going to talk about ways that Core Data can help you manage this chaos and give it some structure. We'll talk about building predictable behaviors and helping you build tunable containers that you can align with your experience metrics. What does that mean? Well, when we think of metrics, there's a couple of different ways that we can think about them. The first is in alignment with our customers. And usually we define these as things that they experience, like having a consistent user interface or a responsive scroll view, and also as delight. But those are really hard for us to capture as engineers. So, we translate them into engineering metrics, things like peak memory consumption, how much battery drains during a given task or how much CPU time is burned during a given task. And, finally, how much IO we do during a given task. To make this a little more concrete, we'll use this application. Some of you may remember the history demo application that was introduced last year at WWDC and I've modified it for the purposes of this talk. There are a few actions here that a customer can take while using our application. The first is they can add a single post to our database by hitting the + button. They can also download any pending data from the server by tapping Download. And, finally, for anything that hasn't yet been uploaded to a server, they can tap Post All. Now, this application has a fairly small set of interactions that a customer can take, and yet, as these happen concurrently, it tends towards chaos. So, we can see that even with this small set of actions, things that go on concurrently could cause a number of different state changes in the application and the worst thing for us is to end up with a user experience that looks like this. This notion of partial completeness doesn't make sense to our customers. In fact, it doesn't make sense to us either. Core Data is here to help with that with query generations. Query generations were introduced in 2016 in our What's New in Core Data session. So, if you're not yet familiar with them, I highly recommend that you check out that session for more information about how they work. What you do need to know is that they require wall journal mode and only work with SQLite. The goal of query generations is to isolate your managed object contexts from competing work. This could be rights to the background or actions that the user is taking that you're not yet ready to manifest in a given context. Query generations provide a consistent, durable view of the database that will return the same results for fetches regardless of what other contexts are writing to the database at a given time. The best part is we can adopt them in one line of code. This is a typical change for reloading a table view. We would just have to insert a call to NSManagedObjectContext setQueryGenerationFrom token with the current query generation. And when it comes time to update them, we can update them as we normally do by using NSMangedObjectContextDidSave notification. And this allows us to manifest changes to the application's data in the UI at the right time. But what if the data that we're writing isn't related to the UI, such as downloading some comments that Scott mentioned earlier? In this case, we don't want that data to manifest in the user interface or cause changes to it because none of the change will be visible to the user. So, we can actually filter out these updates by using history tracking. Persistent history tracking was new in iOS 11 and macOS 10.13. We introduced it in our session, What's New in Core Data, last year at WWDC, and for more information about how it works and what the underlying features are of it, you can use that session as a reference. Persistent history tracking is a great way to get a persistent record of each transaction that connects to the database and this is useful to us for a couple of different reasons. For the purposes of this talk, though, we'll be considering NSPersistentHistoryChange, which gives us a changedObjectID and a set of updatedProperties. And NSPersistentHistoryTransaction which gives us a set of changes and an objectIDNotification. So, let's consider the following set of changes. As you can see, these are posts that are being inserted to our database and when this happens, given our table view, we would want to refresh the UI, which we can do by using the objectIDNotification. These are analogous to NSManageObjectContextDidSave notifications and can be merged in using the same API. But if we downloaded a list of comments that we don't want to manifest in a user update for, we can filter them. Using this small amount of code, we can filter out the changes from a given transaction to decide if any of them were relevant to the post entity and in that way we won't refresh the UI and cause an unnecessary blip or stutter to the user. But as you can see here, we're actually only using a small amount of the post content. In fact, we're only using two properties, the image and the title. And so we can do better than just filtering out by entity. We can actually filter out by updated properties using the history changes and in this way we can create highly-targeted updates to our user experience that align with changes that will be visible to them. But Core Data can also help you support new interactions for your users. As your data becomes more complex and grows in scale, some editing operations can get more expensive. For example, consider a simple photos browser. Typically, when our applications grow, we want to -- we want to introduce new functionality that makes it easier to perform repetitive tasks, such as multiple selection. And Core Data can support this by using batch operations. In fact, in just a couple lines of code we can mark an entire set of photos as a favorite or not. And in just one line of code, we can purge or delete a set of records from the database using a batch delete operation. And these operations scale in ways that aren't possible by faulting the objects into memory. For example, during a delete, a traditional delete by calling NSManagedObject.delete will grow with the size of the records in the database. And as you delete objects and their memory gets faulted into the context, this gets more and more expensive the larger your database gets. But with batch operations, we can perform the same mutations in just a fraction of the memory. And this has the curve that we want as data increases, where the larger the data set is, the less memory we use, using up to about 7% of the memory of a traditional delete at 10 million rows. So, this is a very powerful way to save resources on your customer's device. But one of the traditional problems with batch operations is that they were difficult to work with because they don't generate save notifications. That's where history tracking comes back in. With persistent history tracking, we can fetch out the transactions from the database that occurred as part of the batch delete or update and we can use the objectIDNotification method to generate a notification that works like a save notification. In this way, the fetched results controller or any other context in your application can update to those notifications incrementally. And so those are some ways you can manage growing data with Core Data, but how about your workflow itself? What can Core Data do for you as a developer and engineer to make it easier to build and test your applications? The first thing is that we can help future you today. As you may know, NSKeyedArchiver is changing. We're adopting secure coding across the entire platform and the KeyedArchiver API has changed significantly this year in support of that. For Core Data, this means that value transformers are changing, so if you have a transformable property in your managed object model and you're not sending a value transformer today, you used to be getting NSKeyedUnarchive FromDataTransformer as your default value transformer. In the future, you'll be getting NSSecureUnarchive FromDataTransformer and this implements secure coding under the covers and you should adopt it today. There was a great talk this morning about exactly this topic called the Data You Can Trust and I highly recommend that you view it to get more information about secure coding and how to make your applications more resilient. You can specify this in the model editor with a transformable property by the value transformer name field. And today, we want you to implement this on your own. This will become the default in a future release and in a future release Xcode will also start emitting a warning for anyone that's using the default value transformer name. If you're building your models in code, you can set it by using the valueTransformerName property on NSAttribute description. This should be transparent for you if you're not encoding custom class types. So, for plist types, this is a no op. You simply change the value transformer name and you'll get the new secure coding behavior. However, if you are implementing custom classes, those classes need to adopt secure coding and you can come see us in the labs for help with that. But we can help you more. At Core Data, we've spent time building in new debugging tools over the years that can help you understand what's going on under the stack. So, this is a picture of our preferred default scheme configuration. We have a couple of process arguments that we have that can help you get more debugging information about SQLite, but the big one you should always run with is com.apple.Core Data.ConcurrencyDebug. And this will catch any queue exceptions in your applications, so areas that you may be transferring objects between main and background queue contexts or areas that you may not be obeying a managed object's actual context. SQLite also has a number of interesting environment variables, so their threading and file assertions are a great way to ensure you have correctness in your application around their API as well as the file system. And auto tracing is a great way for you to see what's going on under the covers, an additional tour on debug logging. com.apple.Core Data.SQLDebug has four levels. The first level is the most interesting and the least of a performance hit. The fourth level is the most verbose, but does cause significant performance hit when you run with it. When you enable SQL debugging and our multi-threading assertions, you'll see a couple of logs in the console and these are the indication that the assertions are enabled and running correctly. With our SQL debugging on, you'll be able to see things like select statements for our fetch requests as well as how long they took. And, if you're set to level four, you'll even get explain, which will show you the query plan for a given select statement. And here we can see that our table view is selected via table scan and then using a temporary B-tree in memory for the order by, which is on the timestamp. This is a potential performance problem and as you're running your application you can use messages like this to see where you may be doing more work than you need to. So, how would we fix this? Well, turns out SQLite 3 can actually tell us. If we open a database and hand it the select query from our SQL logs, we can enable a mode called Expert, which will analyze the query and give us the ideal solution of optimizing it by creating a covering index. And we can do this in the model editor by adding a fetch index to our post entity. Here I've configured it to run on the timestamp and fetch them out in descending order because we're showing the most recent posts at the top of the table view. When we run the application again, we see the same select logs. Except that this time we see that the select query hits the covering index during the query. Explain shows us that the query will use the covering index for its order by. Core Data supports many types of indexing, including compound indexes using R-trees. And these are great for creating any kind of query or optimizing a query that uses a bounding box in its select statement. This is most commonly done with locations and we can set this up by adding another index to our post entity, which works in the latitude and longitude property that I added for the purposes of this slide. We change the query type in this box by selecting R-tree. And then we can set up our predicate on the fetch request to say get all of the posts that happen inside of continental China. This predicate is a little more advanced because it uses functions inside the actual select statement to hit the index that we created in the managed object model. When we run our application without this predicate and without this index, we see the same results that we saw before where we're hitting only the timestamp index. But when we run it with our new index and predicate, we see that SQLite is using the index to generate faster results for both of the between statements. Unfortunately, because our timestamp index doesn't have any bounding predicates on it, SQLite can't use it for the sort. So, the optimization that we've chosen here is to use a compound index to first filter out the result set to a smaller set of objects and then we'll do an in-memory B-tree sort for the order by. As you can see, this index increases the performance of our fetch by about 25%. In this case, my performance test was run over a size of about 100,000 rows and we saw around 130 milliseconds of improvement for just the fetch. Which brings me to my next topic of testing with Core Data. As you may know, we really like tests. Tests are awesome. And, at Core Data, we use them internally for both correctness as well as learning. They're a great way to learn about the functionality of Core Data and how our API behaves under a given set of conditions. They're also a great way to verify your assumptions about how Core Data works and how it's going to help your customers have a better experience with your application. As you saw in the previous example, we can verify that our R-tree index actually does give us a performance benefit even though it's using an in-memory B-tree sort. They also capture your product requirements, and this is really important to us at Core Data because it helps us communicate around your expectations. With tests, we can see what you're doing in code and how you expect those lines of code to behave for your customers. There are some important things that you can set up to make this easy for yourself, such as a base class that generates a persistent container. This base class on the screen happens to use a file URL of /dev/null for the persistent store and this is a great way of making tests that operate on a small set of managed objects run very, very quickly because they'll run entirely in memory. When you do this, SQLite materializes an in-memory store for you that can be very efficient, but because it's in memory, if you have a lot of data, this will cause a lot of memory growth in your test suite. You should have at least one test, though, that actually materializes your store file on disk. And this is because if you can't open your store for your test suite, it's highly likely that your customer can't either. If your persistent container is in the application delegate, you can have a test base class that grabs the container out and writes directly to that store. But I must caution you to take care when you do this, because that means that you're writing to the store file that's in use by the application, so if you run your test on a personal device, you'll see the effects of the unit test when you open your application the next time. What if I told you I could insert 100,000 records in just seven lines of code? I'm cheating a little bit. I was going to leave this as an exercise to the reader, but this type of scaffolding is a great way to help you build a test suite that evaluates your invariance around your data. By building these methods ahead of time, as your data changes or you become aware of new use cases for your application, you can iterate on these to build new edge cases, build new structures for your object graph, or evaluate the behavior of certain functionality under the covers, such as performance. This is the unit test scaffold that I used to build a performance test for the R-tree query. In just a handful of lines of code, we get high confidence on the performance of our fetch. And these types of tests are very informative when you're trying to evaluate tradeoffs between different features and functionality in Core Data. These three lines of code generate a new managed object context and container for us for our test to use. Now, this is important primarily because the setup and teardown logic in tests can sometimes affect their performance. So, you'll need to take care to analyze whether or not you're actually testing the teardown performance or the setup performance or the actual runtime performance of the queries you're evaluating. And after you've run these tests, you can file good bugs. We love bugs, especially from you guys because we're building a product to help you build your applications. But, bug reports without tests or without a sample application are very hard for us to communicate around because, as I mentioned earlier, they don't capture your product requirements and expectations in the same way that well-structured tests do. In fact, in an application attached to our radar that has a test suite or even just a bare sample application with some UI that explains your constraints and concerns to us, we can get back to you much more rapidly about what's going on and what you should do about it. They also help us verify the correctness of our fixes later. So, if you're going to file a bug, please take some time and write a test for us. That's all I have today. Come see us at lab tomorrow. We are here from 1:30 on in Technology Lab 7 and I highly recommend that you check out Testing Tips and Tricks tomorrow in Hall 2 at 3:20. Thanks.  Welcome, and hello. I'm Ben Chester, a software engineer on the Wallet and Apple Pay team. And, I'm here today to talk to you about how to create great customer experiences using Wallet and Apple Pay. And to do that, we're going to cover four topics, and see a fantastic demo from my colleague. Firstly, we're going to cover some of the key things that have happened in the last year. Secondly, how to create great experiences with Apple Pay, using some of the new enhancements we're announcing today. Thirdly, how to get the most from passes. There's some updates to designs, and some new changes you can adopt for richer content. Finally, we will talk about digital to physical commerce. And, to do that I'm going to show you an example that we use every day at Apple Park, the Caffe Mac's app, and some others. So, let me first start by telling you about Apple Pay Cash. Apple Pay Cash makes it easy to get paid, and pay friends and family using iMessage. Now, you can pay in-store, with apps, and on the web. And, it's available in the U.S. Most importantly for you, ensure you accept Discover debit in your apps, and with your payment processor, so you can take Apple Pay Cash. Another feature that we announced last year, that is even a greater option, is inline setup. Now, inline setup lets your users with no cards set up one, and then return to the purchase in one step. All you have to do is always show the Apple Pay button if the device supports it. If the user has no cards, you can use the Set Up with Apple Pay button, shown here, to make it even clearer to the user. Now, when the user taps that button, the setup flow is presented inside of your app, which reduces the risk of users leaving your experience. And, this is commonly known as abandonment. And, to support this, no additional work is required from you. Once the card is set up, it returns the user to the purchase immediately. We even automatically activate the card via SMS. Now, even with all of this, it's still faster than manual entry. And, you should adopt this in your apps, if you haven't already, for a great user experience. That's not the only feature we announced in the last year. To help with faster checkout, we also introduced error handling, and that's been great for developers. Error handling gives you control over nonfatal errors in the Apple Pay Sheet. And, this allows you to provide your own custom error messages to the user for billing and shipping issues. And, that's so they can more easily resolve those issues, and get back to purchasing. And, that's great for higher conversion. Now, some tips for error handling. One thing we want to make clear, is that it's still important to expect fuzzy data. The user has one set of data in Apple Pay, and that's shared across many apps and websites. And, we don't want the user-- don't want to force the user to conform to everybody's individual business logic, as that requires the user to reenter information, which just increases friction, and will reduce your overall conversion rate. Instead, you should do your best to accept a range of inputs. For example, Zip and Zip + 4. And, infer what details you can from other fields such as city and state from Zip. Now, that's not all the features that we announced. We also launched in some new countries. And, you may remember this slide from last year. Well, this year, we're introducing the following countries, and we're also coming to Norway and Poland soon. Now, that's a great range. But, one country I want to talk about in particular, is China. This year, we've taken Express Transit to China, launching with Beijing Transit, and Shanghai Transit. Now, this works fantastically. You can buy a card directly from Wallet, and get going straightaway. But, Transit is not limited to China alone. You can use Apple Pay to ride throughout the world, anywhere credit and debit cards are accepted, such as Moscow and London. Well, that's it from updates from the last year. Let's talk about creating great Apple Pay experiences. What is a great experience with Apple Pay? Well, it's one that is easier, by allowing the user to checkout as early as possible, with what we call upstreaming. It's one that is faster, by making Apple Pay the default payment option, with what we call defaulting. And, clearer by making the Apple Pay flow as simple as possible, with what we call streamlining. Now, I want to go through each of these, starting with streamlining. Here we see a fantastic example of streamlining. The Apple Pay button is directly placed on the product page. And, this makes it so easy, as Apple Pay is [inaudible] the user's flow as possible. Now, if your experience instead is focused on multi-item checkout, then placing the Apple Pay directly on a product page might not work so best for you. Instead, you should focus on Express Checkout. In this example, the customer has the option to checkout with Apple Pay right from the basket, and the user should be able to reach this checkout from anywhere in your flow. And, you can combine both of these experiences for the best results, if that does make sense for you. Whenever you use this Apple Pay button, it's critical to use the one provided in the SDK. Let me tell you why. Well, it's localized in all device languages. And, it's available in a number of styles and colors. It scales across all the supported devices, and any changes that we make, you're going to get for free in the future. It's also supported all the way back to iOS 8.3, and WebKit 10.1 on iOS, and macOS 10.12.1. However, we're heard that the button does not always fit all of your needs, and so today, we're announcing three new types in iOS 12. And, I want to show you them now. The first one is Book with Apple Pay. Now, you should use this when booking a hotel, or with a ride-sharing service, for example. Simply pass the new Book type to start using it. Secondly, Subscribe with Apple Pay. Use this for subscription-based purchases. Pass the subscribe type. And, thirdly, Check out with Apple Pay. This is best used if you have multiple references to checkout on the same page. Now, this really makes it clear to the user exactly which checkout performs Apple Pay. That's not all the changes to the button. You know that we love rounded corners and continuous curves, but we understand that not everybody shares exactly the same feelings, and so this year, we're allowing you to control the corner edits of the button. You can configure this however you see fit to match existing styles of your apps or websites. If you want square corners, simply set the button radius to 0. And, if you set no value, our existing corner radius will be used. Now, similarly, on older versions, the button will fallback to using the current rounded corner radius, ignoring any value that you've set. The new types and corner radius are available in iOS 12 today. And, they're coming to WebKit in a future release. Most importantly, fallback to using older types on unsupported versions. Consider that cascade. Using the new types will help your customers purchase more easily and clearer than ever before. Now, the placement and use of the Apple Pay button is one way to make checkout easier. But, that benefit to customer experience can be multiplied if you combine this with guest checkout. Now, you should think about guest checkout as your user's first experience. Now, blocking first-time purchase increases friction, which can lead to abandonment, as we described earlier. The user should want to create an account with you, not have it forced upon them. Apple Pay offers you a great way to do this. You can use data from the Apple Pay sheet post-purchase to make account creation easy. I want to show you an example of that. As soon as the purchase is complete, and the confirmation is displayed, account creation is now offered. User's email address from the Apple Pay transaction has been used, and all the user has to do is enter a password to confirm the creation of the account. You can make this even easier, using password AutoFill, and the enhancements that we've added in iOS 12. So, that's everything for upstreaming. Now, let's cover making things faster with defaulting. Most important thing about defaulting is that your users will look for and expect Apple Pay. So, you should make purchasing faster by setting Apple Pay as the default option. And, Apple Pay is a great default option, because it has the user's most up-to-date customer information. Even if the user has no card set up in Apple Pay, as we showed earlier, it still makes a great option. Let's see an example of defaulting in action. Here we see Apple Pay as the default option right at the top, and the Buy with Apple Pay button is clearly visible, and easily reachable by the user. If you apply these guidelines, you can make it faster than ever for your users to checkout with Apple Pay. So, that's easier and faster ways, but let's look at how to make things clearer with Streamlining. Most important piece of information about Streamlining for you, is to prefer customer information from Apple Pay. Users have this data already set up, and you can make their experience clearer by using it. When you request user's data, it's important to only request relevant fields. Any additional fields that you add will only make things more difficult. If data is needed that Apple Pay does not support, you should collect this before checkout begins. For example, the number of items of a particular product, or a voucher code. Now, this year, we've helped to streamline the Apple Pay experience even further, by making two changes. The first one I want to talk about is zero total support, and I want to show you why that's important. Here we see the Apple Pay sheet with a discount being applied, and the total is above zero. But, if that discount had brought that all the way down to zero, we've previously not been able to present this sheet. Well, now we do. You can see the discount has brought the total to zero. Apple Pay sheet can still be used to collect the necessary shipping details, and other information you need for the purchase. And, zero totals also work well with subscriptions. If the introductory month is free, you can make this clear to the user with a zero total. Now, remember, it's important to clearly state the price of the subscription, as well as the promotional discount they can receive. Zero total support is available in iOS 12 today. And, it's coming to WebKit in a future release. Make sure to fallback to existing behavior on older versions, where zero total is not supported. Now, the second way we have made the experience more streamlined in iOS 12, is with three new supported networks. And, this means you can filter the cards in the Apple Pay sheet more granularly. And, there are three new networks: Electron, Maestro, and VPay. It's important for the best possible experience to accept as many networks as you can. These are available in iOS 12, and coming to WebKit in the future. So, we've covered ways to make things easier, faster, and clearer with Apple Pay. I hope you will try these all out, and make your experiences better than ever, using our new enhancements. One final topic I want to cover is Apple Pay on the web, and the WC3 Payment Request API. Apple worked closely with a W3C working group over the past few years to help develop the Payment Request API. Now, that it's announced, you may be wondering if you should be using that, or our existing Apple Pay JavaScript API. Let's compare the benefits of using one over the other. The Payment Request API is a cross browser solution, however, it's important to know that Apple Pay is only supported on Safari. If you choose the Payment Request API, you do miss out on some of the following features. Error handling with custom error messaging, as I showed you earlier. But also, automatic selection of affiliated and cobranded cards. And, therefore, your ability to adjust prices of the user's selection of these cards. In markets and regions where it's important, phonetic name collection is also not supported. And, the Apple Pay JavaScript API has a wider range of support, going back to iOS 10, and macOS 10.12. Now, you should evaluate what makes sense for you. And, if you choose the Payment Request API, consider using the Apple Pay JavaScript API as a fallback for older clients. To make it even easier to get started with the Apple Pay JavaScript API, today we're launching a demo page. Let me show you that. The demo page walks you through all the steps required to get set up. And, it all starts with the Apple Pay button. You can configure sizes, and styles, and locale, and see how they change in real time. The code required to drill the button is dynamically updated below. And the page even allows you to test out the Apple Pay purchase flow. Simply configure the JSON in the request inside the page, and the Apple Pay button there will be used to present the payment sheet. The results of the authorization and success or failure are displayed to you directly. Now, it's important to note, your cards will not be charged when you do this. It's a great resource to get started, and I hope you try it out. The demo page is available here online today. And, that's everything we have for covering Apple Pay today. So, now I want to move on to talking about getting the most from passes. Passes are where it all began for us, and they hold a very special place in our hearts. And, we have some fantastic improvements I'm going to tell you about. But first, I just wanted to recap what makes passes so special. They are so easy to use contactlessly or with a bar code. And, they're synced across all your devices using iCloud. And, we're able to intelligently show them on the lock screen and in Search for quick access. And, they continue that seamless Apple Pay experience into the physical world, which works great for tickets, movie passes. Those are great reasons why you should use passes. But, how do you get the most from passes for your users? Well, to do that we're going to cover four topics. And, we're going to start by talking about adding passes to Wallet. And, we have some best practices that you can follow. Secondly, making passes look great. There's some new design options, and some tips for making passes work across all your devices. Thirdly, rich pass content. New ways to make your passes more deeply integrate with the system. And, finally, contactless passes, and how you can get started. So, let's begin with adding passes to Wallet. Many of you have seen or experienced the full model UI that was presented to add passes. But, there's a better way. For almost all situations, we recommend using our automatic pass adding API. To use this, simply call the addPasses function in PKPassLibrary, with the array of passes you intend to add. Now, this presents a simple alert to the user, requesting them to add or to review those passes. And, that's much less friction compared to the full-screen model UI. And, the user still has the option to review the passes in full. And, you should handle that in the completion, and present that viewController if requested. In the demo you're going to see later, you'll see how this improves the experience. So, let me tell you about some other best practices to follow for adding passes. Now, we suggest adding passes that were created outside of your experience, when the user enters the app for the first time, if they're coming from an unsupported platform. They've not previously had the opportunity to add those passes, and that's a great time to offer it to them. And, you should add related passes to Wallet in a group, to reduce the friction. And, you can make it easy for people to quickly add passes that they do not have from your order history views, or previous transactions, for example. And, from those same views, if your user has a pass in Wallet already, you should link them directly to it. So that no matter where the user starts their journey, in Wallet, or your experience, they can get to the pass. If you follow all these tips, and use automatic pass adding, your users will love passes. But, how do you make a pass that looks great? Most important information for you is that the pass fields that you display to the user should have the relevant and critical information on them. And, there're things that the user definitely needs. And, you can make them stand out by using vibrant colors. And, you should design a pass that looks great across all your devices. Now, to do that, you want to avoid reproducing existing passes, physical ones. We have some great templates, and layout options to make these fields stand out and make sure that your entire audience can work with these passes as they're accessible. And, secondly don't encode user information inside the strip image that we can see here. You should reserve that for the fields. That's particularly important on Apple Watch. Because there's no space to display that. And, the fields are front and center. Similarly, the thumbnail image is also not displayed, and the user cannot access the pass details. So, but we know sometimes there's information that's difficult to put all on the front of pass. And so, today we want to make things a little bit easier for you by adding additional row support. Let me show you that in action first. Here we see a new third row enabled, showing the section and the start number. Now, that's a great way to layer additional content, when your existing information is already a little bit long. And, this is really easy to add to your passes. Let me show you how to do it with a pass JSON. Here we see the new row key be using the auxiliaryFields, by setting a 0 or a 1. You can indicate that the field should be split over the two locations on the front of pass. "Row" can only be used in the auxiliary fields, and only for event ticket-type passes. We support values of 0 and 1, and that's going to be coming to you in a future seed. On older versions, "row" will be ignored. And, in that case, auxiliary fields will be displayed on one row, up to a limit of 4 fields per row. So, make sure you consider this when designing your passes. And, test across supported devices and versions. So now, we've got passes in Wallet, looking great. How do we make that content even richer? Let's get the most from a pass. You can include information that the system can leverage to provide the user enhanced experiences. And, one of those you may already know about, is relevancy. You can add relevancy by setting the locations, relevantText, and relevantDate pass JSON fields. And, this allows the system to present the pass to the user just at the right moment. We handle multiple relevant passes. And, if you're issuing passes for a single journey, multiple ones, then you should make sure to add relevancy information for each pass. And, allow the system to correctly present them one at a time to the user. It's not a problem at all. Now, it feels like magic when their passes are so easy to access. And, to date we're looking at making passes even richer, with something we call semantic tags. The fields that we showed earlier in the pass JSON are a great mechanism for display. And, as the developer, you have full control over the field names, and the format of that content. However, that means it's difficult for iOS to reason about every field. As everybody uses unique key and label names. Semantic tags changes that by providing a structured way for you to augment the display data with a fixed format data. Let me show you how easy it is to add to an existing pass for an upcoming film I'm going to see. Here we see a sample of primary, secondary, and auxiliary fields from the pass JSON. This film's called "Revenge of the Passes." It's showing at Apple Park, and it's running for two hours and 45 minutes in theater F5. Now, that's all the information I'm going to need when I get there to see the film. And, it's going to be readily available to me. So, let's take that event field at the top, and add some semantic tags to it. To do that, we add a semantics dictionary to the field. And, inside that dictionary, you can have relevant key names from my documentation to provide machine-readable content. For example, with the movie, the name used for display may differ from the full official name used internationally. And the same might apply to your venue name. These should be the official names that are different from display. You're also able to add additional related data that is not important for display, but associated with that field. For example, the phone number. Now, sometimes, you have semantic data that is not associated with fields. And, to capture that, we can use a top-level semantics dictionary. Here, I've added a semantic tag indicating that while this pass is relevant, silence should be requested. I've also indicated the duration of the event in seconds, and specified the event type. You may have seen this in the Keynote, Siri will then be able to offer the user the ability to quickly enable do not disturb at the right time. It just works. And, it's great for things like movie tickets, or event passes. Now, in summary, semantic tags are a great way to add machine-readable content to your passes. And, we have over 70 supported transit and event tags. All these keys will be available in our online documentation soon. And, one important thing to note, relevancy information works in combination with semantic tags. And so, you should add both to your passes for the best experience. Now, bringing all this together, I want to talk to you about contactless passes. Contactless passes are a great way for users to interact with Wallet. And, it's a much improved experience over scanning a barcode. I want to show you one of our favorite uses of contactless passes. Internationally renowned, Wembley Stadium in London had famous football players record an advert just to show how easy it is to use. It really couldn't be any easier. And, that contactless feels like magic when you combine it with relevancy for lock screen access, swiping down for notification. Contactless passes do require an NFC certificate to get started. Now, you should contact us for access if you have a great use case. One other important thing to remember is that contactless readers you select must support Apple's value-added services protocol. However, that's easier than ever. You can find readers from any of these companies that support this protocol. Now, one final use of contactless passes you may have seen in the Keynote, is student ID cards. Students will be able to use these to access and pay anywhere physical student ID cards are used today, right from Wallet. We worked closely with two companies to bring student ID cards to educational institutions across the U.S. Now for a demo of some of the features that we've covered here today. I would like to welcome to the stage, my colleague, Katie. Thanks, Ben. Hi, my name is Katie, and I'm an engineer on the Wallet and Apple Pay team. Today, I'm going to be showing you a Swift application, where a user can purchase an event ticket for a party that I'm throwing this weekend for my team. My colleagues will be able to use the app in order to purchase a ticket, and make me a little money on the side. This app will demonstrate two things. First, how to use the best practices and new API's that Ben had mentioned earlier in order to create the most seamless Wallet and Apple Pay experience. And, second, how to structure your pass's JSON in order to enhance your customer's experience. So, let's go ahead and take a look at this app. So, here we're presented with a summary about the event, a date, a time, a location, and a cost. Here's an example of upstreaming, where we place the Buy with Apple Pay button directly on the product page. So, let's go ahead and purchase this ticket. You can see, we're presented with the Apple Pay sheet, and this is a good example of streamlining, asking the user for the least amount of information possible. In this case, only their email address. So, let me go ahead and Face ID in order to complete this purchase. Here, we're shown more information about what we just bought, about the event. And, here at the bottom we have a QR code, and all the way below that, obstructed from view, is an Add to Apple Wallet button. You might have seen many apps in the App Store today do something like this, embedding a QR code in their app. If you have a QR code on the front of your pass, there's no need to embed it within your app. So, let's go ahead and add this pass to Wallet. You can see, we're presented with the pass for the user to review. It has information about the event, and a QR code. So, let's add it. Now, let's go over to the JSON, and-- for this pass, and see what changes we can make in order to enhance the customer's experience. There are two major changes I want to make here. First, when people come to my event, I want to check them in using a contactless reader. And, second, when the user gets close to my event location, and in this case, Discovery Meadow, I want the pass to become relevant on the lock screen. So, let's first take a look at this barcode dictionary here. Since I'm going to be checking people in using a contactless reader, there's no need to have a QR code on the front anymore. So, let's remove this dictionary. And, instead, let's replace it with an NFC dictionary. Here, this dictionary contains information that we've passed from the device to the reader in order to identify them when they check in. Next, let's add location data to this pass, so it can become relevant on the lock screen. Here, we set a latitude, a longitude, in this case it's Discovery Meadow, and relevant text that will be displayed in the notification on the lock screen. Now, let's take a look at the Swift application, in order to see what changes we can make there as well. Let's first take a look at this presentAddPass method. This was called when we tapped the Add to Wallet button. Here, we got the pass from a function we called, called eventPass. Let's go ahead and replace this with our newly modified pass. We then instantiated a PKAddPassesViewController with that pass, and presented it. But, as Ben had mentioned earlier, there's a better API we can use in order to automatically add passes to Wallet. So, let's go ahead and remove these two lines of code, and replace it with that API. That API's on the PKPass library, so let's create an instance of that. And, let's call the API. The function's called addPasses. Takes an array of passes, has a completion handler with a status. And, there are three possible statuses that can be returned. DidAddPasses, which means the user wants to add the pass to Wallet. DidCancelAddPasses, which means the user does not want to add the pass to Wallet. And, shouldReviewPasses. In this case, the user wishes to be presented with a preview of the pass before deciding to add it to Wallet or not. And, if this is the case, you should do what we did previously and instantiate a PKAddPassesViewController and present it. Now, let's go up to the viewDidLoad method. As we saw, there was a QR code here. Again, if your pass has a QR code on the front, there's no need to have it in your app. And, since our pass is now NFC-capable, we don't need this view anymore. So, let's go ahead and remove these lines of code. Here is the PKAddPassButton, or our Add to Wallet button. In order to-- another way to enhance the user's experience is when the summary view is presented right after they've purchased the ticket, we immediately ask them if they want to add the pass to Wallet, rather than having a button onscreen. Of course, you can have this Add to Apple Wallet button somewhere else in your app, if the user decides to add the pass later. So, let's go ahead and remove this button. And, instead, we're going to replace it with a View in Wallet button. That way, the user can tap this button, and they'll be brought directly to a Wallet, and be shown the pass that they just added. And, finally, when the view is presented, let's immediately ask them if they want to add the pass. Now, that we've made these changes, let's deploy to the device, and see what it looks like. Again, we're presented with the same view, a summary about the event. I'll go ahead and purchase this pass again. And now, you can see we're immediately asked if we want to add the pass to Wallet. And, you can see in the background code, the QR code is no longer there. And, the Add to Apple Wallet button is replaced with the View in Wallet button. So, I want to add this pass to Wallet. And, let's go ahead and view it. You see, we're immediately taken to the pass, and you can see the QR code is no longer on the front. And, instead we have a Hold Near Reader displayed at the bottom, meaning that our pass is now contactless, and I could scan it a reader when I arrive at the event. Now, let's lock the device, and since we're close to Discovery Meadow here in San Jose, let's see if the pass is relevant on the lock screen. Here's the notification for the pass with the text that we set in the JSON. Let's tap our notification. The pass is immediately displayed, and I happen to have a contactless reader here on stage, so let's simulate me arriving at the event. And, it's as simple as that. We hope that you can adopt these simple changes in order to enhance your customer's experience when they interact with Wallet and Apple Pay in your app. And now, I'll hand it back over to Ben. Thank you Katie, for that fantastic demo. What an improvement those changes made to the experience. So, the final topic we want to cover today is digital to physical commerce. Now, using Apple Pay is a great way to buy, but that's not where the user's journey ends. It's important to carry through that seamless experience into the physical world. Now, to do that, I want to show you two examples. Starting with the Caffe Macs app, that's used by thousands of employees every day at Apple Park. Here, I launched the app, and I'm immediately presented with the best offers on offer today. It's a great way to engage me. It doesn't require me to browse through all the items in the app. You might be wondering why you don't see the Apple Pay button immediately. And, that's because I'm not inside Caffe Macs. As soon as I get close, using GPS and Bluetooth beacons, the app detects that change, and the button appears. This allows me to buy right from the starting page. It's a great example of upstreaming. Sometimes I order more than one thing, and Caffe Macs has me covered. They support both the multi-item express checkout, as well as a single-item purchase flow right from the product page. I can also check out from anywhere in the app, using the Pay tab below. Now, this is a great pattern to follow to make checkout easy. When I'm ready to pay, I Face ID, and I'm done. Here, you can see that following our guidelines, and requesting as little information as possible. I'm already logged in across multiple Apple apps, so no additional fields are required. However, if I wasn't, this would be a really great place for you to request the minimal information to make collection of the items easier. Finally, a confirmation to let me know that the order's been done, and I should wait for a notification. As I walk over to the pizza station, the notification arrives to let me know that it's ready. And, when I get there, my lunch is waiting for me. A ticket with my name on it, and a member of the Caffe Macs staff confirms me by name, and I'm away to devour my pizza. Now, that's a great experience from start to finish. And, another experience that we love is a food ordering app from China, [Chinese spoken]. Now, they start with using the iPhone's camera to scan a QR code when entering the restaurant. And, that takes them straight to the menu in the [Chinese spoken] app. They use images in their menu to make it easy to know what you want, and how to checkout fast. In the checkout flow, the app offers the user a virtual payment credential, issued by [Chinese spoken], which uses our in app provisioning flow to put a card directly into Wallet. Now, this works great even if the user has no cards at all, in Wallet or otherwise. They can set one up, and order right away. And, the card that's provisioned can be used in all e-commerce, and in-store merchants anywhere in China that takes Apple Pay. If you use a provisions [inaudible] card into Wallet, Apple Pay's the exclusive option to checkout. Fantastic example of defaulting. As soon as I Face ID, the food is ordered. That's a fantastic experience bringing together all of these technologies: GPS, beacons, the camera, in-app provisioning, and notifications. If you combine that with a great physical customer experience-- you know, if you leverage everything we've shown you here today, you too can build digital to physical commerce experiences. So, I have one piece of final housekeeping. If you are a payment processor, or a merchant, most of you don't need to worry, but you should just check your server-side certificate expiry dates in the developer portal. They are easy to renew, and you should renew before expiry to avoid disruption taking Apple Pay. That's everything we have to talk about today. If you want to learn more, our online documentation can be found here. And, if you have any questions about Wallet, Apple Pay, or Core NFC, you can join us immediately after this session in Technology Lab 1 at 10 A.M. Thank you all for coming today, and I hope you've had a great WWDC. Bye.  Hello and welcome. I'm Lindsay Verity from Advertising Platforms and I'll be joined by my colleague, Carol Hsu, and we're here today to tell you what's new in Search Ads. Search Ads was first introduced to the App Store in the U.S. in October 2016 as an efficient and easy way for you to promote your apps directly in App Store search results so customers could discover your apps while protecting their privacy. Search Ads Advanced has since expanded to the App Stores in the U.K., Australia, New Zealand, Canada, Switzerland, and Mexico. In December 2017, we introduced Search Ads Basic to the App Store in the U.S., a minimal effort cost-per-install offering that's fast become a favorite of developers worldwide to promote their apps on the U.S. App Store. And the response to Search Ads from developers of all types has been incredible. Postsnap, a U.K.-based photocard app has used Search Ads to expand their offering to new markets and they told us Search Ads has given us a huge increase in high-value users in multiple regions. Now we're using Search Ads to help us launch other apps too. And JoyTunes, creators of instrument learning apps, are seeing incredible results. They say the quality of users is so good that the numbers upgrading from free to subscription is double that of any other pay channel. And, for Playtika's World Series of Poker, it's the quality and engagement of users that matters and they said Search Ads is an extremely powerful product that has brought high-quality users and engagement rates 10% higher than the average. It's been really great and wonderful hearing these stories and so many more like them. And so, today, I'm excited to announce that this summer, Search Ads Advanced is coming to six more app stores. Thank you. Japan, South Korea, Germany, France, Italy, and Spain, giving you even more opportunities to reach high-intent, high-quality users in more of the markets you've told us are most important to you. And, what's more, we'll be introducing Search Ads Basic to all 13 App Stores where Search Ads will be available. And we've worked hard to make sure that Search Ads Basic remains a simple, easy, effortless way for you to promote your apps across multiple app stores. Let me show you how easy it's going to be. Imagine I'm a developer and I want to start to promote my new app, NoMoss. I simply select the app from the drop-down list and note that I know see an ad example on the right and I'm also presented with a list of the app stores where Search Ads is available and also where my NoMoss app is available too. I want to permit my app in as many app stores as I can and so I'm going to leave all of those selected. Next, I enter my monthly budget, which is a single budget across all of the app stores where my app promotion will run. My monthly budget for NoMoss is $5000. And, finally, I input what is the maximum amount I'm willing to pay for an install. Now, note Search Ads is recommending a maximum cost-per-install of $1.50, which is based on what it knows about my app and also what similar apps are willing to pay for an install, so I'm going to take that recommendation. I then save and I'm taken to my dashboard where I'll check back later to see the results. I can now see confirmation that my new app promotion has been saved and it should start running soon. From my dashboard, I can also add another app that I want to promote and I can also make changes to any of my existing app promotions. So, I am currently running promotions in the U.S. App Store for my LightRight app and I'd like to expand that to more app stores. To do that, I simply click on the edit button at the top of the apps card and my LightRight app is only available in 10 of the app stores where Search Ads is offered, so I'm only presented with those 10 options. Now, I can pick markets individually, Canada and France, for example, or I can select all. I save. And I can now see that my app is being promoted in 10 of 10 available app stores. Now, because I'm promoting my app across more app stores, I also want to increase my monthly budget to enable me to reach more users and achieve more installs, so I'm going to change my monthly budget from $1000 to $3500. My change is saved and I'm done. It's really that simple and we can't wait for more of you to try it. And that was Search Ads Basic. So, let's talk about Advanced and, more specifically, new creative options. Developers have told us they love the ease of Search Ads Creative and how we automatically create the ads using the metadata and imagery you already provide for your App Store product page. And App Store customers respond really well to those ads because of their noninvasive appearance and because they're an accurate representation of what they can expect from the app's experience. Since February, you've had the option to add up to 10 app screenshots and 3 videos on your App Store product page. And just last week, we were excited to introduce our newest Search Ads Advanced feature, Creative Sets, giving you the ability to leverage these additional App Store assets to create more ad variations to help you better align your ad group keyword themes with specific audiences. Now, let me turn it over to Carol, who's going to tell you more about how Creative Sets work and give you a demo. Thanks, Lindsay. As Lindsay just showed, you can use Creative Sets to display different app previews in screenshots that align to keyword themes or specific audiences. Since Creative Sets uses App Store approved assets, this ensures the best user experience. I'm here to show you how it's done. Before we begin, imagine I'm a developer and these are all of the assets that I've uploaded to App Store Connect for my app, TripTrek. TripTrek is an app that lets users document every step of their travels and treks. The first three assets will be used in my default ad. The default ad matches what shows in organic search results; however, there are 2 specific keyword themes that I want to focus on with my Search Ads campaigns, fitness and mapping. I can use Creative Sets to choose the assets that best align to those themes. Let's go through the steps of how I do that by adding Creative Sets to my Search Ads campaigns and ad groups now. Here I am, already logged into my Search Ads account. And I'm on the campaigns view. Let's go ahead and create a new campaign together. I'll start by clicking Create Campaign, choosing my app, TripTrek, the storefront that I want to promote it in, and I'll give it a campaign name. Let's call this one Active Travel. I'll assign a budget and for now I'll skip over the daily cap and the campaign negative keywords. We'll go ahead and create the first ad group in this campaign. I'm going to call it fitness to match the keywords that I plan to bid on. I want this to run on both iPad and iPhone, so I'll leave that setting as is. And since, of course, I want this to run all day every day, I'll leave the ad scheduling alone for now. And I'll enter a default max CPT bid. I'm going to leave Search Match on so that Search Ads automatically matches my ads to relevant searches. And I'm going to choose from some of the recommended keywords some of the keywords that match my fitness theme. Maybe we'll go with hiking tracker, hiking trails, activity tracker, and step counter. And then I'll add a few more of my own. How about fitness tracker and fitness challenge, since that's one of the features in my app. Since I want my ads to show to all users, I'll leave the audience settings alone for now too. Here I can see a preview of the default text ad and the default image ad, which shows alongside the ads from my Creative Sets and matches the organic search result. But, since I want to use assets that are more closely aligned to my fitness theme, I'm going to create a Creative Set for that, so let's go ahead and add one now by clicking Add Creative Set. I'll name this Creative Set Social Hiking and choose the localization language. The set of languages depends on the storefronts that I'm promoting in and on the localized assets that I submitted to App Store Connect. I'll leave this as English for now. Next, I need to choose the right combination of assets to create an effective creative set. Here I can see all of the screenshots and app previews from the available set of assets on my app product page organized by display size and in the order that I submitted to App Store Connect. I'll only see the display sizes that have enough localized assets to satisfy a Creative Set. In App Store Connect, I had elected to use the 5.5-inch display, which is why I don't see any other iPhone display size assets here. If I'm having trouble remembering which devices are which display size, I can go to the upper right-hand corner and click on View Device Display Sizes to see that information. I'll go ahead and close this now. If I hover over an asset, an icon appears in the lower left-hand corner and I can click on that to get a closer look. I can also easily click through to see the other assets as well. Let me close this now and choose the assets for my Creative Set. For the iPhone 5.5-inch display, I have all portrait assets, so I need to select at least 3 in order to complete my Creative Set. I can see which of my assets are already being used in the default ads by this label here that says default on the first few assets. I'm going to choose assets that are different from the default ones and that are aligned with my fitness theme. Here we go. These look good. And then for the iPad 12.9-inch display size, I have all landscape assets, so I need to select at least one of these in order for my ads to show on iPad. I'll go ahead and pick this one for now. Once I've chosen assets for each display size, I click save and see that my creative set has been added to my ad group and now I'm ready to start my campaign. The Active Travel campaign now appears on my campaigns view and that's how you add a Creative Set to a new campaign and ad group. Next, let me show you how to add a Creative Set to an existing campaign and ad group. Let's go into my Navigation campaign. And from there, let's go into the City Maps ad group and I'll click on this new Creative Sets tab. This is a brand-new view in Search Ads where I'll be able to see not only how my Creative Sets are performing, but also how the default text ad and the default image ad are doing. I currently only have the default ads running and I don't have any Creative Sets yet for this ad group. So, I'll add one now by clicking Add Creative Set. Let's go ahead and name this Creative Set Mapping Features to align to my mapping theme. And I'll leave the localization language as English again. And for the iPhone 5.5-inch display size, I have all portrait assets, so again I need to select at least 3 to populate my ad. In this case, I'm going to choose more than 3 assets, including some that are already used in the default ads, and let Search Ads automatically create the various combinations of ads from all of these assets. Search Ads uses machine learning to continuously optimize to show the best performing ads. Since I'm only interested in testing ad variations for iPhone at this time, I'm going to elect to use default assets for iPad, which means only the default ads will show on iPad. I click save and now I've added my first Creative Set to this ad group and I can create up to 10 Creative Sets per ad group. I can return to this view at any time to check how this Creative Set is performing versus the default ads. So, that's how I added Creative Sets for my two keyword themes, fitness and mapping. I hope you'll give this a try yourself. Now, let me hand it back to Lindsay. Thank you, Carol. Thank you. So, to summarize, Creative Sets use App Store approved assets, so it continues ensuring a really great customer experience. And now with more ad variations, you can more closely align your ad images to specific keyword themes for audiences and our machine learning ensures you'll always achieve the best results. It's been great sharing these updates with you today. Just to recap, Search Ads Advanced is coming to 6 more markets this summer and Basic will be coming to all 13 App Stores where Search Ads is available. Creative Sets are available now, so if you're a Search Ads Advanced user, start experimenting with different ad variations and see how they can further improve the performance of your existing campaigns. And we really love to hear from you on your experiences with this new feature. As always, you can learn more at our website, searchads.apple.com. Thank you so much for listening. It's been really great sharing this with you today.  Good morning. Who here has an app on the store that uses Apple Pencil? All right, a few of you. And who has an app on the store with Apple Pencil features that is not a drawing or a painting app? Oh, one, yes. Well, today, not only are we going to learn a few things about how to make your app great with Pencil, we're also going to see that Pencil can be for so much more than just drawing. Pencil can do a lot of things and it can be a bit daunting to add Pencil interactions to your app, especially if you've never worked with it before. Today, we're going to look at seven properties of Pencil that, when implemented, will add entire new dimensions to your app. And to do that, we're going to add Pencil interactions to an existing app. So, here's my app. It's a fairly straightforward sudoku game. The puzzle screen shows the current puzzle we're solving. Tapping a cell brings up the keyboard where the player can enter a number, and there are a number of buttons for showing or hiding the timer, depending on how competitive you are, for getting a hint, and for changing various settings about the current game. So, even though this is not a drawing app, doesn't it seem perfect for Pencil? Let's make it happen. And the first thing we're going to look at is that Pencil makes marks. I don't know about you, but if I pick up a pen and I try to write with it and it doesn't write, I toss it out. We worked really hard on Apple Pencil and we don't want people tossing out their Apple Pencil. So, don't require tapping a button or entering a special mode to draw with Pencil. Pencil should make a mark the moment it touches the screen. When I put Pencil down, I can go ahead and write my guess immediately. Pencil is familiar; it should behave consistently with what people using our app have come to expect. This means that tapping controls should behave the same whether you touch them with Pencil or your finger. Here, I can interact with the various setting sliders using Pencil just like I would with my finger. One thing we definitely never want to do is have controls that will have different consequences, or worse, do nothing when tapped with Pencil rather than a finger. This would be very unpredictable and confusing. Pencil has a number of sensors. It can sense tilt, force, and orientation, and these sensors can add layers of depth and expressivity to our interactions, but there are a few missteps to avoid. A good way to use the sensors is to make marks more expressive. For example, the continuous variation of pressure and tilt can modulate the thickness of a stroke. Or the orientation to affect the width of a marker. We don't want to use sensors to affect things not directly related to Pencil input. For example, here, depending on how hard we press, we get a new puzzle of varying difficulty. That's just a confusing behavior. Sensors should affect the kind of marks that Pencil makes, not things disconnected from it. Another way to handle sensor data is discretely. For instance, to modify the input based on the pressure exerted. So, an idea for a game here is to let people pencil in a tentative guess, depending on how hard they write. Writing lightly lets you put in a guess in lighter ink, and writing with more pressure lets you commit that answer in black ink. We can think about a sensor reading such as pressor as a continuum and treat it as such to modulate a quality of a mark. We can also split that continuum binarily, as we just did for this pencil-in feature. In this case, we'll want to make sure to give clear feedback as to which side of the split we're on. We definitely don't want to break that continuum into more than two discrete parts though, as it would become very hard to differentiate. People can tell the difference between writing soft or writing hard, but they can't tell the difference between 5 or 8 or 15 different levels of pressure. Handwriting is tremendously expressive, and as a result, so is Pencil input. As we get our game for people to try, we notice that people will often write slightly outside of the boundaries of a cell and this is a totally normal side effect of handwriting being so personal and freeform, but it means we need to be flexible with how we handle input. What we shouldn't do is impose unnecessary constraints on input, like rejecting it if it doesn't fit perfectly within the cell. So, what we're going to do here is assign the answer to the cell it covers the most and we also scale and move it to make it fit neatly into the cell, which is a nice little touch. Now, an interesting question comes up. What should we do if the player starts drawing or writing outside of the main sudoku grid? One thing we can try doing is just nothing, but it doesn't feel very good. It goes back to our earlier point about making a mark. It breaks the impression of fluidity and it might even lead some people to think that their Pencil is impaired, or the battery is discharged. Something we can try is to show transient marks, marks that disappear shortly after they've been made, to communicate that everything is working fine, just the margins don't do anything special. Another option we can try is to just let people draw and annotate in the margins just for fun. And this could be particularly great if later we add multiplayer to our game. Now, there is no right or wrong answer here. It is up to you and what you want your app to be; but doing something is always better than doing nothing, even if that something is just to communicate that Pencil is working. If you're not drawing with Pencil, you're writing. And because you're writing on an iPad and not a piece of paper, we have to ask ourselves questions about whether we want to process the input further. For the case of our game, should we convert handwriting to digital or should we keep it analog as it is? One reason to convert it to digital is to be consistent with keyboard input. And one reason to keep it analog is that it's fun and personal. Again, here there are no right or wrong answers. It will depend on your specific circumstances. You do want to keep it consistent so that people using your app know what to expect. And if we do choose to keep the input analog, it's always a good idea to digitize it under the hood to enable things like searching for text or, in the case of our game, so that we can validate the final answer to the puzzle the player came up with. So far, we've been talking a lot about Pencil and it can be easy to forget that there is a human holding it. Different people will hold and use Pencil differently. For example, if I watch a left-handed person use our game, we'll notice that their hand and arm gets in the way of the controls on the left side of the screen. The positioning of these controls wasn't much of an issue when our app was used primarily with touch, but it becomes an issue with Pencil. So, what we can do is move the control to the top or bottom of the screen or just make them repositionable. And, finally, Pencil is really great when it provides shortcuts. That is, letting people do things faster by drawing and writing rather than tapping on the screen. So, how do we erase a cell in our game? It's a two-step process. The first step is we tap the cell we want to erase, and the second step is we press the backspace key. So, can we turn this two-step interaction into a one-step interaction with Pencil? Something that would feel pretty natural is letting people cross out their answer, so let's give that a try. Seems to work pretty well. But, as I give my game for people to try, I notice something interesting when they try to erase a cell. Some people will strike it through, others will scribble it over, and yet others might draw something like an X mark. Handwriting is really expressive; different people will cross things out differently. So, in this case, it's probably better not to ask people to perform a specific mark they would have to remember. In addition to being hard to remember, specific marks might have different meanings in different cultures. For example, in the U.S., it's pretty common to use a checkmark to mean correct. In other countries, that might not mean anything or even mean incorrect. So, what we do is we just let people erase a cell by scribbling over it. Whether they slash it out, draw an X, scribble over it, draw a straight line, it doesn't matter. All of it works and erases the cell. Let's look at another two-step interaction, getting a hint. First, we'd have to tap the hint button and then we tap the cell we want a hint for. And this is another interaction that we can bring down to one step using Pencil. Perhaps we can let the player draw a question mark on the cell they want an answer to. And this feels pretty good. And in this case, because the question mark is mirrored in the icon of our hint button, I feel pretty good about using that mark as a shortcut for hint. Now, these seven properties are by no means an exhaustive list of all the ways Pencil can make your app great, but I hope they've shown you a side of Pencil that perhaps you did not expect and I hope they help you start thinking about what adding Pencil interactions would mean for your app. Thank you.  Hello, and good morning everybody. I'm glad you could all make it out this morning. My name is David Owens. And I'm an engineer on the Xcode team. And today, with my colleague Jordan Rose, who's an engineer on the Swift team, we're going to be talking to you about building faster in Xcode. Now, depending on your projects, their configurations, and their complexities, there's going to be a different set of opportunities that you're going to be able to take to improve, or in some cases, significantly improve the way in which your builds perform. So today, when we talk about building faster in Xcode, we're going to be looking at it in two different perspectives. The first is going to be around increasing your overall build efficiency. And the second, it'll be about reducing the amount of work that you do on your builds, and especially your incremental builds. Now, I'm going to be walking you through some of the project-level items, including how to parallelize your build process. How to declare and configure your run script phases. And I'll be walking you through some new functionality in Xcode 10 around measuring your build times. Now, Jordan is going to be walking us through some of the source-level improvements that we can make to our projects, including understanding our dependence using Swift. Dealing with complex expressions in Swift. And how to limit your Objective-C to Swift interfaces. So let's talk about parallelizing your build. Now, Xcode configures your projects through the use of targets. And targets specify the output or the product that you would like to build. Some examples are iOS app, framework, and unit tests. Now, there's another piece of information, that's the dependency between these targets. And Xcode provides us two ways to define our dependencies. There's an explicit means, which we do through the target dependencies phase. And there's an implicit means, which is primarily done through the linked binary with libraries phase. And we'll be taking a look at those more in depth in just a few moments. Now, throughout this section I want to use a sample project to ground our discussion. And so we're going to take a look at a dependency graph for that project. Now, a dependency graph is simply a listing of all of the targets. And this case we're going to have five targets that we're going to be building. And it has that dependency information between those targets. And based on these two pieces of information Xcode can derive our build order. Now, let's take a look at what this looks like on a timeline graph. So as we can see, each of these targets are building in in order, sequentially. And they each have to wait until the previous target is done building. Now, there's nothing wrong with this build timeline, per se. But it does represent a waste of potential hardware utilization, especially if you have a multi-core or a mini-core machine like an iMac Pro. And what that means to you is a waste of time as a developer. So instead, we want to move to something that looks more like this. Now, there's a couple things -- or a few things I want to note about this. First, the amount of work that we actually did in building our project, it did not change. However, the time to build did decrease. And in this case, it actually decreased by a fairly significant margin. And we were able to decrease our build time by making better utilization of the hardware that we have available to us. So, if parallelization -- or parallelization is such a good thing, why don't we just create a build graph that looks like this? We just build everything at once up front in our build timeline. Well, in the best case, you're going to deterministic build errors. And this because that dependency information is actually a vital part of your project configuration. And when it's set up like this, you're trying to build, for example, your game target before you've built your dependencies. So, this is not a good state to be in. So, how do we get there? How do we get from the long, serialized build timeline to the better parallelized build time? Well first, we need to make sure that Xcode is actually set up and configured to allow our targets to be built in parallel. And we do that through the Scheme Editor. You can get to the Scheme Editor by opening the Scheme Chooser and selecting Edit Scheme. And specifically, you'll need to look at the Build Action. And in there, the Build Options. Now, there are two listed here. The first is Parallelize Build. And the second is Find Implicit Dependencies. You'll want to check Parallelize Build. This will allow Xcode to use the dependency information across your targets so that it can attempt to build your targets in parallel. So let's look at how your dependencies are actually configured within Xcode. This is done through the Build Phase Editor. And you can get to the Build Phase Editor by going to your Project Navigator and selecting your project. In this case we're looking at the Game Target -- or the Game Project. Next, you want to click on Build Phases. So, let's take a look at the Game Target. This is the Build Phase Editor for our Game Target. And we're going to look at how its dependencies are configured. And I want to call your attention first to the Link Binary with Libraries phase. Now, this is the phase of your build process where you define all of the items that you would like to link with your target. In this case I have two items. I have Physics and Utilities. Now, these are targets within our project and our workspace. So Xcode can create an implicit dependency on those targets. If you're using other linking features such as Autolink or the other LD Build Flags build setting, those are not going to be made available to you here implicitly. So you either have to make an explicit dependency in this build phase, or in the Target Dependencies build phase. So you can see here that we have another item here called Shaders. And Shaders is something that is not used at link time, but instead it's used by another build phase within our current target. So it's important that we let Xcode know that this is a dependency and that we need to wait for the Shaders to finish its compilation and build before we can actually build the current target we're on. Now, this target actually exists in a different project. And if you would like to make a reference to that project you can do so by dragging the project as a child of the current project you're working in. So I want to walk through the rest of the dependencies of our project. Our Shaders target has a dependency on our Utilities target. Our Utilities target has a dependency on our Physics target. And lastly, our Tests have a dependency on our Shaders and our Utilities targets. So now that we have an understanding of the configuration of our project, let's look at the steps that are necessary to turn this serialized build process into one that we can build more in parallel. And we're going to start by looking at our test dependencies. Now, I've broken down the dependencies into three different classes of dependencies that I want to talk about. This first dependency is the dependency that I call the "Do Everything" dependency. Right? It should a little bit clear here that this test is testing way too many components. It's testing our Game. It's testing our Shaders. And it's testing our Utilities. Now in this case, it'd be better to simply break up our tests so that it's testing each individual component. And we're going to see by doing this, we're going to introduce our first bit of parallelism into our build process. Right? Our test target, which was built in all three can now build just the component that it's looking for in the Game tests. And then our Shaders tests and our Utilities tests can be moved to be built in parallel with our other targets. And they can be built as soon as their respective components are done, Shaders and Utilities. Now, the next type of dependency I want to look at is the dependency that I call the "Nosy Neighbors." This is the dependency that needs to exist. It's look at another target. But it only needs a little bit of that target. But instead it's getting everything that's in that target. So if we look at our game, it has a dependency on Physics, Shaders, and Utilities. This is actually okay. The suspect one is the dependency between our Shaders target and our Utilities target. Now, our Shaders target produces a meta library, which is essentially just a bundle of GPU code that's going to run on our graphics card. And our Utilities target just produces a normal frame, which is just CPU code. So there's already a little bit of a suspect dependency here. When we dig into it we see that the utilities target actually has a build phase in it that's generating some information that's used by both targets. Which is totally fine. It's just that Shaders doesn't need anything else from the Utilities target. So it's best to break out that into its own target. And we're going to see that this small incremental change actually has a large and significant impact on our overall build timeline. So the new green box that just moved in is our new code target. So we were able to shrink our utilities target down because we moved that work into Code Gen. And since Code Gen has no other dependencies, it can move to the very front of our build process. It can also be built in parallel with our Physics target, which is the red box on the bottom. And lastly, because Shaders no longer depends on Utilities, it doesn't have to wait for both Utilities and the Physics target to be built. And instead it can be built as soon as that Code Gen target is done. Now, the last dependency I want to talk to you about are the ones that I call the "Forgotten Ones." Throughout the evolution or the lifecycle of our products and our code, we tend to move code around and delete things. And we get things like dead code. Well, we get the same thing that happens with our dependencies. Sometimes we simply forget to clean them up. And so in these cases, it's actually safe to just remove that dependency. And this last change tightens up or build graph even further by allowing the Utilities target to be built right after the Code Gen target instead of having to wait for all of the Physics target to be done. Now, previously in Xcode, when you built targets that have a dependency on another target, you have to wait for the dependent target to finish its entire build process. Well, we have a new feature in Xcode 10 that allows us to introduce some parallelism into your build for free. And we do this by starting the compilation of your target as soon as the build phases that go into its dependencies that satisfy our compilation are complete. So things like linking can now be done in parallel. Now, if you have run script phases, this is one of those build phases that your target is going to have to wait on to finish in order before it can start taking advantage of some of these new parallelization benefits. So let's talk about run script phases. Run script phases allow you to customize your build process to meet your needs. And so with this flexibility comes some responsibility for you as a developer as well. And so we want to walk you through the configuration process and make sure that you have your run script phases set up and configured well so that your builds behave. Now, this is your Script Phase Editor. It can also be found within your Build Phase Editor. Now I want to call your attention to first the script body here. You can either put your entire script contents here. Or you can do what I've done and reference another script that's within your project. Now, throughout the entirety of your run script phase, there are a set of build settings that are available to you. And I'm making use of one of those right now, which is the source group. This gives you a convenient way to not have to provide absolute paths or try to do some relative path hacks to get your stuff to work. The next section are you input files. Now, these are very important for your run script phase. As this is one of the key pieces of information that the Xcode Build System will use to determine if your run scripts should actually run or not. So this should include any file that your run script phase, the script content, is actually going to read or look at during its process. Now, some of you may have a lot of inputs into your run script phase. And so this task might seem a little bit daunting. And so new in Xcode 10 we have the ability for you -- or we've provided you the ability to maintain this list in an external file, something we call a File List. Now, a file list is a simple text file that has all of its inputs listed on its own line. You get access to all of the same build settings that were available to you throughout the context of your run script phase. The important thing to note here, though, is that these files cannot be modified or generated throughout your build process. They are read when your build process starts. And all that information is used then. Next I want to talk to you about your output files. Your output files are another key piece of information that's used throughout your build process. And Xcode will use this information to determine if your run script phase actually needs to run. And of course, we have support for output files -- or file lists for your output files as well. So I want to recap for you when your run script phase is actually run. If you have no input files declared, the Xcode build system will need to run to your run script phase on every single build. So this is one of key reasons it's important to declare your inputs. Now if any of your input files change, including the contents of your file lists or any of the inputs that the file lists point to, Xcode will know that it needs to rerun your run script phase for you. And finally, if any of your output files are missing, the Xcode build system will run your run script phase to give you the opportunity to generate those missing output files. Now also new in Xcode 10, we have documentation for the run script phases. So, it goes through more detail of what I just explained and it tells you about all of the additional build settings that are made available to you, including how you can use those file lists within your own script content. Now, when setting up your run script phases and declaring all of these new dependencies, and including when you modify the dependencies in your targets, you may run into a dependency cycle. And a dependency cycle is simply an interdependency graph somewhere there is a loop that's created. Well, new in Xcode 10 we have better diagnostics to detect these cycles and will give you an error, including the ability for you to expand this box to get all of the inputs that the Xcode build system knows that went into creating the cycle. So, cycles are bad for a couple of reasons. One, they represent a configuration issue within your project. And two, they can be the source of spurious rebuilds within your project. Or of getting out-of-date information in your build process. So we also have updated health topics on dependency cycles, including some specific sections that we call out of the most likely dependency cycles that you run into, and ways that you can fix those. So, the last thing I want to talk to you about today, is measuring your build time. We have two new pieces of functionality in Xcode 10 for this. The first is we have introduced in-line task times to give you the duration that each of your tasks has taken to run. Now, I want to point out something about your build logs. There's a filter bar across the top. And specifically, the All the Recent filters. When you have "All" selected, it's going to show you all of the tasks that went into creating your entire final product outputs. Which is usually not what you want to look at. What you want to look at, especially when you're trying to diagnose issues in your incremental builds, is the Recent tab. That's going to show you all of the build paths that went into the previous build operation. Now, another new feature in Xcode 10 is a timing summary. And you can get to this timing summary by going to the Product menu, selecting Perform with Action, and Build with Timing Summary. When you do that, you're going to get a new log section out at the end of your build log. And if we focus in, you're going to see that it's going to give you an aggregate timing of all of the tasks that went into your last build operation. So this is another important reason to look at the Recent filter tab. And there's one specifically I want to point to, the Phase Script Execution. So, you can see in our last build that we just did, we had a shell script that ran. There was only one of them. It says one task. And it's taken 5 seconds. If you're seeing these on every single one of your incremental builds, this is a good indication that you have something misconfigured in your run script phase. And that's something you might want to address to help decrease your overall build times. Now, this build timing summary is also available to you from the Command line by passing in the Show Build Timing Summary flag. And so now I want to bring up Jordan, who's going to talk to you about some of the source-level improvements that you can make to your project. Thanks, David. All right. So we've gone over a bunch of ways where you can improve your Xcode projects just by doing one small change. And before we get to the source-level and file-level topics, I want to talk about one more that's new in Xcode 10. And it's a particular workaround that we know that some of you have been using on your projects to make them build faster if they have a lot of Swift files. You've already heard about this. It's the Whole Module setting being used in a debug configuration. So, in previous versions of Xcode, for some projects, turning on the Whole Module Compilation mode, even for debug builds, produced a faster overall build than when used in Default Incremental Modes. And, this did improve build times because it was able to share -- Swift's compiler was able to share work across files in a way that the Incremental Mode was not. But it also meant that you were giving up your incremental builds and would rebuild the entire target's worth of Swift files every time. So in Xcode 10 we've improved the incremental build to have some of that same work sharing across files. So you should no longer need to use Whole Module mode to get good build times. So, if you've done this in your project, then you should go back to your Build Settings Editor and select the debug configuration under the Compilation Mode build setting and hit Delete. That will get it back to Xcode's default setting of an incremental build. I'm not going to talk much more about this because you already heard about it. We mentioned it in the "What's New in Swift" talk on Tuesday. And if you do want to know more, we'll cover this and other topics about your build in greater depth in tomorrow's session "Behind the Scenes of the Xcode Build Process." So we have a lot of topics that we're trying to get through today. And David's already covered half of them. I'm going to talk about the remaining three dealing with complex expressions at the top of that list. And the reason for this is because it's the best one that exemplifies a key takeaway for both of our sections. When a build is taking a long time, there's often a key piece of information that you can provide to Xcode to improve the situation. And so we're going to look at that first in the context of complex Swift expressions. So here's an example of some code from my latest project. And the problem with this struct is that I use it all over the place. And it's perfectly fine to have a struct. It's perfectly fine to have a struct with a property. And it's fine to have a struct with a property with an inferred type. But the expression that we're inferring that type from here is a little bit complicated. It's not something simple like -- Oh, I took out a build from my slides. So I've given away the answer here. If this were something like 0.0, then this inference of double here wouldn't really have been necessary. But since we've got this big, complicated expression involving reduced and the power function from the system frameworks, you might not have even guessed that "double" was inferred type of this property. And so by providing this information here, you've saved work that the compiler would have to do in every file that uses this struct. And you've also saved work that your coworkers would have to do to figure out what really is the type of that big number property. So a lot of times you can get this extra key piece of information that will help your build times is also an example of a good software engineering practice. Let's take a look at another example involving closures. This time I'm trying to define a function that will return the sum of the non-optional values of its arguments. And if all three arguments are nil, it will return nil. And I'm trying to use one of Swift's cool features where if you have a closure with a single expression in its body, then the compiler will use that expression to help determine the type of the closure. Sometimes this is really convenient. Other times it can lead to code like this. That's pretty ugly. I don't think I'm going to get past code review with that one. We've got some nested turnery operators and some explicit comparisons against nil. And then a force and wrap to go with it. I don't really think this is going to fly. And it's got another problem, too. Because this expression is getting so large, with so many independent pieces, the Swift compiler will report that it's not able to compile this in a reasonable amount of time. Now, this is the ultimate in slow builds when even the compiler gives up. And really, it's telling me something about this code. So, my first option here would be to do the same thing as the previous example and provide additional types. With a closure, you can do that just before the In Key word. But, this may not be the best solution for this particular problem. So let's go back to what we had before. Recall that I said that I'm trying to write a single expression here so that it can be used to help determine the type of the closure. But in this case, that's not really necessary. We already know from the call to Reduce what this closure has to be. Reduce is being called on an array of optional integers. And the result type has to match the return type of the function. So we already know that this callback for Reduce is going to be operating just on optional integers. That means there's no need to put a single expression in that closure. And it's perfectly okay to break it up into separate, more readable statements. So here's a direct translation of the code that I had before. But I also have the freedom now to make it something more Swifty. This is a lot more readable. A lot more maintainable. And it compiles in a quick, reasonable amount of time. Now, the last example I'm going to show in this section is something that won't apply quite as broadly as the previous two. It's about this type Any Object. Now, Any Object is a convenient type that describes any class instance. So not a struct or an enum. Definitely a class. But we don't know which one. But it also has an additional feature carried over from Objective-C's ID type. And that's this method call syntax. If you try to call a method or access a property on a value of type Any Object, Swift will allow you to do so, as long as that method is visible somewhere in your project and exposed to the Objective-C runtime. However, this does come at a cost. Because the compiler doesn't know which method you're going to call, it has to go search out any possible implementations throughout your project and the frameworks you import and assume that they might be the one that's going to be used. It has to do this because if none of them match, it needs to present you with an error. So instead, we can do something much better and much more, again, declarative of our intent. We can define a protocol. Now, this can be done in the same file, or a different file, but the important part is that once we change this delegate property to use our protocol instead of Any Object, the compiler knows exactly which method it's calling. And now you also have the opportunity for all of your implementing types to be checked that they implement the method correctly. So, we've talked about several techniques here for decreasing the amount of work the compiler does once it's already decided to recompile a file. But what about not recompiling the file at all? What makes the compiler choose whether a file needs to be recompiled? For that, we need to understand Swift's dependency model. Now, Swift's dependency model is based around files. And it's a little bit tricky because in Swift there are no header files. We just see everything that's defined somewhere in our target by default. In this case, I'm declaring a struct point in the file on the left. And if I bring in a file on the right, the compiler knows that I'm referring to that first declaration. The same is true for the use of the X and Y properties in that file on the right. Now, this file-based dependency means that if I change the file on the left, both files will need to be recompiled. And that's important because we're actually trying to call this initializer. And we want to make sure that we're calling it correctly. The compiler is smart enough to know that when you make change within a function body, in this case making the assertion more appropriate, that only that file will need to be recompiled. Other files won't have to change how they use the API's from the first file. However, it does need to be conservative. And so if I add a separate type to this file, a human can tell that this path segment struct won't affect the file on the right. But the compiler will still be conservative and rebuild them both. Let's see how this applies to the game example that David was using earlier. So here we have the app target and the Utilities framework. And I'm showing some of the Swift files that are in each target. So if I change a file in the App target, well, we know already that that file needs to be recompiled. And of course, anything that depends on that file will also need to be recompiled. But there's no chance that anything within the utilities target will be recompiled. It's in a separate target. It has an explicit dependency. And it doesn't have implicit visibility between those two sets of files. Now, similarly, if I change something in the framework target, then I would need to recompile that file and anything else in the utilities framework that depends on it. However, these dependencies are more coarse-grained. And so Xcode will also recompile everything that's in the Game target as well, unless the changes are entirely confined to function bodies. So to recap those rules, the compiler needs to be conservative. Even if a human can tell that a change doesn't affect other files, that doesn't necessarily mean that the compiler can. However, one change that the compiler does know how to handle is function bodies. It knows that this doesn't affect the file's interface. And therefore, will not require other files to be recompiled. This per-file dependency basis happens within a module, which is where Swift declarations are implicitly visible to one another. When you're dealing with cross-module dependencies via your imports or your bridging header, these are dependencies on the entire target. So this is all good information about Swift dependencies, and Swift targets. But I know a lot of you out here have mixed Objective-C and Swift targets. And so the last section is going to be focused on that, on how to reduce the interface between the Swift and the Objective-C code in a mixed-source app. And to do this, we're going to have to talk about the parts of a mixed-source app. And this diagram's going to get a little complicated, so bear with me. And if you're watching on video, you may need to pause and restart. Feel free. We start off with the headers that describe your Objective-C interface. This is the parts of your app that are written in Objective-C that you may want to expose to Swift. Or perhaps you're just declaring headers for other Objective-C parts of your app. Then we have the bridging header. This is the header that collects all of the information that you want to expose to the Swift part of your app. This is a build setting in Xcode that controls which header is used. And once it's set, the Swift compiler will know to expose those Objective-C interfaces to your Swift code. The Swift compiler will then produce a generated header, which does the same thing in reverse. It describes which parts of your Swift code will be exposed to Objective-C. That can then be used in your Objective-C implementation files, which probably also use some of those headers from the first step. And then of course, you might have Objective-C code that is not dependent on any of the Swift code. But that's less interesting for this part of the talk. So, I'll step through that from left to right again. We have the Objective-C headers. The bridging header for getting some of that information into Swift. Your Swift implementation files. A generated header for presenting that information back to Objective-C. And then finally, your Objective-C implementation files. And in a diagram like this, all of these arrows represent dependencies. Not dependencies on a target level, but within on a file-by-file level within a target. And so, what we want to do is focus on the generated header and the bridging header, because if we can shrink the content in these headers, then we know that there's fewer chances for things to change. And therefore, less need to rebuild. So let's take a look. For the generated header, your strongest tool is going to be the private key word. So in this example, I have a view controller that I'm defining in Swift. And it has an it an IBOutlet property and an IBAction method. By default, these will be exposed in your generated header because they're methods and properties exposed to Objective-C. And they're not declared as private. But most of the time you don't need to expose these to any other files in your project. They're just for interacting with Interface Builder. And so, in this case, I can mark these private and watch as the property and method vanish from the generated header. Another example of this is when dealing with methods exposed to Objective-C for use with Objective-C runtime features like #selector. In this case, I'm using foundations Notification Center API, which takes a selector to use as a callback when the notification is sent. Once again, the only requirement here is that the method is exposed to Objective-C. It doesn't actually need to be used from any other files in my project, Swift or Objective-C. So I can mark it private. And once again have that reduction in the shape of my generated header. In cases like this, there's often another option as well. And that's to switch to block-based API's. In many cases, this can even clear up your code because you can implicitly capture state from the function that's registering for the notification rather than having to carry it along as some kind of context object. Now, the last tip for reducing the contents of your generated header is actually a very old one. You can migrate to Swift 4. And you've already heard that you're going to have to do that this year. That Xcode 10 will be the last set of releases where Swift 3 mode is supported. And so, this is something you'll be doing anyway. Edit. Convert. To Current Swift Syntax. However, when you do this migration, you may have actually selected to keep the Swift 3 compatibility mode for a particular build setting. And that's the Swift 3 @objc imprints. This is an option when you migrate to Swift 4 to keep on a rule from Swift 3 which exposes internal methods and properties to Objective-C automatically on any subclass of NS Object. Now, if you are writing in Swift 3, you may be relying on this feature. But there's a lot of cases where you were not actually depending on this in any way. Not in the runtime sense. And definitely not at compile time. So, once you get to the point where you've explicitly marked all of your Objective-C dependencies as either @objc or IBOutlet, IBAction, whatever, as appropriate, then you can also select this build setting and hit Delete to get it back to the default mode where the OB-C attribute will only be inferred for methods and properties that satisfy protocol requirements or those that override methods that come from Objective-C. So we've talked a lot about the generated header and what you can do to your Swift code. But you have Objective-C code as well. And the Objective-C code, likewise, causes rebuilds. And so a bridging header looks something like this, usually. It's got a bunch of other headers in the project that you're trying to expose to Swift. And we can zoom in on one of these headers here, the MyViewController header and see that it's a perfectly normal declaration of a view controller. But also that it itself includes another header. What that means is that if any of these headers change, the Swift code in your target has to be recompiled because it might depend on something that changed. This is suboptimal. And now we can notice that in this example, the only reason we're importing the MyNetwork Manager header is to declare this property, this network manager property on the view controller. And it's possible that that property is never actually used from Swift. In which case, it's unnecessary for us to be declaring it here. So what you can do is use categories, Objective-C's equivalent of extensions, to break up this interface. So I'm going to define a new file here, MyViewController Internal, and use the special nameless category syntax that allows me to declare additional properties while still taking advantage of the property synthesis feature in my main Add Implementation block. Now I can just move the import and the property down to the category. And voila! The headers that are being imported into Swift have gotten much smaller and are much less likely to change now and cause an unnecessary rebuild. And there's one more note. This file here that I defined, well, it's possible that nothing else in my Objective-C code needs to access this property, either. In which case, there's no need for a separate file. I can put this category directly into my .m. There's nothing wrong with doing this. Everything will work fine. And as I said before, property synthesis will still work for the network manager property. So what have we seen? We used private and block-based API's, and turning off that Build setting to shrink the contents of the generated header. And, we've broken out separate contents from the Objective-C headers that we declared, which shrink the contents of the bridging header. Less content means less work done on each build. And it also means fewer opportunities for changes, which means fewer chances for rebuilds. We win on both counts. So let's wrap things up. David and I talked a lot about quite a few different topics, of ways that you can get more information from Xcode and that you can provide more information to Xcode in ways that can speed up your builds. And this covers both increasing the build efficiency when you're doing a build and reducing the work that you have to do at all in a rebuild. So, we went through this kind of fast. So if you want to see it again, check out the video page. And you can also come find us in the labs at noon today and tomorrow in the afternoon. Thank you very much. Enjoy the rest of the conference.  Thank you all for coming. And welcome to the session, Advances in Rresearch and Care Frameworks. My name is Srinath and I'm a software engineer. Now, if you're familiar with our WW talks in the past, you might remember that our sessions have focused primarily on our two open source health frameworks. ResearchKit and CareKit. This year, although we'll still be talking a lot about these frameworks, we'll also be covering some new APIs and features that we've been working on in the larger health space. But for those of you who are somewhat new to these frameworks, I highly recommend checking out our two sessions from last year. What's new in CareKit and ResearchKit was hosted by Sam Mravca. Where she gives a really good overview of both ResearchKit and CareKit. The other session, Connecting CareKit to the Cloud was by Kelsey Dedoshka, where we introduced a new CareKit bridge API that allows developers to sync care plans between patients and providers using any HIPAA compliant backend. And on that note, I'm really excited to show you all some of the amazing work that's been happening at Penn Medicine. The Penn Life Gained Appl utilizes CareKit and the CareKit bridge API to help patients through both pre and post-bariatric surgery at Penn Medicine. The app leverages care plans, healthcare data and other interactive components to help patients go through their weight loss journey. Data is constantly being synced between the iOS apps used by the patients, and the iPad apps used by care providers in order to closely monitor and interact with their patients in real time. The team has received extremely positive feedback from both patients as well as clinicians with regards to the impact that these apps have had on their bariatric program. And now that we have touched upon where we have come since last year, I would like to take a quick step-out to all of our health frameworks as a whole. The creation of these frameworks stems from our overarching desire to improve the world of health, through technology. And in that process, we really want to empower our users and provide more tools to developers and researchers, so at the end of the day, you and all of us can help improve and advance two core areas of interest, research and care. This year, I'm really excited to be talking to you about some of these topics which we care so deeply about in a variety of ways. Starting with the framework focus, where I'll talk to you about updates we have made to our ResearchKit framework. From there, we'll move on to a more condition focused approach, specifically around movement disorders like Parkinson's, where Gabriel will come on stage and talk to you about this brand-new API. And finally, we'll bring it all together with a demo where we will showcase how you can utilize these new APIs and features directly in code. Now let's get started with ResearchKit. Over the past year, we have been putting in a lot of effort to improve out openness and engagement with our community. Some updates to our ResearchKit UI and modules, as well as some new additions to our existing library of Active Tasks. Now, let's get started with community updates. I want to cover two main topics, repository privileges and schedule updates. Over the past few months, we have been expanding our access rights and providing write privileges to members of our community. In fact, we've chosen five all-star contributors and given them direct access to the ResearchKit repository that also allows them to merge MPRs. A huge thank you and congratulations to Erin, Fernando, Nino, Ricardo, and Shannon. Next, I want to touch on our schedule updates. Historically, we've been pushing to master and stable at the same time. Now, we realize that this inhibits the ability for developers, like you, to leverage some of our in-house features like localization, accessibility, and QA, which is why this year we will be pushing to stable two to three months after our push to master. We hope that you can use this time to check out our latest updates, provide feedback and submit PRs. And the changes that you make, will make it soon into our stable, tagged release branch as opposed to you having to wait for an entire release cycle. And now that we have touched upon community updates, I would like to dive a little bit deeper and talk about updates we have made to the ResearchKit framework itself, starting with UI updates. Now, for comparison, this is what our Mini Form step looks like in ResearchKit 1.5, as taken from our ORK test app. And here is the same Mini Form step as it will be available in ResearchKit 2.0. As you can see, we have worked very hard to update the overall look and feel of ResearchKit UI to closely resemble the latest iOS style guidelines. Let's take a closer look. We've moved the progress label from the center of the navigation bar to the right side and applied some styling. This allows us to leverage the large titles feature in nav bars and apply it to all of our step titles. Now, to keep the consistent styling, we are also adding a new card view in order to improve the overall user experience for those who are answering multiple questions and surveys, as it now provides a clear break in action that you're requested to perform. Now, this card view is applied by default to all our steps and forms. But we are also exposing a Boolean property that you can set to falls for backwards compatibility. And finally, we are also adding a footer container view, to improve the navigation flow. The cancel button is now part of the footer view, which is always sticky to the bottom. What this means is that your users will no longer have to scroll all the way to the bottom of the step, in order to access forward navigation options. And having all these controls in one place makes it much more intuitive. And now, moving on, one of the most commonly used modules in ResearchKit is informed consent, which is used to generate a PDF and attach a user signature to it. Now, we realized how important it is to be able to survey some of these confidential documents to your users within the context of your app. Which is why we are adding a new ORK PDF viewer step. This is built on top of the PDF Kit framework that came to iOS just last year. Let's take a closer look at some of the functionalities that this step provides. Quick navigation to easily switch between pages. Real-time annotations to mark up the document if necessary. Search functionality for your users to query the entire document for keywords or phrases. And the ability to share this PDF or save it using the standard iOS share sheet. And what's even better is how easy it is to incorporate this in your app. You create an instance of ORK PDF viewer step with a unique identifier, and simply provide us the file path to the PDF document that you wish to display. And now, I'd actually like to switch gears and talk about one of the core components of ResearchKit. Active Tasks. For those of you who are unfamiliar, Active Tasks are prepackaged modules that allows users to perform certain tasks or take certain tests in a given amount of time. When the user completes the step, the developer receives a delegate call back with an ORK resolved object. Now, this object consists of a variety of data points, which include things like user responses, timing information and data recorded from different sources like Accelerometer, Gyroscope, healthcare data, and even from your microphone. And this year, we are also adding support for health records. Now, let's take a look at how you can utilize this in your app. Now, your objective here is to create a recorder configuration that can query health clinical data types. And you need to provide us with two important parameters. First is of HKClinicalType, and a second is an optional of type HKFIRResourceType. And once you have created this record of configuration, you attach it to the step. So, now when the user is about to perform the task, they will be prompted with HealthKits new authorization UI. And only if they grant access will we be able to run the query. And once the user completes the task as part of the delegate call back in your ORK result object, you will also have information that you requested for from health records. Now, to understand and leaner more about these different health records types, I highly recommend everyone here, to also visit the session, Accessing Health Records with HealthKit, that is happening right here at 3 p.m., where they will talk about everything related to health records in great detail, including some very important best practices. And now that we have covered updates to the Active Task module as a whole, let's talk about the Active Tasks themselves. This year, we are adding new modules focusing on three main areas of health; hearing, speech, and vision. Let's get started with hearing. We are adding a new dBHL tone audiometry step. This implements the Hughson-Westlake Method and allows you to determine the hearing threshold level of a user in the dBHL scale. And to facilitate this, and ensure that you get the most accurate results. I'm really excited to say that for the very first time we are open sourcing calibration data for our AirPods. Now, this comes with three tables. The first one is a volume curve for AirPods on all iOS devices. The second one is the sensitivity per frequency. Where sensitivity is measured in decibels sound pressure level. And finally, we are also providing the reference equivalent threshold sound pressure level tables or RETSPL. Now, it's really important to note that the RETSPL table is still in beta. This means that we are actively running internal validations and tests. And over the next few weeks, we will be updating these tables as we start converging on accurate data. Now let's take a look at how this Active Task actually works. The user is required to hear tones at a particular frequency at varying levels of dBHL value. When the user hears a tone, they are expected to tap the button to indicate that they have heard it. And at that point we will start decreasing the dBHL value as indicated here by the green dots. Now, when the user fails to tap the button after a given timeout period, we will start increasing the dBHL values as indicated by the red dots. And over time we will feed these data points to the Hughson-Westlake Method in order to determine the hearing threshold level of the user in the dBHL scale. Now, from a developer perspective, the entirety of tone generation occurs in three phases. The first one, is a pre-stimulus delay, which is a developer-specified maximum number in seconds that we use to generate a random delay between one and that value before we start playing the tone to the user. This is to ensure that the user cannot cheat the test by just randomly tapping on the button. Next, we are also providing a property for tone duration, which governs the actual duration for which the tone will be played. And finally, is the post stimulus delay, which is the amount of time allocated to the user to respond for that specific tone. To incorporate this in your app, you create an instance of ORKdBHL tone audiometry step with a unique identifier and provide us values for some of the parameters, including a frequency list, which is an array of frequency that you wish to be played back to your user. We're also exposing more properties that you can further customize for your specific use case. Now, when the user completes this task as part of the delegate call-back, the ORK result object will be returned to you. Let's take a look at what that object looks like for this particular task. So, at the top level you get a variety of information that includes things like output volume, and also includes an array of sample objects. These objects in turn encapsulate things like channels, which refers to whether the tone was being played in the left or right channel to the user, as well as the threshold value, as determined by the Hughson-Westlake Method. This also consists of an array of unit objects. And the unit objects in turn, provide details like the dBHl value at which that particular tone was being played, and a variety of timestamps, which also includes when exactly did the user tap the button. So, now let's move on to our next task that falls under the hearing category. We are adding an environment SPL meter. This implements an A-weighted filter, in order to measure the environmental sound pressure level in DBA, or in other words, it tells you how noisy it is. Now, this step also accepts a threshold value, and this makes things interesting, because now you can use the step as a gating step. So, for example, if you want your users to perform the tone audiometry task, you can add this task before that to ensure that your users are not in an environment that is too noisy to accurately perform the task. To add this you create an instance of ORK environment, SPLMeterStep with a unique identifier and provide us the threshold value. We're also exposing additional properties that you can further customize. And now, let's take a quick detour and move on to our next category, speech. We're adding a speech recognition module, which leverages the speech recognition framework on iOS, now, this gives us direct access to a real-time speech recognizers that supports over 50 different languages, or locales. As part of this task, the user is asked to either repeat a sentence, or describe an image. And once they are done speaking, the users will be automatically taken to a next step, where they can edit the generated transcript. So, for example in this case quick and fox were incorrectly interpreted as quite and box. Your users can just tap on these words an edit them if necessary. Now, it's important to note, that as part of this task, we are returning to you a very, very rich dataset that consists of three main things. A direct audio recording to what the user said, the transcription generated by the speech recognition engine, and the edited transcript by the user. To add this to your app, you create an instance of ORK speechRecognitionStep, and provide us with either a UI image, or a string. You can also customize the locale for this recognition. And also, you can surface real-time transcription as the user is speaking. Now, let's take a closer look at one of the subsets of our result object. This is of type FS transcription as surface by the speech recognition framework. The formattedString provides the transcript, and the array of segment objects, essentially breakdown the transcript into substrings, as well as provide a confidence letter for each substring. On top of this, they also provide an array of alternative string, as you can see here in this illustrative example. Now, these results can be used to derive syntactic, semantic and linguistic features, as well as speaking rate in order to evaluate the speech patterns for various medical conditions, including cognition and mood. Our next task, Interestingly, is a combination of speech and hearing. The speech and noise Active Task. This allows you to perform fully automated speech audiometry. And conventional tone audiometry uses pure tones, which are essentially sign waves. Now, there have been recorded instances, where users are able to clearly distinguish pure tones, but find it extremely difficult to distinguish words when they are intermixed with noise. And this closely reflects real-world examples of early-stage hearing loss. For example, when you are unable to understand what the person sitting in front of you is telling you in a noisy restaurant. Now, before I go into further details let's take a look at how this task actually works. The actor tried three green pictures. Now, as you notice an audio file was played to the user, and once it completes, they are immediately requested to repeat what they have just heard. Now, the speech and noise Active Task uses audio files generated by combining five words from a close-set matrix, that was validated internally for things like word familiarity, uniformity, difficulty, and also to ensure that the sentences generated by the synthesizer were phonetically balanced and more importantly, consistent. These files are then programmatically mixed with background noise at varying levels of signal to noise ratio, or SNR. And developers will also have the option to set the value for the gain for all of the noise signals. Now, speech reception threshold, is defined as the minimum value in SNR at which a user can understand only 50% of the spoken words. Now, our vision for this test is over the next few weeks, we'll be uploading over 175 different files that corresponds to 25 lists, each with 7 sentences. And in the long-run, we want to be able to support this test for multiple languages, especially ones where currently speech and noise is not possible due to a lack of speech database or other testing resources. So, if you are a researcher in this particular field and if you have a request for specific locale, I highly encourage you to reach out to us and we'll do our best to accommodate your request. To add this to your app, you create an instance of ORKSpeechInNoiseStep, and point us to the audio file that you wish to play. You can also specify the gain applied to the noise signal. And lastly, let's touch on vision. the Amsler grid is a tool that is used to detect problems in a user's vision that can be caused due to conditions like macular degeneration. Now, this test is conventionally performed in a doctor's office on a traditional piece of paper, displaying a graphic like the one you see right here. Users with perfect vision would see exactly this, whereas users who suffer from some conditions would start seeing distortions on this graph. Now, please don't panic if you're seeing distortions right now. That was intentional and it was added for dramatic effect. Now, users simply have to point to places on the grid where they see distortions. By replicating this grid, we are able to bring the functionality of this task to the users at home on their device. Users simply have to annotate areas on the grid where they see distortions. And we believe that developers can leverage some exciting iOS features like [inaudible] or the depth sensing camera to increase the experience of a user who is taking this particular task. Now, all of these Active Tasks are really great for data collection and analysis. But they are designed to be performed at a specified time for a given period. And as we start to dig into specific conditions, we have realized that some of the more complex problems in health require constant monitoring, and therefore, introduce the need for passive, noninvasive data collection. And to talk more about that I would like to introduce Gabriel up on stage. Hello. My name is Gabriel and I'm here on behalf of the core motion team to introduce a new research API. The movement disorder API. As Srinath was mentioning, this is a passive, all-day monitoring API, available on Apple Watch, which will allow you to monitor the symptoms of movement disorders. Specifically, two movements disorders which are relevant to the study of Parkinson's disease. Now, because of the targeted use case as a research API, you will need to apply for a special code signing entitlement in order to use this API. This application process will be done through the Apple developer web portal, starting in seed two, though if you just can't wait and I don't blame you, there will be sample data sets as well as demo code available in the research kit, GitHub repository for this talk. So, let's start talking about those two movement disorder symptoms. As some of you may know, Parkinson's is a degenerative neurological disorder which can affect the motor functions of those with the disease. One of the identifiable symptoms of Parkinson's is a tremor. And this API monitors for tremor at rest, characterized by a shaking, or a trembling of the body when somebody is not intending to move. Now, there are treatments including medications, which can help suppress and control the symptoms of Parkinson's. However, these very same treatments can often have negative side effects. Side effects such as dyskinesias. And one dyskinetic symptom that this API is able to monitor for is a fidgeting or swaying of the body known as choreiform movement. So, to recap, you have tremor, a symptom of the disease as well as dyskinesia, a side effect of the treatment. Let's take a quick look at what tools researchers and clinicians currently have in order to assess these symptoms. Typically, these types of assessments are done in clinic. A clinician will ask a patient with Parkinson's disease to perform physical diagnostic tests in order to rate and evaluate the severity of their condition. These ratings provide a quantitative, though subjected to the rater, measurement of their condition at that point in time, when they're in the clinic. In order to get a broader and more complete picture, patients are also encouraged to keep diaries where they log their symptoms manually. However, this can be cumbersome for patients and some will understandably forget or be unable to describe the full extent of their symptoms every single day. Wouldn't it be great if there was a passive, unobtrusive way to monitor for these symptoms. Well, by using the movement disorder API, researchers and developers like yourselves, will be able to build apps which collect these types of the metrics, continuously whenever a patient is wearing their Apple Watch. Not only does this give you a quantitative measurement of those symptoms, displayed here as the percentage of the time observed. But it also gives you a longitudinal analysis, where you can track changes to those symptoms over time. These algorithms were designed and piloted using data collected from Parkinson's patients in internal clinical studies. And we hope that you can use these tools and these learnings to build new care experiences that improve the quality of life of people with Parkinson's. But before you can do any of that, you're going to need to know how to use the API, right? All right. Well, let's take a look at some code. The first thing you're going to want to do is request motion authorization from the user in order to use their movement disorder data. Once you've done that, you'll want to call the monitorKinesias function in order to enable symptom monitoring. Now, this symptom monitoring does turn on additional Apple Watch sensors, so this will have an impact on the battery life of your users, though they should still be able to have a days' worth of data collection on a single charge. As you can see, the maximum recording duration is seven days. I know that many of you are going to be conducting studies that are longer than seven days, and if that's the case, simply call the monitorKinesias function again in order to extend your data collection interval. This will begin to store tremor and dyskinesia results on the user's device, on your behalf. At a certain point afterwards, you're going to want to return to that application so you can query for those records. Let's take a look at what the query function looks like. As you can see, in this line, recording for any new tremor records that were stored on the device since our last query date. These records are stored on device by the API, but they also will expire after seven days. And so, before it expires, you're going to want to take ownership of those records either by serializing them and storing them on this device yourself, or transferring it to a different platform so you can visualize it and analyze it. The data will be returned to you as an array of minute long object results. So, one hour's worth of data, 60 result objects. Let's take a look at what one of those result objects will look like. As you can see, the result objects return the percentage of the time the percentage of that one minute that the algorithm was able to observe the presence, or the absence of the symptom. For dyskinesia, here on the right, you can see that that's pretty simple; unlikely or likely. tremor gives you a few more options. Let's go through them. Since this is a tremor at rest any active or chaotic motion will simply be returned as percent unknown. And this is the same category that we use for low signal levels we're unable to make a determination. However, if the algorithm is able to make a determination, it will also return the severity of the tremor, ranging from slight up until strong. Now, to show you just how well the passive monitoring of the movement disorder API is able to work in conjunction with the active monitoring of a ResearchKit Active Task, I would like to invite up to the stage Akshay, who is going to integrate both into one stellar research application. Hello, everyone. And welcome to the Advances in Research in Care demo. In this demo, we'll see some research get update and also implement the movement disorder API. As part of our research kit repository on GitHub, we have already added an ORK Parkinson's study app. This app implements the movement disorder API and also visualizes the tremor and dyskinesia symptom data points that we just saw. Let's go ahead and see what this app looks like right now. We have an Apple Watch app where we implement the movement disorder API, collect the tremor and dyskinesia symptom data points and send them to our phone app. In our iPhone app, we have a questionnaire, a few Active Tasks, and also visualize these tremor and dyskinesia symptom data points. Let's look at what the code looks like, and for this demo, we'll start with the basic bald plate application and try to recreate this app. Here's my Xcode workspace, and as you can see, in my ResearchKit, I now have ORK Parkinson's Study App. We have a task list viewController, where we'll be adding all our Active Tasks and questionnaires. A graphviewController, where we'll be visualizing these tremor and dyskinesia symptom data points. And an assessment manager, where we will be implementing the movement disorder API. When a Parkinson's disease patient, or a PD patient visits their doctor, they're asked a certain set of questions, and these questions include questions about their activities of daily life, something for example on a scale of 0 to 10, how is your pain level today. Or, what kind of non-motor symptoms are you feeling? We have already added a subset of such questions in our app. Now, these questionnaires are usually followed by seven physical tests. And one of the physical tests is assessing the clarity of speech. So, let's go ahead and add the speech recognition Active Task. In my task list view controller, I'll go ahead and add the speech recognition Active Task. And as you can see, we just added an ORK ordered task of type speech recognition. And if you notice, one of the parameters is the speech recognizer locale, which is an item provided by ResearchKit that represent all the locales that are supported by speech recognition API, so that you, as developers don't have to worry about conforming if your locale is supported by these speech recognition API. Now, let's move on to our assessment manager. As Gabriel mentioned, we have a variable call manager, which is of type see and movement disorder manager. And if you notice, we have a function that calls the monitorKinesias method for the maximum duration of seven days. Let's call this matter in our initializer. Now, whoever creates an object of a type assessment manager will simply start the tremor and dyskinesia symptom query. Once we have collected these data, we need a way to query these data points also. So, let's go ahead and add a method that record these data. I add a new, query new assessments method, where I'm calling the query tremor method for a given start date and an end date, and the query dyskinesia symptoms method, for the same start date and the end date. For this demo, we have already ran this query and collected the tremor and dyskinesia symptom data points and saved them as part of JSON file. Let's go ahead, use those JSON files and create ResearchKit graphs on them. I'll move over to my graph view controller, and here, as you can see, I have a create graph method, which reads the JSON files and creates ResearchKit graphs from them. Let's call these methods in our view data log. Perfect. Now, let's run this. As you can see, we added this speech recognition Active Task, and added the movement disorder API. Here's what our Parkinson's study app looks like. We have the questionnaire on top. Let's quickly go ahead and run through one questionnaire. As Srinath mentioned earlier, we now have a card view for all these survey items. And also, all these steps in ResearchKit adhere to the iOS paradigm. Let's quickly finish this questionnaire, and that's it. Now, let's move on to the speech recognition Active Task. The first two steps talk about how to use the speech recognition step. And as soon as I press the start recording button, I would be repeating the text that I see. A quick brown fox jumps over the lazy dog. I am directed to the next step, which is the other transcription step. And as Srinath mentioned, this step is optional and could be replaced from the task by setting the property allow edit transcript to no. Perfect. Now, let's look at the graphs that we created off of tremor and dyskinesia symptom data points. Since the ResearchKit graphs look really nice in the landscape mode, I'll quickly turn my phone, and let's look at the graphs. Here as we can see, we have all the tremor and dyskinesia symptom data points for a particular day starting from 7 a.m. to 6 p.m. We can see tremor slight, mild, moderate, and strong. And also, dyskinesia likely. Perfect. With this, I would like to call Srinath back up on stage and continue with the session. Thank you. Thanks for that great demo. So, now let's take a look at a quick recap of what we went over today. We started off by talking about updates that we have made to our community by expanding privileges and also updating our release schedule. We showcased the look and feel of ResearchKits new UI, and we also added some new Active Tasks focusing on three main areas of health. Hearing, speech and vision. Gabriel spoke to you about the new movement disorder API that's available on the Apple Watch from WatchOS 5. And now, we'll look to all of you as current or new members of the community to continue to engage with us and provide feedback. We also encourage you to take advantage of our new release schedule so that way you'll be able to leverage some of our in-house features like accessibility, localization and QA. And on that note, as we continue to expand on our library of Active Tasks, we look to all of our developers, researchers and health professionals to help us improve on these. These Active Tasks are just building blocks, which we hope you can utilize to create much bigger research studies, care plans and treatment mechanisms. And as you do, we encourage you to contribute back to ResearchKit, so we can continue to improve our foundation and expand on the breadth of Active Task that's available for everyone to use. For more information about ResearchKit, please visit our website researchkit.org. For additional information about the talk, you can visit the following URL. I also encourage you to stop by our lab session today. Our teams will be down there and will be really excited to answer any questions you have, as well as discuss more about some of our new updates. And finally, we really look forward to seeing what you all do with some of these new updates in the coming days. Thank you.  Hello. Welcome everyone. My name is Gaurav. And today we are going to talk about machine learning. Last year, we launched Core ML. And the response from developers, from you guys have been tremendous. We are just amazed by the apps you have made, the [inaudible] phenomenal. So let me first begin by saying thank you. Thank you for embracing Core ML. And we are -- we love seeing so many of you using it and giving intelligent features to our users. We are in this together. Thank you. It's an applause for the devlopers. Okay. So if you recall, Core ML gives you an easy way to integrate an ML model in the app. The idea is very simple. You get an ML model, you drag and drop in Xcode, and with just three lines of code, you can run state-of-the-art ML model with millions of parameters and billions of calculations in real time. It's just amazing. And you give -- your users get real time machine learning as well as privacy-friendly machine learning. All you have to do is to drag and drop an ML model in Xcode and Core ML takes care of the rest. I think the big question remains is where do I get these models from? So last year, we provided you two options. The first one was you could download some of these models, popular models from our website but, more importantly, we also released Core ML tools. Core ML tools allow you to tap the work which is done by amazing ML community. So the idea is, again, simple. You choose your favorite learning library, train your model in that training library, convert it into Core ML from that and then just integrate it into your app. When we released Core ML, we released with only five or six training libraries support for five or six training library but within a year, we have support for all the famous training libraries out there. We are enhancing our tools to even allow you more customization. And we are going to talk about more about Core ML tools in tomorrow's session. Another thing we did towards the end of the year, we released Turi Create, our open source machine learning library. We are going to talk about Turi Create in tomorrow's session. But this year, we want to give you something even more. We want to continue our journey. We want to give you something native, something Swifty, something that harnesses the power of our Xcode, something that puts the focus on you, our developers, something that just demystify machine learning for you. Hence, we are introducing Create ML -- Our machine learning framework in Swift. So Create ML completes the left-hand side of the equation. The idea is you make a model in Create ML and you run it in Core ML. You do complete end-to-end machine learning in Swift, our favorite language. So you are not dealing with language oddities where you are training in one language and then running in, for instance, another language. Create ML is simple and very powerful. It is tailored to your app. It leverages core Apple technologies, and you do everything on your Mac. So for this year we are going to focus on three very important use cases. The first one is images, second is text, and the third one is tabular data. These are the top use cases that we believe will benefit you. So you can do things like custom image classifier. Idea is that you make your own image classifier that can recognize product from your product catalog. You can do things like text classifier so you can make your own sentiment analysis, topic analysis, domain analysis. And you can also do classical regression and classification on tabular data. For example, let's just say you want to predict the wine quality using its chemical composition. The possibilities are endless, and we are going to discuss them in detail in the next 30 minutes. However, before we do, let's take a look at common workflow. First, let's just say you are trying to enable an experience in your app, make sure that machine learning is the right thing to do there. So don't just blindly apply machine learning. Make sure machine learning is the right thing to do there and define a machine learning problem. Second, collect data. Make sure this data reflects the real usage of your app. So, for example, if you're making a custom image classifier that is going to be used by users on their iPhone, so collect pictures from your iPhone. Do not collect -- collect less screenshots but have more iPhone pictures. Then you train your model. Finally, an important step here is to evaluate this model. The model evaluation is done on a separate handout set. If you're happy, you write out the ML model. But let's just say the results are not good. You should either retrain your model with different parameters or you collect more data. Create ML actually helps you across all four stages of this workflow. We have powerful in-built data [inaudible] utilities, data source and data table that we will talk in the remainder of the presentation. You can actually train your model using only one line of code. And the training is done hardware optimized. There are built-in evaluation metrics, so you don't have to write your own precision and recall and confusion metrics calculation. Use them. And finally, when you're happy, just write out the model. Now we will take a deeper look in all three use cases: images, text, and tabular data. So let's start with images. And to do that, I will invite Lizi Ottens, Senior Engineer in Machine Learning team. Thank you. Thank you, Gaurav. Since enabling image-based experiences are some of the most powerful and interactive ones that you can add to your apps, today we'll take a look at how to train custom image classification models. Image classification is the problem of identifying what label out of a set of categories you'd like to apply to an image. Depending on the type of training data, you can target domain specific use cases to enable in your apps. The first step is to collect training data. In doing so, we'll take a look at a fruit classifier and see how you would do so. First, you'd want to gather many varied types of images that reflect the true data that you'll end up seeing and then label them. First, you can do this as a dictionary with the string label corresponding to arrays of images. Or what we've noticed is many popular data sets are organized in hierarchical directory structures such that the label is the name of the folder that contains all images within it. There are also other data sources such as single folders that contain labeled filenames. And in the Create ML API, we've provided conveniences to extract these structures. Now training is the more complex part of the equation. So once you have your data, this is what you will get next. And what you can do is you can start training a very complex model from scratch on your input images. And for this you need lots and lots of label data. You need big compute and you need a lot of patience. But another well-established technique in the industry is transfer learning. And since Apple has lots of experience in training complex machine learning models, we already have one in the operating system that you can take advantage of. So what we do is we apply transfer learning on top of this model that already exists in the OS, and we augment it, retraining the last few layers to your specific data so you no longer need millions of images. You can train a good classifier using the amount of data that you have. This results in faster training times. And for developers that we've worked with, we've seen them go from hours of training down to minutes for thousands of images or for small data sets, even seconds. This also results in much smaller models going from hundreds of megabytes down to just a few megabytes for thousands of images or even kilobytes. The goal of Create ML is to abstract much of this and make it simple and easy to use. But to prove it, let's take a look at a demo. First, to set up the problem, I started by running an app that's using a state-of-the-art image classification model that's already in the industry. This one, though, is quite large. It's 100 megabytes in our app. And if we run it, we have some fruits but it's not quite what I was looking for. I'd really like it if, instead, we could classify these particular ones. So what we can do is we can switch to a new playground and import CreateMLUI and walk through how to do this using the UI for it. We can define a builder. Initialize it. And to enable drag-and-drop training, we can show the builder in the live view. This brings up a prompt in the live view to drag in images to begin training. And here I set aside some photos of fruits. Here's some blueberries and other types. And you can drag them in and automatically an image classifier model begins training on the Mac. All of this is accelerated by the GPU on however many categories you end up training on. It automatically tells you what the accuracy is on the training data set, but what's more helpful is to try this on new images that the model hasn't seen before to predict how it will do on real use cases. So I can drag in this other folder containing unseen images. And now the model is evaluating all these new types of fruits. And if you scroll, you can see what the true label is of each type as well as with the predicted one was by the model. And if you're happy with this accuracy, what you can do is you can take the model and drag it into your app. I'll add it here. And if we take a look, this model is 83 kilobytes. It's a huge savings down from hundreds. So we can delete the old model that we were using before. And in the view controller, we can initialize this new one, ImageClassifier. We can then re-run the app, bring up the simulator, and see how it does on some of those fruits. On the raspberry, it can now correctly predict it since we trained the model to recognize raspberries. We can even see if it can distinguish from strawberries and it can now. But there are other workflows you can use. Perhaps you want to do this programmatically or perhaps you want to automate it. We can also walk through how to use Create ML to do so. So now we can switch to another playground and import Create ML. Since we'll be using URLs, we also can import foundation. And since, on our desktop, we still have these folders of fruits, we can say where they are and also say where the testing fruits are. And then the next step is to actually train the model. So we can define a model, and we can initialize an image classifier. And now if we take a look at what auto complete shows to us, we can see we can provide training data in the form of a dictionary of labels to arrays of images or we can use a data source or even specify model training parameters if we want to. Let's use a data source. And we'll use label directories since that's how our data is organized and specify the training directory. And since we're running in the new [inaudible] mode of Xcode playground, I just need to hit shift enter and the model begins training right away. You can even pull up the console and see output of one, its extracting features and how many iterations it's running through. Afterwards, you can also open quick looks and see the name of the model and how many instances it's trained on. Now we might want to evaluate on the testing data that we've set aside. So what we can do is we can call evaluation on another data source since that folder is organized the same way, specifying the URL of the testing data. You can hit shift enter and now the model is evaluating testing images. Once it's complete, we can also look at the quick look and see how many examples it evaluated on as well as how many classes were in that folder altogether and the accuracy. If we're happy with that, we can write it out. And say that I want to write it to the desktop with the name fruit classifier ML model. Once I do, you can see this new model appears on the desktop. We can double-click it and take a look and see it's exactly the same. This is also 83 kilobytes. Furthermore, we can integrate it back into our app the same way. Let's recap. We saw two ways of training image classifier models in Create ML. One was with the UI which makes it super simple to drag-and-drop your training data and evaluation data to produce an ML model. The other way was with the Create ML API. If we walk through some of this code, we can see the first thing we had to do was import Create ML. The next was to specify where our training and testing data was and then actually begin training the model by specifying how our training data was laid out. We can then evaluate on the testing data and finally save ML model. If you want to automate this, you can also turn these into scripts, which is a very popular way of saving what you've done and re-running it whenever. You can then change permissions on the file and run them like so. Or for other workflows, you can always use Swift command line [inaudible]. So we've seen today how to train image classification models using a few different workflows. But next, I'd like to pass it off to Tao to talk about natural language. Thank you. Thank you, Lizi. Hello everyone. My name is Tao. I'm an engineer here at Apple working on the Core ML team. You just saw how easy and intuitive to train an image classifier with just a few lines of code. Now I'm going to show you the same can be done for natural language. In this year's release, we're going to support two natural language tasks: text classification and word tagging. Today, I'm going to focus on text classification. For details on word tagging, please join the natural language session that happens tomorrow. Text classification can be used in a few machine learning applications. For example, sentiment analysis. The energy of developers is amazing. That's a positive note. You want your app to know it. Spam analysis. If you saw this message in your mailbox, you know it's very likely it's spam. So you want your app to know that as well. Topic analysis. The Warriors just had an amazing comeback win. That's a sport post. You want your app to be able to classify that. So to train such a classifier, the first thing you do is to collect some training data. With Create ML, we support a few different ways for you to organize your training data. For example, label directories. Here you have two folders. One named positive, the other one named negative. Within each folder, you have a number of articles with just raw text whose truth label is simply the name of the folder. Alternatively, you can prepare your training data using simple CSV where you prepare your raw text and the truth label separated by comma. We also support JSON formatting the training data and know that we just talk about the training data organization and you can actually organize your test data in the exact same way. Now with your training data and test data ready, what other steps involve to train such a text classifier? A typical workflow would look something like this. You start with your raw text. You do a language identification to figure out which language it is in. You convert that into tokens. And then you convert that into some feature values and then you can apply a machine learning model that gives you some predictive value that you have to map to some desired label. And then you can compare that label to your truth label and start iterating on it. With Create ML, though, we took away all these complexities so that all you need to do is to prepare raw text with their truth label and start training immediately. Now let me give you a concrete example like how you can train such a classifier and use it. For example, we have this simple app called Stay Positive whose purpose is to encourage positive post. If a user entered I hate traffic, the background turns red and it will disable the post button. I love driving my car at five mile per hour just chilling in traffic. That's a positive post. We encourage you to post it. Just imagine what our Internet would look like with this app running on everybody's phone? Now, in order to do that, let me give you a live demo. So to train such a classifier, the first thing I do is collect some training data. On my desktop, I have a train folder and also a test folder. In train folder, we have two folders. One is named positive, the other one negative, and there are a number of articles in each folder. And test folder is organized in a very similar way. So the first thing I do is to import Create ML. Now I need to tell the [inaudible] where to find my training data. For that, I'm simply using a URL capability and then I can start training my model using the label directories that Lizi just showed you. Look. The training has started. As you can see on the bottom there, there is some progress report for you to check. Looks like training has finished. Now you can check some basic performance numbers on this model. For example, model.trainingMetrics that shows you this model has been trained on over 2000 examples and accuracy is 100%. But how does it perform on some unseen data? So I'm going to do the same to define test data and then evaluate that model on the test data. As you can see, we have 77 test examples, and we are achieving over 94% accuracy, which is very good. I'm sure you want to iterate on that if you want to see like even a higher number, but this number is pretty good enough for my app so let me just test it out. So to save out the model, what I need to do is define a URL where it's saving to and then write out a model to my desktop. Looks, that model has been saved. So now I need to switch back to my app. Just drag and drop it. There you go. Now I can start use it. I will do let model equal to textClassifier which should auto complete. And then I'm going to insert some basic inference code. In this inference code, as you see, the first line I do is using model.prediction to get prediction. And then in order to hook up with this simple app UI, I just convert that into some double value. Let's give it a try. Yeah. Let's try some example we have showed you. I hate traffic. Negative. I love driving my car at five mile per hour just chilling in traffic. Positive. Let's try something different that'll be fun. Machine learning is hard. Create ML makes it so easy. Positive. So that's how you train your customized text classifier and drag it into your app to use it. Here's a recap. So to train such a classifier, the first thing you do is to specify your data. You specify your training data as well as your test data and then you can create your model on the training data. To evaluate its performance, you evaluate a model on the test data. Finally, to use your model in your app, you simply save it out using this write API. To summarize, with just a few lines of code, you can train your customized text classifier simple intuitive. With that, I'd like to hand back to Gaurav who is going to talk about tabular data. Thank you. Thank you, Tao. Besides images and text, another common source of data that occurs very frequently when you're solving a machine learning problem is tabular data. What I mean by tabular data, I mean the data is in special format or in a table format. This kind of data occurs fairly frequently. For example, let's just say you're trying to predict house prices using number of beds, number of baths, or square footage. Generally the data is arranged in a tabular format. You want to predict the quality of wine using its chemical compositions. Chances are data will be arranged in table format. Or something even simple like where to hop, which bar to hop tonight using happy hour or its price, the data will be in tabular format. To handle the data which is in tabular format, we actually introduce a new data structure which we call as MLDataTable. MLDataTable is based on [inaudible] technology that we will discuss in detail tomorrow. There's something interesting about these data tables. The rows contains the observations or examples. So here, house number two has four bed, three bath, and 500K price. The columns contains what we call as features. So the beds are features, baths are features, square feet, etcetera are features. There is one special column that we want to predict, in this case price, and this column is known as target or response variable. The whole idea behind tabular data is that we want to predict target variable as a function of one or many of these features. So what are the common sources that we support? Well, CSV, JSON as well as you can actually have code. So let's talk a little bit more about MLDataTable. First, you can read data simply by using CSV. What is more important that you can access the column using a subscript notation. So all you do is house or the price and you get an entire column of price. You can add two columns, subtract two column, multiply two column, divide two columns. And the way you do it is in very natural looking syntax. So you just simply say house or the price divided by house or the square foot to get price per square foot. Behind the scenes, this calculation is done using [inaudible] evaluation and through vector operations. It can also do some of the other interesting things. For example, you can split data table in training as well as you can even do filtering. So for example, if you're only interested in large houses, you can create an indicator variable and filter it out. There a lot of operations that data table support. I urge you to try it out in Xcode playground. They're fun. Now once you have data in data table, you would like to do the training on it. Create ML supports a large number of algorithms such as Boosted Tree Regression, Random Forest, etcetera. And all of these algorithms are represented by their class. In order to train your model, you only have to write one line of code. Basically, you tell what is the target and where you're getting the data and which is the algorithm you are instantiating. So in this case, let's just say you are running Linear Regression or Regularized Linear Regression, you just actually tell it that the data is house data and the column is price. If you do Boosted Tree Regression, just replace Linear Regression with Boosted Tree and you're all set. Now Random Forest like that. Plus we also provide a high level abstraction MLRegressor that automatically runs all these algorithms and choose the best one for you. This is in line with our philosophy that you should focus on task. So the task is to predict the price. You should not focus about nitty-gritty details of the algorithm. Having said that, in case you're an expert, you can actually use Boosted Tree and change its parameters also. So a complete end-to-end would look like this. It follows exactly the same pattern as image and text. First, you specify the data. Second, you just create your model. Third, you evaluate the model. And once you're happy, you save it out. So tabular data, image data, or text data, they all follow the same pattern. So let's just take a quick summary of what we saw in this session. So Create ML is our ML framework insert. It's very simple to use and it is very powerful and it leverages core Apple technologies. You do end-to-end machine learning in Swift on your Mac. We also discussed about our workflow. Once again, you start from an experience. What is the experience you're trying to enable? Then define the problem. Then collect the data. Make sure this data is reflective of the real-world usage of your scenario. Then you train the model. And finally evaluate it. And once you are happy, you just save it out. Create ML is in Swift. And it's available on macOS Mojave. You can use it in Xcode Playground, Swift Scripts and [inaudible]. So please try it out. We would love to hear from you. We are here to receive your feedback, and we hope that you will love it as much as we do. We will be in the machine learning get together as well as the labs. So there is -- tomorrow there is a get together. We will be in labs also, so please give us your feedback. There are also related sessions in the WWDC App. We have Core ML session tomorrow morning and ML session tomorrow afternoon, Vision sessions on Thursday. And we have labs on Wednesday and Friday. Thank you.  Good morning. Wow. Thank you. Thank you, very much. And once again, welcome to your first session at WWDC. My name is Joaquim. I don't think I have to reintroduce myself. Karan, Dongyuan, and I are really excited to talk to you about internationalization, at long last. So, what are we going to talk about? I'm going to do a brief introduction, like I started doing. And we're also going to do some deep dives on making your app's layout adaptive. And also, cover some really fun discussions on text, as well. So, why is this important? This applies to every single app. Internationalization is a crucial concept and we're going to go over some of the details as to why it applies to every single app, regardless of the set of languages that you support. And once again, this is all about reaching out to more people. It's enabling everybody to learn about and use your app. Regardless of the language they speak or the country that they live in, or the country that they grew up in. And it's important to note that those last two might be different, as well. And that's something that's worth keeping in mind. So, internationalization covers a wide range of topics, and potentially, every single aspect of your app. And if you're new to the topic, probably, the best way to think about it is that internationalization is best applied as a constant process. It's a discussion that belongs in every single stage of your app. Whether it's the initial inception, its design, to its implementation, and even distributing it on the App Store. Things like your App Store screenshots and even your app's name should be open for discussion in an international context. And like I said before, this covers a wide variety of topics. At Apple, we have a lot of firsthand experience about this. So, we estimate that over 70% of our customers are outside of the United States. And this means that when somebody downloads your app they expect that app to match your language preferences. And not only that, but also, be aware of any details that might be related to their region settings, as well. We support exactly 38 different written languages and many, many more keyboards and input methods for people to type and text. And what this means for you, is you have a rock solid foundation to get started with. Every single one of our API's supports these languages and input methods. And this means that you can get started right away by using these and you're off to a great start in providing really great international support to your app. So, for example, this might entail presenting dates and times. This wouldn't be an international talk if I didn't talk about dates and times. But it's important to think about, right? Because people have different expectations around the world. This is the [inaudible] Sao Bento in Portugal. It's a lovely train station. But my focus today isn't so much the train station as much as the way the times are presented in the train station. And if you go there, you might notice that everything is in 24-hour time. And this is true, not just in train stations, but it's commonplace everywhere in many countries. And so, your app should know about these and present time accordingly, depending on the region. And you can do this using date formatter. It's a set of APIs we provide that lets you do this. And it really does all the heavy lifting for you. Another example is calendars. There are many different calendrical systems around the world. And it's also important to note that many people use more than one on a daily basis. They might use one for religious or festive events and another for business. And these all have different properties like leap months and leap years and different numbers of days and different numbers of months. And so, being precise about these and knowing how to present these appropriately means being internationally aware. This is an example of the iOS Calendar app showing the Chinese Lunar calendar on top of the Gregorian calendar. And we have a set of API's in Calendar and alongside Date Formatter that help you do this. Could also talk about units and measurements. Whether it's presenting things in the metric versus the imperial system, or even temperatures like Celsius or Fahrenheit. Your app should be aware of the appropriate regional defaults for these, and also, respond to changes in the user's preference. And not only that, whether they're units or not, even numbers themselves. There are so many things to cover there from the decimal separator to the thousand separator. And even, the entire class of digits used to represent and talk about numbers. These can change between regions. And if you use number formatters, your app gets this behavior for free. And this is important not only for consistency, but also, understandability. Text is, also, another great topic with many different areas. And depending on the language or languages that you grew up learning and writing, and the scripts that those language are written in. You might have thought that there are some properties that are completely immutable across other scripts and other languages. And that might not, necessarily, hold. One example, probably the biggest example, is script directionality. So, some languages, like Arabic and Hebrew, and in this case Urdu, are written and read from right to left. Which, is completely the other way from English, from what you might be used to. And taking this further, many books, most books published in traditional Chinese and Japanese are presented in vertical and right to left. And so, knowing about these formats and knowing about the context in which it's appropriate to present these and adapting your layout for these could be an important aspect of your app. And are why frameworks alongside our text frameworks like TextKit and CoreText can help you make sure you're doing the right thing in every single one of these cases. Going a little bit higher level, even things like names. Depending on where you grew up, you might have a first, middle, and last name. Or a family name that comes before your given name. And again, knowing how to present these, and even asking your user about these could be a crucial part of your app. And this is something that users expect that you get right and know about the details and intricacies of formatting names. And we have PersonNameComponentsFormatter API that help you do this. Okay. So, throughout all of these concepts it's also important to note that many of the users might interact with and speak more than one language on a daily basis. Whether it's because they moved countries, or simply because that region, in and of itself, uses more than one language. And taking this example further, even if your app only supports one language it's very likely that your customers are using your app to publish and consume content in their own native language. And so, regardless of the set of languages that your app supports, these are all still very important topics to consider and talk about throughout the design and implementation of your app. So, if all of this is new to you, worry not. Like I said before, we have a great set of APIs that really do all the heavy lifting for you and take care of all these different aspects. And are aware of all the different details between languages and regions. These are some examples of the Formatter APIs that I talked about. And we've, also, got some great sessions that go into great detail on how to further tailor these for your app, as well. So, let's start our first deep dive in to layout and some of the goals related to internationalization when it comes to adaptive layouts. Because, really, the core goal of an adaptive layout is to present all kinds of different information. And when you're adapting your application for other languages, probably, the biggest guarantee that you'll get is that these translation lengths are going to be different. They're going to be much shorter or much longer, depending on the language. And this is something that your design and layout should adapt for. On top of this, there's also directionality, like I mentioned before. Because some languages are written in right to left this has some design considerations, not just in the text that you have in your app, but also, the way you present information. Especially, horizontally flowing information and how that general flow should adapt for both script and writing directions. So, a great starting point to help you do all of this is Auto Layout. You might have heard of Auto Layout, before. This is a powerful technology that is at the core of our layout engine. And Auto Layout, instead of describing explicit frames or positions of your controls and labels, describes constraints or relationships between these views. And therefore, how they're positioned relative to each other and how they're allowed to grow relative to each other. On top of this constraint-based system we have the idea of leading and trailing constraints. And what this means is that this, essentially, describes properties that are left and right in a language like English that automatically evaluate to right and left, respectively, in a language like Arabic and Hebrew. And this means that with Auto Layout you can create these adaptive layouts that flow depending on the writing direction without having to write special code for either one of the writing directions. Another great starting point is to use high level components and containers that we provide in our UI frameworks. Because these use all of these concepts and are aware of how to be adaptive. Some examples are collection views and stack views that we provide, both in UIKit and AppKit. And you can create really complex layouts and even embed these within each other to, potentially, even just create the whole layout of your app just using these. Just as an example for Stack View itself, I could arrange these horizontally in a Stack View. So, I have the city name on the left and the time on the right. And this is, this could just be put into a horizontal stack view. And because this flows left to right in English and because I'm using a StackView if I run my app in Hebrew, this is the result I would get. Because that information would adapt automatically. Because StackView is aware of these concepts and uses Auto Layout under the hood. If you'd like to learn more about Auto Layout and details on how to adapt your application for different writing directions, these are some great starting points to check out, as well. So, when it comes to the text in your app and the content in your app the key goal here and, obviously, the number one recommendation is to simply not assume fixed widths. If you don't do that you're off to a great starting point in terms of allowing your app to adapt to, not only different changes in length horizontally. But even, potentially, allow your labels to grow vertically and adapt to multiple lines if necessary. And this is something you'll have to decide in your app and your app's design on what you want to prioritize in terms of what is allowed to grow and use up the real estate of your app. If your labels and your controls are either in stack views in these high-level components that I mentioned before, or positioned using Auto Layout itself, this work is pretty much done for you. You're already allowing your labels to grow and your controls to take up the content that they need to take up. And then, it just becomes a matter of prioritizing how you want to let them grow relative to each other. So, this is great. Let's say you've done all this. And how, exactly, do you make sure that your app is checking off all the right boxes in terms of adaptive layout and responding to all of these things? Well, the good news is that Xcode provides a number of features for you to test. And not only that, it allows you to test early and find these out quickly in the development, and early on in the development of your app. One example is pseudolanguages. Pseudolanguages are great, especially, when you haven't yet added language support to your app. So, what you can do from within Xcode is run your application in a pseudolanguage and this changes a few details of the layout and text of your app. One example is the bounded string pseudolanguage. This adds a few characters at the beginning and end of every single UI string that you display. And then, you can see, make sure that there aren't truncations or unexpected clippings in the content of your app. So, this is a really useful one. And we have many more pseudolanguages that can help you out, as well. Specifically, for Auto Layout we have Auto Layout warnings built into Xcode. So, Xcode can tell you about common antipatterns right within an interface builder. And this can be things like fixed width constraints or too few constraints on a control or label that might, potentially, introduce issues at runtime, especially, in other languages. So, with that, I'd like to hand it over to Dongyuan, who's going to give you a demo of all of this in action. Thank you, so much. And hope you have a great week. Thank you, [inaudible]. Thank you, Joaquim. Hi, I'm Dongyuan. Let me show you some techniques to [inaudible] and some common pitfalls to avoid in real world. Here's an app called Vacation Planet, which is the first interplanetary travel agent that allows you to book travel for not only other countries, but also, as you can see in this table view, other planets. So, I wanted to go to the moon for quite a few times. So, I can do that, this time. And here are all the locations available on the moon and their distance from Earth. Let me choose the location like the Clavius Base. And here is our Travel Details page. It seems pretty cheap for one trip. So, I'm going to buy more tickets. Let's buy three of them. And there we go. As you can see, because we design and developed the app in English our layout works great in English. However, we still want to make sure it can adapt for other languages. Because we are still early in the development cycle and we haven't localized the app, yet, we can use the pseudolanguages in Xcode. To do that, I'm going to go through the Current Scheme, click Edit Scheme. And in the scheme editor I have an Application Language selector. I can select one of the pseudolanguages like the Bounded String Pseudolanguage. It's very useful for exposing potential clipping or truncation issues. Let's run the app in this configuration. As you can see, this pseudolanguage adds a few special characters at the beginning and end of every UI strings. Our tag line here is still good. But the Browse button is truncated. Let's try to fix that. When I select the Browse button I can see that there's a wording in Xcode. The wording shows that we have a fixed width constraint and that may cause clipping. If I want to know more information, I can click on the I button. I'm going to fix the issue by click the growth, sorry. By click the Warning sign. Here are three options. I'm going to select the first one, which is simply remove the constraint to allow the button to be wider when there's more content. Let's do that. Okay. No other layout issues. And then, I'm going to show you a very simple way to verify the change that was made. I'm going to go to the Assistant Editor at the top right corner, the middle button here. Here I can select Preview and Main tell Storyboard. The Preview pane allows you to view your layout in multiple screen sizes and different languages without running the app. On the bottom right corner, I'm going to select the first, the same Bounded String Pseudolanguage we just used. Great. Because we don't have the fixed width constraint, anymore, our Browse button can now accommodate a slightly longer string. Another very useful pseudolanguage is the Double Length Pseudolanguage. It's for verifying your layout against potential languages that have longer string length, like German, Finnish, or Russian. They can sometimes be two times longer than English. Let's see if that works. Now, the Browse button is still okay. But our tag line is clipped by the screen boundaries. When I select the label, I can see that we only have a center X constraint. We don't have any leading or trailing space constraint, so that the label can overflow. Let's add a leading space constraint to fix that. Again, I'm holding down the Control key, drag from the label to [inaudible] view, and select Leading Space to Safe Area. I can select the constraint to adapt at just its value. Let's put something reasonable, like 20, for the margin. And here, I can see instantly that my label is no longer clipped. However, it's still truncated and it's a very important message we want to show to all of our customers. That's not ideal. Instead of truncation, I can allow the label to wrap into multiple lines when necessary. Here, when I select the label I can see that we have Lines property set to one, which means we only allow one single line for the label. If I change the value to zero I can allow the label to wrap into any number of lines, when necessary. I can see that my label is now three lines in this Double Length Pseudolanguage. If I switch language back to English my label is still one line, which is exactly what we expected. I, also, encourage you to check the app down here, where you can select different screen sizes. I encourage you to check your layout in the smallest device, like iPhone SE, because clipping and truncation are more likely to happen in the smaller devices. Now, I wonder if our layout can adapt for right to left languages, like Arabic or Hebrew. As I said earlier, we don't have localizations for those languages yet. We can run another pseudolanguage. Let's open the Scheme Editor, again, this time in application language. And I'm going to select the Right to Left Pseudolanguage and run the app in the simulator. Okay. Browse. As you can here, our table will, now flows from right to left without us making any changes. That's because we used UITableView and other standard UIKit components. The system had done the hard work for us. All the section titles are now at the right side, which is the leading side for right to left. The chevrons here are at the left side, which is the trailing side. I'm going to go to Jupiter, this time. And you may notice that the Back button is now at the top right corner, instead of top left. That is, actually, very natural for right to left languages. Let's select a location on Jupiter. So, here's our Travel Details page. Everything seems to be okay, except the stepper and the Traveler label. The first issue is that the stepper should be at the trailing side, which is the left side for right to left. The Traveler label should be at the leading side, which is the right side, here. And also, there's an unnecessary spacing for the Traveler label. Let's fix that in that Interface Builder. Let me locate our trouble zone cell, is here. Okay. Zooming in a little bit. When I select the Traveler label and the stepper I can see that there's no constraint on them. That's not good. And of course, Xcode has a warning for that. So, one way to resolve the issue is to add a leading space constraint for the Travelers and a trailing space constraint for the stepper. Leading and Trailing will translate to right and left for right to left languages. However, here I'm going to show you an even simpler way, which is to use UIStackView, a high-level container view that uses Auto Layout under the hood. By using Stack View I can get right to left support for free. Let me select the two views. And click the Embed down here at the bottom right corner. Select Stack View. Now, my two views are inside this UIStackView. The only thing left is to add constraints to the Stack View itself. I'm going to select Stack View, click the Add Constraints button. I'm entering four zeros, here, because I want the Stack View to fill the table view cell as much as possible. And I like to select the Constrain to margins here, because I want some default margins for the Stack View. And I want the leading edge to align with the cell separators. And add four constraints. There you go. And I'm going to verify my change. Yep. Browse. This time, I'd like to visit Earth. And I'd like to go through Lisbon, Portugal, to visit [inaudible]. Do that. Now, as you can see, because our Traveler label and the stepper is now inside a UIStackView, we get right to left support for free. Let's book this travel. Great. Now, let me summarize what we talked about. So, to make our app layout great for our global audience, there are a few simple steps. First, is to use high level containers like StackView whenever possible, as they provide a lot of heavy lifting for you. And believe me, they are just simpler to use. For finer control, remember to use Auto Layout and make sure to use leading and trailing constraint so that you can adapt for right to left languages. For early testing without even localizing your app, you can use the pseudolanguages in Xcode Scheme Editor. And please, don't ignore the Auto Layout warnings in the Interface Builder. They are very useful for avoiding clipping, truncation, and overlapping issues. Now, over to Karan to talk about text. Thank you, Dongyuan. Good morning, everyone. Let's talk about text. At Apple, a high quality typographical experience is a key part of our design process. And it's very important to us how text looks on screen and how it's designed. And also, the way that it translates to other languages. So, let me walk you through some key aspects that we keep in mind when we localize our own apps into other languages. And how you can take advantage of these things to make your apps look great in other languages. Now, I'm going to talk about three main topics, set up some fundamentals with languages and scripts. And then, dive into typefaces and styles. Let's talk about languages and scripts. What is a script? So, when I'm talking about script, I don't mean a Bash script or a Python script. I'm talking about the way that a language is written. So, the writing system. The letters that you use to write a language. These are some of the scripts that we support that are written from left to right. We, also, have scripts that are written from right to left. The key thing to note about all of these scripts is that they're multilingual. So, each script supports a huge variety of languages that are written in it. And as you can see, the Latin script here supports everything from English to Vietnamese. And this is true for other scripts, as well. For example, the Cyrillic script supports a variety of different languages. And if you look at right to left scripts, we see that the Arabic script, not the Arabic language, but the Arabic script supports a variety of different languages, such as Arabic and Persian, Urdu, etcetera. So, what you're seeing on screen, yes, there's a lot of visual variety. But this is not just for show. There's, actually, a lot of implications when you develop your apps and different scripts because some concepts don't map across scripts as easily as others. So, let me show you a few examples of this. Let's talk about typefaces. So, here you see the Health app in English. So, you see a lot of labels. Now, you see the same app in Catalan. And lastly, you see the same app in Vietnamese. Now, the thing that I'd like to call your attention to is that all the text here on screen is rendered using our system font, San Francisco. And the other thing I'd like for you to notice is that everything is rendered beautifully. That's because San Francisco has support for a huge variety of different languages. And when you use the system font in your apps, you're guaranteed to get that support for free. Now, any text label that you create in Xcode will get San Francisco by default. But if you want to go one step further, use a text style. We support a variety of different styles in our OS that are meticulously implemented to support a variety of different use cases. And also, to map across different languages, really well. So, when you use a text style you guarantee a consistent high quality experience to your users. You can also go one step further. If you set your label to automatically adjust its font size, it will do so with respect to the user's text size setting. And this is really handy for people who use smaller or large sizes for their text. Like myself. So, I highly encourage you to use that and this will ensure that your text styles scale appropriately. But let's say, that like us, we're making this Vacation Planet app to go to Jupiter, to go to Mars. And we wanted to have this fun vacation look to it, which it doesn't right now, because everything is in the system font. So, we decided that the title of the font should evoke the personality of the app. And so, we looked at a few different fun fonts to choose from. We looked at this one, for starters. But as Joaquim mentioned from the start, we like to keep localization as a central part of our development and design process. So, the first thing we did was to check does this work for all the languages we need to support? Well, we already had a French localization. So, we tried it out. As it turns out, it doesn't. So, we kept looking at our short list of options. And we looked at this other typeface. And when we tried it out in French, voila! It works because it supports all the characters needed for French. So, that's great news. So, the next step for our app was to expand to Vietnamese. But we didn't have a Vietnamese localization, yet. So, one great tool that you can use is FontBook, which comes installed on every Mac, even if you're not a developer. It comes installed on every Mac. And in FontBook you can easily search for the name of a language, like Vietnamese, here. And we see here, that in the font that we chose Vietnamese is in the set of supported languages. In fact, this font also supports Cyrillic and Greek script. So, we are somewhat sure that if we expand to Russian and Ukrainian and Greek later that this font will work for us. Now, I should advise some caution here. Just because FontBook says that a font supports a given language doesn't mean that you don't have to, actually, test your app in that language. You still need to make sure that the font really works for that language, by trying it out. So, we've got our beautiful fun font for our Vacation Planet app. And now, we're going to expand even further. So, we want to do more languages. Specifically, we want to localize into Chinese. Okay. So, we sent off all our strings for translation and they've come back. And our app is now perfectly localized into simplified Chinese. Cool. But it's not really, though. Because look at what happened to the title. So, in English, we have this fun font, but in Chinese we are using the system font. That's because our font only supports the Cyrillic, Greek, and Latin scripts. So, of course, the way to fix this is to repeat the same process for Chinese. And here now, we have out fun font in Chinese. It's as simple as that. And unfortunately, I can't really help you with that. That's a stylistic thing and you have to choose your own fonts. But I can show you how this is done in code. So, this is really straightforward. First of all, you start with your font for your development language, normally. Such as, in our case, it's English. So, we choose our Latin script font and get our font here. And the key concept that I want to introduce you to, here, is called a Cascade List. So, a Cascade List says if I'm looking to render this Chinese character and this first font doesn't have it, what font should I use after this to look for this character? Now, if you don't specify a Cascade List you're going to get the system font. But if you do have a Cascade List, then you can specify other fonts to try before falling back to the system font. So, in this case, we create a Cascade List with a font descriptor for our font for Chinese that we hand-picked. Now, if your app supports multiple scripts you add multiple things here and that's as simple as it works. And once you've got a Cascade List you create a new font descriptor. And then, you create a new font. Pretty straightforward. And also, make sure that if your app used dynamic type, which it really should, then your font should adjust to that, as well. And again, that's as simple as an API call. Let's see some examples. So, this is the new Word of the Day screensaver in macOS Mojave. And as you can see, for this new design we've chosen a rounded style. And of course, it wouldn't make sense if we didn't, also, choose equivalent rounded styles for all the languages that the screensaver supported, which includes Japanese. And also, new in macOS Mojave, simplified and traditional Chinese. Another example is the Messages app, in which you can respond to a message using a tap back reaction. Now, here you'll see that the ha-ha in English has been not only translated to other languages. But has, also, been matched in style, so you get the same bubbly fun look in all the languages that we support. That's it for typefaces. Let's talk about styles. Again, I should start with the definition. What do I mean by a style? So, broadly speaking, I'm referring to aspects of your text that you choose once you've chosen a typeface. So, let's say the font weight, like how bold it is, or whether it's italicized, and what the size of the font may be. So, the key thing to keep in mind is that some aspects translate better to other languages and others don't. So, let's take a look at an example of where something might not translate. So, here we have a simple string in English. We've italicized Mars and 2 Travelers to indicate that they're variables, so they can change. And this is how it translates into traditional Chinese. There are a couple of issues here. Mars is italicized in English. It's not italicized in Chinese. Why? Because italicization is not a concept in Chinese. And it's not a concept in, actually, most scripts outside of Latin, Cyrillic, and Greek. So, a design that uses italics, probably, won't work. The other thing to note is that in Chinese there are no spaces between words, and also, there is no concept of uppercase and lowercase letters. So, you kind of lose a natural distinction that you get in English when you translate it into Chinese. A couple of small things to note, also, is that the word order is different. And also, because the 2 still comes from the Latin script and is still italicized, it doesn't look great. So, how can you fix this? Well, the keys to realizing that you're doing emphasis, not necessarily italics. And emphasis can be done in multiple different ways. This is a great way to do emphasis that works across several different languages. In fact, it works for all the languages that we support, which is to bold a given word. Lastly, let's talk about emphasis at a character level or sub-word level. Let's say we have an app with a Search feature and we want to highlight the part of the results that matched to make it clear to the user what's happening. Now, this works really well for English, by using a bolder weight for the matched segment. It works really badly for Hindi. So, what you're seeing here, so anybody who can read Hindi will tell you that everything on the right-hand side looks completely broken. Any time you see a dotted circle inside a Hindi word, something has gone very, very wrong. And the reason this is happening is because even though it's the same font family, different weights within that font are, actually, a different font. And you can't have the appropriate joining behavior for languages like Hindi if you have two different fonts. So, one easy way to solve this problem is use a different kind of emphasis at a character level. So, at a character level you can use something like using a different color. For example, here we use black color for the math segment and gray for the remainder of the word. This works really well. Now, you'll see some dotted circles on the keyboard. That's perfectly fine. They're dependent marks. And here's the same example in Arabic. Again, the same approach is used all over iOS and macOS, and it works really well for many purposes. And I should also mention that this is really easy to do with attributed strings. Finally, let's go over everything that we've talked about. First off, it's important to start planning early in your apps, not only the development, but also, the design phase. Once you know the languages you're going to localize into and plan to extend to in the future, internationalize as you go. Technologies like Auto Layout, StackViews, and dynamic type are very simple to adopt while you're in the process of building your feature. But if you finish your project and try to come back and use them, then you may have to completely rearchitect your app. And that will be a lot of work. If you're doing something for which you feel, oh, yeah, there should be an API for this, right? There probably is an API for it. So, make sure that when you're doing something like formatting any kind of data, to make sure to look to see if there's a formatter class. And also, make sure to look to see if there is an API for anything you're doing with text before trying to ruin your own implementation. And lastly, ensure that nothing is lost in translation. So, the key thing to realize here is that every localization of your app is a unique experience. And you need to make sure that your intent that you specified in your development language actually makes it over to all the other languages that you support. And that nothing is lost along the way. Thank you, very much.  Thank you. Thank you. Good afternoon, everyone. My name is Phil Azar, and I'm a software engineer on the Power Team at Apple. Today, along with my colleague, David, I'm excited to share with you what's new in energy debugging. Battery life is hugely important to our customers. The Power Team strives to make sure that everyone can get through the day on a single charge without having to plug their device in. We work with our developers here at Apple to try and optimize battery life by guiding them and helping them make design choices that are energy efficient. Apps are also hugely important to our customers. In fact, we find that most usage on device is directly attributed to third-party apps. This is incredible, and it makes it more important now than ever before to focus on energy efficiency in the design of your application. To that end, we're going to talk about three things today. First, we're going to talk about some battery life concepts that you can employ in your application to make sure that you are being as energy efficient as possible. Then, we're going to talk about some tools that we have available for you to understand and quantify where energy is going in your application. And finally, I'll pass it on to my colleague, David, who is going to talk about a new tool that we have available to take your energy debugging one step further. So, let's go ahead and get started and talk about some general concepts. To make a battery life great for our users, we have to start with first principles and understand what makes battery life battery life. So, let's start. What is energy? Fundamentally, if you think back to physics, energy is the product of power and time. As your app is running on any of our platforms, it'll be consuming energy at various rates. This is because the different things that your app does consume different amounts of power. Additionally, the more time it spends consuming that power the more energy consumption you'll face. We can plot this graphically. Here, you can see as your app is running there are various peaks and troughs of power consumption. It will follow that the area under that curve is energy, and this relates directly back to your application in its various modes of running. When your app is active and when your app is idle, it's going to consume different amounts of power. When your app is active, we say that the power being consumed is at its highest point. This is because the user is directly using your application for whatever it was or intended for. Then, when your app is idle but still running, the power consumption drops. Finally, when your app is suspended, there's still a basal level of power consumption, and that's interesting to note. When your app is doing any of the work that it's been designed to do, it's going to be asking the system to bring up hardware that it needs to do that work, and the energy associated with that hardware being brought up and used is called overhead. Your app doesn't have direct control over overhead, but it really does influence through anything that it does. Then, when your apps first utilize those hardware resources, this is called active energy. So, now, your app has access to, let's say, the radio or has access to, let's say, the camera, and it's using that subsystem, this energy being consumed is going to be called active energy. So, then, it stands to reason that the battery life problem is actually a two-part optimization problem. We have to think about being efficient about the active energy that we are consuming, and we also need to be thinking about the overhead that we'll be incurring by asking for different hardware resources on the system. So, I've mentioned hardware and these subsystems that supposedly consume energy. So, what exactly consumes energy on the system? As an app developer, you're going to run into a number of different hardware subsystems in your app development process. But there are four subsystems that we think on the Power Team will contribute most highly to your energy consumption. These are as listed here; processing, networking, location, and graphics. Let's run through these and try to understand what they mean. Processing is what you might imagine. It's going to be the energy consumed when your app utilizes system resources on, let's say, the SOC. Such as DRAM, CPU, etcetera. It's really the workhorse component. Energy consumed here is going to be highly dependent on the code that your app is executing and the workload that you've asked your app to perform. So, in a nutshell, the more operations and code your app executes, the more energy it will consume in the form of processing. Networking is the next major subsystem that we think about when we talk about what consumes energy on our devices. Networking energy is what you might imagine. Whenever your app asks to do any form of networking over cellular, Wi-Fi, and Bluetooth, it's going to consumer energy in the form of networking. This energy is traffic-dependent. The more traffic that your app asks to be sent over any of these technologies, the more energy it will consume. So, put it bluntly, the more network requests that your app asks for, the more energy you'll consume in networking. Location follows suit but it's a little different. In a location subsystem, when your app asks to fix location using GPS, Wi-Fi, and cellular, it's going to consume energy in the location system. The location energy is going to be accuracy and frequency dependent. If you're asking to fix a user's location with a high degree of accuracy and at a very high cadence, you're going to get a lot of energy consumed in the form of location. So, putting it all together, the more time spent tracking location in your application, the more energy you'll consume as location energy. Finally, we have graphics. In the graphics subsystem, you would imagine that process components such as he GPU and the CPU contribute to the energy consumed by graphics. This is going to be animations and UI dependent. So, when your app is asking for any animations to be displayed or any UI to be rendered, it's going to consume energy in the form of graphics. This is highly complexity dependent. The more complex your animations and UI are the more energy that you'll consume in the form of graphics. Finally, a good rule of thumb is to say that the more rendering that your app does, doing animations or UI, the more energy you're going to consume in the form of graphics. So, we talked about these four subsystems, and what's the take-away message? There's a common thread ties them all together in our app development, and so that the more work you do, the more energy you're going to consume. We can't necessarily say do less work because that means our app might do less. So, then, the point here is that we need to optimize the work we do and make it as energy-efficient as possible. But it's not so simple. Thinking about energy efficiency is a process. It's not just so that we can make an optimization and suddenly our energy is going to be more efficient or our app is going to be better for battery life. We have to get into this mode of thinking that our app has a set of resources that it's using, and we need to use those resources efficiently. So, with that being said, let's take a look at some examples of real-world situations where we can think about energy efficiency and really start this process off. Let's talk about when our app is in the foreground. When our app is in the foreground, it will likely be providing the main user experience. For many of us, this is the most important and critical part of our application. With that being said, energy efficiency in the foreground is about focusing on providing value to your user, ensuring that whatever you're doing provides some immediate impact for the user experience. One tenet we can follow is to only do work when required. Sounds pretty straightforward. Well, let's take a look at an example and illustrate why this is so important. Let's say you're building a media application, and the primary goal of the media application is to present content to the user at a regular cadence. Well, a really robust solution would be to implement a sort of timer-based approach to refresh the content feed. This will ensure that the content of the user is seeing is as fresh as possible without any sort of interaction. This isn't a very energy-efficient approach, and let's sort of understand why. If we plot the power over time curve for a solution like that, we see that every time our timer fires, we have a little bit of active energy that's consumed. But the really important part here is that we have a ton of overhead, and this is because every time we ask to display new content, we likely have to bring up subsystems such as networking, graphics, and processing to do all that work and display that content, and the user might not actually want it. So, we'll end up burning a lot of energy consistently while that application is running. We can do better. If we think about what the user actually wants, the fresh content, we can implement a solution that is on demand. Now, in this new solution, user interaction or some kind of a notification from our server will provide us the new content and display it to the user. This solution isn't that different, but it's an energy-efficient approach and makes a dramatic impact on our power over time. Let's take a look at why. Now, if we imaging that our app is running in the foreground, and a user interaction occurs, we would refresh our content feed and display it to the user. Then, our app will go idle as our user is using it, let's say, to scroll or just to read the content that's been displayed. You'll notice that the overhead here is still a little bit high, but it's been significantly reduced. The trick here is that we've allowed the subsystems we no longer need to go to sleep and idle off. Another tenet that we can follow to reduce our energy consumption in the foreground is to minimize complex UI. So, I mentioned before that in graphics our energy consumption is highly complexity-dependent, and we always want to make our apps look as good as possible. So, we're going to spend a lot of time building this UI that looks great and animations that are pleasing to view. However, this can have unintended side effects, and let's look at an example to illustrate why. If I'm a video player, my goal is to let a user watch a video. Simple. But I could be tempted to add new controls and UI above that video, let's say, in the form of related videos or a scrubber or maybe volume controls. This allows a greater degree of control to the user to use this application and enjoy the video they're watching. This is actually insidiously energy inefficient, and let's understand why. On many of our devices, there's a display optimization in place that allows for video playback to be very energy efficient when there is no UI on screen. This is something that is not immediately clear when you're building an application like this. However, it makes all the difference. So, a good approach to take advantage of this optimization and counteract this sort of energy inefficiency we see is to have a simple auto dismissal of our UI controls. And this could mean that any related content that we put on the video or in the UI layer simply goes away if the user is not interacting with it. This makes a big difference on our energy consumption during video playback, as this display optimization is critical for maintaining quiescent energy-efficient playback. So, we've talked a lot about the foreground, but what about the background? Many of us who are building applications such as music players, or maybe even alarm clocks, are focused on the background. Our main experience comes from our app running effectively in the background. Well, when we're in the background, we have some things that we need to be aware of. Likely, our app is going to be running in conjunction and concurrently with other systems on device. Let's say I'll be using iMessage or maybe even Facetime. To that end, we should focus on minimizing our workload to ensure energy efficiency when we're in the background. Well, this is a pretty broad statement. So, let's kind of try to understand it. When you're in the background, you may be able to utilize subsystems that are already being used by other apps on the system. However, it's important to note that the majority of the priority for the energy consumption is going to go to those applications that are in the foreground. So, then, we should focus on minimizing our workload to make sure we don't interrupt those experiences. One way we can start thinking about this is to coalesce all of our tasks. If there's a lot of maintenance work, let's say, that we need to do in the background, or we have a lot of networking activity that needs to be performed, let's say, then it would be best for us to group those together and do them all at the same time. That way, we have the minimal impact on anything else happening on the system. A really common example that many of you may face is to upload analytics and working with application analytics. It's likely that when you're collecting these analytics you'll be sending them immediately because this is a very robust solution, and it allows you to build a dataset that is protected against crashes in your application. Well, doing that may not be very energy efficient. If we were to send our analytics every time we went into the background, we would risk overusing our networking hardware. And here's how that looks like when we take a look at the power over time curve. Every time we enter the background, we would spin up networking resources to send these analytics, and then we would come down and go idle again. This may not look like a lot with just three on this graph, but you can imagine if your application is experiencing heavy usage, this adds up over time. The right way to do this is super straightforward, and it's simply to send these in deferred batches. We have a lot of APIs that support this coalescing principle, and one of the biggest ones is NSURLSession. Using NSURLSession with a discretionary property and a background session will enable you to take advantage of this sort of an optimization very quickly, and this is the right way to do it. Let's take a look at what the energy over time looks like now, if we've done this. We can see here that while it might take a little longer for our app to do any sort of uploading for analytics, the energy that we're going to consume is going to be far less, and it's going to be condensed to one single burst. This is effectively the result of coalescing any tasks when you're running in the background. You get a high energy for a short period of time completing those tasks, but then once you're finished you no longer have to worry about doing those tasks and potentially interrupting an experience of another application. Another example that seems sort of straightforward is to end your tasks quickly. With many APIs on the system that allow you to take advantage of background running, things like UI background task and UIKit, or VOIP and PushKit. And these APIs have ways for you as an app developer to indicate that you no longer need to run in the background. So, it stands to reason that as an app developer, if you're using any of these background modes, you would call these completion handlers, let's say, to let the system know you're done. Well, that doesn't always happen, and in a lot of cases, we might actually forget or not want to end our task. So, we let our tasks expire. There's a great energy impact to this, and it's really something that people don't necessarily see when they're developing their application. Let me demonstrate why this is energy inefficient with the power over time curve. You could imagine if you enter the background for any reason and your task starts, you finish some time afterwards. Then, if we let our task expire, as we've said, we enter this sort of idle phase where you're consuming energy and our app is running in the background for whatever reason we've asked our API for, but there's not really much else happening. And then, we have a long tail of overhead because we've kept the system awake and subsystems we thought they needed to be using their own resources are now waiting for us to finish. The quick solution to this is to simply call your completion handlers whenever they're available. And as I mentioned, UI background task is one of the biggest ones. When we enter the background from the foreground, we can call this API and UIKit. If we don't let our system know that we don't need, if we let our system know that we don't need to do any work anymore, we save a lot of energy and allow hardware systems to go idle when they need to go idle. Here's what that looks like if we call these completion handlers. You could see here that the tail of active energy that we saw before is gone, and now we've greatly reduced our tail of overhead as well. A simple solution, but it has a big impact on your overall energy consumption. So, we've talked about some ways that we can start thinking about energy efficiency as a process. If we focus on optimizing the work we do in all of our use cases, we can really work on optimizing the energy that our application consumes. For a deeper dive into the things we talked about and to maybe get a little bit more hands-on with the code behind some of these optimizations we discussed, I really recommend that you check out our video from last year, How to write energy-efficient apps. In that session, you'll find that there are a lot of interesting resources and more examples on how you can use energy-efficient designs in your application. So, now that we've talked about some ways that we can improve energy efficiency in the design of our application, and we've spent a lot of time talking about ways that we can improve our energy efficiency through thinking about the hardware systems behind our application, what are the ways that we can quantify this? Let's say we've made a change, and we want to understand the real impact in our application. Well, right now, let's talk about some tools that we have available for you today to do that sort of work. Today, we have two tools available that you can use to quantify your energy impact. The first tool is the energy gauges, which are accessible directly through the Xcode debugger. The energy gauges are a great way for you to rapidly iterate on your codes energy consumption and to help you understand at a very high level where your energy consumption is going by subsystem. And then, if the gauges aren't good enough, you can jump right into the instruments from the Developer Toolkit. The instruments will allow you to do a deeper dive into the various subsystems on the device. And understand at a lower level how these actual subsystems are performing and what they're doing. Let's take a look at the energy gauges first. As I said, these are accessible directly through the Xcode Debugger UI, so they're pretty easy to use. Let's jump into the UI. As you can see, we've selected the row that says energy impact, and now we have this main area in the UI that's composed of three major sections. On the top left, we have the canonical gauges themselves. These gauges range from low, high, and very high, and represent the average energy impact of your app at an instantaneous moment. It's important to know that where the gauge actually falls doesn't necessarily mean good or bad. It means that whatever your app is doing, it's consuming this much relative amount of energy. It's important because it's up to you as an app developer to think about your use case and whether or not you would expect it to do that. To the right of that, we have the average component utilization, and this is going to be a pie chart that shows you all of the different components relative to the total amount of energy that you're consuming, what percentage those components are consuming. This is really useful because it's representative of those subsystems we talked about earlier, and it helps to identify if you have an excess amount of overhead or maybe if one component is taking too much energy, and you don't expect it. And then, immediately below that, building off of the average component utilization chart, we have a time series that represents the average utilization of each component as your app is running in real time. We could also see here that you have the state that your app is actually running in, foreground and background, and also it would list suspended. This is a really awesome tool for understanding how your app is behaving in real time. So, as I said, the energy gauges are really great for doing high-level characterization work and rapid profiling. That's the key. When you're iterating on your code, you're trying to get something to work as an app developer, and you're trying to put something together, it may not seem immediately clear how you could really think about energy, but the gauges are a great way to start. But let's say that you've done that and the gauges aren't really enough for you. That's where the instruments come in, and directly through the energy gauge's UI, we have access to three instruments that we think best correlate to the subsystems we talked about before. These include the time profile, the network profiler, and the location profiler, and if you were to click through into any of these from the energy gauge's UI, you would be able to transfer your current debug session into any of those instruments. Let's take a look at one of the instruments here, the Time Profiler, and try to understand the UI. Now, the instruments have a very standard UI, but what's interesting about it is that it's very useable. And let's take a look. Here, we can see the Time Profiler UI, and on the top, you see a bar that's representative of the different controls that you have of the actual instruments. On the top left, ou can see you have a Play and Pause button as well as your target that you're using to profile. And then, on the right, you see a plus button that allows you to very quickly drag and drop other instruments into you profiling pane, which can be found here. And now, this profiling pane actually allows you to see what instruments are running and currently profiling your application. Here, since we're using a Time Profiler, we see the CPU usage and a graphical representation of how much CPU usage is being consumed over time. Directly below that, we have a weighted call graph. Since we're using the Time Profiler, we're trying to understand how our CPU is being used by the application. To that end, there's a weighted call graph that allows you to see exactly what is being called in your application and how much weight it has on CPU time. And then, directly to the right of that, you have a summation of the heaviest stacked race in your application that basically says what is the heaviest stack during this profiling run? There are a lot of other great instruments that you can use, and here are some of them now. This means that the instruments are really great for a couple of things. The first thing is that the instruments are really great for root cause analysis. Let's say you have a problem in a specific subsystem, so just processing or networking. You would be able to identify pretty rapidly what that problem might be using the Time Profiler or the Network Profiler. The instruments are also really great for doing in-depth profiling of your application. If you implement a CPU efficiency improvement of some kind; let's say you cut down the time that it takes for an algorithm to execute, the instruments are a really good way to understand if that's the, if the intended effect of your optimization is going through on that subsystem. But there's also one more thing that the instruments are really awesome for that I haven't talked about today, and that's untethered profiling. There's a single instrument that you can use called the Energy Log, which allows you to do an untethered profiling run on a provision device while using your application. It's accessible directly to the developer settings, and when you start running it, you can use your phone as you normally would and use your application as you might expect for any number of use cases. And then, afterwards, when you're finished, you can stop the recording directly from the developer tools and jump into Instruments and upload that trace. This is really useful for understanding if there are any environmental problems that you're having that might be impacting your energy consumption. Now, we've talked about the tools; we've talked about the concepts; now, I want to do a demo and work through an example about how we can actually use these in tandem and solve energy problems and make our app more energy efficient. So, today, we've prepared a simple game called Energy Game, which draws sprites onscreen and allows the application to inject a number of bugs. It's a very simple application that we've built, and it only has an app delegate in a View Controller, but the primary purpose is to show you how to use our tools rapidly to iterate through your code. So, I'm going to go ahead and build Energy Game here through the Xcode UI and let it run. Then, you'll see on the right side that all it really does is draw a little little battery sprite at a random time. There it is. Very simple. If I jump straight into the Xcode debugger and jump to energy impact, now, I can see my gauges. And so this is the UI that we just talked about. It's the same three areas that we discussed, and you could see right now that all my app is doing that we've designed it to do is just placed some sprites onscreen. But you notice that I'm doing networking, and my overhead seems to be high for simply no reason. Well, this is because we're also doing a little bit of networking and uploading the spike count every time a new spike is drawn onscreen. And so, through the Xcode energy gauges, you can actually see the impact of doing that. So, I'm going to go ahead and stop this now and jump into my code to understand where this is coming from. So, if I go to my View controller, where I actually add a new sprite, I've had a function here to upload the sprite count, which creates a simple connection object and uploads the sprite count every time a new sprite is added. I'm going to go ahead and comment the cell and then jump into my app delegate and move it to the only upload the sprite account when I'm in the background. And for the sake of this demo, I've named that my networking optimization. I'm going to go ahead and rebuild Energy Game and show you the effect this has on the energy gauges. Now, Energy Game is running again. I'm going to jump back to the Xcode Debugger UI, jump back to Energy Impact, and now we don't see any networking energy, and we don't see any overhead, which is good. So, that's simple optimization, simply moving a networking request from one area to the other and preventing it from happening often allowed us to greatly reduce our energy impact in our quiescent use case. So, now, I'm going to go ahead and inject a bug and try to see how we can see a bug when we use Xcode energy gauges. Bug1 is a simple bug that you can see on the bottom left here that will essentially cause a CPU spin in the background. This is a case that many of us might face in regular and real world development. I'm going to go ahead and inject this bug. And now that I've injected it, I'm going to background Energy Game, and as you can see in the Energy Gauge's UI, we transfer to the background. We do a little bit of networking because I moved that networking call to the background. But now, we also see that our CPU is going wild. So, this is the power of the gauges. We've now, we know that we're injecting a bug, but we can see that bug directly in the gauges. So, now, to find the root cause, I'm going to go ahead and jump into the Time Profiler and transfer my debug session, as we discussed before. So, now, I transferred my debug session, and it will begin running automatically. And as you see, the weighted cobra apples start populating in a moment; here, we can see that the dispatched thread here is consuming the most CPU time. Let's go ahead and dig into it. And we can see that we have closure at something called appdelegate.compute. Well, let's jump back to our application and try to understand what that is. So, for the purpose of this demo, when we entered the background in Energy Game, we called something called computation. Computation is a really terrible function. It basically starts spinning wildly with a while true loop when we inject Big1. So, it's very simple for the purpose of this demo, but using both the gauges and the time profiler, we were able to dig back directly to where this was happening, and we can see that this while true loop is not good. So, I'll go ahead and comment this out because I love commenting out code instead of deleting it, and I'll go ahead and rebuild Energy Game. We'll just jump back into the gauges to see that everything is okay, and now we'll go ahead and inject Bug1 again, and I'll go to the background. And we see our expected networking activity but no CPU spin. Voila! We've solved it, using two tools in about 30 seconds or a minute. That's the power of these tools. They're able to let you rapidly iterate and root cause problems that you might face on day-to-day development. So, let's go back to the slides. So, there's some takeaways from this demo. The first takeaway is that the gauges, as we said, are great for rapid iteration. They allow you to quickly see where your problem might be happening, and they allow you to take the next step in figuring out how to solve it. The second takeaway is that the instruments are great for in-depth profiling. And finally, the third takeaway is that we want you to think about energy efficiency as a primary objective in your application development. We have powerful tools available for you to quickly understand where your energy is going and to root cause problems that might be energy related. So, let's say you've done all of that, and you've shipped your application. From the App Store it's getting used; all your customers are greatly thankful that you shipped it on time. What's next? Let's say you still see customers saying that your app is bad for battery life. What sort of recourse do you have? Well, now, I'm going to pass it on to my colleague, David, who's going to talk to you about how you can face those challenges and solve them using our new tools. David. Good afternoon. Hi, I'm David, and I'm here today to talk about some new great tools for energy debugging. If you're an iOS developer with an app in the App Store, or in TestFlight, then this part of the talk is for you. I'd like to start with the following question, now that you've shipped your app, how do you know how our app is doing in the wild? In other words, how do you know if your customers are experiencing energy issues that are leading to bad battery life? Now, a customer may leave a review on the App Store, saying, "My battery went down a lot while using this app." But they might not be able to tell you what happened. Or even worse, they may delete your app and not leave any feedback at all. So, it can be challenging to find out if you have energy issues in the wild. And even if you know that there are energy issues, how do you debug an issue that occurred on your customer device? You can make use of tools like instruments and gauges that Phil talked about, but unless you know what to test for, it can be challenging to reproduce. There can be environmental factors such as poor Wi-Fi conditions that occurred for your customer whereas on your desk, you have great Wi-Fi conditions. So, these are some really challenging questions. So, to help answer these questions, I'm excited today to talk about a new way of debugging energy issues using Xcode Energy Logs and Xcode Energy Organizer. First, I'll talk about Xcode Energy Logs, which is a new way of reporting energy issues on device. Later, I'll cover Xcode Energy Organizer, which is a new tool for viewing Energy Logs. With these tools, for the first time ever, you'll have the data that you need to find and to fix energy issues. So, let's get started. Xcode Energy Logs are a new way of reporting issues from device. We start with high CPU energy events, which is when your app is using lots of CPU. Each Energy Log will have a weighted call graph, which will point out the energy hotspots within your code. These logs will be made available from TestFlight and the App Store, so you'll have real world data, what's actually happening with your customers. And with these logs, you'll be able to begin improving the battery life experience. Let's talk about when an Xcode Energy Log is generated. Let's say your customer is using your app, which starts to put a really heavy load on the CPU. This can be natural, depending on what your app is doing. Well, let's say it's putting a really heavy load on the CPU for a long time. This causes a high CPU energy event to be detected. Now, there are two key thresholds that are checked for for a high CPU energy event. The first threshold is when your app is spinning 80% CPU for more than three minutes while in the foreground, and the second threshold is more than 80% CPU for more than one minute while in the background. In this latter case, your app may actually get killed to prevent runaway background usage. Each instance of a CPU Energy Log indicates that your app uses so much CPU that it was worth flagging. What this means in practical terms is that it was responsible for up to a 1% battery drop in a typical case. Now, you may be saying to yourself, 1% battery doesn't sound too bad. But to put this in context, on an iPhone 6S with an additional 1% battery, your user could have had eight minutes of additional talk time or six minutes of additional browsing or 30 minutes of additional music. And if your app continues to burn at this rate, the battery would have dropped even more. So, writing CPU-efficient apps is really important, and your users will notice. An Energy Log has three things that can help you figure out what has happened. First is the context by which what happened that triggered the report. For example, it will say that your app spent for 8% over three minutes. The second piece of information is the metadata about where the Energy Log was created; for example, on an iPhone versus an iPad and on, say, Build 30 of you app. The third and most important piece of information is the weighted call graph that will show you the energy hotspots in your code. So, let's talk a little bit more about the weighted call graph, how it was generated, and how you can use it to debug energy issues. Let's say your program is comprised of a main function and a number of methods, Method 1, Method 2, Method 3, and Method 4. Your code begins to execute until a high CPU energy event is detected. Up to this point, backtraces are continuously sampled at a periodic interval of once per second, where each backtrace is a sample of an active frames in execution. The first backtrace, for example, shows that main Method 1 and Method 2 were active. The second backtrace shows that main Method 3 and Method 4 were active and so on. Now, we can try to combine these backtraces together to form an overall picture. What we see here is a weight call graph, and this weighted call graph is really useful. Here, we can see that main was present in six out of the six samples that we collected, meaning that main was running 100% of the time. Of that, we see that Method 1 had five samples whereas Method 3 had only one sample. And within Method 1, we see that Method 2 and Method 3 had three samples and one sample respectively. So, this gives us an overall picture of where the code was being executed and how much time was being spent. So, when an Energy Log is created, there's a collection of periodic backtraces sampled at one per second. For each backtrace contains a list of the active frames being executed by the CPU, these backtraces are aggregated by sample count into a tree where the samples, where more samples mean more heavily executed code. And you can use these weighted call graphs to identify unexpected workloads in your app. So, now that we know what an Energy Log is, how do we access them? First, Energy Logs are created on device. Then, your beta testers and your customers, who have opted in, will upload these logs up to Apple. Now, there might be hundreds or even thousands of these logs, so we will aggregate these logs for you, sort them, and present them in a list of top energy issues to you. And you can download and view these logs using the new Xcode Energy Organizer tool. The Xcode Energy Organizer is your command center for debugging energy issues in the wild. Energy Organizer makes it really easy to view energy logs. The Energy Organizer is connected to TestFlight in the App Store, so you'll see a list of all your iOS apps. You'll be able to see some statistics of how often these energy issues occur in the wild. You'll have a list of the top energy issues sorted by how many devices that was impacted. You'll have a view of the weighted call graph for a number of different logs, which you'll be able to page through, using page through logs, and you can use Open in Project to jump directly into your code base so you can begin debugging these energy issues. And now, I'd love to show you a demo. Now, I've made sure that I've signed into my developer account and that I've uploaded our Energy Game app up to TestFlight in the App Store. To bring up the Energy Organizer, I could just go into Window here, and click Organizer. And this is the Energy Organizer UI. I make sure that the Energy tab is selected at the top, and if you've used the Crashes Organizer, you will already be familiar with this UI. On the left, we have a list of all of our apps. Next to that, we have a list of our top energy issues. In the center is our Weighted Call Graph, and on the right-hand side are some statistics about the energy issue. So, let's go ahead into the left here and select Energy Game, which is the game that we're working on. And then, make sure that we're on the correct build. We see here a list of our top energy issues, sorted by how many times it's affected. Let's jump into this first energy issue, which hit 64 of our devices. On the right-hand pane here, we have some more details about what happened, as well as a breakdown of how often that energy issue happened, and we can see that it happened across a mix of iPads, iPods, and iPod Touches, and we can see a distribution of how often it happened in the past two weeks. Let's take a look at the weighted call graph. We see that a lot of time is being spent in this dispatch call block calling into this app delegate computation function. Now, I can use this button here to jump us directly into our code base. So, we are back directly into our code. On the left here is one of the sample backtraces from our weighted call graph. We can see that we're spending a lot of time in this computation function. Now, this is the very function that Phil was talking about earlier on his demo. And we can see that he's already commented this part of the code out, so he's already addressed this energy issue. So, let's jump back to the organizer. I can go ahead and click this button here and mark this issue as resolved. And what this does is the next time we open the Energy Organizer, we'll see that we've already taken care of this issue. All right, let's jump to the second issue, which hit 42 devices. Now, before going into the weighted call graph, I'd like to draw your attention to three features at the bottom here. First is this page through logs where I can select one out of five sample energy logs out of the 42 that we've hit in the wild. As I page through these, you can see that the weighted call graph looks a little bit different, which is okay because these backtraces and these weighted call graphs are samples. However, we've grouped these together by similarity, so these logs should look fairly similar to you. This button here, when I click it, shows that all the system library frames that were hidden previously. And this button here clicks all, shows you all the frames that had low sample counts. Now, by default, we've hidden most of these frames for you so that we only show you the most important frames. Let's take a look at this function. It looks like a lot of time is being spent in this heavy timer function. Actually, I heard Phil talking about this bug off stage, and he said that he was going to take a look at it, so I'll let him deal with it. I can go ahead and rename this and move on to the next bug. Let's take a look at one more bug. Here, I can see there's a lot of time being spent in set next update timer and add new sprite. What is this function? Let's investigate. I'll jump directly into the code, and I can see that a lot of time is being spent in this add new sprite function. Okay. Adding new sprites can be expensive, but the question to ask ourselves is, is this an expected workload? And the answer is, in this case, not really because we only expect to be adding sprites once every few seconds. So it doesn't quite make sense why this is chewing up so much CPU. Let's take a look at the backtrace to see who is calling us. We're being called by set next update timer. So, what is this function doing? We see that within set next update timer, we're calling in to this add new sprite. At the end of a function, we're calling in to this update timer to schedule the next time this function is called. This timer is set to fire sometime between now and next update interval. Now, next update interval is decremented by 1 until it hits 0, and then it's re-initialized according to this line of code here. Now, here's where the problem is. Time interval since last update date can potentially be negative, and we've seen cases of this happening, especially when users try to game the system. Maybe they're playing a game, and they want to reset the clock. Maybe they want some extra lives or some extra chances, so they go into System Settings and change the clock to 24 hours ago. Well, in this case, this causes next update interval to be negative, and when we schedule a timer for a time that is sometime in the past, that timer will fire immediately and then call itself again and again. So, we effectively have an infinite loop here. Fortunately, this is really easy to fix. We just go into this function here and change this to less than or equal to 0 so that even if next update interval is negative, we can break out of the loop. Now, this is a really great example of an energy issue that is really difficult to catch during normal testing but is made obvious once you have the data from the field. That's the power of Energy Logs, and that's the power of Energy Organizer. Let's take a look at the three key takeaways from this demo. You can use the Energy Organizer to discover top energy issues in the field. Take a look at the top issues, take a look at how often they're happening, and take a look at what kind of devices and builds are affected. Second, you can view energy hotspots using the weighted call graphs. So, look out for the frames with unusually high sample counts, and watch out for the unexpected workloads. Finally, use OpenEnd Project to jump directly into your code so you can make and inspect fixes with what's going on. Let's summarize what we've learned today. First, think about energy use and treat energy as a first-class citizen in every aspect of your design, development, and testing. Second, make use of the great tools like energy gauges and instruments to profile your app. And third, take a moment to explore the new Xcode Energy Organizer to understand and fix energy issues in the field. For more information, please come check out the following URLs, and feel free to come by the Power and Performance Lab on Friday from 9 to 11. Thank you and have a great evening.  Hello and welcome to advanced debugging with Xcode and LLDB. I'm Chris Miles, one of the engineering managers on the Xcode team, and I'm really excited to be here today. Now, I'm well aware there's nothing between you and Beer Bash but our session, so thanks for coming. We'll get you out on time, but we've packed in lots of great content for you guys today. So let's get straight into it. I'd like to start by talking about Swift debugging reliability. The main point to make here is that there's good news. The team has landed a lot of reliability fixes in Xcode 10. The -- thank you. The compiler and debugger teams have been able to smooth over many of the rough edges that were causing Swift debugging headaches. I'd like to tell you about a couple of them. In some cases, usually with more complex projects or build configurations, attempts to po an object or evaluate an expression in the console may have failed with an error such as this. The AST context that this error refers to is an expression context that LLDB needs to reconstruct the state of the compiler from when it built your project. And, in some cases, such as if there are module conflicts, the expression context cannot be reliably reconstructed, and your expression would fail. In Xcode 10, LLDB have implemented a fallback mechanism for when this problem case occurs. So if it can't reconstruct the context, it will fall back to creating a simpler context for the current frame and use that to evaluate your expression. Another failure case that some developers may have had was due to the debugger being unable to materialize variable types while debugging. This would manifest itself in Xcode looking something like this. Where on the left, you can see the variables' view shows all the names of the variables in the frame but there'd be no type information or values visible. And attempts to print out the value of a variable would fail with errors like these. Now, thanks again, in big part to your bug reports, the team has been able to track down and fix these many edge cases where debug information was not being reliably generated. So I want to thank you guys on behalf of the team again for filing issues when you've encountered any problems while debugging. And if you find any more problems with Xcode 10 while debugging a project, please continue to file bug reports for us. For those of you at the conference, if you find a problem with your project, please come along to the labs. We have an Xcode debugging and profiling lab tomorrow morning from nine till 12. Bring your project and ask for an Xcode debugger engineer or an LLDB engineer, as they would love to see your projects. Now I'd like to move on to telling you about some of my favorite debugging tips and tricks that I like to use to enhance my debugging webflow. In fact, I'm not just going to tell you about it, I'd like to show you with a demo. Now the project we'll be using today is an iOS app called Solar System. And you may have seen Solar System appear in such keynotes as the Keynote and State of the Union. We're going to be debugging an area of the Solar System called Moon Jumper. Moon Jumper allows the user to hold a phone in the hand and jump in the air. The app measures the force of your jump and then translates that to moon gravity and shows you a visual representation of how high you would have jumped if you were standing on the moon. You can set a bar to choose a limit, so you can try and challenge yourself to jump as high as the bar, in terms of moon gravity. Now in making some enhancements to Moon Jumper, such as some visual enhancements and a GamePlay mode, while we're not ready to ship, we've done a test pass and come up with a list of bugs that we need to look at. So these are the bugs for Solar System. I'll be tackling the iOS bugs first, and later on, Sebastian will come out and solve the macOS bugs. Now, as it says, none of us are getting out of here to the Beer Bash until we fix all of these bugs, so there's nothing more exhilarating than peer programming with 2000 people. Let's get straight into it and start with the first bug. The jump failure animation does not match the specification. So what's that talking about? Well, switching to the simulator because we're using the simulator to speed up debugging and development. I've wired up a tap just to recognize them, so every time I tap the astronaut he performs a successful jump to the height of the bar. Now this bug is talking about the case where the astronaut doesn't reach the height of the bar, so let's reproduce that. Switching to the editor, I'm going to use the jump bar to navigate to the jump function and set a breakpoint at the start of that function. Now I'll tap the astronaut, and we'll start a jump and now we are paused in the debugger. The first thing to point out is actually up here in the tab bar. We now have four tabs. This debug tab was just created for us by Xcode, and for those of you that like working in tabs like I do, this is an Xcode behavior that you can define. So to do that you use the Xcode menu to edit behaviors and that takes you to the preferences in the behavior's tab. And here you can configure many behaviors. In this case, you would need to configure the pause's behavior in the running section. And this is the behavior that will be actioned when Xcode pauses in the debugger. So you can see here I've configured it to show a tab named debug, and Xcode will always switch to that the tab when pausing in the debugger. So that's great for users who like to work in tabs like I do. Now switching back to the code, we can see that there's a condition based on this property did reach selected height, so I'd like to have a look at the value of this property. So switching to the debug console, I can use po to look at the value of that property and it's currently set to true. So the [inaudible] always sets this to true, and I'd like to change it to false so we can reproduce the bug. Now I could go to the code and modify it, tap just to recognize it to set it to false, but I don't like to make changes to my code just for debugging purposes if I can avoid it. So in this case, what I can do is use the debugger to do it for me. I can use the expression command. And here I can give it any Swift expression, such as did reach selected height equals false, and it will evaluate that and execute it. So now, we can see that this property has indeed changed to false. And if I step over the line using the debugger, we can see that we've entered the false side of the branch. So now, when we continue, we can see that the astronaut doesn't quite reach the height of the bar and falls down. So that's the case we're trying to reproduce. I'd like this to happen every time I tap on the astronaut, and I don't want to have to pause and type this expression each time, so what I'm going to do is configure this breakpoint to do that for me. If I right click on the breakpoint, I can select edit breakpoint and this gives me a popover window with some configuration to customize how the breakpoint behaves. I'm going to select the debugger command action and enter the expression command with the command that I used in the debug console and make this an auto-continuing breakpoint. So what we've configured here is a breakpoint that once execution enters this function, the breakpoint will trigger execute this command for us to change the value of the property and then automatically continue. So now, every time I tap the astronaut, he'll perform this unsuccessful jump and fall back down. Now what's the problem that we need to fix? The specification said that after falling down, the astronaut should stand back up again, so let's fix that now. I'm going to navigate to this function update UI for jump failed, and we can see this function uses UIKit Dynamics to simulate failing the jump. So it starts by creating a UI Dynamic animator and then calls a function to add behaviors to create the physics effects. And then the astronaut is meant to be reoriented and recentered in the dynamic animator did pause delegate callback. Now we can scroll down and see that that delegate callback is implemented successfully, so that looks fine. But I'm noticing that no delegate is set on this object, so if I add that code in here, I think this is the change I'll need to fix the problem. Now, at this point, I can recompile and rerun and try and verify my fix, but I'd like to shortcut that whole cycle if I can. So what I'm going to do is use a breakpoint to inject this change for me so I can see if this does fix the problem quickly and conveniently. So I'll create a break point, double click to bring up the edit window, which is a shortcut for bringing up the edit window. And then, once again, use a debugger command action to inject an expression, and I can type in that line of code that I think will fix the problem and make this an [inaudible] continuing breakpoint. So what I've configured here, even though I've made the change in the code, I haven't recompiled, of course. So I'm using the custom breakpoint to inject that change for me so I can test it using the current live application. So now, if I tap the astronaut, he performs this unsuccessful jump and falls over and stands back up again, so it looks like we fixed the problem. I like this effect, so I'm going to do it again. Now let me just launch notes again. And we can tick off the first bug as fixed. Nothing better than checking off fixed bugs. Now the next three bugs are to do with the new GamePlay mode I've been working on. So I'm going to press play in the simulator to show you that, and the challenge here is to try and jump higher than the bar 10 times. And the bar starts low and raises each time. And you can also see some score labels have been added to the top. So if I tap the astronaut, you'll see he's still configured to perform an unsuccessful jump, and then the attempts label at the top should have incremented but it didn't. And that's the first bug we've got here, where these labels flash but do not change. We've also got an issue with the end-of-game state not being handled properly and a problem with the layout of those score and attempts labels, but we'll come back to those. So returning to this bug, if you didn't notice it, let me tap the astronaut again and keep an eye up on the top on the attempts label. You'll see that it does flash but it doesn't update. So what this tells me is that the label is getting a value set on it because we're seeing the transition animation but the value is incorrect. So I'd like to find the code that is modifying this label to have a look at what the logic is at that point. So we note UI label is getting the text property changed, so what I'm going to do is switch to the breakpoint navigator and down at the bottom here select this plus button to create one of many specialized breakpoints. You can see we've got breakpoints for Swift errors and exceptions and even test values, but for this case, I'm going to use a symbolic breakpoint. So that creates a new breakpoint and brings up the editor for that, and here we can enter any function or method name such as UI labels, set text, which we'll need in this case, and we enter it in Objective-C format because UIKit is in Objective-C framework. The thing to point out, if I dismiss that, is that below the breakpoint, there is a [inaudible] added and this is the feedback from the debugger to tell us that it was able to resolve this breakpoint to one location in UIKit, Core. Some symbols may result in multiple locations, and you would see them all here. And if you saw no [inaudible] entries, that would indicate that the debugger wasn't able to resolve your breakpoint, so it wouldn't hit. So with that in place, I'm going to tap the astronaut again, and now we see that we've hit the breakpoint in UI label set text. Now we don't have the source code for UIKit, of course, so we're looking at assembly code, but there's no need to remain in the dark even if you're in assembly code of one of the system frameworks. We can inspect the arguments passed into a function. You just need to know the calling convention for the architecture, and you can inspect the registers to see the arguments. Well, I'll admit I never remember what those registers are but thankfully, I don't have to because the debugger provides pseudo-registers. Dollar arg one is translated to the register that holds the first argument. So in this case, we can have a look at the receiver of that Objective-C message, which is a UI label instance. Now we see that it has a value of 17 feet, and that indicates to me that it's this height label here, so it's not the label we're interested in, but while we're here, let's look at the other arguments. If you're familiar with Objective-C message send, you may remember that the second argument should be the selector. We don't see that here but that's because LLDB doesn't implicitly know the types of these arguments. So in some cases, we need to typecast it and now we see the selector for this message. Now the third argument is the first parameter passed into the method. In other words, it's the string passed into the set text. So this is a great convenience for inspecting the arguments, if you're in an assembly frame for one of the frameworks. But like I said, this wasn't the object or the label we're interested in, so let's press continue, and now we've hit the breakpoint again. So we can inspect dollar arg one to see what the receiver is, and it looks like it's the same height label with zero feet now. And I can see the problem with my strategy. While the astronaut is jumping, the code is updating the height label in real time, so the breakpoint is going to hit on this object quite frequently, and it's going to take us a long time. It'll be very difficult to hit a breakpoint on UI label set text for the attempts label. So what I think I should do is only set this symbolic breakpoint after the jump animation has completed, so let me show you a way to do that. I'm going to switch to the breakpoint navigator. And if I double-click on the indicator for the symbolic breakpoint, we can bring the editor back up and we can use this condition field. We can enter an expression here that returns true or false, and the breakpoint will only trigger if that expression evaluates to true. So if we had a property such as jump animation in progress, we could edit an expression to test if that was false and then the breakpoint would trigger. I don't have a property in this case, so I'm going to show you a different method. I'm going to delete that symbolic breakpoint and instead, scroll down to this function jump completed and set a breakpoint here. Now jump completed is called after the animation has finished so that it can update UI and update game state. However, we don't want a break in this function. What I want to do is configure this breakpoint to actually set the symbolic breakpoint in UI label set text for me. And I can do that by adding a debugger command action, which is breakpoint set, and I'm going to use the option one shot true. And a one shot breakpoint is a temporary breakpoint that only exists until it's triggered and then it's automatically deleted. And we can give it the symbolic name, UI label set text, and make this an order continuing breakpoint. So what we've created, in this case, is a breakpoint that when the execution enters the jump completed function, it sets a temporary breakpoint in the location we're interested in and then continues. So then we'll only hit that set text breakpoint after flowing through this function. So let's press continue, and we see that the jump animation completes in the simulator, and now we hit the breakpoint on UI label set text. So now we can have a look at the receiver of that message by using po dollar arg one. And we see that is indeed a different UI label instance with a value of zero, so that's likely to be one of these at the top. So we think we've found the right object, so let's have a look at the code that's modifying this label's value. We can do that in the debug navigator by selecting the next frame up in the stack. And now we've found the code that's modifying the label value. It's currently passing in a string of zero using the label text variable, and looking up, we can see that label text is always set to the label's current value, so that's not going to change. It looks like value text is the variable that contains the new value, so probably just a typo; let's fix that. Let's make it value text. And then, what I'd like to do, rather than recompile and rerun to test this change, I'd like to actually test this change in the context of the current running application like before. So I'm going to add a breakpoint below the current line because remember, we've changed the code but we haven't recompiled. So while the label will still be set, we'll add our line below that to set it to the value we think it should be. So another custom breakpoint, and we can use expression once again to inject that code and make this auto-continuing. So now, if I press continue, code execution will continue to flow through here, and we see that the attempts label is indeed updated. I'd like to make sure that also works. Well, thank you. I'd like to make sure that also works for the score label after a successful jump, so let's navigate back to here. I can remove this jump-completed breakpoint that was creating the one shot breakpoint because we don't need that anymore. And I can disable the breakpoint in jump because we don't want to modify did reach selected height anymore. And now, when I tap the astronaut, he performs a successful jump, and we can see that all the labels update properly, so that bug looks great to me. So let's return and check that one off. All right. The next bug is a problem with the end-of-game state. So the game is meant to end after 10 attempts, so I could tap the astronaut, wait for the animations to play out, and progress through the game to get through that state to try and reproduce it. But the animations take some time, and I may need to do this numerous times while I test and verify my fixes, so I'd like to skip all the jump animations. So let me show you how I'll do that. I'm going to navigate to update UI for jump succeeded, and we can see this function modifies some colors and then calls jump astronaut animated true. So it looks like I just need to call jump astronaut animated false. I could change the code and recompile, but like I said before, I don't like to change my code for debugging purposes if I can avoid it, so let me show you the technique I'll use instead. I'm going to set a breakpoint on this line. Let's clear the debug console and initiate a new jump by tapping the astronaut, and now we're paused on this line. So I need to ask the debugger to replace this line with a call to jump astronaut animated false. Well, the code's compiled in, we can't replace it, but what we can do is ask the debugger to skip over this line, to not execute it but skip over it. And then we can use expression to inject the change that we -- or the call that we'd like to make. So how do we skip over a line? Well, let me draw your attention to this green annotation label thread one. We call this the instruction pointer. It points to the line containing the instructions that will be executed next. And this icon here, that kind of looks like a grab handle, in fact, is a grab handle. If I hold the mouse down, I can move this around, and I'm able to change the instruction pointer while paused, so I can move it down one line and let go and then we get a scary message from Xcode. And basically, what it's saying is, with great power comes great responsibility. And I'll be honest with you, this is the riskiest feature I'll tell you about today. But it's only risky because the debugger allows you to move the instruction point wherever you like. It does not care, but it cannot guarantee that the application's state will remain intact. So you could, for example, cause memory management issues if you end up referencing an object that has not yet been initialized or over-releasing an object, for example. But we've all been to the advanced debugging session now, so we know what we're doing. So let's press the blue button. All right. So we've skipped over that line, and now in the console, we can use expression and call jump astronaut animated false. And now we press continue to see if all this worked, and indeed, the game state updated, and we skipped all the jump animations. So I'd like that to happen every time I tap the astronaut, so I'm going to configure this breakpoint here to do that for me. So first, we need a debugger action that skips over one line, and the command for that is thread jump. And I give it the option by one, and this tells the debugger to jump over one line of code for the current thread. And then we just need to call our expression and we can do that by pressing the plus button and adding another command action. Expression jump astronaut animated false. And we make this order continuing. So what we've got here is a breakpoint then when execution reaches this line but before it executes this line, the breakpoint is triggered. It will perform the command to jump over that line and then use expression to call the function call that we'd like to make instead. So now, if we tap on the astronaut, we can rapidly progress through the game state and skip all the animations and easily reproduce our bug. So, as I said, the game was meant to end after 10 attempts, and we've gone well beyond that, so let's have a look at the game state. That is all stored in a property up here called GamePlay, so I'm going to set a breakpoint on that property and start a new jump. And now we've paused at the next reference to that property. I'm going to use po to look at the current state of that object, and here we see the debugged description for this GamePlay object. And we have a custom debugged description here, so it's worth pointing out that po requests the programmatic debug description of an object and you can customize these. I'll show you how we did that for GamePlay. If I switch to the source code and scroll to the bottom, you can see that we've added an extension to conform GamePlay to custom debug string convertible. And conformance requires that you implement a property called debug description and return a string. And you can return whatever string you like to represent this object for your debugging purposes. You can do the same for Objective-C objects as well by implementing debug description. Compare that with the command p GamePlay. P is an alternate LLDB command, which uses LLDB's built-in formatters to represent the object. So here, we see two representations of the same object, and default formatter shows you the full [inaudible] type name, the memory address, and then a list of all the properties and their values. So we can see here that there is a max attempts property, and it's correctly set to 10, so it looks like there's perhaps a logic error where, after attempts is incremented, it's not successfully determining that it's past the maximum. So I'd like to find the code that's modifying attempts to see what the logic looks like. I'm going to open the variables view and expand the view controller to see all its properties and down at the bottom, I'm going to use the filter to enter GamePlay to find that property. Expand its properties and then I'm going to select the attempts property. And down here, I'm going to open the contextual menu and select watch attempts. Now what this does is creates what's called a watchpoint. In the breakpoint navigator, below all the breakpoints, you'll see there's a new group called watchpoints, and we have one watchpoint for attempts. And a watchpoint is like a breakpoint, but it pauses the debugger the next time the value of that variable is changed, so we can remove this property breakpoint because we no longer need it and press continue. And now we've paused at this watchpoint, and we found the code that's modifying the attempts variable. I can disable this watchpoint because I no longer need it. And we can look at the code here while the game's playing, increment attempts, and if successful, increment score. So I'm not seeing any logic that will detect if attempts has exceeded maximum and transition to the end-of-game state. So I think that's all that's needed, but in this case, I'd like to test my hypothesis before I make any actual code changes. So I'm going to create a breakpoint and configure it to inject that change to see if it fixes the problem before I make any code changes. So once again, I can add a debugger command action with an expression and I think what we need is if attempts is greater and equal to max attempts, then we change the game state to ended and make this order continuing. So now it's easy just to test if that actually fixes the problem by pressing continue. Execution will continue through this breakpoint. Inject the code and we can see that it does look like it fixes the problem. I'd like to verify that from the start of the game and I can quickly and easily do that by clicking play again and rapidly progressing through 10 attempts. And at the tenth, we see that it does indeed detect end-of-game state, so it looks like that's the fix we need. And now, don't forget to apply that to your code. So I'm just copy that out, drag the breakpoint to delete it. And then I can paste that in and that looks good. So let's check that one off, and we've only got one more left for this section and that's the layout of the attempt and score labels. Now, the layout of this application has been left up to the engineers, and as good engineers do, we've found an efficient location stuffed right up in the top corners. But the team decided that wasn't very appropriate, so they've sent it back and asked us to try again. So I'd like to mock up a new layout for these score labels. Now I could get out my graphical application and start mocking it up, but I'm an engineer and I like to mock up using code. In fact, I'm a debugger engineer, so I like to mock up using the debugger with a live application and real data. So let me show you how to do that. Let's navigate back and put breakpoint in the jump function. What we need to do is first find a reference to a view that we can play around with, so I'm going to clear everything and open that up and start a new jump. So we're paused in the debugger in this jump function within the view controller. So if you have of course a property or an outlet for a view, then that's a good reference. But if you don't, then you need to get the memory address of a view. So let me show you some ways to find the memory address and how to manipulate a view only by memory address. Well, like we said before, the debug description contains a custom description. So looking at the view controller's view, we can see the default debug description for a UI view has the class of the view and then the memory address. So one way is to just get the debug description for objects. So that's easy to get it for this one because there's a property for it. But how about all of the views below this view controller's view? Well, we need to look at the view hierarchy and one way to do that is to use this button here, which invokes Xcode's visual view debugger. It will snapshot the hierarchy and give you a 3D exploded view, and you can use that to inspect views that way. Sebastian's going to talk more about that in a few minutes, so let me show you an alternative way, which is good for simpler hierarchies and keeps you in the debug console. And that's using a debug function on UI view called recursive description. So we should be able to call po self.view recursive description. However, that doesn't work. Why is that? Well, recursive description only exists for debugging purposes. It's not part of the public API and so isn't [inaudible] to Swift. And Swift is a strict language and doesn't allow you to call functions that haven't been strictly defined. However, Objective-C [inaudible] code can run wild and free in Objective-C world and you can pretty much do whatever you like. I mean it's a dynamic language so you can call functions like this. So what we need to do is to tell the debugger to evaluate this expression in an Objective-C syntax. And the way to do that is to use expression with the option - l objc. That tells expression that you're about to give it Objective-C code even though you're in a Swift frame. And we'll give it -O, tell it that we also want the debug description the same as po would do and -- to indicate that there are no more options. The rest of the line is just raw expression input. So we should be able to then give it the Objective-C format of this method call. Unfortunately, that doesn't quite work and the reason for that is that expression will create a temporary expression context for the Objective-C compilation, and it doesn't inherit all the variables from the Swift frame. So there's a way around that, though. If we just put [inaudible] view in back ticks. Back ticks is like a [inaudible] step that says first, evaluate the contents of this in the current frame and insert the result, and then we can evaluate the rest. And now we get the recursive description. So using this, we can see all the debug descriptions for all the views. And I'm interested in the scoreboard views, which host these labels, so we can find the memory address for one of those. And now we can use [inaudible] po memory address, which you might be familiar with if you're an Objective-C developer. Well, that doesn't work and that's because Swift doesn't treat numbers as pointers and de-reference them for you. So once again, we need to do this from an Objective-C context. So we could do the same thing we did before, but I find this to be so convenient that I like to shortcut this down to just a simple short command. So I'm going to do that by using command alias, and I'm going to call that command poc. So now that I've created an alias, I can simply poc that memory address and see the debug description for that object. I'd like to show you another way to look at the description of an object if you only have its memory address. And in Swift, you can use a function called unsafe bit cast. Give it the memory address and then it's unsafe because it's up to you to provide the correct type, so I'll give it scoreboard view.self. And now we see we can use unsafe bit cast to see the debug description for an object. Now the great thing about unsafe bit cast is that it returns a typed result, so we can call our functions and property names on it such as .frame. And in this case, I'd like to inspect a center point and then modify that center point. Let's change it to 300 [inaudible] we can see it has changed to 300, but the view in the simulator hasn't moved. Well, why not? Well, we're paused in the debugger, so cronomation isn't currently applying any view module changes to the screen's frame buffer. But we can ask cronomation to do that for us, just use the expression ca transaction.flush and that tells cronomation to update the screen's frame buffer. So now, I can just use these two lines to fix the new positions and continue flashing and we can move [inaudible] around. And in fact, I find this to be so convenient that I kind of wanted to wrap all this up in just a single command to nudge views around, and so that's what I did. Let me show you that. I'm going to switch to terminal and open a Python file. Why a Python file? Well, LLDB is scriptable using Python, where you get full access to the LLDB API. So I've created an LLDB Python script to create a nudge command, which takes an x offset, a y offset and a view expression, and you can use that to nudge views around while paused in the debugger. Now, it might look like a sort of long script but most of that is argument pausing. The core of it, in the middle, is just calling out to the expressions we would call in manually. We don't have time unfortunately to go into detail in this script. But we're going to make this available for you guys to download, so you can see how it works and use it as the basis for your custom debugging commands. Let me show you how to enable a script like this. Just edit your .lldb in it file in your home directory and add a line command script import. I'd also like to add some of the aliases that I find convenient, such as the poc alias I created before, and an alias for flushing the transaction. I think I'll remember this one. I'm going to copy command script import so we can just paste it in to the debug session to save us restarting that session. And now we have a command called nudge. So I can, let's say, nudge zero horizontally, minus five vertically, give it the memory address of that view and start just nudging it around in the simulator. The great thing about LLDB is if you just hit enter on a blank line, it repeats the previous line, so it's great for nudging. And I can nudge it across to the right a bit, just to get it right. And then let's do the other view. We can give it any view expression, say the attempts view down to here. The other feature of nudge is once you've given it a view expression, you don't have to repeat that expression. It remembers that and applies it to the same view that you've specified previously. So something like that looks fine. It's a better layout than we had before. And what I can do now is take the information provided by nudge, such as the total offset applied to that view relative to its original center point, and then your frame value. Back to my code and modify my layout code or my auto-layout constraints, and I've easily mocked up a new layout for my scene. Now the last thing to do -- well, firstly, don't forget to check off debug, very important. And then the last thing to do before restarting or recompiling and rerunning is to disable or remove any breakpoints that are injecting expressions because you don't want those lines to be executed twice. Simply selecting them or a group of them and hitting delete is a quick way to delete those. And so those are some of the debugging techniques that I like to use to enhance my debugging workflows. Notice how we were able to diagnose and fix all four bugs without having to recompile or rerun. This can be a huge timesaver, especially for complex projects and can be crucial when trying to solve hard to reproduce bugs. So thanks for [inaudible] I hope you enjoyed that and can use these techniques in your debugging sessions. I'd just like to quickly recap all of the features and tricks that we went over during that debug session. So firstly, we looked at how we can use Xcode behaviors to dedicate attempt to debugging and how to use LLDB expressions to modify program state. We can use auto-continuing breakpoints with debugger commands to inject code live, and we can create dependent breakpoint configurations using breakpoint set one shot as a debugger command action for another breakpoint. Even when in assembly frames, we can easily inspect the function arguments using po dollar arg one, dollar arg two, et cetera, and we can skip lines of code by dragging the instruction pointer or using the command thread jump. We can request that the debugger pause when a variable is modified using watchpoints, and we can even evaluate Objective-C code in Swift frames using expression -l objc. We can request that view changes are flashed directly to the screen, even while paused in the debugger, using the expression ca transition flush. And you can add custom LLDB commands, either aliasing commonly used commands to correct shortcuts or by completely customizing and creating your own command using LLDB's Python scripting. And don't forget to check out our session website. We'll be posting that nudge script soon, so you can download it, check it out, and use it as the basis for your commands. There's one more thing I wanted to cover with you guys and that's just the current LLDB print commands. So you might be familiar with po. We used it a lot during the demo, and we saw that po requests the debug description of an object, and you can customize that. And that's because po is simply an alias for expression -- object description or expression -O, compared with the p commands, which is simply an alias for expression. And it uses LLDB's built-in formatters to show a representation of that object. The third command that's important to know is frame variable. It differs from the previous two in that it doesn't have to compile and evaluate an expression at all. It simply reads the value of the name variable directly from memory and then uses LLDB's built-in formatters. So the choice of which command to use is more personal preference and the type of information you want to see while debugging. But it's important to remember that if you ever end up in a situation where expressions are failing or so po and p may not be working for you, if you need to inspect a variable in the current frame, then frame variable should still work for you. And with that, I'd like to hand over to Sebastian, who's going to tell you about some advanced view debugging techniques. Thank you. Thank you, Chris. I'm excited to show you tips and tricks how to get the most out of Xcode's view debugger. And we'll also take a look at the enhancements we made for Xcode 10 to provide a great debugging experience when you adopt a dark look in macOS [inaudible]. And we'll take a look at this in a demo. So let me switch to the demo machine, and I'll be using the same project that Chris has been using and you already saw that there are two more bugs we have to solve. However, I'm not going to be using the iOS App, I'm going to be using the [inaudible]. So we can see the Mac version of our Solar System app here, we can see that looks pretty good in dark mode but there are two bugs that we have to solve today. First of all, the planet image is not centered horizontally correctly and that is a very obvious bug. You can see on the right-hand side this Earth image is shifted to the right-hand side, so we'll take a look at this problem. And the second bug is that the description in a popover is not readable in dark mode. Let me show you what that is referring to. When I switch to this app, I can bring up the orbital details information in this popover here. You can see that the labels at the top and nice and readable; however, the label at the bottom is so hard to read, I really have to select the text to read it. So these are the two bugs we have to take a look at. Let me hide the to do list and let's jump right in. So what I want to do is I want to capture the view hierarchy of this application with Xcode, then inspect it. We'll find the problem and then hopefully find the fix, so we can all have a beer. The problem is when I switch to Xcode to capture the view hierarchy, this popover will be dismissed since the application goes into the background, and we won't be able to capture its view hierarchy. So what we have to do is we have to capture the app in its active state, and I will show you two ways how to do that. And you can see when I now switch to Xcode how this popover is being dismissed. First of all, we can use the touch bar, and I'll show you what that looks like by bringing up the touch bar simulator from Xcode's window menu. I'll switch back to the Solar System app and bring up the popover again. And taking a look at the touch bar, you can see that there's this spray can icon, and when I tap that on the touch bar, you can see that there's a subset of the debug option that Xcode provides in its debug bar. So it's a very convenient way to access these from your touch bar. And as you can see, I can bring those up with Xcode being in the background, so you can access them even when you're, for example, developing your app in full screen mode. And one of these options allows me to capture the view hierarchy. Now I'm not going to do that because I know that not everybody has a touch bar [inaudible] so I will show you an alternative. I'm going to close the simulator, and I will make use of command click to perform the click on the button in Xcode's debug bar. Command click is a system-wide gesture that allows you to perform mouse events without activating the application that the mouse event is performed on. So this allowed us to invoke the capture of the view hierarchy. The debugger paused the application while it was still in its active state, and we can see that the UI still renders as if the app was front most and the popover hasn't been dismissed. If you're wondering why that spinning [inaudible] is coming up, that's because the application is paused in the debugger and doesn't respond to mouse events anymore. Now, you may be wondering, if we take a look at the view debugger here, why the popover isn't visible. Don't worry, the view hierarchy has been captured. I'll get to how we take a look at the popover once we get to that bug, but first I want to take a look at the layout issue of this image view here. So I'll select the image view here and I'll zoom in a little bit. When we take a look at the inspect on the right-hand side, we can see that this is an underscore NS image view simple image view. Now the fact that this is prefix with an underscore usually hints at the fact that this is an internal class from a system framework and not what we use when we set up image views in code or in interface builder. So let's take a look at this object in the view hierarchy. And I can do that by using the navigate menu, select reveal in debug navigator. We can now, on the left-hand side, see it relative to its super and sub views. Now, we can see that super view is in fact an NS image view, so that's what we're looking for. We can also see that its super view is a planet globe view, and the super view of the planet globe view is an NS effect view. So I will select the image view here and we can now also see other properties of the image view on the right-hand side. So let's inspect the layout of this view. I'm using auto-layout in this application, so I want to take a look at the auto-layout constraints. I can show the constraints using this button here, and we can now see all the constraints that currently affect the layout of this view. You can see that there's, for example, an [inaudible] constraint here. We can also see this vertical line here, which is an alignment constraint. When I select this constraint, we can see all the properties in the inspect on the right-hand side. If you're wondering why the view debugger only shows you wireframes right now, that is because it only shows the content of the views that currently participate in the layout of the selected view. And since all those views themselves don't have content, we currently only show wireframes. So with the constraint selected, let's take a look at the inspect on the right-hand side. We can see that it aligns the horizontal center of the image view with the horizontal center of the planet globe view and it does so with a constant of zero. So it aligns it horizontally centered in its super view. Now let's select the planet globe view from the debug navigator, and we can see that it's a little bit larger towards the left-hand side but it aligns on the right-hand side. So that's a little bit weird because we just saw the constraints aligning them correctly or exactly horizontally, but that's not really what we see in the view debugger. So let's take a look at the constraints of the planet globe view and see if we can understand what's going on. I'll select the leading constraint here, and taking a look at the inspect again, we can see that it aligns the leading edge of the planet globe view with the leading edge of the NS visual effect view, which we saw in super view. So it simply inserts it relative to its super view and it does so with a constant of 30, so that makes sense. And then the trailing constraint here aligns the trailing edge of the planet globe view with the trailing edge of the super view, and it also does so with a constant of 30. Now the fact that this constraint doesn't attach to anything on the right-hand side is a little bit suspicious and makes me wonder if we see the whole picture here. In such a situation, it's usually a good idea to see if there's any content that's currently being clipped that we don't see by default. And you can do so using this button down here to show clip content. Now, when I enable this functionality, you can see that the planet globe view actually extends past the window bounds towards the right-hand side, and now the horizontal centering constraint makes sense again since it actually correctly centers it in the super view. However, the super view just extends past the window. This is a very common issue if you set up your constraints in code, where you accidentally swap first and second item and thereby get the layout direction wrong. Or you accidentally invert the constant, and in this case, we used 30 instead of using negative 30 to insert it towards the left-hand side. So what I want to do is I want to try out that fix to invert the constant. I will make use of the same technique that Chris introduced earlier by simply applying it through LLDB. So, with this constraint selected, I'll select edit, copy. And I will bring up the console area at the bottom, and what this copying gives me is the [inaudible] pointer to the selected object, and that works for all objects that you select in the view debugger as well as in the memory graph debugger. It makes it super convenient to use them in the console. So let's -- Thank you. Let's print the debug description and we can confirm that the constant is in fact positive 30. That's what we saw in the inspector as well. So let's set the constant to negative 30. I'll type e, which is the super short form of expression by casting the pointer and then say set constant to negative 30. We have the same problem that the path application doesn't update that Chris saw earlier, so what I'm going to do or what I have to do is I have to [inaudible] the ca transaction to [inaudible] application schedule update of its UI. I can use the handy command that Chris added earlier, so I can take the command from here. And we can now see that the planet image is horizontally centered correctly, so inverting the constant was the correct fix. So we were able to confirm this, so let's apply this change in our code. With the constraints selected, you can see that on the right-hand side I can see this back trace here, and that is the allocation back trace of the constraint and tells me exactly in which frame it was allocated. So it allows me to jump right to the code where I created that constraint. Now to have this creation back traces show up, you have to enable mailx backlogging and let me show you how you enable that. You go to your scheme up here and [inaudible] edit scheme. And in the diagnostics tab of the scheme options, under logging, you can enable mailx stack logging for all allocations in prehistory. And that will provide you with these handy allocation backtraces for all selected objects in the view debugger as well as in Xcode's memory graph debugger. Now, when I mouse over this stackframe here, we can see the full name of that frame. And we can see that it is the set up planet globe view layout method in the scene view controller. And I can jump to these points in code using this jump to arrow. And I will do so by holding down shift control option, which brings us this navigational [inaudible] and allows me to open this file in a separate window. Now you can see that the line of code where the constraint is allocated is highlighted. We can see the constant of 30, and I can invert it to negative 30. I'll save this file and close it and we're done with our first bug. Perfect. So the second bug was that we wouldn't be able to read the description inside that popover, so let's take a look at that. First of all, I want to disable constraint mode and clipping so we see the constant again. And I will also clear the console. Now, I showed you at the beginning how to capture this [inaudible] in its active state so we would be able to get to the view hierarchy of the popover while it's open; however, we don't see it. That is because the view debugger only shows you a single window at a time. Let me show you how to take a look at other windows. When I scroll up in the view hierarchy and walk up the view hierarchy to eventually hit the window of the current window that we see, we can see that it's hosted by a window controller. And if I collapse this root level item here, we can see that there's actually another root level item, which is exactly what we're looking for. Our popover. So if your application has multiple windows, and that is true for macOS and iOS, they show up as multiple root level objects in the outline on the left-hand side. So take a look there, if you think your application should have multiple windows that should be captured by the view debugger. So we can take a look at this in 3D, and we can see that there's this large view that's obscuring clicks to the labels. I would like to take a look at the labels to inspect them. There's a trick to click through views in the view debugger. You can simply hold down command. So I can click through this view in front and select this blue label. So let's take a look at the text color for this label. I want to first look at the labels that look great in dark so that we can hopefully derive a solution for the problematic label at the bottom. So let's take a look at the text color and we can see that this is a RGB color resulting in this blue and we can also see that the inspector provides us with a name for this color. Now this indicates that this color is coming from an asset catalogue in our project and with Xcode 10, you can provide multiple variants of a color for a single color that you define. So, for example, you can provide one for light and one for dark. And which variant is picked is determined by the appearance of a view, and you can get that information in the inspector as well. And I scroll down here to the view section in the inspector. You can see that there's appearance and effective appearance. Now appearance is not set explicitly on this view. And this is a very common scenario because most views inherit the appearance from either one of the super views, the window or the application. But you can see the inherited effective appearance here. And that is vibrant dark, so that determines which color of the color that you define in your asset catalogue is selected. Okay. Actually, while I'm down here in the inspector, I want to point out the description property, which is the debug description of that object. And Chris showed you earlier how to provide a custom debug description for your objects. So you don't only benefit from providing a great debug description in your console when you use po on an object but you also benefit from it in the view debugger, since it shows up right in the inspector. Okay, so let's get back to the text color. And I want to select the second label since that also looks good in dark. And in this case, we can see there's also a name color. It's a label color, but it's prefixed with system. And that indicates that it's not coming from our own asset catalogue but from the system, and of course, system colors automatically adapt to appearance changes as well. Now, taking a look at the problematic label here, we can see that it's this very dark gray, and it doesn't have a name. That means it's a custom color that doesn't adapt to appearance changes. So what we want to do is we want to change its color, its text color, to a system color. So with this object selected, I'll hit command c. I'll type e, paste in the [inaudible] pointer and say set text color to NS color text color. Again, I have to [inaudible] to see a transaction. And we can see that the popover now updates, and the font is nice and readable. Now, I'm not going to apply this fix in my storybook file. But one thing I want to point out is it's very important that you verify that your application still looks good across all appearances when you make a change to your [inaudible] now that there are multiple system appearances. And I will show you how to do that. I will continue running, and instead of switching my entire system appearance to light to take a look if the label still looks good on a light background, Xcode 10 provides you with a way to override the appearance only for the target application. And you can use this button in the debug bar. And I can select light here, and you can see that the application now is presented in a light appearance. And I can bring up the popover, and we can verify that the text is nice and readable. So that confirms that we fixed our issue. Now, since this is a very common action to take a look at your application in different appearances, we actually made that option available in the touch bar. I will show you [inaudible]. I'll bring up the touch bar simulator again, and with the popover opened, I can select this option here. And you can now see all the override options right in your touch bar, so even if your application is in full screen mode, for example, I can access these. Let me switch to high contrast light, which enables the accessibility feature of high contrast as well as overrides the appearance to light, so you can make sure that your app looks good in that configuration as well. And, of course, I can go back to the system appearance as well. So we confirmed that our issue is fixed. And that allows me to check off this on our bug list [inaudible] with this, we're at the end of the demo and I will go back to slides. So let's recap what we just saw. I showed you how to use reveal in debug navigator to orient your current selection in the hierarchical outline on the left-hand side. I showed you how to show clip content, and we made use of the auto-layout debugging functionality to track down our constraint problem. I showed you how to use the object pointers that you can easily access by just copying the selected objects and use them in LLDB. We had a look at the creation backtraces, which are available if you have mailx stack logging enabled in your scheme options to jump right to the code and applied a change that we needed for our constraint. We had a look at the debug description that's conveniently available in the inspector of the view debugger. And we had a look at click through to select a view that was behind another view. And regarding debugging dark mode, we saw that you can easily override the appearance of the target application right from Xcode's debug bar or from the touch bar. I showed you how to capture a Mac app in its active state. And we had a look at the name color information and its appearance information that is now available in Xcode's view debugger inspector. If you want to learn more about adapting the dark mode in your Mac application, please check out the videos for these two sessions. That brings us to the end of this talk. If you want more information about the talk, including the [inaudible] script that Chris showed you earlier, it will be posted on the session's website. And if you have any questions to any of the content of this session, or debugging in general, there's a profiling and debugging lab tomorrow morning at 9 am. Chris and I will be there. We'll be happy to answer any questions you may have. And there's also an iOS memory deep dive talk, in case you're interested, in memory debugging that is also tomorrow. With that, I hope you have a fantastic time at the Beer Bash and enjoy the rest of the conference.  Hey, guys. Thank you. Thanks for coming by, guys. Welcome to Designing Fluid Interfaces. My name is Chan. And, I work on the human interface team here at Apple. And, most recently, I worked on this, the fluid gestural interface for iPhone 10. So, me, Marcos, and Nathan, we want to share a little bit about what we learned working on this, and other projects like this in the past. So, the question we ask ourselves a lot is, what actually makes an interface feel fluid? And, we've noticed that a lot of people actually describe it differently. You know, sometimes, when people actually try this stuff, when we show them a demo, and they try it, and they hold it in their hands, they sometimes say it feels fast. Or, other people sometimes say it feels smooth. And, when it's feeling really good, sometimes people even say it feels natural, or magical. But, when it comes down to it, it really feels like, it's one of those things where you just know it when you feel it. It just feels right. And, you can have a gestural UI, and we've seen lots of gestural UI's out there, but if it's not done right, something just feels off about it. And, it's oftentimes hard to put your finger on why. And, it's more than just about frame rates, you know. You can have something chugging along at a nice 60 frames per second, but it just feels off. So, what gives us this feeling? Well, we think it boils down to when the tool feels like an extension of your mind. An extension of your mind. So, why is this important? Well, if you look at it, the iPhone is a tool, right? It's a hand tool for information and communication. And, it works by marrying our tactile senses with our sense of vision. But, if you think about it, it's actually part of a long line of hand tools extending back thousands of years. The tool on the left here was used to extract bone marrow 150,000 years ago, extending the sharpness of what our fingers could do. So, we've been making hand tools for some time now. And, the most amazing thing is that our hands have actually evolved and adapted alongside our tools. We've evolved a huge concentration of muscles, nerves, blood vessels that can perform the most delicate gestures, and sense the lightest of touches. So, we're extremely adapted to this tactile world we all live in. But, if you look at the history of computers, we started in a place where there was a lot of layers of extraction between you and the interface. There was so much you had to know just to operate it. And, that made it out of reach for a lot of people. But, over the last few decades or so, you've sort of been stripping those layers back you know, starting with indirect manipulation, where things were a little bit more one-to-one. A little bit more direct all the way to now, where we're finally stripping away all those layers back, to where you're directly interacting with the content. This to us is the magical element. It's when it stops feeling like a computer, and starts feeling more like an extension of the natural world. This means the interface is now communicating with us at a much more ancient level than interfaces have ever done. And, we have really high standards for it. You know, if the slightest thing feels wrong, boom, the illusion is just shattered. But, when it feels right, it feels like an extension of yourself, an extension of your physical body. It's a tool that's in sync with your thought. It feels delightful to use, and it feels really low-friction, and even playful. So, what gives us this feeling? And, when it feels off, how do we make it feel right? That's what this presentation's all about. We're going to talk about four things today. And, we're going to start with designing some principles, talking about how we build interfaces that feel like an extension of us. How to design motion that feels in tune with the motion of our own bodies, and the world around us. And, also designing gestures that feel elegant and intelligent. We're also going to talk about that, now that we've built this kind of stuff, how do we build interactions on top of it that feel native to the medium of touch, as a medium? So, let's get started. How do we design an interface that actually extends our mind? How do we do this? Well, we think the way to do it, is to align the interface to the way we think and the way we move. So, the most important part of that is that our minds are constantly responding to changes and stimulus and thought, you know? Our minds and bodies are constantly in a state of dynamic change. So, it's not that our interfaces should be fluid, it's that we're fluid, and our interfaces need to be able to respond to that. So, that begins with response. You know, our tools depend on the latency. Think about how hard it would be to use any tool, or play an instrument, or do anything in the physical world, if there was a delay to using it? And, we found that people are really, really sensitive to latency. You know? If you introduce any amount of lag, things all of a sudden just kind of fall off a cliff in terms of how they respond to you. There's all this additional mental burden. It feels super disconnected. It doesn't feel like an extension of you anymore. So, we work so hard to reduce latency. Where, we actually engineered the latest iPhone to respond quicker to your finger, so we can detect all the nuances of your gestures as instantly as possible. So, we really care about this stuff, and we think you should too. And, that means look for delays everywhere. It's not just swipes. It's taps, it's presses, it's every interaction with the object. Everything needs to respond instantly. And, during the process of designing this stuff, you know, oftentimes the delays kind of tend to seep in a little bit. You know? So, it's really important to keep an eye out for delays. Be vigilant and mindful of all the latencies or timers that we could introduce into the interface so that it always feels responsive. So, that's the topic of response. It's really simple, but it makes an interface feel lively and dynamic. Next, we want to allow for constant redirection and interruption. This one's big. So, our bodies and minds are constantly in a state of redirecting in response to change in thought, like we talked about. So, if I was walking to the end of this stage here, and I realize I forgot something back there, I could just turn back immediately. And, I wouldn't have to wait for my body to reach the end before I did that, right? So, it's important for our interfaces to be able to reflect that ability of constantly redirecting. And, it makes it feel connected to you. That's why for iPhone 10, we built a fully redirectable interface. So, what's that? So, the iPhone 10's an actually-- it's pretty simple two-axis gesture. You go horizontally between apps. And, you go vertically to go home. But you can also mix the two axes, so you can be on your way home, and peek at multitasking and decide whether or not to go there. Or, you can go to multitasking and decide, actually, no, I want to go home. So, this might not seem that important, but what if we didn't do this? What if it wasn't redirectable? So, what if the only gestures you could do was this horizontal gesture between apps, and then a vertical gesture to go home, and that's it. You couldn't do any of that in-between stuff I just mentioned. Well, what would happen is that you would have to think before what you did, before you performed the gesture, you'd have to think what you want to do. And so, the series of events would be very linear, right? So, you'd have to think, do I want to go home? Do I want to go to multitasking? Then you make your decision, then you perform the gesture, and then you release. But, the cool thing is when it's redirectable, the thought and gesture happen in parallel. And, you sort of think it with the gesture, and it turns out this is way faster than thinking before doing. You know? Because it's a multi-axis gestural space. It's not separate gestures. It's one gesture that does all this stuff. Home, multitasking, quick app switching, so you don't have to think about it as a separate gesture. And, helps with discovery. Because you can discover a new gesture along the path of an existing gesture. And, it allows you to layer gestures at the speed of thought. So, what does that last one mean? So, let me show you some examples. And, we've slowed down the physics on the simulation, so you can actually see a little bit what I'm talking about. So, I can swipe to go home, and then swipe to the next page, or springboard while I'm going home. I can layer these two gestures once I've internalized them. Another example is that I can launch an app and realize, oh, actually I need to go to multitasking, and I can interrupt the app and go straight to multitasking, while the app is launching. Or, I can launch an app and realize, oh, that was the wrong app. And, I can shoot it back home, while I'm launching it. Now, there's one other one where I can actually just launch an app, and if I'm in a hurry, I can start interacting with the app as it's launching. So, this stuff might not seem really important, but we've found it's super important for the interface to be always responding, always understanding you. It always feels alive. And, that's really important for your expectation and understanding of the interface, to be comfortable with it. To realize that it's always going to respond to you when you need it. And, that applies as well to changes in motion, not just to the start of an interaction, but when you're in the middle of an interaction, and you're changing. It's important for us to be responsive to interruption as well. So, a good example is multitasking on iPhone 10. So, we have this pause gesture where you slide your finger up halfway up the screen, and pause, and so we need to figure out how to detect this change in motion. And so, how do we do this? How do we detect this change in motion? Should we use a timer? Should we wait until your finger has come below a certain velocity for a few amount of time, and then [inaudible] bring in the multitasking cards? Well, it turns out that's too slow. People expect to be able to get to multitasking instantly. And, we need a way that can respond as fast as them. So, instead we look at your finger's acceleration. It turns out there's a huge spike in the acceleration of your finger when you pause. And, actually the faster you stop, the faster we can detect it. So, it's actually responding to the change in motion, as fast as we know how, instead of waiting for some timer. So, this is a good example of responding to redirection as fast as possible. So, this is the concept of interruption and redirection. This stuff makes the interface feel really, really connected to you. Next, we want to talk a little bit about the architecture of the interface. How you lay it out, conceptually. And, we think when you're doing that, it's important to maintain spatial consistency throughout movement. What does that mean? This kind of mimics the way our object persistence memory works in the real world. So, things smoothly leave and enter our perception in symmetric paths. So, if something disappears one way, we expect it to emerge from where it came? Right? So, if I walked off this stage this way, and then emerged that way, you'd be pretty amazed, right? Because that's impossible. So, we wanted to play into this consistent sense of space that we all have in the world. And so, what that means is, if something is going out of view in your interface, and coming back into view, it should do so in symmetric paths. It should have a consistent offscreen path as it enters and leaves. A good example of this is actually iOS navigation. When I tap on an element in this list here, it slides in from the right. When I tap the back button, it goes back to the right. It's a symmetric path. Each element has a consistent place where it lives at both states. This also reinforces the gesture. If I choose to slide it myself to the right, because I know that's where it lives, I can do that. It's expected. So, what if we didn't do this. Here's an example, where when I tap on something, it slides in, and then when I hit back it goes down. And, it feels disconnected and confusing, right? It feels like I'm sending it somewhere. In fact, if I wanted to communicate that I was sending it somewhere, this is how I could do it, right? So, that's the topic of spatial consistency. It helps the gesture feel aligned with our spatial understanding of the world. Now, the next one is to hint in the direction of the gesture. You know, we humans are always, kind of, predicting the next few steps of our experience. We're always using the, kind of, trajectories of everything that's happening in the world to predict the next few steps of emotion. So, we think it's great when an interface plays into that prediction. So, if you have two states here, initial state and a final state. The object-- and you have an intermediate transition. The object should transition smoothly between these two states in a way that it grows from the initial state to the final state, whether it's through a gesture or an animation. So, good example is Control Center actually. We have these modules here in Control Center, where as you press they grow up and out towards your finger in the direction of the final state, where it actually finally just pops open. So, that's hinting. It makes the gestures feel expected, and predictable. Now, the next important principle is to keep touch interactions lightweight. You know the lightness of multitouch is one of the most underrated aspects of it, I think. It enables the airy swipes and scrolls, and all the taps and stuff that we're all used to. It's all super lightweight. But, we also want to amplify their motion. You want to take a small input and make a big output, to give that satisfying feeling of moving or throwing something and having a magnified result. So, how does this apply to our interfaces? Well, it starts with a short interaction. A short, lightweight interaction. And, we use all our sensors, all our technology, to understand as much about it. To, sort of, generate a profile of energy and momentum contained within the gesture. Using everything we know, including position, velocity, speed, force, everything we know about it to generate a kind of, inertial profile of this gesture. And then we take that, and generate an amplified extension of your movement. It still feels like an extension of you. So, you get that satisfying result with a light interaction. So, a good example of this is scrolling, actually. Your finger's only onscreen for a brief amount of time, but the system is preserving all your energy and momentum, and gracefully transferring it into the interface. So, what if it didn't have this? Those same swipes, well, they wouldn't get you very far. And, in order to scroll, you'd have to do these long, laborious swipes that would require a lot more manual input. It would be a huge pain to use. Another good example of this is swipe to go home. The amount of time that your finger's onscreen is very light. And, it's-- ends up making it a much more liquid and lightweight gesture that still feels native to the medium of multitouch. While still being able to reuse a lot of your muscle memory from a button, because you move your finger down on the screen, and back up to the springboard. And, it's not just swipes, it's taps too. It's important for an interface to respond satisfyingly to every interaction. The interface is signaling to you that it understood you. It's so important for the interface to feel alive and connected to you. So, that's the topic of lightweightness and amplification. The next one is called rubberbanding. It means we're softly indicating boundaries of the interface. So, in this example, the interface is gradually and softly letting you know that there's nothing there. And, it's tracking you throughout. It's always letting you know that it's understanding you. What happens if you didn't do that? Well, it would feel like this. It would feel super harsh and disconcerting. You kind of hit a wall there. It would feel broken, right? And, you actually wouldn't know the difference between a frozen phone, and phone that's just at the top of the edge of the screen, right? So, it's really important that it's always telling you that you've reached the edge. And, this applies to transitions, too. It's not just about when you hit the edge, it's also when you hand off from one thing to another thing. Tracking. So, a good example of this is when you transition from sliding up the dock to sliding up the app. It doesn't just hit a wall, and one thing stops tracking, and then the other thing takes over. They both smoothly hand off in smooth curves, so that you don't feel like there's this harsh moment where you hand off from one thing to another. Next one is to design smooth frames of motion. So, imagine I have a little object here moving up and down. It's very simple. But, we all know this object is not really moving, right? We're all just having the perception of it moving. Because we're seeing a bunch of frames on the screen all at once, and it's giving us the illusion of it moving. So, if we took all of those frames of motion, and kind of, spread them out here. And we see the ball's in motion over time, the thing that we're concerned about is right around here, where there's too much visual change between the adjacent frames. This is when the perception of the interface becomes a little choppy. You get this visual strobing. And, this is because the difference between the two frames is too much. And, it strobes against your vision, so. Here's an example of where you have two things both moving at 30 frames per second. But the one on the left looks a bit smoother than the one on the right, because the one on the right is moving so fast, that it's strobing. My perception of vision is, kind of, breaking down. I don't believe that it's moving smoothly any more. So, the important thing to take away is that it's not just about framerate. It's what's in the frames. So, we're kind of limited by the framerate, and how fast we can move and still preserve a smooth motion. So, this one's in 30 frames per second. If we move it up to 60 frames per second, you can see that we can actually go a little bit faster, and still preserve smooth motion. We can do faster movement without strobing. And, there's addition tricks we can do too, we can do things like motion blur. Motion blur basically bakes in more information in each frame about the movement, like the way your eyes work, and the way a camera works. And you can also do-- take a page from 2D animation and video games by stretching, this-- this technique called motion stretching stretches the content in each frame to provide this elastic look as it moves with velocity. And so, in motion, it kind of looks like this. So, each of the different techniques, kind of, tries to encode more information visually about what's going on in the motion. And, I want to focus a little bit on this last one here, motion stretching, because we do this on iPhone 10, actually. You know, when you launch an app, the icon elastically stretches down to become the app as it opens. And, it stretches up in the opposite direction as you close the app. To give you that little bit extra information between each frame of motion to make it a little bit smoother-looking. Lastly, we want to work with behavior rather than animation. You know, things in the real world are always in a state of dynamic motion, and they're always being influenced by you. They don't really work like animations in the animation sense, right? There's no animation curve prescribed by real life. So, we want to think about animation and behavior more as a conversation between you and the object. Not prescribed by the interface. So, to move away from static things transitioning into animated things, instead think about behavior. So, Nathan's about to dive deep into this one. But, here's a quick example. So, in Photos, there's less mass on the photos, because it's conceptually lighter. But, then when you swipe apps, there's more mass on the apps. It's conceptually heavier, so we give more mass to the system. So, that's a little bit about how to design interfaces that think and work like us. In-- it starts with response. To make things feel connected to you, and to accommodate the way our minds are constantly in motion. To maintain spatial consistency, to reinforce a consistent sense of space, and symmetric transitions within that space. And, to hint in the direction of the gesture. To play into our prediction of the future. And, to maintain lightweight interactions, but amplify their output. To get that satisfying response, while still keeping the interaction airy and lightweight. And, to have soft boundaries and edges to the interface. That interface is always gracefully responding to you, even when you hit an edge, or transition from tracking one thing to tracking the other. And, to design smooth dynamic behavior that works in concert with you. So, that's some principles for how to approach building interfaces that feel like an extension of our minds. So, let's dive in a little deeper. I'm going to turn it over to Nathan de Vries, my colleague, to design motion-- to talk about designing motion in a way that feels connected to motion, to the motion of both you and the natural world. Thanks, Chan. Hi everyone. My name's Nathan, and I'm super excited to be here today to talk to you about designing with dynamic motion. So, as Chan mentioned, our minds and our bodies are constantly in a state of change. The world around us is in a state of change. And, this introduces this expectation that our interfaces behave the same way, as they become more tactile, it shifts our expectations to be much higher fidelity. Now, one way we've used motion in interfaces is through timed animations. A button is tapped on the screen, and the reins are, kind of, handed over to the designer. And, their job is to craft these perfect frames of animation through time. And, once that animation is complete, the controls are handed back to the person using the interface, for them to continue interacting. So, you can kind of think of animation and interaction as being-- as moving linearly through time in this, kind of, call and response pattern. In a fluid interface, the dynamic nature of the person using the interface kind of shifts control over time away from us as designers. And, instead, our role is to design how the motion behaves in concert with an interaction. And, we do this through these continuous dynamic behaviors that are always running, that are always active. So, it's these dynamic behaviors that I'm going to, really focus on today. First of all, we're going to talk about seamless motion. And, it's this element of motion that makes it feel like the dynamic motion is an extension of yourself. Then, we're going to take a look at character. How, even without timing curves, and timed animations, we can introduce the concept of playfulness, or character, or texture to motion in your interfaces. And finally, we'll look at how motion itself gives us some clues about what people intend to do with your interface. How we can resolve some uncertainty about what a gesture is trying to do by really looking at the motion of the gesture. So, to kick things off, let's look at seamless motion. What do I mean by seamless motion? So, let's look at an example that I think we can all familiarize with. So, here we have a car, and it's cruising along at a constant speed. And then, the brakes are applied, slowing it down to a complete stop. Let's look at it again, but this time we'll plot out the position of the car over time. So, at the very start of this curve it's, kind of, straight, and pointing up to the right. And, this shows that the car's position is moving at a constant rate, it's kind of unchanging. But then, you'll notice the curve starts to bend, to smoothly curve away from this straight line. And, this is the brakes being applied. The car is decelerating from friction being introduced. And, by the end of the curve, the curve is completely flat, horizontal, showing that the position is now unchanging. That the car is stopped. So, this position curve is visualizing essentially what we call seamless motion. The line is completely unbroken, and there are no sudden changes in direction. So, it's smooth and it's seamless. Even when, actually, new dynamic behaviors are being introduced to the motion of the car, like a brake, which is applying friction to the car. And, even when the car comes to a complete stop, you'll notice that the curve is completely smooth. There's this indiscernible quality to it. You can't tell when the car precisely stopped. So, why am I talking about cars? This is a talk about fluid interfaces, right? So, we feel like the characteristics of the physical world make for great behaviors. Everyone in this room finds the car example so simple because we have a shared understanding, or a shared intuition for how an object like a car moves through the world. And, this makes us a great reference point. Now, I don't mean that we need to build perfect physical simulations of cars that literally drive our interface. But, we can draw on the motion of a car, of objects that we throw or move around in the physical world around us and use them in our own dynamic behaviors to make their motion feel familiar, or relatable, or even believable, which is the most important thing. Now, this idea of referencing the physical world in dynamic behaviors has been in the iPhone since the very beginning with scrolling. A child can pick up an iPhone and scroll to their favorite app on the Home screen, just as easily as they can push a toy car across the floor. So, what are some key, kind of, characteristics of this scrolling, dynamic behavior that we have? Well, firstly it's tapping into that intuition, that shared understanding that we all have for objects moving around in the world. And, our influence on those objects. The motion of the content is perfectly seamless, so while I'm interacting with it, while I'm dragging the content around, my body is providing the fluidity of the movement, because my body is fluid. But, as soon as I let go of the content, it seamlessly coasts to a stop. So, we're kind of maintaining the momentum of the effort being put into the interface. The amount of friction that's being used for scrolling is consistent, which makes it predictable, and very easy to master. And finally, the content comes to an imperceptible stop, kind of like the car, not really knowing precisely when it came to a stop. And, we feel that this distinct lack of an ending kind of reinforces this idea that the content is always moving, and always able to move, so while content is scrolling, it makes it feel like you can just put your finger down again, and continue scrolling. You don't have to wait for anything to finish. So, there are innumerable characteristics of the physical world that would make for great behaviors. We don't have time to talk about them all, but I'd like to focus on this next one, because we personally find it incredibly indispensable in our own design work. So, materials like this beautiful flower here, the natural fibers of this flower have this organic characteristic called elasticity. And, elasticity is this tendency for a material to gracefully return into a resting state once stress or strain is removed. Our own bodies are incredibly elastic. Now, we're capable of running incredibly long distances, not because of the strength of our muscles, but because of their ability to relax. It's their elasticity that's doing this. So, our muscles contract and relax once stress and strain is removed. And, this is how we conserve energy. Makes us feel natural and organic. The same elasticity is used in iPhone 10. Tap an icon on the Home screen, and an elastic behavior is pulling the app towards you. Bring it exactly where you want it to be. And, when you swipe from the bottom, the app is placed back on the Home screen in its perfect position. We also use elasticity in scrolling. So, if I scroll too far and rubberband, like Chan was talking about, when you let go, the content uses elasticity to pull back within the boundaries, helping you get into this resting position, ready for the next time you want to scroll. So, let's dig in a little deeper on how this elasticity works behind the scenes. You can think of the scrolling content as a ball attached to a spring. On one end of the spring is the current value. This is where the content is on the display. And, the other end of the spring is where the content wants to go because of its elasticity. So, you've got this spring that's pulling the current value towards the target. Its behavior is influencing the position of the content. Now, the spring is essentially pulling that current value towards the target. And, what's interesting about a spring is, it does this seamlessly. This seamlessness is, kind of, built in to the behavior. And, this is what makes them such versatile tools for doing fluid interface design. Is that you, kind of, get this stuff for free. It's baked in to the behavior itself. So, we love this behavior of a value moving towards a target. We can just tell the ball where to go, and we'll get this seamless motion of the ball moving towards the target. But, we want a little bit more control over how fast it moves. And, whether it overshoots. So, how do we do that? Well, we could give the ball a little more mass, like make it bigger, or make it heavier. And, if we do that, then it changes the inertia of the ball, or its willingness to want to start moving. Or, maybe its unwillingness to want to stop moving. And, you end up with this little overshoot that happens. Another property that we could change is the stiffness of the spring, or the tensile strength of the spring. And, what this does, is it affects the force that's being applied to the ball, changing how quickly it moves towards the target. And, finally, much like the car, and the braking of a car, we can change the damping, or the friction, of the surface that the ball is sitting on. And, this will act as, kind of, a brake that slows the ball down over time, also affecting our ability to overshoot. So, the physical properties of a ball and a spring are, kind of, physics class material, right? It's super useful in a scientific context, but we've found that in our own design work they can be a little bit overwhelming or unwieldy for controlling the behavior of objects on the screen. So, we think our design tool should have a bit of a human interface to them. That they need to reflect the needs of the designer that's using the tool. And so, how do we go about that? How do we simplify these properties down to make it more design friendly? So, mass stiffness and damping will remain behind the scenes, they're the fundamental properties of the spring system that we're using. But, we can simplify our interface down to two simple properties. The first is damping, which controls how much or little overshoot there is from 100% damping, where there will be no overshoot to 0% damping where the spring would oscillate indefinitely. The second property is response. And, this controls how quickly the value will try and get to the target. And, you might notice that I haven't used the word duration. We actually like to avoid using duration when we're describing elastic behaviors, because it reinforces this concept of constant dynamic change. The spring is always moving, and it's ready to move somewhere else. Now, the technical terms for these two properties are damping ratio and frequency response. So, if you'd like to use these for your own design work, you can look up those terms, and you'll find easy ways to convert them. So, we now have these two simple properties for controlling elastic behaviors. But, there's still an infinite number of possibilities that we can have with these curves. Like, there's just hundreds, thousands, millions of different ways we can configure those two simple properties and get very different behavior. How do we use these to craft a character in our app? To control the feel of our app? Well, first and foremost, we need to remember that our devices are tools. And, we need to respect that tools, when they're used with purpose, require us to not be in the way, not get in the way with introducing unnecessary motion. So, we think that you should start simple. A spring doesn't need to overshoot. You don't need to use springy springs. So, we recommend starting with 100% damping, or no overshoot when you're tuning elastic behaviors. That way you'll get smooth, graceful, and seamless motion that doesn't distract from the task at hand. Like, just quickly shooting off an email. So, when is it appropriate to use bounciness? There's got to be a time when that's appropriate, right? Well, we feel if the gesture that's driving the motion itself has momentum, then you should reward that momentum with a little bit of overshoot. Put another way, if a gesture has momentum, and there isn't any overshoot, it can often feel broken or unsatisfying to have the motion follow that gesture. An example of where we use this is in the Music app. So, the Music app has a small minibar representing Now Playing at the bottom of the screen, and you can tap the bar to show Now Playing. Because the tap doesn't have any momentum in the direction of the presentation of Now Playing, we use 100% damping to make sure it doesn't overshoot. But, if you swipe to dismiss Now Playing, there is momentum in the direction of the dismissal, and so we use 80% damping to have a little bit of bounce and squish, making the gesture a lot more satisfying. Bounciness can also be used as a utility, as a functional means. It can serve as a helpful hint that there's something more below the surface. With iPhone 10, we introduced two buttons to the cover sheet for turning on the Flashlight, and for launching the Camera. To avoid accidentally turning on the flashlight by mistake, we require a more intentional gesture to activate the Flashlight. But, if you don't know that there's a more intentional gesture needed to activate it, when you tap on the button, it responds with bounciness. Has this kind of playful feel to it. And, that hint is teaching you not only that the button is working, but that it's responding to you. But, it's kind of teaching you that if you press just a little bit more firmly, it'll activate. It's like teaching you. It's hinting in the direction of the motion. So, bounciness can be used to indicate this kind of thing. Now, so far we've been talking about using motion to move things around, or to change their scale, change their visual representation on the screen. But, we perceive motion in many different ways. Through changes in light and color, or texture and feel. Or even sound. Many other sensations that we-- our senses can detect. We feel this is an opportunity to go even further, go beyond motion, when you're tuning the character of your app. By combining dynamic behaviors for motion with dynamic behaviors for sound and haptics, you can really fundamentally change the way an interface feels. So, when you see, and you hear, and you feel the result of the gesture, it can transform what was otherwise just a scrolling behavior into something that feels like a very tactile interface. Now, there's one final note I want you thinking about when you're crafting the character of your app. And, that's that it feels cohesive, that you're staying in character. Now, what does this mean? So, even within your app, or across the whole system, it's important that you treat behaviors as a family of behaviors. So, in scrolling for example, when I scroll down a page, using a scrolling behavior, and then I tap the status bar to scroll to the stop of the page, using an elastic behavior. In both cases, the page itself feels like it's moving in the same way, that it has the same behavior, even though two different types of behaviors are driving its motion, are influencing its motion. Now, this extends beyond a single interaction like scrolling. It applies to your whole app. If you have a playful app, then you should embrace that character, and make your whole app feel the same way. So, that people-- once they learn one behavior of your app, they can pick up another behavior really easily, because we learn through repetition. And, what we learn bleeds over into other behaviors. So, next up, I'd like to talk a little bit about aligning motion, or dynamic motion, with intent. So, for a discrete interaction like a button, it's pretty clear what the intent of the gesture is. Right? You've got three distinct visual representations on screen here. And, when I tap one of them, the outcome is clear. But, with a gesture like a swipe, the intent is less immediately clear. You could say that the intent is almost encoded in the motion of the gesture, and so it's our job, our role, to interpret what the motion means to decide what we should do with it. Let's look at an example. So, let's say I made a FaceTime call, a one-on-one FaceTime call, and in FaceTime, we have a small video representation of yourself in the corner of the screen. And, this is so I can see what the person on the other end sees. We call this floating video the PIP, short for picture in picture. Now, we give the PIP a floating appearance to make it clear that it can be moved. And, it can be moved to any corner of the screen, with just a really lightweight flick. So, if we compare that to the Play, Pause, and Skip buttons, like, what's the difference here? So, in this case, there's actually four invisible regions that we're dealing with. No longer do we have these three distinct visual representations on screen that are being tapped. We kind of have to look at the motion that's happening through the gesture, and intuit what was meant. Which corner did we intend to go to? Now, we call these regions of the screen endpoints of the gesture. And, when the PIP is thrown, our goal is to find the correct endpoint, the one that was intended. And, we call this aligning the endpoint with the intent of the gesture. So, one approach for this is to keep track of the closest endpoint as I'm dragging the PIP. Now, this kind of works. I can move the PIP to the other corner of the screen, but it starts to break down as soon as I move the PIP a little bit further. Now, I actually need to drag the PIP quite far, like past halfway over the screen. Pretty close to the other corner. So, it's not really magnifying my input. It's not really working for me. And, if I try and flick the PIP, it kind of goes back to the nearest corner, which isn't necessarily what I expected. So, the issue here is that we're only looking at position. We're completely ignoring the momentum of the PIP, and its velocity when it's thrown. So, how can we incorporate momentum into deciding which endpoint we go to? So, to think about this, I think we can set aside endpoints for a moment, and take a step back. And, just really simplify the problem. Ultimately, what I'm trying to do here is move content around on the screen. And, I actually already have a lot of muscle memory for doing exactly that with scrolling. So, why don't we use that here? We use scrolling behaviors all the time, so we have this natural intuition for how far content goes when I scroll. So, here you can see that when I scroll the PIP instead, it coasts along, and it slows down, using this familiar deceleration that we're familiar with from scrolling. And, basically by taking advantage of that here, we're reinforcing things that people have learned elsewhere. That the behavior is just doing what was expected of the system. Now this new, hypothetical, imaginary PIP position is not real. We're not going to show the PIP go here in the interface. This is what we call a projection. So, we've taken the velocity of the PIP, when it was thrown. We've, kind of, mixed in the deceleration rate, and we end up with this projected position where it could go if we scrolled it there. And so, now instead of finding the nearest endpoint to the PIP when we throw, we can calculate its projected position and move there instead. So now, when I swipe from one corner of the screen to another with just a lightweight flick, it goes to the endpoint that I expected. So, this idea of projecting momentum is incredibly useful. And, we think its super important. I'd like to share some code for doing this with you, so that you can do this in your own apps. So, this function will take a velocity like the PIP's position velocity, and deceleration rate, and it'll give you the value that you could use as an endpoint for dynamic behavior. It's pretty simple. If we look at my FaceTime example of the pan gesture ending code, you can see that I'm just using the UIScrollView.DecelerationRate. So, we're leaning on that familiarity people have with scrolling and how far content will go when scrolled. And, I'm using that with my projection. So, I take the velocity of the PIP and the deceleration rate, and I create that imaginary PIP position. And, it's this imaginary, projected position that I then use as the nearest corner position. And, I send my PIP there, by retargeting it. So, this idea of using projection to find out the endpoint of a position, is incredibly useful for things being dragged or swiped, where you really need to respect the momentum of the gesture. But, this projection function isn't just useful for positions, you can also use it for scales, or even for rotations. Or, even combinations of the two. It's a really versatile tool that you should really be using to make sure that you're respecting the momentum of a gesture, and making it feel like the dynamic motion in your app is an extension of yourself. So, that's designing with motion. Dynamic motion. Behaviors should continuously and seamlessly work in concert with interactions. We should be leaning on that shared intuition that we have for the physical world around us. The things that we learn as children about how objects behave and move in the physical world, apply just as readily to our dynamic interfaces. You should remember that bounciness needs to be purposeful. Think about why you're using it, and whether it's appropriate. And, make sure that as you add character, and texture, that you're balancing it with utility. And finally, remember to project momentum. Don't just use position, use all of the information that's at your disposal to ensure that motion is aligned with the intent of where people actually want to go. And then, take them there. So, to talk a little bit more about how to fluidly respond to gestures and interactions, I'd like to introduce my colleague, Marcos, to the stage. Thanks for having me, everyone. That was great. Thanks, Nathan. Hi everyone. My name is Marcos. So far, we've seen how important fluidity is when designing interfaces. And, a lot of that comes from your interaction with a device. So, in this section, we're going to show you how touches on the screen become gestures in your apps. And, how to design these gestures to capture all the expression and intent into your interfaces. So, we're going to start by looking at the design of some core gestures like taps and swipes. Then, we'll look at some interaction principles, that you should follow when designing gestures for your interface. And then, we'll see how to deal with multiple gestures, and how to combine them into your apps. We're going to start by looking at a gesture that is apparently very simple, a tap. You would think that something-- you would think that a tap is something that doesn't have to be designed, but you'll see how its behavior has more nuances than it seems. In our example, we're going to look at tapping on a button, in this case, on the Calculator app. The first thing to remember is that the button should highlight immediately when I touch down on it. This shows me the button is working, and that the system is reacting to my gesture. But, we shouldn't confirm the tap until my touch goes up. The next thing to remember is to create an extra margin around the tap area. This extra margin will make our taps more comfortable, and avoid accidental cancellations if a touch moves during interaction. And, like my colleague Chan was saying, I should be able to change my mind after I've touched down on the button. So, if I drag my finger outside the tap area, and lift it, I can cancel the tap. The same way, if I swipe it back on the button, the button should highlight again, and let me confirm the tap. The next gesture we're going to talk about is swipe. Swipes are one of the core gestures of iOS, and they're used for multiple actions like scrolling, dragging, and paging. But, no matter what you use it for, or how you call it, the core principles of a gesture are always the same. In this example, we're going to use a swipe to drag this image to the right. So, the interaction starts the moment I touch down on the image with intention to drag it. But, before we can be sure it's a swipe, the touch has to move a certain distance. We learn to differentiate swipes from other gestures. This distance is called hysteresis, and is usually 10 points in iOS. So, once the touch reaches this distance, the swipe begins. This is also a good moment to decide the direction of the swipe. If it's horizontal, or vertical for instance. We don't really need it for example, but it's very useful in some situations. So, now that the swipe has been detected, this is the initial position of a gesture. After this moment, the touch and the image should stay together and move as one thing. We should respect the relative position, and never use the center of the image as the dragging point. During the drag, we should also keep track of the position and speed up the touch, so when the drag is over, we don't use the last position. We use the history of the touch, to ensure that all the motion is transferred fluidly into the image. So, as we've seen, touch and content should move together. One-to-one tracking is extremely important. When swiping or dragging, the contents should stay attached to the gesture. This is one of the principles of iOS. You enable scrolling, and makes the device feel natural and intuitive. It's so recognizable and expected that the moment the touch and content stop tracking one-to-one, we immediately notice it. And, in the case of scrolling, it shows us that we've reached the end of the content. But, one-to-one tracking is not limited to touch screens. For instance, manipulating UI on the Apple TV was designed around this concept. So, even if the touch is not manipulating the content directly, having a direct connection between the gesture and the interface puts you in control of the action, and makes the interaction intuitive. Another core principle when designing gestures, is to provide continuous feedback during the interaction. And, this is not just limited to swipes or drags. It applies to all interactions. So, if you look again at the Flashlight button on the iPhone 10, the size of button changes based on the pressure of my touch. And, this gives me a confirmation of my action. It shows me the system is responding to my gesture, but it also teaches me that pressing harder will eventually turn on the flashlight. Another good example of continuous feedback, is the focus engine on the Apple TV. So, the movements on the Siri remote are continuously represented on the screen. And, they show me the item that is currently selected, the moment the selection is going to change, and the direction the selection is going to go. So, having our UI respond during the gesture is critical to create a fluid experience. For that reason, when implementing your gestures, you should avoid methods that are only detected at the end of the gesture, like UISwipeGestureRecognizer. And, use ones like the actual touches, or other gestureRecognizers that provide all possible information about the gesture. So, not just the position, but also the velocity, the pressure, the size of the touch. In most situations though, your interfaces must respond to more than one gesture. As you keep adding features to your apps, the complexity and number of gestures increases, too. For instance, almost all UIs that use a scroll view will have other gestures like taps and swipes competing with each other. Like in this example, I can scroll the list of Contacts, or freely touch on one of them to preview it. So, if we had to wait for the final gesture, before we show any feedback, we would have to introduce a delay. And, during that wait, the interface wouldn't feel responsive. For that reason, we should detect all possible gestures from the beginning of the action. And, once we are confident of the intention, cancel all the other gestures. So, if we go back to our example, I start pressing that contact, but I decide to scroll instead. And, it's at that moment that we cancel the 3D touch action, and transition into the right gesture. Sometimes, though, it's inevitable to introduce delay. For instance, every time we use the double-tap in our UIs, all normal taps will be delayed. The system has to wait after the tap, to see if it's a tap or a double-tap. In this example, since I can double-tap to zoom in and out of a photo, tapping to show the app menu is delayed by about half a second. So, when designing gestures for your applications, you should be aware of these situations, and try to avoid delays whenever possible. So, to summarize, we've seen how to design some core gestures, like taps and swipes. We've seen that content and touch should move one-to-one, and that is one of the core concepts of iOS. You should also provide continuous feedback during all interactions, and when having multiple gestures, detect them in parallel from the beginning. And now, I'd like to hand it back to Chan, who will talk about working with fluid interfaces. Thanks, everyone. Nice job. Alright, I'm back. So, we just learned about how to approach building interfaces that feel as fluid, as responsive, and as lively as we are. So, lets talk about some considerations now that we're feeling a little bit more comfortable with this, for working within the medium of fluid interfaces. And that begins with teaching. So, one downside to a gestural interface is that it's not immediately obvious what the gestures are. So, we have to be friendly and clever about how we bring users along with us in a way that's friendly and inviting. And so, one way we can do that is with visual cues. So, the world is filled with these things, right? You can learn them once, and you can use them everywhere. They're portable. And so, when you see this, you know how to use it. So, we've tried to establish similar conventions in iOS. Here's a couple examples. So, if you have a scrolling list of content, you can clip the content off the bottom there, to indicate that there's more to see, that invites me to try and reveal what's under there. And, if we're dealing with pages of content, you can use a paging indicator to indicate that there's multiple pages of content. And, for sliding panes of content, you can use an affordance, or a grabber handle like this, to indicate that it's grabbable and slidable. Another technique you can use is to elevate interactive elements to a separate plane. So, if you have an interactive element, lifting it up to a separate plane can help distinguish it from the content. So, a good example of this is our on/off switch. We want to indicate that the knob of the switch is grabbable, so we elevate it to another plane. This helps visually separate it, and indicate its draggable nature. So, floating elements, interactive elements like this, above the interface can help indicate that they're grabbable. Next, we can use behavior, you know, to show rather than tell to use-- how to use an interface. So, we can reinforce a dynamic behavior with a static animation. So, an example of this is Safari. In Safari, we have this x icon at the top left to close the tab, and when you hit that button, we slide the tab left to indicate it's deleted. This hints to me that I can slide it myself to the left. And, accomplish the same action of deleting the tab through a gesture. So, by keeping the discrete animation and the gesture aligned, we can use one to teach the other. And, there's another technique we can use, which is explanations. This is when you explicitly tell users how to use a gesture. So, this is best when used sparingly, but it's best when you have one gesture that's used repeatedly in a bunch of places, and you explain it once up front, and then you just keep using it, and keep reinforcing it. Don't use it for a gesture that's used only intermittently. People won't remember that. Now, I want to talk a little bit about fun and playfulness. Because this is one of the most important aspects of a fluid interface. And, it only happens when you nail everything. It's a natural consequence of a fluid interface. It's when the interface is responding instantly and satisfyingly. When it's redirectable and forgiving. When the motion and gestures are smooth. And, everything we just talked about. The interface starts to feel in sync with you. And, something magical happens where you don't feel like you need to learn the interface, you feel like you're discovering the interface. And so, we think it's great when we allow people to discover the interface through play. And, it doesn't even feel like they're learning it, it feels fun. So, people love playing with stuff. So, we think it's great to play into our natural fiddle factor. You know, play is our mind's internalizing the feel of an interface. So, it's great when we're building this stuff, when we're prototyping it, just to build it. You know, play with it yourself. See how you fiddle with it. Hand it to others see how they play with it. And, think about how you can reinforce that with something like an animation, or behavior, an explanation. And, it's surprising how far play can go, and having interface teach itself to people. Let's talk a little bit about fluidity as a medium. How we actually go about building this stuff. You know, we think interfaces like this are a unique medium, and it's important that we approach it right. So, the first thing is to design the interactions to be inseparable from the visuals, not an afterthought. The interaction design should be done in concert with the visuals. You shouldn't be able to even tell when one ends and another begins. And, it's really important that we build demos of this stuff. The interactive demo we think is really worth a million static designs. Not just to show other people, but to also understand the true nature of the interface yourself. And, when you prototype this stuff, it's so valuable for you because you get to almost discover the interface as you're building it. You know, this technique is actually how we built the iPhone 10 interface. And, it's really important because it also sets a goal for the implementation. We're so lucky here at Apple that we have this amazing engineering team to build this stuff, because it's really hard to build. And, it's so important also to have that kind of magical example that reminds yourself and the engineering teams, and yourselves that what it can feel like, you know? And, it's really important to, kind of, remember, remind yourself of that. And, it makes-- when you actually build it, it makes something that's hard to copy, and it gives your app a unique character. So, you know, multitouch is such an amazing medium we all get to play in. We get to use technology to interface with people at an ancient, tactile level. It's actually really cool. You know, all those principles we talked about today, they're at the core of the design of the iPhone 10 gestural interface, you know, responsive, redirectable, interruptible gestures, dynamic motion, elegant gesture handling. In a lot of ways, it's kind of the embodiment of what we think a fluid interface could be. When we align the interface to the way we think and move, something kind of magical happens. It really stops feeling like a computer, and starts feeling like a seamless extension of us. You know, as we design the future of interfaces, we think it's really important to try and capture our humanity in the technology like this. So, that one of the most important tools of humankind is not a burden, but a pleasure and a delight to use. Thank you very much.  Welcome to session 508, Getting and Using a MapKit JS Key. I'm Eric Gelinas from the MapKit JS team. In this video, we will learn how MapKit JS authorization works, create a MapKit JS key, create an authorization token, and in the end we'll make a map. Let's dive into how authorization works in MapKit JS. Anything you host on the web, including MapKit JS is in plain text. One of the great things about the web is the ability to learn techniques from others simply by viewing source. Just open the web inspector and see how it's done. See a map you like? View its source, see how it works. It's certainly how I learned. Unfortunately, that sometimes means that your API authorization credentials can get copied and used by mistake without your permission and that unauthorized use can count against your usage limits. Once the limit is reached, your site may no longer be able to display maps. The MapKit JS authorization system is designed to give you more control over how your credentials are used. You can set a time when access should expire and restrict access to a specific domain. It's the web and others will still be able to view course and poke around in the web inspector, but if somebody ends up with your credentials for any reason, there will be limits on how they can use it. Let's discuss how MapKit JS authorization works at a high level. Once you have a MapKit JS key, you'll save it somewhere safe. This will never be shared over the web. Instead, you'll create tokens with restrictions and signed with your key. This token is what you will use to authorize with the MapKit JS API. So, let's see how creating a MapKit JS key works. A MapKit JS key is your credential to use with the MapKit JS API. This is a one-time set-up. Here's how. Create a maps identifier. Create a MapKit JS key and associate it what that identifier. And download your key and save it to a save place. Before we start, head over to developer.apple.com/account and log in. From there, click on Certificates, IDs, and Profiles in the left-hand menu. Our first step is to create a maps identifier. This identifies your project in the Apple Developer website, but it also serves to track usage limits. You may want to create different identifiers for production and development environments. To create a maps identifier, select Maps IDs from the menu. Click the + button in the upper right. Give this identifier a description. The description should be the name of your website as it will appear to users. Give this identifier a unique ID. We recommend using reverse domain style for this ID and it will need to start with the string maps. For example, maps.com.yourdomainname.yourapp. Now we'll create a MapKit JS key. This key is a secret shared between you and Apple and should never be stored in front end code, checked into source control, or shared with others. First, click on All under the Keys subheading in the left-hand menu. Then, click the + icon in the header. Then, give this new key a name. Then check the MapKit JS checkbox. Then, click the Configure button. Now that you've created a new key, we can associate it with the maps identifier you created earlier from the maps drop-down menu. Maps identifiers can only be associated with one key at a time. When you've selected your maps identifier, click Continue. Finally, review your changes when prompted and click Confirm. Your key is now ready to download. Click the Download button to save your auth key file to your computer. You can only download this file once, so keep it in a safe place. If you do lose it, you'll need to create a new key and then revoke the old key. When downloaded, your auth key file will look something like this. As I said before, this key is meant to be a secret between you and Apple. This is the only time you'll be able to download your auth key file. We recommend you quickly save it somewhere safe. If you ever lose your key or it is compromised, you can always come back to the Apple Developer website to revoke it and create a new key. You just created a MapKit JS key. Now we will create authorization tokens which can be used to authorize MapKit JS over the web. Tokens are created containing your developer credentials and are signed with your auth key file. Claims are added to this token, which are used by Apple to verify a client's authorization. Let's dive deeper into how to make authorization tokens. MapKit JS authorization tokens are based on JSON Web Tokens RFC 7519, or JWT for short. This is an industry standard designed to specifically transmit authorization claims over the web. You can find more information on this standard at jwt.io. There you will also find libraries for signing JWT tokens in most programming languages. All JWT tokens are made of the following three parts. A header, a payload, and a signature separated by periods and each Base64 URL encoded, making a token which can easily be passed to a request header. We have two recommended JWT configurations for your authorization token: short-lived tokens and long-lived tokens. Let's start with short-lived tokens. These authorization tokens offer the most protection against misuse by ensuring that authorization tokens can't be used for long if copied from your source code. This approach requires that you provide a server endpoint to respond to requests for MapKit JS for new authorization tokens. MapKit will refresh from this endpoint any time it needs to authorize throughout a user's session. First, the payload must contain an ISS claim, which is your Apple Developer Program ID. You can find your Team ID in the account section of the Apple Developer website. An IAT claim, which is when this authorization token was issued in seconds. An EXP claim, which is when this authorization token is meant to expire in seconds. For short-lived tokens, we recommend 30 minutes. And, finally, a claim of origin. This can restrict browser access by matching the origin header string. Though this claim is optional, we recommend it be used for all authorization tokens, especially in production. Second, the auth key file contents which you downloaded earlier. This full file including white space, header, and footer, should be used to sign this token. See the documentation for your JWT signing library for instructions specific to your programming language. Finally, the header section of the JWT token will contain a KID claim, which is your MapKit JS key ID. Note that this is not the same as your maps identifier. A TYP claim, which should be the string JWT to identify this is a JWT-style token, and an ALG claim, which is the hashing algorithm to use. This must be set to ES256, which when passed to your JWT signing library will output a JWT token ready to be used by MapKit JS. Since short-lived tokens may expire before your user is done with their session, your server will need to provide an endpoint for MapKit JS which will respond to requests to new tokens. Later, I'll show you how to make MapKit JS aware of your endpoint, but for now, here's an example of an endpoint that I've created as an express JS route. This route simply returns a new access token set to expire in 1800 seconds or 30 minutes every time it's called. Regardless of your token's expiry, MapKit JS will call this endpoint any time it needs to authorize throughout a user's active session. Since your endpoint needs to return a new token every time it's called, you'll need to let browsers know not to cache. You can do this by setting cache control headers. If set up correctly, you should have a route on your server which returns a token looking something like this. A long-lived token is different. Its expiry is set much further out. You can use the same token for many requests over a longer period of time. This type of token doesn't require a server since the token is not likely to expire during a user's session. For that reason, it can be easily used on static websites or in development environments. You can sync expiration and renewal of this token with your release cycle. We strongly recommend that you attach an origin claim to long-lived tokens. Let's see how this is done. In this example, we set long-lived tokens to expire in six months. As I said before, you can tailor this to your application's needs. For example, if your team has a two-week release cycle, you can set your expectation to sync with that or even add a couple weeks as a buffer. Setting the origin claim is always recommended, but especially on tokens with long expiry. This is your strongest defense against misuse of your credentials. Now with this token handy, we're ready to make a map. The simplest example of this is to create an HTML page, including a script tag linking to MapKit JS on the Apple CDN. Remember when I said that I would show you how to make MapKit JS aware of your token endpoint? That happens when you provide a callback function when you initialize MapKit JS. For short-lived tokens, MapKit JS will fetch new tokens as needed throughout an active user's session. Note that when MapKit JS invokes your callback function, it'll pass it a function, which for this example we're calling done. Call this function with your new token to allow MapKit JS to proceed with the authorization process. If you're using long-lived tokens, simply call the done function immediately in the body of the authorization callback function. MapKit JS will use the same long-lived token any time it needs to refresh its authorization. Finally, set a map container and center for your map. Now, let's start our server and see this all in action. If everything is set up correctly, your browser should display a map. If you're using short-lived tokens in the web inspector, you should see a server endpoint being called, returning an authorization token. Now that MapKit JS has its authorization, it'll do the rest. MapKit JS will request from your route whenever it needs a new token. So now you know how MapKit JS authorization works. You know how to create a MapKit JS key, create an authorization token, as well as how to make a map initialized with your new token. Remember, get your MapKit JS key and store it in a safe place. Only send authorization tokens over the web. If a key becomes compromised, revoke it on the Apple Developer website as soon as you've created and updated your website with a replacement. For more information, visit developer.apple.com/wwdc18/508. Have a look at our session, Introducing MapKit JS, in the Executive Ballroom on Tuesday at 5:00 PM. Thank you. Hey everyone. Welcome to our session on what's new in App Store Connect? My name is Daniel Miao. And I'm an engineering manager on the App Store Connect team. So what's new? Well, first off, you may have noticed we changed our name. This is really just a small part of our ongoing commitment to building the best tools and services focused on you, our developers building apps for the App Store. Now along these lines, we've also been undergoing a few renovations on our App Store. So last year, we went back and we redesigned the iOS App Store. And following the success of that redesign, this year we also redesigned the Mac App Store. Now with this Mac App Store redesign, we've added support for a few new features including app subtitles and app previews. You can take advantage of these features by signing into App Store Connect or by using the XML feed if you use that for automation today. Now for today's session, we're going to focus on enhancements we've made to App Store Connect. And we're going to talk about it within the context of your app lifecycle. This lifecycle typically begins with the design phase. This phase can be both visual or in code. And as you make your way around this lifecycle, you eventually end up with your app on the App Store followed with an analysis of how your app is doing on the App Store. Now more specifically, we're going to focus on a few key areas of this lifecycle. We're going to talk about changes to provisioning, user management, delivering your app into our system, beta testing the builds, and analyzing your biggest business drivers. Now looking at all these different areas, you can imagine how something like automation could bring all these together quite nicely. And we tend to agree which is why we're very excited because this summer we'll be releasing an all-new product focused around automation and we call it the App Store Connect API. Now the App Store Connect API is simply a RESTful interface into your app management experience. To authenticate with the API, you'll be using industry-standard JSON web token authentication. Some of you may already be familiar with this with other APIs you've worked with. In designing the API, we've designed it around strict convention. This means whether you're looking at something like naming or error formats, no matter what corner of the API you're in, everything should feel familiar. And finally, we've been writing extensive documentation for this API. You might be looking for references around resources and their attributes or maybe you're simply looking for a guide around how to best use this API for your use cases. Well, either way and for many other scenarios, this documentation will be the place to go. We'll be launching the API this summer. And we will be launching with support for the areas of the lifecycle we highlighted earlier. Now as we add more features and support for other areas, we will be delivering those features to you. So going back to our lifecycle, let's talk through some enhancements and some API integrations. We'll begin by talking about provisioning. Now today, many of you rely on Xcode to manage your provisioning automatically. But for some of you who are looking for a little more control, today you sign into the Apple Developer website to generate the resources you need to sign your apps. Well, with the API this summer, you'll be able to do a lot of these things directly. The API will support generating provisioning profiles, creating and revoking signing certificates as well as managing your devices and your app bundle IDs. This should make it a little bit easier for you to integrate your provisioning activities into your automation process. So early on in the lifecycle while you're thinking about provisioning, you're probably also thinking about managing your users. Now the API is going to support inviting new users to App Store Connect, modifying which apps your users can see, managing your user's roles as well as updating profile details. Now we didn't want to stop at simplifying your management experience with just the API. We also wanted to take a look at how we could simplify where you go to manage your users in the first place. Today, you go to both App Store Connect and the Apple Developer website. But starting this summer, you'll be able to go to just one place. And that's App Store Connect. Coming to App Store Connect, you'll be able to think about all of your users within the context of just one set of roles. And you can manage just one account for each of your Apple IDs. Now we know that App Store Connect and the Apple Developer website, they're very different systems. So you might be wondering how is all this going to come together? Well, that's why we've built a way for you to see how your user's permissions will be changing once this process is complete. Once we have this preview available, we'll let you know via Developer News and on the App Store Connect home page. From here, you can click into a page that looks sort of like this. Here you'll see a list of all of your users as well as a summary of how their roles will be changing. Of course, some of your users won't be changing at all. Now if you click into a single one of these users, you'll be able to pull up a modal giving you much more information about that user including what their permissions will look like going forward. We encourage you to come in to App Store Connect and take a look at each and every one of your users and make any necessary changes before we began this process. And once this process completes, we encourage you to come back one last time to make sure your users are in a state that makes sense to you and after that you simply come back to App Store Connect for any future changes. All right. So while some of you were thinking about provisioning and user management, others were working on your apps. Now a natural part of the app development process is, of course, spinning up builds. At some point, you reach a build that you're ready to deliver, that way you can distribute it to your customers. Now many of you today are using our tool called Transporter. For those of you who aren't familiar, Transporter is our command line tool that does many things and one of those is delivering your builds into our system. For those of you who use Transporter today, you know that macOS is a supported platform. Well, we also know that many of you use Linux for things like continuous integration, which is why this summer we'll be adding Linux as a supported platform for Transporter. Now you'll continue using Transporter the same way you do today. And Transporter will continue allowing you to validate your builds before you deliver them. This will save you a lot of time because you'll know whether your builds are in a good state before you push us all of the bits. And once we launch the API this summer, in addition to the username and password authentication that you can do today, you'll also be able to use the same API tokens that you use with the rest of the App Store Connect API. Okay. So now that we have a few builds in the system, it's time to beta test them. And, of course, beta testing means TestFlight. We've got a few enhancements to TestFlight. And I'd like to invite Tommy McGlynn up to the stage to tell you all about them. Thanks, Daniel. I'm really excited to be here again talking to you about TestFlight. We've got a new feature to tell you about, and it deals with tester acquisition. Today, all you need to invite someone to TestFlight is their e-mail address. You simply enter their e-mail and they'll be sent an invitation. Tapping on the invitation will launch TestFlight where they can install your beta app. It's pretty awesome, but if you're dealing with a lot of testers, it can be cumbersome. And if you're working with testers who don't have an e-mail address, there isn't an easy way to invite them. We think we can do even better, and we call it TestFlight public link. This is really exciting. Public link is a unique URL that represents an open invitation to your beta app. You can share it anywhere you'd like. And it can be used by anyone to become a new tester. So public links can be shared anywhere that a regular link can be shared. This means you can send a public link directly to someone, or you could share it on social media to reach a wider audience. If someone wants to become a tester, they simply tap on a link. This would launch TestFlight where they can install your beta app and instantly become one of your testers. It's really easy. You don't have to collect any information. You simply make the link available and testers can join. So let me show how we can create a public link. In TestFlight, groups allow you to organize your testers and decide which builds each group of testers should have access to. We'll need to create a group before we can generate a public link. And we can do that by clicking new group in the left nav and then giving the group a name. Now before we create a public link and start bringing testers in, we want to make sure there is a build that they can test. So first we'll add a build to this group. We do that by going to the builds tab and clicking the plus button next to build. This will show all the builds we've uploaded. And we can choose one that we want to distribute. Now that this group has a build, we can create a public link for this group. We'll go back to the testers tab and click enable public link. This generates a unique URL that we can share anywhere to reach new testers. And testers who join through that link are automatically added to the group. So when you're ready to deliver a new build, you simply add the build to the group. You can see how this is a much faster way to bring in up to 10,000 active testers. If you want more control over the number of testers or you're not quite ready for your beta app to go viral, you can easily set a custom tester limit on each public link. This would put a cap on the number of testers who could join through that link. You also have the ability to disable the public link at any time at which point new testers would no longer be able to join. If someone tries to open a link once it's been disabled, they'll receive a message letting them know this beta isn't accepting any new testers. I also want to show what happens when someone taps on a public link and doesn't have TestFlight installed. They'll land on a localized page that explains how to install TestFlight and get started as a beta tester. This will make it a lot easier for brand-new testers to begin testing your app. I'd also like to mention that you'll soon be able to do all of this without using the App Store Connect UI at all. This will be possible using the new App Store Connect API. You'll be able to automate the creation of groups, assign builds to groups, manage public links, add and remove testers, and update test information. All of this can be automated using our new REST API. Thank you. We have an entire session devoted to the new App Store Connect API. And I strongly encourage you to check it out tomorrow at 3:00. That's TestFlight public link. And I'm really excited to see what you're able to do with this new feature. Thank you. Thanks, Tommy. Well, TestFlight public links and the new TestFlight APIs are just two more ways for you to streamline your beta distribution process. So now that we've gone through a successful round of beta testing, prepared our App Store metadata, gone through app review, and now our app is in the hands of our customers. Now it's time to take a look at some hard numbers to see how our customers are actually responding to our apps. So the first place we can look at this is in sales and trends. Just this week we launched an all new sales and trends overview page. This page gives you a summary of your biggest business drivers like app units, in-app purchases, and sales. As you scroll down this page, you'll see your top apps and how they contribute to each of these metrics. Here, when you look at these sales numbers, these numbers include not just app sales but also in-app purchase sales from within those apps. As we scroll down towards the bottom, we'll see these same metrics broken down by territory and by device. After looking at all this information, you'll know better where to focus your efforts. Now another place we can look is in your generated reports to see how your app is doing. Today, in order to download your generated reports, you use a reporter tool but beginning this summer you'll be able to do the same thing directly with the API. And we'll be supporting the download of both financial reports and sales reports. This will make it easier for you to integrate reporting in to your process. So we've been talking about a lot of features. And so far they've mostly been centered around your web and your desktop experience. So now we'd like to move on and talk about our all new mobile experience. And I'd like to invite Alex Miyamura up to the stage to tell you all about it. Thanks, Daniel. We know that the involvement with your apps doesn't stop the moment that you step away from your Mac. You may have a version of one of your apps that you've been working on hard for quite a long time that you're just itching to release and it's now in app review. You may just have released a version of one of your apps, or you may just want to see how your apps are doing, check out their sales and trends or their ratings and reviews. And for that reason, we've created a brand-new App Store Connect experience for iOS. This experience is centered around giving you the ability to access your apps data on the go as well as empowering you to take certain quick actions and letting you know exactly when you can take them. Now let's start with sales and trends. When you tap into your trends tab, you'll see this gorgeous graphical summary of your app's performance over the past 7 days. You'll see your app's units and proceeds. Scrolling down, you'll see sales, updates, and then in-app purchases and app bundles. Now we know that you implement diverse modernization strategies across your apps. Some of you have paid apps, free apps, apps with in-app purchases, or you may leverage subscriptions. Now we know that and we wanted to give you the ability to select exactly which of these graphs are most relevant to you. So we've added this edit function where you can select the graphs you'd like to see. So let's say that, in my business, I only have free apps. I'll go ahead and unselect proceeds, sales, in-app purchases, and app bundles. Hit done and, voila, now I just see units and updates, exactly what's relevant for my business. Next, we're going to take a look at the time selection control underneath the trends title. In iTunes Connect mobile, we allow you to see the status from 7 days all the way to 26 weeks. Now this was amazing if you are interested in longer-term trends, right? But what about an app that you just released? You might not even have 7 days' worth of data. And if you want to compare those 7 days to the previous 7 days, well obviously you're not going to have that. So in App Store Connect, we introduce the 1 day view. In this view, you can see how your apps are during day over day even just a few days after launch. Of course, some of you are still going to be interested in longer-term trends, right? So we still offer you the ability to view your apps data over 2 weeks, 5 weeks, 13 weeks, and 26 weeks. You can also drill down into each one of these graphs in more detail. So we're going to take a look at units right now. When you tap into one of these graphs, you'll see a breakdown across your free apps, your paid apps, and in-apps for the iOS/tvOS App Stores as well as the redesigned Mac App Store. You can also take a look at a deeper dive into any one of these cells just by tapping on them. So let's go ahead and tap the free iOS/tvOS app cell. Now you can see data about each one of your apps in more detail and also your apps top territories. If you're interested in seeing more territories, you can tap the show more button. Now you can see the territories that your apps are available in worldwide. You can also take a look at your apps data individually as opposed to this aggregate view by taking a look at the my apps tab which we'll do next. When you tap into the my apps tab, you'll see a list of your apps, obviously. But what if you're a member of more than one development team? So do we have anyone in the audience that is, indeed, a member of more than one development team? Cool. So it's actually quite a bit of you. So that's awesome. And that's exactly why we've introduce the ability to select which development team you'd like to see within App Store Connect. So we'll tap the little account button. That brings up settings. And you'll see the account cell. We'll tap into that, select the team that we'd like to see within App Store Connect, and dismiss this. Now we see exactly the apps that we're interested in right now. Let's go ahead and take a look at one of the apps within our apps list, Mountain Climber. When we tap into Mountain Climber, you'll see four sections. The first section that you see is the iOS app section. And here you'll see your iOS versions. If your app has a tvOS app version, you'll see the tvOS app section and, obviously, the tvOS app versions underneath. Now we also have App Store information. And remember that I noted that you can see your apps trends data individually. Well, all you have to do is tap into that cell and you'll be taken there. Now, finally we have notifications. And we're going to do something a bit unorthodox here. We're actually going to start from all the way at the bottom with notifications. There are two types of notifications that you can receive within the App Store Connect app. The first one is for app status changes. And this is something like when your app is in review and then goes to pending developer release when it's approved. The next one that you can take is for reviews with a certain star rating. And so we're going to go ahead and select the five-star review because we want to thank users that are enjoying our app as well as the one-star review because we want to engage users that, for whatever reason, are not having the best of experiences. Now that we've selected our notifications, we're going to go back to the app view. The reason why we started with notifications is because one of the key tenets of the App Store Connect experience on iOS is giving you the ability to take a quick action around your apps and letting you know when you can do so. So we know that you've taken -- you've spent countless hours working on your apps. You've distributed them to beta testers via TestFlight. You've taken those testers feedback, iterated upon that with your apps, and finally you have a build that's ready for release to the world. Now one of the last things you have to do before releasing your app to the world is, of course, submitting your build to app review. One of the notifications that we just set up in the App Store Connect app was for app status changes. This means that when your app is approved by app review, you'll receive a push notification that tells you this happy news. The second that you received one of these notifications, you can open App Store Connect wherever you are. And we'll go to the version view. You'll now see that our app Forest Explorer is pending developer release. We'll scroll down and you'll see two buttons. The first one that you'll see is this big red button. It's really obvious. Reject this binary. We don't want to do that, but now we see release this version and that's exactly what we want to do. We'll tap there. We'll get a confirmation dialog and we'll hit release. Now we sent that build off to the world. Sometimes you don't get the happiest push notifications and this is one of them. Right. Sometimes your app may be rejected by app review. And, obviously, that's something that no one wants to see. Later in our session, my colleague Daniel is going to go over some tips and tricks from app review to make sure that this never happens to you. But what should we do now? We need to make sure that we can get this app approved and out there to the world. Now what we're going to do is open up App Store Connect. And under App Store information, you'll see this warning badge next to resolution center. Within resolution center, you'll be able to see app reviews feedback for your app. You can tap into one in more detail and, of course, reply to it on the go. Now once we send our response back to app review, they'll get a chance to review it and hopefully our app will be approved at once. Now once your app has been submitted to app review, once it's been approved, and you've used App Store Connect to release it, the next thing that you'll be receiving his customer feedback. And here's one of the notifications that you may receive, a five-star review. Now recall we set up notifications for one and five-star reviews within App Store Connect. Since we set these up, you'll know the very minute a customer submits one of these reviews. You can access all customer feedback within App Store Connect by accessing the ratings and reviews section where you'll be able to see your current rating across any one of the territories you've selected or, obviously, all territories. You can also take a look at your reviews, read them in more detail, and reply to them. Now this is a satisfied customer so we're going to thank them for the positive feedback. That covers App Store Connect notifications and the quick actions that you can take from them but there's one more thing. We've also optimized App Store Connect for iPad. So let's take a quick look. As I mentioned earlier, you can view a certain apps sales and trends data. And here is sales and trends for Forest Explorer. You can see units. And we'll scroll down to updates and also drill down into the territories view. Now I'm sure you want to try this out on your own. And our app is actually available now. So if you haven't downloaded it already, please go ahead and do so. We've worked hard to bring an experience -- an improved mobile experience to you. And we hope you'll enjoy using it as much as we enjoyed creating it. And with that, now back to Daniel. Thanks, Alex. So that's a beautiful new app that'll give you even more power to manage your apps on the go. So next we'd like to talk about a guideline change we made just this week that will let those of you with paid apps offer time-based free trials for your customers. Now those of you who are using subscriptions today know that free trials are a great way to attract new customers to your services. But we also know that subscriptions aren't the model that works for every app which is why we're happy to now have a path for paid apps. So we're going to take a look at what this setup looks like. So let's say we start with a paid app. The first thing you would do is turn your paid app into a free app. Now on top of this free app, you're going to add two non-consumable in-app purchases. The first is a free tier 0 priced non-consumable. And you'll present this to your customers when they launch the app so that they can opt-in to the free trial. Now the second non-consumable is the in-app purchase that you use to unlock your app functionality for that customer for good. You can present this at any point; in the beginning when they launch the app or when their free trial is complete. Now in order to use this setup, there are a few guidelines to look out for and to follow. First, make sure you name your trial in-app purchase with this naming convention. And next, you'll want to make sure that it's very clear to the customer exactly what they are signing up for. So please make sure that they know how long the free trial is going to last, how much it'll cost to unlock the functionality, and what kind of features and content won't be available anymore after the trial if they decide not to continue. All right. So we talked about a number of new features today. So let's take a second to recap them. First, we're launching an all-new App Store Connect API this summer. We're unifying where you go to manage your users. We've extended app Transporter to support Linux. We're launching TestFlight public links, which will make it a lot easier for you to invite large numbers of testers. We've launched a brand-new sales and trends overview page, a new App Store Connect for iOS page, and a path for you to offer trials to customers on your paid apps. Now these are features that will be launching between now and the end of the summer. But we also have two significant features we launched over the past year that we'd like to highlight. And these are intro pricing and pre-orders. So introductory pricing, for those of you who use subscriptions, is a great way to incentivize new customers to sign up for your subscriptions. Now you do this by offering your customers a discounted introductory price at the beginning of your subscription. There are three different pricing models we offer for this. First, we have free trials. Now with free trials, your customers sign up for free. They use your services for the introductory period. And at the end of the period, they move on to your regular subscription price. Next, we have a model that we call pay upfront. With this model, your customers sign up and they pay one time at the beginning of your introductory period. And, again, at the end of the period, the regular subscription price kicks in. And finally, we have pay as you go. With pay as you go, your customers pay a recurring discounted price during the introductory period. Now once the entire period has been completed then they move on to the regular subscription price. Now we have two related sessions that happened yesterday that have more information around the StoreKit side and best practices around both intro pricing and subscriptions. They are Best Practices and What's New with In-App Purchases and Engineering Subscriptions. So check those out if you have a chance. So next we have pre-orders. Now pre-orders are a great way for you to drum up excitement for your app before your app is actually available for download or purchase. In order to set up a pre-order, you can take any app that has not yet been released on the App Store and you can enable it. After you submit it to app review and that app has been approved, you can release that app to the App Store as a pre-order. Once that pre-order period is over and your customers have signed up for your pre-order, your app will move into ready for sale state where your customers can now download or purchase that app. It's also at this transition that customers that have signed up for your pre-order are charged for the price of your app. Now let's take a look at how you might set this up in App Store Connect. We begin here on the pricing and availability page. Again, if this app has never been released to the App Store, you'll see a section in the middle here labeled pre-orders. Here, you can enable your pre-orders and set a release date. This release date is a date on which your app goes from being available as a pre-order on the App Store to being available as a download or purchase on the App Store. In other words, this is when it goes from pre-order ready for sale to ready for sale. Now once you've gone through app review and your app is in pending developer release state, you'll see a button in the top right corner. This is the release pre-order button. You click this button to make your app available on the App Store as a pre-order. Once you do this, you'll see a banner at the top of the page telling you when your pre-order started and when it completes. This completion date is simply the release date that you set for your app. Now if you'd like to release this app before the release date or more immediately, you can click the release app now button in the top right corner. And that's intro pricing and pre-orders in a nutshell. So we have to close out this session with a few best practices from app review. These are a few tips that'll help your apps to get through review a little more smoothly. First, please enter contact information and keep it current throughout the review process. Sometimes app review needs to reach out to maybe ask a few questions to clarify how your app works. This will help us to get your app through review a bit more quickly, but we can't reach out to you without contact information. Next, please submit demo account information if your app requires a log in. There's a section on the version page where you can enter this and keep it updated. Please make sure the account -- this account information is current throughout the review process. And if there is a server side to the log in, which usually there is, please make sure that the server side account is also enabled throughout the process. You may be wondering does app review ever look at the notes? Well, yes they do. They look at it with every single review. So if there is any information that might make it clearer for app review or maybe tells them about any kind of non-obvious features of your app, be sure to include that information in these notes. When you submit screenshots, please make sure to include customer experience. We want to see what the customer sees in these screenshots. And finally, if you're asking your customers for permissions around things like their location or maybe access to their photo library, please include, in that permission modal, why you're requesting that information and how it's going to be used. And those are a few notes from app review. For more information about anything we've talked about today, please visit this link. A recording of this video will be posted there along with related documents and other things, more information about what we've talked about. We also have a few related sessions. Tomorrow, as Tommy mentioned, we have our Automating App Store Connect session at 3:00 where we're going to take you into a deep dive of the API and show you how to use the API at a much deeper level. We also have two labs coming up, one tomorrow and one the day after. So if you'd like to come and talk to us, we'd love to talk to you. And finally, there is an App Store lab that happening, as many of you know, on the other side of this building. So if you have any questions, feel free to sign up for that. Or if you just have a quick question, we now have a walk-in table this year that you can just drop by with a question for. Thank you so much for spending your time with us and we hope you enjoy your dinner and the rest of this conference. Good morning, everyone. Wow. Thank you. Welcome to Advanced Dark Mode. I'm Matt Jacobson. I'll be joined later on stage by my colleague, Jeff Nadeau. We're engineers in the Cocoa Frameworks group at Apple. We are super excited to talk to you today about the awesome Dark Mode in Mojave. Now, in the intro session yesterday, you learned all the things you need to get started adapting your app for Dark Mode, like rebuilding on the macOS 10.14 SDK. Making use of dynamic colors instead of static or hardcoded colors. Making correct use of template images and materials. And, most of all, making use of the new features in Xcode 10 to define custom color and image assets specifically for Dark Mode. Now, if you need a review on any of those topics, I highly recommend going back and watching the intro session on video later. Now, most UI will look great in Dark Mode using just those techniques. In fact, some of our system apps required no other changes. It was great. But we know some cases will require a little bit more work and that's what we're going to get into in this session. We're going to cover six main areas today. First, the appearance system, how it works, and how you can make use of it in your custom views. Second, materials, what they are, and how you can best make use of them in your UI. Then I'll hand it over to Jeff and he'll talk about vibrant blending, which is an awesome way to make your views look great. As well as reacting correctly to selection using something called background style. Finally, he'll wrap it up with some discussion on how to back deploy your app to older versions of macOS while still supporting Dark Mode, as well as some general tips and tricks for polishing your apps for Dark Mode. All right. Let's get started. So, in Mojave, your app will need to look great in light and dark. And the way you'll do that is using something called NSAppearance. NSAppearance is the theme system used throughout Cocoa and the key part about it is you only have to maintain a single view hierarchy and NSAppearance will help it look great in light and dark. Now, in addition to being at the core of Dark Mode, we've already been using NSAppearance for several years and it's underlied [phonetic] such features as the high contrast mode of macOS as well as the touch bar UI designed specifically for that awesome piece of hardware. Now, previously we've had one main appearance, one main light appearance for aqua windows and we called aqua. And, of course, in 10.14, we're introducing a second appearance for aqua windows for Dark Mode called darkAqua. These objects contain all the assets that views draw with. So, any time you use system dynamic colors or standard effects or named images or even just standard Cocoa controls, this is where all that stuff is coming from. And AppKit will automatically provide appearances for all of you views and windows based on the user's light/dark preference in system preferences once you link on the macOS 10.14 SDK. So, here's our beautiful Chameleon Wrangler app that Rachel and Taylor created in the intro session and you can see once we linked it on the macOS 10.14 SDK, AppKit went ahead and automatically gave it the darkAqua appearance. Now, that's great, but what if we want to change the appearance? For example, what if we wanted to change the appearance of this notes view? We might think that in the dark appearance we still might want the notes view to appear light. Well, you can do that using something called NSAppearanceCustomization. Now, this is a protocol, but it's not a protocol you have to go off and adopt in your applications. It's already adopted by NSView and NSWindow and, in Mojave, NSApplication conforms as well. It's a pretty simple protocol. It just adds two properties. First property is appearance and this is where you can override the appearance for a particular object. Now, it's an optional NSAppearance because, if you set it to nil, the object will simply inherit its appearance from its ancestors. There's also effective appearance and this is a read-only property that you can use to find out what appearance a view will draw with. And of course, to use this, you'll have to get the right NSAppearance object and you can do that pretty easily using the NSAppearance named initializer. Just pass aqua or darkAqua based on which appearance you want and then you can go ahead and just assign that to the appearance property of the object that you'd like to customize. So, in this case, we'll assign the aqua appearance to the appearance property of the text view and now it uses the light appearance. All right. That was pretty easy, so let's take a look at another case. You might have a window that kind of hangs off of a particular view. And you probably want its appearance to match the view it hangs off of. Now, we could just assign the aqua appearance to this window just like we did to the view, but what we really want is something a little stronger. We want its appearance to inherit from the view and we can do that-- first of all, AppKit will automatically do this for us for a number of common windows, like menus, popovers, tool tips, and sheets, so you don't have to worry about it in those cases. But, for custom cases like this, there's new API in Mojave that you can use to do this. It's called Appearance Source. Now, this is a property that takes any object that conforms to that NSAppearanceCustomization protocol-- so, views and windows-- and you just assign it to the appearanceSource property and the window will inherit its appearance from that object. So, in this case, we'll assign the text view to the appearanceSource property of that child window and now it's appearance will always inherit from that view no matter what it is. In fact, you should think of the appearance system as a sort of hierarchy. Similar to the view hierarchy you're probably familiar with, but extending to windows and the application as well. And when we ask AppKit for a view's effective appearance, AppKit will simply walk up this hierarchy until it finds an object with a specified appearance and that's the appearance we'll use. OK. So, now that we know how objects get an appearance and how the appearance system works, let's talk about how you can use it in your custom views and controls. Here's an example. Let's say I wanted this custom header view here to use a different color in light and dark appearance. Now, we already know in Xcode 10 I can go into the asset catalog editor and specify specific color assets for light and dark. But then how do I use that in my custom view? Well, here's one way that seems tempting but won't work and I'll show you why. First, we'll add an NSColor property to our view. And in init, we'll use that color to populate our layer. And if the color changes, we'll go ahead and update our layer there too. Let's try that out. OK. It looks pretty good in light, but if we switch to dark, we can see our color didn't actually change. And that's because even though our NSColor is dynamic, the CG color that we get from it is static. It won't change for the appearance. And, since we configured our layer in our initializer, we didn't get a chance to run any code when the appearance changed. Now, the key takeaway from this is you need to do your appearance sensitive work in specific areas. Specifically, the update constraints, layout, draw, and update layer methods of NSView. Now, AppKit will automatically call these methods as needed when the appearance changes. And if you need to trigger them manually, of course you can always use the needsUpdateConstraints, needsLayout, and needsDisplayProperties and AppKit will automatically call them. So, let's go back to our example. Instead of overriding init, we'll implement updateLayer and there we can go ahead and safely populate our layer by asking our NSColor for a CG color. And if our color changes, instead of updating our layer right there, we'll just set the needsDisplay property to true. AppKit will come back around automatically and call updateLayer. So, let's run it again. Still looks good in light. And now it uses the correct color in dark just like we wanted, so that's great. Now, what if we want to do something a little more complicated that might not be expressible just with dynamic colors or images? For example, maybe I would like to add this nice white glow behind Chloe's beautiful face here, but only in Dark Mode. How would I do that? Well, for cases like that, we have new API in Mojave that you can use to match against your view's appearance. Let me show you how it works. So, in this view, I'll override the layout method and I'll switch on effectiveAppearance bestMatch(from:, I'll pass an array with all of the appearance names that my view happens to know about. In this case, aqua and darkAqua. Then it's just a matter of implementing behavior for each of those appearances. So, for the aqua appearance, I'll simply use my imageView with Chloe's face as a subview. And for darkAqua, I'll not only use that imageView, but I'll also through my glowView behind it. Finally, I'll implement a default case and this is for appearances my view doesn't know about and that includes potential appearances Apple might come out with in the future. OK. Let's take a look at what it looks like. So, there it is in light, no glow, that's what we wanted. Switch to dark and we have that glow. That's great. All right. Let's talk for a minute about high contrast. So, I said before that we've been using NSAppearance for the high contrast mode of macOS. And one of the nice side effects of doing all this work to support Dark Mode is it makes it really easy to support high contrast really well as well. As a reminder, high contrast is enabled through the increase contrast checkbox in system preferences. And in this mode, colors are changed so that control bounds and other kinds of boundaries are more easy to see. Now, in this mode, AppKit automatically replaces the aqua and darkAqua appearances with high contrast counterparts. Now, these high contrast appearances inherit from their normal contrast versions. So, what that means is any code you've written to take advantage of Dark Mode will automatically apply in high contrast Dark Mode. But you can go even further. In Xcode 10, if you check this high contrast checkbox in the asset catalog editor, it'll allow you to specify color and image assets specifically for the high contrast versions of the appearances. Now, you can also use those appearance names in code. You might be temped to think, well, great, I'll just pass them to NSAppearance themed and I'll get the NSAppearance object and I'll do something with that, but that won't work. Those appearances are only available through system preferences. But what you can do is pass them to bestMatch(from:) just like we did before for Dark Mode to implement custom programmatic behavior. OK. Let's talk for a minute about sublayers. I know a lot of you out there have views that manage their own sublayers and there are important things to be aware of for Dark Mode. Primarily, you need to know that custom sublayers will not inherit your view's appearance automatically. Now, the easiest fix for this is to switch them from being sublayers to subviews. If you do that, AppKit will automatically handle the appearance inheritance for those views, just like any other view. Otherwise, you'll have to manage those layers manually using a couple techniques that I'll talk about now, viewDidChange EffectiveAppearance and the concept of the current appearance. So, first viewDidChange EffectiveAppearance. This is a new method on NSView that you can override to find out when your view's effective appearance changes. Now, this is a good time to perform any custom invalidation you might need to do or drop any caches that are no longer relevant. But remember you don't need to invalidate the view itself here, AppKit will do that for you automatically. Second, the concept of the current appearance. Now, this is a thread local variable that you can access through a class property on NSAppearance. If you're familiar with concepts like the current NSGraphics context or the current NSProgress, you already know what I'm talking about. If not, just remember that this is the appearance used to resolve dynamic colors and images. AppKit will set up the current appearance automatically for you before we call any of those special NSView methods we talked about before, like updateConstraints, layout, draw, and updateLayer, but you can also set it up yourself where necessary and let's take a look at an example why you might do that. So, here's a custom that maintains some sublayers. I'll override this new viewDidChange EffectiveAppearance method and I'll set my sublayer needsDisplay. Now, if I didn't do this, my sublayer wouldn't update when my view's effective appearance changed. It would just stay the same. And then in my layer delegate routine, I'll save off the current appearance for later and then I'll go ahead and set the current appearance to my view's effective appearance. Then I can go ahead and update my layer. Now, if I hadn't set the current appearance before this, this code wouldn't be using my view's appearance and so it would end up looking wrong. Finally, when I'm done, I'll just restore the old current appearance. Here's another thing to be aware of if you're managing layers. You might have code that looks like one of these two examples. Either you're setting the contents of a layer to an NSImage or you're using the layer contents for content scale API to create layer contents from an image for your layer. If you have code like this, you should know that the image will not automatically inherit the appearance. As before, the best fix is to switch to views. In this case, NSImageView. NSImageView will take care of this detail as well as a bunch of others automatically, so do that if you can. Otherwise, you'll need to create a CGImage from your NSImage for your layer. And you'll do that using the cgImage(forProposedRect:, context:, hints: API on NSImage. And you'll have to be careful to do this at a point where the current appearance is correct. So, a good place to do it is in your updateLayer method. All right, so that's appearance. Now let's talk about materials. Now, you've probably heard that materials are one of the building blocks of the modern Mac UI, but you may have wondered to yourself, well, what exactly is a material, so let's start with a definition. Materials are dynamic backgrounds that make use of effects like blurs, gradient, tinting, translucency, and they provide a sense of depth or context to your UI, as well as just a bit of added beauty. Here's a pretty typical Mac desktop and you can see all the different places where we're using these material effects-- actually, this isn't even all of them. Now, AppKit automatically provides materials in a number of common places, like the title bars and backgrounds of windows, table views, sidebars, popovers, menus, and even other places as well. But you can create a material yourself and add it to your UI using a view called NSVisualEffectView. If you're not familiar with NSVisualEffectView, quite simply, it's a view that shows a material. And if you want to use one, you'll need to be aware of three main properties that you'll have to set up and I'll go through these in order. The state, blendingMode, and material properties. So, first, the state property. This controls whether the material uses the active window look. Now, by default, the material will just match its containing window and so when that window is active it'll look active. When the window's inactive, the material will look inactive. But you can also specify this specifically to be active or inactive if you'd like to control it manually. Second, the blendingMode property. This property controls whether the material punches through the back of the window. Let me show you what I mean by that. Here's a preview using two different materials. For one, this title bar material, if we peel it back, we can see that it's blending the contents within the window, including that color image there. So, it's not punching through the back of the window. There's also the sidebar material and if we peel it back we can see it's blurring the contents behind the window, so it's punching through the back so it can see to the windows behind it as well as the desktop. So, by default, a visual effect view will be in behind window mode, but you can control that using the blendingMode property. Finally, the material property. This property encapsulates the material effect definition. What do I mean by that? That means the exact recipe of blur, translucency, gradience, tinting-- that all depends on the material property. Now, when we first started using materials in Yosemite, we had two main materials, the light and dark materials, and those served us really well at the time, but since then we've really expanded our use of materials across the system. And now with Dark Mode, it no longer really makes sense to specify a material just as light or dark. Instead, we have something called semantic materials. Now, if you're familiar with semantic colors, you know that they're named after where they're used, not necessarily what they look like. Same thing for semantic materials. The menu material, for example, will always look like system contextual menus, regardless of light versus dark. And in Mojave, we're introducing a bunch more semantic materials so that you can always use the right one for your specific use case. In fact, these semantic materials are now our preferred way of using materials and we're deprecating these non-semantic materials like light, dark, medium light, and ultra dark. If you're using one of these materials, now is a great time to go ahead and switch over to a semantic material that's right for your use case. Just to give you an idea of where we're using these semantic materials across the system, here's the Finder using the title bar and sidebar materials. Here's Mail using the header view and content background materials. Here's our Chameleon Wrangler app using the underPageBackground material. And here's system preferences using the window background material. Now, of course this window background material, as you've probably heard, is one of these special desktop-tinted materials new in Mojave. And the way these work is they pick up a slight tint from the desktop picture based on the window's location onscreen. And the idea here is to help your window blend in with the windows on the rest of the system. Again, the easiest way to get one of these desktop-tinted materials is to use the automatic support in NSWindow, NSScrollView, NSTableView, and NSCollectionView. The default configurations of these objects will come with this desktop-tinted effect. You can also configure NSBox to get these materials by setting its type to custom and selecting one of these fill colors. It'll use the corresponding NSVisualEffectView material. Here's an example. I'll set my box's type to custom and then I'll set its fillColor to the underPageBackgroundColor. Of course, I can also use NSVisualEffectView, I can set it's material property to the underPageBackground material. Now, the advantage of using NSBox is it's back-deployable actually all the way back to Leopard. VisualEffectView, on the other hand, gives you a little more flexibility and I'll give you an example of that later. So, just as a reminder, these materials will show their untinted color in light. And in dark, they'll show that desktop-tinting effect. But remember that the tint effect can be disabled. Let me show you why. So, in Mojave, you can choose an accent color for the system. And if I switch this over to graphite, you'll probably first notice that all the controls lost their colored accents, but those desktop-tinted materials also lost their tint. So, just make sure you're not depending on that tint being there in any way. Now, VisualEffectView by default will show its material in its frame rectangle like this. And that's pretty great, but what if I wanted to show a custom UI element with this material, like say a chat bubble. How would I do that? Well, here's one way that seems tempting, but won't work, and I'll show you why. We'll first implement the draw method on NSView and then I'll go get my custom chat bubble BezierPath. And then I'll fill with the controlBackgroundColor in that path. Now, if you do that, you'll find it looks something like this and it looks pretty good, but if we zoom in closely, you'll see that the bubbles are not getting that desktop-tinting effect that we want. It's just a plain gray. So, what went wrong? Well, this effect is provided by the Quartz window server like a lot of our other material effects. And what this means is it updates asynchronously from your application and this is great for performance, but it also means that you can't directly draw with that color or get it's RGB values. Instead, you can use the maskImage property of VisualEffectView to do something very similar. maskImage is an optional NSImage on VisualEffectView that VisualEffectView will use to mask its material, the material that it shows. And in addition to using standard art-based images, you can use drawing handler images to simulate drawing with the material. Let me show you an example. So, I'll go back to my view, I'll override layout, and I'll go ahead and add a VisualEffectView. I'll set its material to the contentBackground material, and then I'll create a drawing handler image using the NSImage size flipped initializer that takes a block. In it, I'll set the white color-- this color doesn't really matter as long as its opaque. And then I'll go ahead and fill with my path. Then I'll set that image ask the maskImage on my VisualEffectView. All right. Let's look at it now. Looks a lot better. It's desktop-tinted. And, if we look side by side, we can really see the difference. So, this technique works with any material, but just remember that only the alpha channel of the image is used for the mask. This is similar to template images. And the mask only masks the material, not any subviews or other descendent views of the VisualEffectView. A common technique is to provide a resizable image for this-- for the maskImage using the capInsets and resizingMode properties of NSImage. And this is really good for performance. OK. With that, I'll hand it off to Jeff, who is going to talk about vibrant blending. Jeff. All right. Thank you, Matt. So, now that we've had a look at our great materials, I want to cover the things that we draw in front of those materials, particularly the materials that we use that pull in part of the background and provide that really awesome blur effect. So, if we revisit our Chameleon Wrangler application, we have this UI here, it's our mood-o-meter. It's where we go to record how our various reptiles are feeling. And it's in a popover, which means that it's automatically getting that awesome popover material backing. And what we want when we're drawing over this backing material is for our content to really stand out on top of that varied background. Something like this. And we do that with an effect that we call vibrancy. So, what is vibrancy? It's a blending mode that we apply to the content that uniformly lightens or darkens the content behind it. It's very similar to a color dodge or burn that you might have seen in your favorite photo editor or design tool. But let's take a closer look. Here we have a glyph that's drawing in about a medium gray, about a 50% gray, but at 100% opacity. And when we apply the vibrant blending effect that we use against dark materials, which we call a lightening effect, we can see that it's not that the opacity has dropped on our glyph, but we're actually lightening the content behind it using the lightness of that gray value. And in fact, when we look at how this works on a range of gray values-- here we have swatches going from 0% to 100% gray, all totally opaque. When we apply our lightening effect, we can see a number of interesting things have happened. Down on the bottom right side, we have 100% light, and because we have added the lightness of white to the content behind, it just remains white. There is nowhere further to go. But on the top left where we were drawing black, there was no lightness to add, which means that it completely disappears. In fact, you wouldn't be able to see it if I didn't have an outline there. And in-between we can see that we have varying degrees of lightening which we can use to establish a hierarchy of content in our application. But where does this effect come from? Well, it's our old friend NSAppearance, it turns out. We have two special vibrant NSAppearance objects, vibrantDark and vibrantLight and these are a complete package. Not only do they include the exact formula that we use for that lightening or darkening effect, but they also have a set of control artwork and color definitions that have been designed to work great with that blend mode. But how does your code use it? Well, it's very simple. In your NSView subclass, you can override the allowsVibrancy property to return true and the blending effect is going to automatically apply to your views drawing and also the drawing of all of its descendants. Typically, when you're drawing in this vibrant context, you want to use one of the built-in label colors, depending on the prominence of your content. Both vibrantDark and vibrantLight have great definitions for all four of these colors that allow you to establish that nice hierarchy. However, you don't have to use these colors. You can use any color that you'd like, but we prefer to use non-grayscale colors. Avoid non-grayscale colors because, if you use them, the blending effect is going to impact the design intent of your color and it's going to wash it out in a way that is not desirable. I'll show you an example of that later. So, revisiting our application, we can go ahead and override allowsVibrancy on our view and in this case we're going to just set it on the view that contains our entire meter in the entire popover. And let's see what that looks like. Well, our slider looks pretty good. It's exactly what we expected. But what happened to the faces? They're all washed out. And what happened here is that when we set allowsVibrancy on the overall meter view, not only are we getting the vibrant blending on that view, but also both of these subviews. And the fix here is pretty simple. If we localize our definition of allowsVibrancy to just the part that's drawing the slider, we get exactly what we expected. Our slider is drawing vibrantly and the colors in our face buttons look exactly the way that we wanted. When you're drawing vibrantly, typically you'd want to apply vibrancy to only the leaf views that are drawing the content that you actually want to have vibrant. And, if you have views that are drawing a mix of content, that means that you probably want to break your drawing out into separate sibling views that you can use to apply vibrancy at the granularity that you want. Further, you should avoid overlapping vibrant and non-vibrant views. If you do this, the blending modes can clash and you might find that some of your content is drawing with a blend mode that it didn't expect. Further, don't subclass Cocoa controls just to override allowsVibrancy. I mentioned earlier that the vibrantLight and vibrantDark appearances have been designed with control artwork and colors that were designed specially for the blend mode and if you remove that blend mode, the contrast on that artwork is not going to be what you expected because we're using the blend mode to provide a lot of that pop against the material, so you should only override allows vibrancy if you're actually overriding drawing and you know what vibrant blend mode or non-vibrant blend mode is appropriate for the drawing that you're doing. That's vibrancy. Next, I want to talk a little bit about background styles, specifically the ones that we use for selections. So, here we have a pretty typical situation in a aqua Cocoa application. In this case, it's a message from the Mail application and we can see that when we have a selection state, we need our content inside of this table row to invert to look good against that blue selection. But when we add darkAqua into the mix, we can see that we can't just naively invert our content anymore. That's not going to work uniformly. And so we need to describe these states semantically. Now, if you're familiar with Cocoa, you've probably seen the NSView.BackgroundStyle enum and that includes a couple of cases, including light and dark, and NSTableView sets this automatically on the TableRowView, TableCellView, and also all of the controls that are immediate subviews of your TableCellView. Now, traditionally, we have set the light background style on unselected rows and the dark background style on selected ones. But, in the face of this brand-new, beautiful theme where the background is effectively always dark, these names don't make sense anymore and so we've renamed them to normal and emphasized, respectively. And these are just more semantic descriptions that better match the way that these enum cases are used in a modern Cocoa application. We also have some additional updates with background styles, including that TableView will now automatically set that background style recursively on all of the controls in your table row, not just the ones that are immediate subviews of your CellView. And so, if you've been catching that background style and trying to forward it along to all these subviews because you wanted to use a stacked view or something for layout, you no longer have to do that on Mojave. That's the applause of somebody who's done this manually. Thank you, I agree. Further, all four of our label colors now automatically adapt to the background style, which means that you can just set up your content hierarchy once, describe it semantically, and it's going to look great in both of these contexts. You can also use these emphasized variants manually and I'll give you an example. So, here we have something that looks a little bit like the icon view in Finder. And we've got two labels that are ascribed with label color and secondary label color. And we want to draw a custom selection behind them, so we've got this custom Bezier path-based selection, maybe we're filling it with alternate selected control color, and we want our labels to match the primary and secondary variants in this emphasized style. And to get that is very simple. All we have to do is set the background style to emphasized on both of our text fields and they're automatically going to provide this nice emphasized variant. And the great thing is that now that we've described it this way, when we switch into Dark Mode, everything just works. We don't have to do anything special to support that. One final note on selections. The selection material that you commonly see in sidebars, menus, and popovers now follows the accent color preference on Mojave. And what that means is that if you're drawing a custom blue selection, it's not going to fit in. Instead, you should use NSVisualEffectView. It has a special selection material just for this and when you use this it's going to automatically follow the preference, as you expect. Now, before I get into the exciting part, the tips and the tricks, I want to say a couple of words about backward deployment because we know that many of you, especially on the Mac, like to deploy your applications back to previous releases of macOS and it was important to us to make sure that you could adopt Dark Mode without necessarily compromising on your backward deployment. And so I'm going to step through a couple of APIs and just examine them for backward deployment, starting with system colors. So, here's a sampling of the system colors that we support that are dynamic for the appearance. And what I want to highlight here is that the ones highlighted in green have been available since at least 10.10 Yosemite, many of them actually far further back. And that means that we think that you have a great vocabulary of colors available to you to describe more or less any UI that you'd like and that all supports backward deployment out of the box. For custom colors, our modern preferred solution for defining them is asset catalogs and these are available back to 10.13. Now, when you do specify dark variants for any of your assets, when you back deploy them, those dark variants are safely ignored on previous versions of the operating system, so that's a-- that's a solution that has backward deployment built right in. But if you want to deploy back further than 10.13, you can use a technique like this where you write a custom color property. And here we just encapsulate the availability check to use our asset catalog color on operating systems that support it and then we can go ahead and put in a hardcoded fallback color for those older operating systems. Desktop-tinted materials is another great new thing in Mojave and if you want to address those materials directly with VisualEffectView, of course that's only available starting in 10.14, but we've been providing-- but we have classes that are providing these materials automatically, including Window, ScrollView, and TableView, which have been available since essentially the beginning of time. In fact, some of these predate macOS 10.0. And so, if you configure them correctly, they're going to on previous operating systems show that special NSColor which looks exactly the way that you would expect in previous versions and then when you run it on Mojave you're going to get that material automatically. And of course NSBox, the custom style that allows you to set a fill color, deploys back to Leopard 10.5 and so does NSCollectionView. And this works whether you're using the legacy NSCollectionView API or the modern one, although we'd prefer that you use the modern one. Finally, enabling Dark Mode is generally gated on linking against the 10.14 SDK, but, as you can see, really, the tools that you need to develop a great Dark Mode application aren't necessarily specific to the 10.14 SDK and you could have developed one just using the 10.13 SDK that you have today. And so, if you have a situation where you can't necessarily update your SDK, we have an Info.plist key that you can use to opt-in to Dark Mode. It's called NSRequiresAquaSystemAppearance and if you set that to NO explicitly, then that's going to enable Dark Mode even if you're linking on an earlier SDK, although we very strongly prefer that you update your SDK. It's a far better solution. You can also set this key to YES to disable it temporarily-- and I want to emphasize temporarily. This is a key that you can use to give yourself time to really build a great polished update for supporting Dark Mode. Finally, some tips and tricks. First of all, when you're updating your application, one of the greatest things that you can do is just audit your use of NSColor just by searching through your code base and seeing where you're using it. And you're going to find a couple situations that you can use to upgrade to make your Dark Mode experience a lot better. And so, for example here, we can find places where we're using named colors that are not dynamic and also colors that have hardcoded components. And when we encounter these kinds of situations, we can look at these and decide one of two things. One, maybe there is a built-in system color that describes what I'm going for and is fully dynamic for the appearance. Or, two, this is a custom color that I think is really important to be specific to my application. And so the first case is pretty straightforward. We were using black color for this label and we can just switch that to labelColor and that's going to be fully dynamic and do what we expect. But in the second case, we might decide that this color is actually really special to our app and that's a really great candidate for moving into the asset catalog. Not only does this clean up our code because we get all of these magic numbers out of our code and into a data-driven source, but we can also then set a dark variant for that color and so we get great Dark Mode support built in. Another common source of issues is offscreen drawing. To do offscreen drawing, you have to make sure that you're being sensitive to the appearance and also other drawing conditions. One really common case of this is using the NSImage lockFocus API to try and draw custom NSImages. In this case, we're going to go ahead and try and draw this badged image where we have a base image and we're applying a badge because something new is happening with our lizard. And, in this case, we're creating an NSImage, calling lockFocus on it, and then doing our drawing. And the problem with this is that once we've used lockFocus, we lose a lot of the semantics. We just have a single bitmap representation. And so if the appearance changes or if many other conditions change, including say the backing scale factor because you've moved your window from a Retina display to a non-Retina display, suddenly this drawing is going to be stale. So, a better solution is to use the block-based image-- image initializer, NSImage size flipped drawing handler. And we can just do the exact same drawing that we were doing before, but inside of this block. And when you assign this kind of image to an NSImageView, you're automatically going to have this block rerun when the appearance changes, scale factor changes, color gamut changes-- anything changes, essentially. And so that's great news because if our, say, badge fill color is a dynamic color, it's going to always resolve against the correct appearance. There are a couple of other ways that you might be doing offscreen drawing. You might be making custom bitmap graphics contexts using NSGraphicsContext or CGBitmapContext. And, depending on what you're doing, these might also be great candidates for replacing with a block-based NSImage. Further, if you're using the NSView cacheDisplay in Rect method to cache your image to a bitmap rep, just be aware that this method is not going to capture some of our more advanced rendering techniques like materials and blurs and it's also just another way that you can produce drawing that goes stale when the appearance changes, so be aware of that. Here's another situation that you might find yourself running into. If you have an NSAttributedString or NSTextStorage and you're manipulating those attributes manually-- say I am in this case, I've just set my attributes to just be a dictionary with a font in it-- you might find that this happens. Your text is drawing black even when you switch into Dark Mode and what has happened here? Well, we're missing a foreground color attribute and when the text drawing engine encounters a range of attributed strings that doesn't have a foreground attribute, it defaults to black. And this is what it has always defaulted to and it's going to continue to be the default for compatibility. So, one way to fix this is to set a foreground color explicitly to one of our dynamic system colors, and that's going to do what you expect. But a better alternative is that if you're doing manual attributed string drawing, you should switch to a Cocoa control, like an NSTextField, which does this for you automatically, or, if you're manipulating the storage of a textView, we have new API called performValidatedReplacement on textView that does a nice thing for you. If you go ahead and replace a string with an attributed string in your textView, it will fill in any missing attributes with the typing attributes from the textView, so that way you can go ahead and specify your new attributed string without having to manually merge all your attributes together. Here's something else that we've encountered in a couple places, which is appearances that are set in Interface Builder. So, if you're going ahead and building and debugging your application and you find that there's some part of your app that just isn't switching, you might have this in your Interface Builder. A hardcoded aqua appearance. And it's easy to miss, because before today, essentially, you were always running under aqua, so you didn't notice it. And the fix for this is easy. If you set this back to the Inherited option in the pop-up menu, your view's going to automatically inherit from its ancestor. An extra special case of this is NSVisualEffectView. It's very likely that if you have a VisualEffectView in Interface Builder or even in code, you're setting one of the two vibrant appearances on it and the great news is that in macOS 10.14 this is no longer necessary. NSVisualEffectView will automatically pick the right vibrant appearance based on the appearance it's inheriting. So, if it inherits darkAqua, it's going to choose vibrantDark and if it inherits aqua, it'll choose vibrantLight. And so the fix for this is easy. In Interface builder, you can set this to inherited and then in code you can set the appearance to nil or just delete your override. Interface-- speaking of Interface Builder, Interface Builder is a great tool for designing and previewing your views visually. And so, for example, here I have a view that is actually a custom view using IB designable. So, I'm rendering a gradient here and I can see it right here in the canvas. And, by default, my canvas is previewing my custom designable view using the canvas's appearance, in this case dark. But down at the bottom, we have a new toggle that lets you go ahead and set it to the light appearance so that you can preview the way that your view looks in either appearance. And thanks to Interface Builder's great support for asset catalog colors, we can actually use our custom asset catalog colors, which have dark and light variants, and we can preview them in the canvas live. And if you see there's a little arrow button built into that pop-up button and you can use that to follow it and go straight to the definition in your asset catalog, so you can see live as you're changing it. And you can do this all without even building and recompiling. When you do build and run, you're going to see a new item in your Debug Bar and it produces a menu that allows you to choose the appearance for your application. And this is really handy for previewing your app in various appearances without having to go and reconfigure your entire system. Not only can you choose light and dark, but you can also choose the high contrast variants and test those as well. And, if you have a Touch Bar Mac, this appears in the expanded Debug Bar as well, so you can do this without even leaving your app to go back to Xcode. Finally, I want to talk about one last tool in Xcode that is really great for debugging your Dark Mode applications. So, here we have our app and really things are looking pretty good. There's nothing out of place, but I find that when I scroll and rubber band a bit, oh, I'm revealing something that I didn't expect. There's a light background hiding back there somewhere, but it's hard to see without doing that little scroll gesture. And this is a great case for using the View Debugger. Using the View Debugger's expanded 3D view, the view that's drawing unexpectedly is really easy to spot. And in this case, we can see that although our collection view was drawing the background that we expected, the scroll view behind it still has a light background for some reason. And when we select it, we can use the Inspector to see how it's being configured. And in this case, we can verify that, yeah, it's just drawing a hardcoded white color and that's a really easy fix. The View Debugger has made a number of enhancements in Xcode 10 that are great for debugging Dark Mode applications, including colors. They can now show you the names of colors, both dynamic system colors and your asset catalog colors, so you can identify where these RGB components are coming from, and it'll show you the configuration of your view for NSAppearance, including the appearance that it's going to draw with as well as whether there's any local overrides of NSAppearance on that object. So, we have covered an awful lot of content and so let's rewind and make sure that we remember it all. We started off with NSAppearance and leveraging it effectively to draw your custom views that adapt based on the theme. Then we learned how to add depth and beauty to our UI using our new and updated palette of NSVisualEffectView materials. We talked about drawing in a couple of interesting contexts, both vibrancy and selections, and then we walked through some of the great ways that Xcode can help you design and debug your Dark Mode applications. As always, you can go to developer.apple.com to re-watch the video for this talk and see any related resources and we have labs today. We have a special Cocoa and Dark Mode Lab, it's at 2:00, and not only will we have Cocoa engineers onsite to help you with your code, but we'll also have human interface designers onsite to help you with your design questions as well. So, go get lunch, think about Dark Mode the entire time, and then come see us. And then, finally, we have an additional Cocoa Lab on Friday at 11:00 as well. All right. Thank you very much.  Good afternoon, everyone. Good af-- alright. I hope you've been enjoying WWDC so far. I know it's been an exciting week filled with announcements, features, updates, developer tools. My name is Shloka Kini, and I work in developer publications, which means-- to borrow a phrase from Cardi B., I don't just code now, I write docs too. Specifically, the docs that'll help you to write amazing applications. Today, I'm privileged to call out some great features in Safari and WebKit. So, if you develop websites, and want to make use of the latest web technologies, and the latest versions of Safari, this talk is for you. And, if you're a native app developer that uses web views, or extensions developers, this talk is for you, too. And, even if you're not in any of these categories, you should still stick around, because the latest version of Safari has some great features that will improve your browsing experience. Now, there've been a lot-- a lot-- of new improvements since we last had a What's New talk. But, today I'm going to highlight a few that can really help you get secure, performant apps, and use the latest web technologies for a rich experience. And, many of them you can get for free. So, let's kick things off with security. And, a few announcements. WKWebView. Now, I know what you're thinking. WKWebView has been around since 2014, so it's not technically new. However, it's worth mentioning again because we are now officially deprecating UIWebView. So, if you're starting a new app, or a new project, and would like to display web content that's not easily put into a native view, use the WKWebView. And, even if you've used UIWebView in the past, switching might be easy for you. It can definitely save you time in development, if you're developing apps for both macOS and iOS, because WKWebView works on both platforms. Unlike UIWebView for iOS, and WebView for macOS. So, you can share a lot of code between the two versions. WKWebView also runs in a completely separate process from the rest of your app. So, unlike UIWebView, even if your web process is compromised, it won't compromise your app. If your web view has complex content, you can't accidentally cause your app to stall. And, even if WKWebView crashes, it's confined to the web view, not the app. WKWebView can provide security benefits while keeping your apps performant and reliable. So, whether it's hard or easy, the benefits you get using WKWebView are worth the switch. The next announcement involves extensions, but extending Safari-- I mean, it's evolved a lot over the years. So, let me start with a quick recap of the history of Safari extensions. Now, in 2010, before we had a platform concept of app extensibility, we had legacy Safari extensions. Now, these were the Safari EXTZ files you could build in Safari Extensions Builder. They could be distributed through the Safari Extensions Gallery, or in some unusual cases, by developers directly. These legacy extensions were incredibly powerful, because they had access to all your browsing data, which made them popular, especially for fraud and malware. We needed to create a safeguard, so that's why we didn't just leave it at these Legacy Safari Extensions. So, the next milestone in our story came in 2014, when we introduced app extensibility for macOS and iOS. App extensions, though, are a way to extend apps, not Safari. However, this move greatly changed how we thought about extensions in Apple platforms. Here, you could clearly extend the system while users are interacting with other apps. And, like apps, they could be built in Xcode. Because of this better extensions model, we wanted to apply some of these concepts back to those legacy Safari extensions, and at the time, the most popular ones were adblockers. So, we introduced content blockers in 2015. Content blockers were a type of app extension built in Xcode, that worked on both macOS and iOS. They have a special architecture that makes them fast. So, any content blocker is faster at blocking than any legacy Safari extension. They don't have the power to slow down browsing, and they're private, because the extensions never see what web pages your users visit. And, by this point, the app extension model offered so many performance benefits we thought, maybe we can bring all these concepts back to the legacy Safari extensions, so we can get the best of both worlds. An extension that extends Safari's functionality, but also extends your app to talk to Safari. So, in 2016, the modern Safari app extensions for macOS were introduced. A way to extend Safari that could be built in Xcode. And, unlike previous extensions, you get them through the App Store, which means they can be free, or you can charge for them. Either way, you don't have to do your own billing. So, compared to those legacy extensions in 2010, content blockers and Safari app extensions have great benefits. So, the best thing for you to do, is if you have a legacy Safari extension, switch over to a Safari app extension. And, if it happens to be an ad blocker, use content blockers. And now that we've done all this work, what can we do about the use of legacy Safari extensions for fraud? Starting with Safari 12, we're officially blocking support for legacy extensions distributed outside of the Safari Extensions Gallery. Legacy extensions will still work in Safari 12 as long as they're in the Gallery. The only exception are those extensions using the deprecated Can Load API, which we turn off by default. We'll continue to accept submissions to the Gallery until the end of 2018. However, we will be coming up with more updates in the following year, and will eventually transition entirely to Safari app extensions. So, the best thing for you to do is learn how to develop extensions in these two models. And, to learn how to do that, check out the docs, courtesy of yours truly and Developer Publications. Now that we've covered the two biggest announcements for native developers using WebViews, and extensions developers, the remainder of these features are primarily going to be about web development. So, let's start with subresource integrity. Now, as a developer, you may serve your content over an HTTPS connection to your user. And, that resulting content may also include content distributed over a third-party server, like a content delivery network. Now, both connections may be secure, both may use HTTPS, which means you maintain the confidentiality, authentication, and integrity of the data transferred. But, what happens if that third party itself is compromised? It could happen. And, in this case, while HTTPS secures the connection, it doesn't secure against a compromised server. It can modify the scripts, and styles you serve to users if that third-party server is compromised. Subresource integrity ensures that you don't serve compromised scripts to your users. So, how does it work? Well, with hashing. First, you add the integrity property for a script or link element in your markup. The value for this property is a hash that you create using a secure hash algorithm. When a user fetches the files, then another hash is calculated. The two are compared, and if they don't match, your script will fail to execute. This process ensures that scripts won't execute if they're compromised. Unless they match what you intended, your scripts will not execute. And, to make sure you don't lose functionality, you can also provide a fallback to reload a resource from your server, in case the third-party script fails to execute. Now, keeping compromised resources from executing keeps users secure. And, intelligent tracking prevention can keep the browsing experience private. Now, I'm sure you heard about intelligent tracking prevention in the Keynote. It's a Safari feature that reduces cross-site tracking by limiting cookies and website data for domains with tracking abilities. And, in previous versions, cookies were kept according to two rules. One, cookies could be used in third-party context for 24 hours after user interaction in a first-party context. And two, for 30 days of Safari use, including that initial 24 hours, those cookies would be kept in isolated storage before being purged. But, now we're tightening the belt a little. And, we're removing the 24-hour general cookie access window for domains with cross-site tracking. But, by default, all cookies are kept in isolated storage, and as developers, I know authenticated embeds are already important to many of your workflows and interactions with web content. So, how do you allow authenticated embeds? Using the Storage Access API. With the Storage Access API, every time a domain with cross-site tracking would like to access cookie in a third-party context, you'll need to request storage access. If the user has not granted access previously, a prompt appears, asking the user whether to permit cookie access or not under this website. By enabling users to provide explicit consent for cookie access, we're empowering them to take control of their cookies, and what websites can track, keeping their browsing experience more private if they choose. Now, next, we'll move on to authentication with automatic strong passwords. Now, I'm sure you saw this in the State of the Union and session earlier this week. Automatic strong passwords is a great way to guarantee that users will always select and save a password that's strong when signing up for a new account. And, this is good for everyone. I mean, I like to think of myself as someone who chooses strong passwords, but give it a little bit of time, and I'll realize that password wasn't as strong as I thought. And, I probably used it in a couple places. For most developers, you won't need to do anything to get this feature, because heuristics will determine if you're on a sign up or login page. But, to guarantee this works, regardless of login flow, add the AutoComplete attribute to the appropriate input fields. Now, the strong passwords we choose are by default 20 characters in length, including upper case, lower case letters, digits and hyphens. Now, while this was designed to be compatible with most services, we acknowledge that sometimes your passwords need to have specific requirements to be compatible with the back-end system. For this reason, there is a passwordRules attribute that you can add to your text elements to specify those requirements. And, on the developers site, there's a password validation tool, to help you test compatibility with automatic strong passwords, and develop your own password rules. Another feature mentioned in the State of the Union, security code AutoFill. Another feature most of you will get for free. This is one I'm going to be making good use for, because I find it tedious to switch between my app and the website, and the messages, and then find out those numbers for the code, and input it and try to remember it. So, having Safari figure out when I have to input a security code, and then suggesting it in the quick type bar? Makes this much more convenient. And, just like before, you get this feature for free, because it uses heuristics, but to ensure that these heuristics work, and you get that quick type suggestion, mark your input fields with the one-time code value in the AutoComplete attribute. For more details, I encourage you to check out the Automatic Strong Passwords and Security Code AutoFill session online. So, that's security. Switch over to WKWebView, more over to content blockers in Safari app extensions, subresource integrity is a failsafe to ensure you don't serve compromised scripts to users, and intelligent tracking prevention improves privacy with the Storage Access API. And, with automatic strong passwords, and security code AutoFill, you get features that are secure and convenient for your users. Whew. You all still with me so far? OK, moving right along, let's talk about performance features, starting with font collections. Now, for those of you who may not have caught it at the top of this talk, my name is Shloka Kini. And that ain't no Anglo-Saxon name. And so, here's my first and last names, using the Devanagari script in the Hindi language. Multiple fonts, different weights and styles, but the same character set. New this year, we support font collections, WOFF 2 and TrueType collections. Bundling related fonts together inside a single collection file can eliminate duplicated tables for character maps. For example, one of our built-in fonts, PingFang has an 84% reduction of file size from using a collection. Font collections can substantially reduce the size of your font files, because the fonts share a table for the same character set. Now, this next feature, font-display, requires no change for most developers. Essentially if you have web content that uses custom fonts, if they don't display for your user for whatever reason, by default we leave a blank placeholder for the text for up to three seconds, before your font displays, to maintain the position of all the content on the screen. But, if this default behavior isn't quite right for you, and you want to have more control over what happens instead of those three seconds, you can use the font-display descriptor. Using different values, you can specify another font as a fallback, or check if the browser has that font in the cache. Now, one cool trick you can use to improve the performance of animated images is using video. Now, I love the colored dust explosion background on my Mac. It's really great, but it's static. I mean I want this thing to bam, pop! I mean, I want motion. I want a GIF. But, animated GIFs take much longer to load, they use more battery power, and give lower performance than a video file showing the exact same thing. Now, in Safari, MP4 video files are supported in image elements, making use of Apple's built-in support for hardware video decoding. My content loads faster, uses less battery, gets better performance, but I can also use MP4s in the CSS background image property. Now, if you adopt this technique, in the simplest way, you could come up with a version that isn't compatible with older browsers. Older browsers don't support MP4s and image elements. Luckily, using existing technology, you can specify a fallback image to display if the MP4 doesn't work. Now, listen up, because now we're going to move on to event listeners. Yes? No. Another feature that has some great defaults, and some customizability in special cases. When any user tries to navigate a web page with a touch screen, they're going to need to scroll. And, for every touch to scroll, a touch event listener can fire, which can interrupt scrolling and cause it to jump a little. Take a look at these two examples. Now, the one on the left is interrupted much more than the one on the right. I mean, it's barely moving. So, what's the one on the right doing right? It's using passive event listeners. By default, we enable passive event listeners on the document, window, and body elements, so any touch events on these elements indicate to the browser to continue scrolling, and not be interrupted waiting for the event listeners to finish. If there are additional elements with event listeners that you want to make passive, you can set the passive property to "true" for those event listeners. Essentially, without preventing default event handling, this flag tells the browser not to wait for event listeners to finish, and lets your users continue scrolling smoothly. Next, we move on to asynchronous calls, with async image decoding. Now, typically, images are decoded synchronously. So, the main thread is blocked. All the images are decoded, and then they display. By blocking the main thread, this blocks user interactions. But, with asynchronous decoding, the operations happen in parallel, and on a separate thread, which means the interactions aren't blocked. And now, new this year, async image decoding happens by default on the first page load, which can cover most cases for web content. However we know that some of you may have special cases. Say, you have a tiled map on your webpage that loads after the initial page load. And, if it has lots of images, some of the tiles may be delayed in their display. Or, maybe you have a carousel of images in your app that you want to fade into each other, but when you try to advance the slides, if the images are decoded synchronously, they might not be ready for display. And, they abruptly switch. But, on the right, asynchronous decoding gives you a smoother fade. Now, if you want to fall into one of these special dynamic cases, you have two options. One, you can add the decoding async attribute to your images elements in markup. Or, you can use the JavaScript API's HTMLImageElements.decode method, which returns a promise, making sure that you know when an image can be added to the dom without causing a decoding delay on the next frame. And, continuing with asynchronous calls is support for the Beacon API. We know, as developers, you want to send data on an unload event. Perhaps to track outgoing links. And normally, asynchronous requests on unload are ignored, so you're stuck using a synchronous request, which can stall the next page load. However, we now support the Beacon API. So, as long as Safari is running, you can send your data to the server and forget about it, with the guarantee that it will be delivered. But, you've heard me talk enough. I mean, I'm sure you want to see some of these security and performance features in action. So, I'd like to call Jason onto the stage to show you how they all work. Jason? Hi, everyone. My name is Jason Sandmeyer, and I'm a developer on Apple.com. In my free time, I enjoy doing arts and crafts, like building birdhouses, and I recently started this blog to share some of my projects and inspire others. I spent a lot of time picking just the right fonts, the right colors. I'm pretty proud of it. But, you know what, I don't just pride myself on good design, I also pride myself on providing a good, secure, and performant experience for my users. So, I'm really excited about all these new performance and security features in WebKit and Safari, and I really want to take advantage of them on my own site. I'd love to show you how easy that can be. So, I have my site loaded on my MacBook Pro here. And-- whoa. OK. Dude! Jason, what did you do? Yes, this isn't the elegant blog I was just bragging about, is it? Let's see-- that's the right URL. You know, I think I know what happened here. When I first started this site, my friends, they warned me that the lifestyle blogging industry can be pretty cutthroat. Clearly, this is sabotage. Someone's replaced my style sheet on my content delivery network. But, luckily, I have a backup, fortunately. And, we can use subresource integrity to add a little bit of an extra layer of security, and ensure this doesn't happen again. So, I'll start by adding the new integrity attribute to my link tag. I should also mention this works on scripts, but we're going to make some changes later, so we'll add that later on. So, the value of this attribute is the hashing algorithm that was used to generate the checksum for the file that I expect my users to see. I've already prepared a hash with SHA256. Next, a hyphen, and then a base64 representation of the hash. Now, let's save this, go back to our page, reload. And, we'll see there's no styles. Because the hash for the downloaded file doesn't match the hash in the HTML. So, Safari has blocked it from being loaded. Now, let's connect to my CDN. And, here's my backup on my desktop. Let's drag in my backup to the CDN, replace the compromised file. And now, when we reload, that looks a lot better. Thanks. So, with subresource integrity, I'll be more confident that my visitors will see the styles and scripts that I expect them to get. Now, let's switch gears a little bit and talk about some performance improvements we can make. I found it insightful to know which links are being clicked on my site, and which ones aren't. It helps me make more informed design decisions. So, I have this click handler that reports which links are being clicked to a server that I control that aggregates that data so I can take a look at it later. But, notice this delay when I click on this Woodworking link that goes to a page that showcases other woodworking-related sites on my page. I'm going to click the link now. Took about a half a second to a second, and this is happening because I'm making a synchronous request in the click handler, which blocks Safari from navigating to the next page. Making a synchronous request ensures the browser doesn't cancel the request when navigating to the next page. But, this is waiting for my server to respond, which can take a while. And, the thing is, I don't really care about the response, I just want to make sure that that data hits my server. So, the Beacon API is actually a perfect replacement for this. I'm going to start by checking that the Beacon API is available in the browser by looking for the sendBeacon method on the navigator object. If it's not available, I'll continue doing what I was doing before. Then, we can just use it. Passing in the endpoint I want to hit, along with the data. Let's save that. We'll go back, reload to get the new script. And now, when I click this link you'll see it's nearly instant. I'm going to click the link right now. And, there we go. So, compared to the XML/http request this is even less code, and it's just as reliable. And now, it'll be much faster for my users to navigate around my site. Thanks. So, next I want to take a look at a problem that I've noticed is more apparent on my iPad here. So, I've organized each step for building this birdhouse as a slide in this crossfading carousel. Tapping the right-facing arrow advances this to the next slide. But, you may have noticed that brief moment of a blank white space where the image should be. Let me go through a few more slides. Let's take a look at some of the code for this carousel, and see what's going on. I think this can be a lot smoother. So, here's my carousel class. I want to focus on this method here, setCurrentSlide. This is the method that's called when the button is clicked to transition to the slide at the given index. Because each slide isn't immediately visible on page load, my carousel only loads the next slide's image when the user taps the button to advance to it. The problem that we're seeing is that the transition is happening immediately. It's not waiting for the image to load. And, after the image has loaded, it still needs to be decoded before it's ready to be displayed on the screen. So, what I really want to do is wait until the image has been loaded and decoded, and I'm sure that we can show the image. And, I can use the new decode method on the HTML image element to make this a lot better. So, I have my image-- a reference to my image element here. The decode method will asynchronously decode the image, and return a promise that resolves when the image has been loaded and decoded. So, I'll just pass my transition function in as the callback for the promise. Now, let's switch back to the iPad. And, we'll refresh to get the new script. And now, when I advance, you'll see this is much smoother. No flashing. It's really great. Thank you. Now, let's switch back to the Mac. Now, finally, at the bottom of my page, I have this animated GIF of a bird furnishing its new birdhouse. This image is-- this video's pretty large-- well, it's a GIF. Seven-- it's roughly a little over 7 megabytes. And, honestly the quality isn't that great. But, I happen to have the original H264-encoded MP4, and now I can just use that directly on my page. So, let's go back to my HTML, and find that image. Here it is. So, I can just change the extension to point to the MP4 file. Reload. And, now I'm using the actual video. The quality's a lot better, and this is only about a megabyte. Plus, it's a little bit longer than the animated GIF. And, as Shloka mentioned, this can also be used in the source attribute to provide a fallback image for browsers that don't support this. So, that's just four of the many new security performance features in Safari and WebKit. I hope you'll take advantage of them on your own site, and I think your users will thank you for it. Now, I'd like to welcome Shloka back up on the stage to tell you about even more new and exciting features. Thank you. Thank you, Jason. And, I had no idea it was that brutal in the blogosphere. You stay safe out there. And, thank you so much for that great demo. To recap performance, using font collections can reduce font file sizes. The font-display property lets you have more control over what happens with custom fonts. Using videos in image elements can help with performance instead of GIFs. Passive event listeners can improve scrolling, and using asynchronous calls, both with the Beacon API, and with image decoding keeps the main thread from stalling. Last, we move onto rich experience. Some cool new features that can really improve your users experience. Starting with drag and drop. Now, first, some general improvements to drag and drop, thanks to some API updates, now you can drag and drop entire directories of files to upload them to the server. No compression or zipping required. And, we support reading and writing MIME types for rich HTML, plain text, and URLs to the system pasteboard. And, specifically for iOS, we've made some new updates to the data transfer API, so now you can customize drag and drop with the getData and setData methods. So, for example, if I wanted to drag groceries into my online grocery shopping cart, I can customize the drag and drop behavior. So, dragging an image element will drop the name of that element and its price into my cart. Now, you can specify what happens with drag and drop behavior, which lets you implement richer user interactions. Next, we move into the API section of this talk, starting with the Payment Request API and Apple Pay. So, let's talk about Apple Pay. Apple Pay's not just a way to pay. It's a way to rethink electronic payments entirely. With Apple Pay, vendors won't directly receive credit card information of your customers, which keeps them more secure. Now, we know that many of you have been requesting a way to support Apple Pay using a standard API. And, I'm pleased to tell you, we listened, and with collaborative efforts, Apple Pay now works with the W3C Payment Request API. So, while you have the option to use this API, remember that to get the benefits of Apple Pay for you and your customers, you will need to make a few changes. For example, adding an Apple Pay button to your interface, rather than adding Apple Pay as an extra option in existing checkout flow. And, at the moment there are a few features incorporated in the Payment Request API, like granular error-handling, handling cobranded cards, and phonetic names. Features that only appear in Apple Pay JS. So, if you need those specific features for Apple Pay, use Apple Pay JS. The next API we're supporting is the Service Worker API. And, if your user's network connection isn't ideal, maybe-- I don't know, they have poor connectivity, or they're completely offline, you want to make sure that you handle that situation gracefully. And Service Workers can do that. A Service Worker is registered by a unique origin, and it can cache offline interactions, and intercept requests made by scripts associated with that origin. Now, every page in your domain can share the same Service Worker instance. So, you can have multiple tabs open at the same time, and all those requests will be intercepted by the same script. So, you can keep a persistent store of resources. Service Workers makes your web page, whether its a web app, or whether you're using SF Safari viewController, more resilient to variants in network connectivity. And, the last of the APIs is the Fullscreen API for iPad. Now, you can customize fullscreen controls for the iPad. Any arbitrary element in Safari. And, clicking on that elements will bring up a complete fullscreen experience. Now, for videos we auto-detect the content, and a Cancel button appears. And, after a short delay, if the content is playing, the button will disappear. Now, if you're presenting content that ends up being blocked by this Cancel button, you can use the CSS Environment Variable fullscreen-inset-top to avoid it. You can also have your content hide at the same time as the button, by using the fullscreen-auto-hide-delay environment variable. Last, a couple of cool callouts, starting with AR. Oh, you've heard so much about AR at this conference so far. And now, you can add AR models to your UI with image thumbnails. So, your websites can take advantage of the brand-new AR Quick Look. And, the code's fairly short. You start with an anchor tag, set the del attribute to "AR" and set the HREF link to your USDZ file, then you file format for AR models. You add a single child, either an image or a picture element containing an image of the model. So, the resulting image looks like this. In the top corner of the image, a small icon appears, indicating an AR model is available if you click on the image. It's a great way to add more depth to the content in your websites. And, for more details on Quick Look, you can check out the session online for Integrating Apps and Content with AR Quick Look. And, last, watchOS. You can already view websites on the MacBook, and the iPad, and an even smaller screen with the iPhone, a screen that can fit in your pocket. But, now we're going to downsize one more time. We've brought you websites on watchOS. Now, I'm personally really excited about this one, because I receive recipes from my mom all the time. I cannot cook, and I see them in Messages and emails and now, when I get that recipe I can see it right there on my wrist while I'm following along. Now, if you use responsive design, great. We do all the work for you and your websites are going to look great on watchOS. But, if you would like to further optimize your webpages for Apple Watch, there's a video for Designing Web Content for watchOS in the WWDC app. Excuse me. Designing, yes. And now, I bet Jason's birdhouse blog could really up the ante with some of these great new rich experience features. So, I'd like to call Jason back on the stage to show us how some of them can be used. Jason? Thanks again. So, I've been thinking about ways to make it more fun for my readers to get started with their birdhouse project. Let's switch back to the iPad. So, I have this list of all the supplies that my readers will need to get started. And, I thought it'd be convenient if I could actually provide them with a way to add to their shopping list, the things they might need, and maybe even purchase some of these directly from my site. Plus, I figured I can make a little extra cash in the process. So, I have the ability to drag and drop the supplies from the left onto this shopping list. And, this works great now on my iPad as well. So, let's take a look at some of the code that's used to achieve this. Doesn't actually take a lot of code to do this. So, for each supply, I add a dragStart eventListener, which stores the element's text, using the Data Transfer API. Then, in my drop zone, which is my shopping list area, I have a drop event listener, that retrieves the previously stored text from the Data Transfer API. And, appends that to the shopping list element. Note that you do also need to add a dragOver eventListener, and for the area where you want the element to be dropped, to prevent the default event, and indicate that a drop is allowed on that element. So, with very little code, I was able to create this fun shopping UI that works great on my Mac, and now on my iPad as well. So, now that I can place supplies in my shopping list, I need a way for my users to actually make a purchase. Let's take a look at how we can provide a great Apple Pay experience with the Payment Request API. So, I've already added the necessary HTML and CSS to my site to display an Apple Pay button, but I've hidden it by default. You should only show the Apple Pay button if the user's device is capable of using Apple Pay. so, let's check for that, using the ApplePaySession.canMakePayments method. If Apple Pay is available, we can show the button. Let's add an eventListener to the button. Now, inside this function is where we'll create a new paymentRequest instance to initiate the transaction. If paymentRequest isn't available, we should consider using Apple Pay JS instead. Here's the constructor for the Payment Request API. It accepts three arguments. We'll start by adding the paymentMethod data object. This contains the Apple Pay paymentMethod identifier, along with some options specific to Apple Pay. Following that, are the payment details. This is where we specify details about the transaction, such as my website's name, the total amount, and each line time. I kept things simple, and decided that I'm just going to charge $5 for everything on this list. Finally, the options argument specifies what information I need to collect from my user to complete the transaction. Let's switch back to the iPad and add some supplies to my list. So, now that we've passed all the information in, we actually need to call another method to show the sheet. And, that's the show method on the paymentRequest. And, this method returns a promise that resolves with a payment response when the user authorizes the transaction. So, [inaudible] with Face ID or Touch ID. In here, is where you would process the transaction. And then, finally, you'll call complete with a value of success or failure, depending on the state of the transaction. Alright, now let's check that out on the iPad. There we go. So, there are a few additional steps you'll need to take, like obtaining a payment session from the Apple Pay server. To learn more about that, please see the sessions page on the Apple developer website for links to those additional resources. Now, finally, I realize I haven't given a glimpse of my readers-- what they're actually building. So, I want to add an image near the top of my page of the final product. But, why stop at a static image? Wouldn't it be great if you could actually see the birdhouse in your own environment, get a sense of its size? So, with the new AR Quick Look feature in iOS 12, we can do this with just a few lines of code. So, we'll go into my HTML. And, I think this is a good place of it. So, all I'm doing here is adding an image. And, linking to a USDZ file, that is the model of my birdhouse, with a rel attribute of AR. Switch back to the iPad, and there's our finished product. That looks pretty nice, but now my users can also tap on this Quick Look, AR Quick Look icon in the corner here. We can see the model, move it around, and I can also place it in the real world, and actually get a sense of what I'm going to build. So, it's actually really easy to do this. Please check out that session if you have the chance. I'd like to bring Shloka back up on stage to wrap things up. Thank you. Thank you so much, Jason. And, the AR model looks really, really cool. And, I think its inspired me to try to build a birdhouse. Not making any promises. So, you can add custom drag and drop features. And custom fullscreen controls for the iPad. You can use the Payment Request API to support Apple Pay, and the Service Worker API to support offline experiences. Or, you can add AR models to your content to give it depth. And now your websites can be viewed in Apple Watch. I've called out several sessions that you can reference for individual features, but if you have any questions right after this talk, stop by the Safari, WebKit, and Password AutoFill Lab. And, check out the link to this session for, of course, documentation resources and related sessions. Now, there are a lot, a lot of features when it comes to Web, and I hope that this quick overview gives you a taste of how Apple constantly improves Safari and WebKit support. So, web developers, native developers, and extensions developers can always offer the best experiences possible for their users. Thank you for so much for enjoying us for this-- for joining us for this session. Hope you enjoyed it. And, enjoy the rest of your afternoon at WWDC.  Good morning. My name is Josh Graessley, and I am really excited to be here this morning to tell you about Network.framework. Network.framework is a modern alternative to sockets. Today, we're going to talk about modernization transport APIs. This will help give you some context to understand what Network.framework is and how it fits into the system and whether or not it's the right thing for your application to be using. We'll introduce you to the API by walking you through making your first connections. We'll talk about how you can use this API to really optimize your data transfers and go way beyond the performance of anything you can do with sockets. We'll talk about how this API can help you handle some complex mobility challenges, and we'll wrap up with information on how you can get involved and start adopting. First, I'd like to spend a little bit of time talking about modernizing transport APIs. Now when I say transport API, I'm talking about any API that lets you send and receive arbitrary data between two endpoints on a network, and that's a pretty broad definition, and there are a lot of APIs that could fall under this category. Perhaps the most ubiquitous is sockets. Sockets has been within us for over 30 years, and I don't think it's an exaggeration to say that sockets has changed the world, but the world has kept changing. And as a consequence, using sockets to write apps for today's internet is really hard. There are three primary areas in which it's very difficult to use sockets well. The first one is Connection Establishment. There's a whole host of reasons that establishing connections can be really difficult with sockets. For starters, sockets connect to addresses, so you have to, most of the time you have a host name, so you're going to have to resolve that host name to an address. When you do that, you often end up with way more than one address. You'll have some IPv4 addresses, some IPv6 addresses, and now you've got this challenge, which address should you try and connect to, in what order? How long do you wait before you try the next one? You can spend years trying to perfect this. I know because we have. Once you get past the dual stack host problems, you run into a whole bunch of other issues. There are some networks that use something called Proxy Automatic Configuration or PAC. On these networks, there's a JavaScript that you get, and you have to pass a URL into the JavaScript, and the JavaScript runs and spits out an answer that says either you can go direct or you have to use this SOCKS proxy over here or that HTTP connect proxy over there. And now your app has to support SOCKS proxies and HTTP connect proxies, and this can be really difficult to do well. And the most difficult thing is that you may not have one of these networks to test on, so you may get a bug report from one of your customers, and they may complain that it's not working well on their environment. And you may want to add code to fix the problem, but that once you've got it in there, you really don't have a good way to test it. You have to end up building the whole environment to reproduce the same environment they have. It can be a real challenge. So connecting with sockets is really hard. The second thing that becomes challenges with sockets is data transfers. There's a lot of reasons that transferring data with sockets can be really difficult. The primary problem is the read and write model itself. If you're using blocking sockets, it pretty simple, but you're tying up a thread, and it's really not a great idea to be tying up a thread while you're waiting to read or write data. You can switch to nonblocking, but then you end up with a whole lot of other challenges that you run into. When you're using nonblocking, you may tell the kernel I'd like 100 bytes, and the kernel will come back and say, I've got 10 bytes for you, why don't you come back later. And now you have to build a state machine to keep track of how many bytes you read versus how many bytes you want to read. This can be a lot of work, and getting it to perform well can be a real challenge. On top of all of that, you really shouldn't be reading and writing to sockets directly because you should be using something like transport layer security or TLS. Sockets don't support TLS, so you're probably using some other library that is handling TLS for you and reading and writing to the sockets on your behalf, or you're writing the glue code between that library and sockets, and you have to figure out how to get all this to work with all the crazy connection logic that you put in ahead of time. There's a lot here that can be really difficult. Finally, mobility can be really challenges with sockets for a variety of reasons. I think a lot of this boils down to the fact that when sockets came out, a lot of the devices required more than a single person to move them, and they were connected with a single wire, and they had a static IP address, and everything was stable and simple. And today, we have these incredibly powerful devices in our pocket with multiple radios that may be on at the same time, and some of them are moving from network to network, and your application has to handle all these transitions well to provide a seamless experience to your customers. Sockets does nothing to help you with this. You can use routing sockets, but it's really, really difficult. We think a transport API should do better. Fortunately, on our platform as an application developer you have a great API in URLSession. URLSession handles all of these problems for you. It's really focused on HTTP, but it also has stream task that gives you raw access to TCP and TLS connections. Now you might be looking at this, and you may not have cheated by looking at the description in the WWDC app. You might think that URLSession is built on the same primitives that you would use yourself. But it turns out that's not the case. URLSession is built on top of something we call Network.framework. URLSession really focuses on all of the HTTP bits, and it offloads a lot of the transport functionality to Network.framework. Network.framework is something we've been working on for a number of years, and in supporting URLSession, we've learned a lot, and we've taken a lot of those lessons to the IETF. A number of our engineers regularly participate in the IETF and meet with engineers from other companies, and they've been discussing a lot of what we've learned in the transport services working group. And in those discussions, we've got some great feedback, and we've brought that back in and improved Network.framework based on that. We are really excited to announce this year that your applications can take advantage of this same library directly now. Now we know that one of the things people love about sockets is that it gives them very fine-grain control over just about everything, and they're really loathe to give that up. So as we developed Network.framework, we wanted to make sure that it did the right thing by default in the way that sockets doesn't, but it gave you all the knobs that sockets does. And it's kind of got this gradient, so the more knobs you turn, the more complex it becomes. It gives you all the power you need, but you don't have to pay for the complexity unless you actually need some of it. Network.framework has incredibly smart connection establishment. It handles the dual stack cases. It handles IPv6 only networks. It handles PAC. It handles proxies. It will help you connect on networks that are otherwise very difficult to deal with. It has an incredibly optimized data transfer path that lets you go way beyond the performance of anything you can do with sockets, and Tommy will cover that in a little bit. It has support for built-in security. It supports TLS and DTLS by default. It's really simple to use. It has great support for mobility. It provides notifications about network changes that are relevant to the connections that your application is establishing. It's available on iOS, macOS, and tvOS as a CAPI with automatic reference counting, so it's easy to use from Objective C, and it has an incredible Swift API. With that, I'd like to turn it over to Tommy Pauly, to walk you through making your first connection. Thank you. All right, hello everyone. My name is Tommy Pauly, and I'm on the networking team here at Apple. And so I'm sure a lot of you are really excited to start seeing how you can start adopting Network.framework in your apps. And the best place to start and just dive right in is by making your first connection. And you're going to be making your connection from your local device, to your server, or to some other peer device that's on your local network. But you may be wondering, what kind of connections are appropriate to use with Network.framework. What are the use cases? And so let's explore first some scenarios of apps that may be using sockets today and would really benefit a lot by taking advantage of Network.framework going forward. So the first one of these I want to highlight is gaming apps. Gaming apps often use UDP to send real-time data about the game state between one device and another. And they really care about optimizing for latency and making sure there's no lag or anything being dropped there. If you have an app like this, you're going to love how Network.framework allows you to really optimize your UDP, sending and receiving to be faster than ever before with the least latency possible. Another type of app that would take a lot of advantage from Network.framework is live-streaming apps. So live streaming often will use a combination of UDP and TCP in their apps, but the key point here is that it's generating data on the fly. If you have new video frames or audio frames, you need to make sure that those are paced well and you're not incurring a lot of buffering on the device or on the network. The asynchronous model for reading and writing in Network.framework is going to be perfect for making sure you reduce that buffering. And the last case I want to highlight are mail and messaging apps. So these are going to be using a lot more traditional protocols, just TLS over TCP. However, it's really critical for apps like this to handle network transitions really gracefully. Oftentimes if you have a messaging app, your user is going to be using your app as they're walking out of the building, texting their friend to let them know that they're on their way. And you want to make sure that you're handling that transition from the WiFi network in the building to the cell network that they're going onto and that you don't hand a long time for that message to actually get to their friend. And these are just three examples of the types of apps that may use low-level networking like this. There are many other types of apps that could take advantage of this, so if you have an app like one of these or some other use case that currently uses sockets, I invite you to follow along and see how your app can benefit. So to get started, I want to focus on that last case, the simplest case of mail and messaging apps and looking at how they establish connections. So when you want to establish your connection to a server, let's say it's for a mail connection, iMap with security, with TLS, you start with your hostname, mail.example.com. You have a port you want to connect to, port 993, and you want to be using TLS as well as TCP. So how would this look in sockets traditionally? Something like this to get started. You would take your host name. You would call some DNS API to resolve that host name. Let's say this is getaddrinfo. You'll get back one or more addresses. You'll have to decide which one you want to connect to first. You'll call socket with the appropriate address family. You will set a series of socket options. Let's say you want to make your socket nonblocking like Josh mentioned before. Then you call connect to start TCP, and then you wait for a writable event. And this is before you do anything with TLS, and that's a whole host of other problems. So how does this look in Network.framework? And we hope that it looks very familiar to you but a little bit simpler. So the first thing you do is you create a connect a connection object. And a connection object is based on two things. You have an endpoint, which defines the destination you want to get to, and this could be the address, the IP address that you had before, but usually, like in this example, we have a host name and a port, and so our end point can just be that host name and port. It could also be a bonjour service that I want to connect to. And then I also have parameters. Parameters define what protocols I want to use, TLS, DTLS, UDP, TCP. It defines the protocol options that I want as well as which paths I want to use to connect over. Do I want to just connect over anything, or do I only want to use WiFi? Once you've configured your connection, you simply call start to get things going, and then you wait for the connection to move into the ready state. And that's all you need to do to bring up a full TLS connection to your server. And I think you're going to love how this looks in Swift. So here's what you do. You first import the network module. Then, you create an NWConnection object. So an NWConnection in either Swift or in C is the fundamental object for reading and writing data. In this case, we have a convenience that initializes your endpoint with a host in the port, so I give it my hostname, male.example.com, and the port. And in this case, it's a well-known port. It's imaps. So I can just put that in Swift very easy, but I could also put any other numeric literal there. And then to define what protocols I want to use, I pass parameters, and since this is a client connection, I only want default, TLS, and TCP parameters. It can be as simple as just writing dot TLS, and now I have a fully-fledged TLS connection. The next thing I do is I said estate update handler to handle all the transitions that my connection might go through. The first state and the most important one that you want to handle is the ready state. Ready means that your app is ready to read and write data on this connection, it's totally established, and if you're using TCP and TLS, this means that the TLS handshake is finished. We also though let you know about the waiting state. So last year in URLSession, we introduced waits for connectivity, and the waiting state of an NWConnection is exactly the same thing. And this is on always by default. So when you create your connection and when you start it, if there is no network available, we won't fail, we'll just tell you we're waiting for a network to be available. We'll give you a helpful reason code, but you don't have to do anything more to watch network transitions yourself. Mobility is an essential, critical part of this API. And we'll also let you know if there's a fatal error. Let's say we had to reset from the server or TLS failed, and we'll give you that as a failed event. So once you've set this up, you simply call start and provide the dispatch queue upon which you want to receive callbacks. So I want to dig into what happens when you call start. What's actually going on? So here's a little state machine, the innards of the NWConnection. When we begin at the setup state and we call start, we move into the preparing state. So the preparing state does a lot more than just calling connect on a TCP socket. For TCP socket, that would just send out a SYN packet across to the server that you're trying to reach. But when you call start on an NWConnection, it actually handles all of the things that Josh was mentioning earlier. It evaluates the network you're on and tries to make the fastest connection possible for you. I want to dig into that a little bit more. So this is what we call Smart Connection Establishment. So the very first thing that we do when you call start is that we take your endpoint, and then we evaluate what are all the networks that are currently available to me. In this case, we have WiFi and cellular. And generally we prefer the WiFi network because it has less cost to the user. So we'll look at that one first. Then we check, are there any special configurations on this network. Is there a VPN? Is there a proxy? And we'll evaluate that for you. In this case, let's say that there is a proxy configured with an automatic configuration file that also lets you go direct if the proxy doesn't apply to your connection. So we'll evaluate both of those options. We'll check if we need to use the proxy, go ahead and connect to it, create a TCP connection there. But if we don't need it, we'll do DNS on your behalf going directly, get back all of the DNS IP address answers, and connect to them one after the other, leaving them going in parallel. We're racing them to get you the fastest connection possible. And then, if something goes wrong with WiFi, let's say the WiFi radio quality goes really bad because you're walking away from the building, we can actually take advantage of the feature called WiFi assist and fall back seamlessly to the cellular network, do DNS resolution there, and try the connections one after the other. So this way your connection establishment is very resilient, handles VPNs, handles proxies for you, and gets you the best connection possible. Now, of course, you may not want to try all of these options. You may want to restrict what the connection establishment does, and so we have many different knobs and controls to let you do that, and I want to highlight just three of them today. The first is you may not want to use expensive networks, like a cellular network, because this connection is only appropriate to use over WiFi. So within the parameters for your connection, there are options to control the interfaces that you use. So if you don't want to use cellular, simply add cellular to the list of prohibited interface types. It's even better, actually to also prohibit expensive networks in general because this will also block the usage of personal hotspots on a Mac let's say. Another way that you can restrict your connection establishment is by choosing specifically the IP address family that you want to use. So let's say that you really love IPv6 because it's faster and it's the future. You don't want to use IPv4 at all on your connection. And you can do this by going to your parameters, digging down into the IP-specific options, and here you'll have options that you'll find familiar from your socket options on a socket today, and you can also define specifically which IP version you want to use. And this will impact your connection as well as your DNS resolution. And lastly, you may not want to use a proxy on your given connection. Maybe it's not appropriate for your connection to go through a SOCKS proxy. In that case, you can simply prohibit the use of proxies as well. So that's what happens in the preparing state. I mentioned before that things can go wrong. You can have no network when you try to establish, and what we'll do after going to preparing is if we find there are no good options, DNS failed, there was no network, maybe you're in airplane mode, we'll move into the waiting state and let you know the reason for that. And we'll keep going back into preparing every time the network changes and the system thinks, yeah, there's a good chance that your connection will become established now, and we'll handle all that for you and let you know every time that we're reattempting. Eventually, hopefully your connection does get established. At this point, we'll move into the ready state. And the ready state, as I mentioned before, is when your connection is fully established. So this is all of the protocols in your stack up to TLS, for example. At this point, you can read and write, and this is also where we give you callbacks about the network transitions that you're going through. So if your connection is established and then you change networks, we'll give you updates about that so you can handle the mobility gracefully, and we'll be talking about this later on in the talk. If there's an error on the connection, either during the connection establishment or after you've already connected, we'll give you the failed state with an error, and then once you're totally done with the connection, let's say you already closed it or you received a close from the other side, and you want to just invalidate the connection, you call cancel, and we move into the cancelled state. And this is guaranteed to be the very last event that we will deliver to your object so you can clean up any memory that you have associated and move on. So that's it. That's an overview of the basic lifetime of a connection object in Network.framework, and to show you how you can use this to build a simple app, I'd like to invite Eric up to the stage. Thanks Tommy. I'm Eric Kinnear, also from the networking team here at Apple, and I'm really excited to build with you an example application using Network.framework. We're going to use the live streaming example that Tommy mentioned earlier to build an application that can take the camera input on one device and send it over a network to be displayed on another device. Because we're going to be continuously generating live video frames, we're going to use UDP to send those packets across the network. So how do we do this? Well, first, we need a capture session with the camera so that we can receive the video frames from the image sensor. For the sake of this example, we're not going to use any video codecs or other compression. We're simply going to take the raw bytes from the camera, ship them across the network, and display them on the other side. In order to make this happen, we need to divvy those frames up into smaller chunks that we can send in UDP packets. Of course, to send those UDP packets on the network, we need a connection. Switching over to the other device, we need a listener that can receive that incoming connection and read the data packets off the network. From there, we simply reverse the earlier process, reassembling the video frames and sending them to the display so that we can see them on the screen. To keep things simple, we've already abstracted out the camera and the display functionalities so that we can focus just on the parts that use Network.framework. There's one piece we haven't yet covered here, and that's the listener. So we're going to take a minute to do that now. Listener functionality is provided by the NWListener class, which you can create using the same parameters objects that you used to configure connections. It's really easy to set up a listener to advertise a bonjour service. In this case, we'll use camera.udp. When a new connection is received by a listener, it will pass that connection to a block that you provide as the newConnectionHandler. This is your opportunity to perform any configuration that you choose on that connection, and then you need to call start to let that connection know that it's time to get going. Similarly, you need to call start on your listener, and again, just like with connections, you provide a dispatch queue where you want these callbacks to be scheduled. So that's listeners. If you think about it, we just implemented the equivalent of calling listen on a UDP socket. Except listen doesn't actually work on UDP sockets. Now we're ready to build our app in Xcode. So here we've got our application, and we've got a bunch of files over here that already handle the camera and the display functionality, so we're going to focus just on the UDPClient class and the UDPServer class. The UDPClient is going to be responsible for creating the connection to the other side and sending the frames across. Likewise, the server is responsible for creating the listener, accepting incoming connections, reading the data off those connections, and sending them up to the screen. Let's start with the client. My client class has an initializer that takes a name, which is a string describing the bonjour name that we want to connect to. I'll create my connection by simply calling NWConnection and passing in a service endpoint. Using the name that I was provided and camera.udp as the type. We also passed the default UDP parameters. As Tommy mentioned, we can use a state update handler to check for the ready and the failed states. Here, when our connection is ready, we'll call sendInitialFrame, which we'll implement in a minute. Because we're using UDP and there's no other handshake, we're going to take some data and send it across to the network to the other device and wait for it to be echoed back before we start generating lots of video frames and dumping them on the network. We need to remember to call start on our connection, and we provide the queue that we created up above. Let's implement send initial frame. Here we're going to take the literal bytes hello and create a data object using them. To send content on a connection, we can call connection.send and provide that data object as the content. We provide a completion handler in which we can check for any errors that may have been encountered while sending. Since we expect this content to be immediately echoed back, we turn right around and call connection.receive to read the incoming data off of the connection. In that completion handler, we validate that the content is present, and if so, we let the rest of the application know that we're connected, and it should bring up the camera hardware and start generating frames. When those frames are generated, the rest of the application knows to call send on our UDPClient class and pass it an array of data objects representing the video frames that we're trying to send. Because we're going to be doing a lot of send operations in very quick succession, we're going to do them within a block that we passed in connection.batch. Within this block we're going to go through every frame in that array of data objects and pass each one to connection.send. Similarly to above, we use the completion handler to check for any errors that were encountered while sending. And that's it. We've got our UDPClient class, and we're ready to go. Let's look at the server. On the server side, we need a listener that can accept the incoming connections. We need to respond to that handshake that we just sent from the client, and we need to read data off the network so that we can push it up to the display. Starting with the listener, we simply create an NWListener using the default UDP parameters. If I wanted to, this is also my opportunity to use those parameters to tell the listener to listen on a specific local port. But since we're using a bonjour service, we don't need to do that. To set up that service, I'm going to set the service property on the listener to a service object of type camera.udp. Notice that I don't pass the name here because I want the system to provide the default device name for me. I also provide a block to the serviceRegistration UpdateHandler, which is going to be called anytime the set of endpoints advertised by the system changes. Here, I'm interested in the case where an endpoint is added, and if it's of service type, I tell the rest of the application the name that is being advertised, that default device name that I ask the system to provide so that I can display it in the UI and have my users type it in somewhere else. I'm going to set a new connection handler on the listener, which will be called every time the listener receives a new incoming connection. I could do some configuration on these connections, but the default settings are fine here, so I simply call connection.start and pass it to queue. Here, I notify the rest of the application that I've received an incoming connection, so it can start warming up the display pipeline and become ready to display the video frames. I'll also call receive on myself, which we'll implement in a minute, to start reading that data off of the network and shipping it up to the display pipeline. Just like with connections, listeners have state update handlers, which I'll use to check for the ready and the failed states. I need to not forget to start my listener, which I do by calling listener.start and passing it that queue we created up above. So I've got my listener ready, I just need to read data off the network and implement this receive function. Here, we start by calling connection.receive and passing it into completion handler. When data comes in off of that connection, we'll see if we're not yet connected. If we weren't connected, this is probably that handshake that the client is starting by sending. We'll simply turn right around and call connection.send, passing that same content back so it will be echoed over to the client. We then remember that we're connected, and on all subsequent received callbacks, we will simply tell the rest of the application that we received this frame, and it should send it up to the display pipeline so that we can see it on the screen. Finally, if there were no errors, we call receive again so that we receive subsequent frames and send them up to the display to book together a video from each of these individual images. So that's it. We've got our UDPClient, we've got our UDPServer, let's try it out. I'm going to run the client on my phone here, and I'm going to run the server on my Mac so we can see it on the big screen. Here, the server just came up, and we see that it's advertising as Demo Mac, which is where I told the rest of the system to just give me the name. That's on my phone. If I tap connect, all of a sudden, I can see video frames being streamed across the network over UDP Live. So here we just saw how quickly I was able to bring up a UDPClient that could connect to a bonjour service, it can send a handshake, wait for that to be processed, take the video frames coming from the camera, and ship them across the network. The server side brought up a bonjour listener. It advertised a service, it received incoming connections, responded to the handshake, and sent them all to the display so that we could see them. And now to take you through optimizing that data transfer in a little more detail, I'd like to invite Tommy back up to the stage. Thank you, Eric. It was a really cool demo. It's super easy to get this going, and so now we've covered the basics, and we know how to establish connections outbound, how to receive connections inbound, but the real key part of Network.framework that's going to be the killer feature here is the way that it can optimize your performance and that we can go beyond what sockets was able to do. And I want to start with the way that you in your application interact with your networking connections in the most basic way, which is just sending and receiving data. And these calls are very simple, but the nuances about how you handle sending and receiving and really make a huge difference for the responsiveness of your app and how much buffering there is going on on the device and the network. So the first example I want to walk through is when we're sending data, in the application very much like what Eric just showed you, something that's live streaming, something that is generating data on the fly. But in this case, let's talk about when we're sending it over a TCP stream, a TCP stream that can back up on the network, that has a certain window that it can send. So how do we handle this? So here's a function to send a single frame. This is some frame of data that your application has generated. And the way that you send it on the connection is you simply call connection.send and pass that data. Now if you're used to using sockets to send on your connections, you either are using a blocking socket, in which case if you have a hundred bytes of data to send, if there's not room in the send buffer, it'll actually block up your thread and wait for the network connection to drain out, or if you're using a nonblocking socket, that send may actually not send your complete data. It'll say, oh, I only sent 50 bytes of it. Come back some other time to send the next 50 bytes. This requires you and your application to handle a lot of state about how much you have actually done of sending your data. So the great thing about a network connection is you can simply send all of your data at once, and you don't have to worry about this, and it doesn't block anything. But then, of course, you have to handle what happens if the connection is backing up, because we don't want to just send a ton of data unnecessarily into this connection if you want a really responsive, live stream of data. And the key here is that callback block that we give you. It's called contentProcessed. And we'll invoke it whenever the network stack consumes your data. So this doesn't mean that the data has necessarily been sent out or acknowledged by the other side. It's exactly equivalent to the time in which a blocking socket call would return to you, or when the nonblocking socket call was able to consume all of the bytes that you sent. And in this completion handler, there are two things you can check for. First, you can check for an error. If there is an error, that means something went wrong while we were trying to send your data, generally it indicates a overall connection failure. Then, if there wasn't and error, this is the perfect opportunity to go and see if there's more data from your application to generate. So if you're generating live data frames, go and fetch another frame from the video stream, because now is the time when you can actually enqueue the next packets. This allows you to pace all of your data out. And so as you see here, we essentially form a loop of using this asynchronous send callback to continue to drain data out of our connection and handle it really elegantly. The other thing I want to point out about sending is the trick that Eric showed earlier that's great for UDP applications that are sending multiple datagrams all at one time. So if you have a whole bunch of little tiny pieces of data that you need to send out or essentially individual packets, you can use something that we've added called connection.batch. So a UDP socket previously could only send one packet at a time, and this could be very inefficient because if I have to send a hundred UDP packets, these are each a different system call, a different copy, and a context switch down into the kernel. But if you call batch within that block, you can call send or actually receive as many times as you want, and the connection will hold off processing any of the data until you finish the batch block and will try to send all of those datagrams all as one single batch down into the system, ideally have just one context switch down into the kernel, and send out the interface. This allows you to be very, very efficient. So that's sending. Receiving, like sending, is asynchronous, and the asynchronous nature gives you the back pressure that allows you to pace your app. So in this case, I have a TCP-based protocol, and it's very common for apps when they're reading to essentially want to be reading some type of record format. Let's say that your protocol has a header of 10 bytes that tells you some information about what you're about to receive, maybe the length of the body that you're about to receive. And so you want to read that header first and then read the rest of your content, and maybe your content's quite long. It's a couple megabytes let's say. Traditionally with a socket, you may try to read 10 bytes. You may get 10 bytes, you may get less. You have to keep reading until you get exactly 10 bytes to read your header. And then you have to read a couple megabytes, and you'll read some, and you'll get a whole bunch of different read calls and essentially go back and forth between your app and the stack. With an NWConnection, when you call receive, you provide the minimum data that you want to receive and the maximum data. So you could actually specify if you want to receive exactly 10 bytes because that's your protocol, you can just say, I want a minimum of 10 and a maximum of 10. Give me exactly 10 bytes. And we will only call you back when either there was an error in reading on the connection overall or we read exactly those 10 bytes. Then you can easily just read out whatever content you need for your header, read out the length, and then let's say you want to read a couple megabytes, and you essentially do the same thing to read your body, and you just pass, well I want to read exactly this amount for my connection, and this allows you to not go back and forth between the stack and your app but just have a single callback of when all of your data is ready to go. So it's a really great way to optimize the interactions. Beyond sending and receiving, there are a couple of advanced options that I'd like to highlight in your network parameters that allow you to configure your connection to get very good startup time as well as behavior on the network when you're actually sending and receiving. So the first one is one that we've talked about many times here at WWDC, which is ECN. This is explicit congestion notification. It gives you a way to smooth out your connection by having the network let the end host know when there's congestion on the network so we can pace things out very well. The great thing is that ECN is enabled by default on all of your TCP connections. You don't have to do anything. But it's been very difficult in the past to use ECN with UDP-based protocols. And so I'd like to show you how you can do that here. The first thing you do is that you create an ipMetadata object. ECN is controlled by flags that go in an IP packet, and so you have this ipMmetadata object that allows you to set various flags on a per-packet basis, and you can wrap this up into a context object, which describes all of the options for the various protocols that you want to associate with a single send as well as the relative priority of that particular message. And then you use this context as an extra parameter into the send call besides just your content. So now when you send this, any packet that's going to be generated by this content will have all the flags that you wanted marked. So it's really easy. And you can also get these same flags whenever you're receiving on a connection. You'll have the same context object associated with your receives, and you'll be able to read out the specific low-level flags that you want to get out. Similar, we have service class. This is a property that is available also in URLSession that defines the relative priority of your traffic, and this affects the way that traffic is queued on the local interfaces when we're sending as well as how the traffic works on Cisco Fastlane networks. So you can mark your service class as a property on your entire connection by using the service class parameter in your parameter's object. In this case, we show how to use the background service class, and this is a great way to mark that your connection is relatively low priority. We don't want it to get in the way of user interactive data. So we really encourage you if you have background transfers, mark them as a background service class. But you can also mark service class on a per packet basis for those UDP connections. Let's say that you have a connection in which you have both voice and signaling data on the same UDP flow. In this case, you can create that same IP metadata object that we introduced before, mark your service class now instead of the ECN flags, attach it to a context, and send it out. And now you're marking the priority on a per-packet basis. The other way that you can optimize your connections is to reduce the number of round trips that it takes to establish them. So here I want to highlight two approaches to do this. One is enabling fast open on your connections. So TCP fast open allows you to send initial data in the first packet that TCP sends out, in the SYN, so that you don't have to wait for a whole handshake to start sending your application data. Now in order to do this, you need to enter into a contract from your application with the connection saying that you will be providing this initial data to send out. So to enable this, you mark allow fast open on your parameters. You then create your connection, and then before you can call start, you can actually call send and get your initial data sent out. Now I want to point out here that the completion handler here is replaced by a marker that this data is item potent, and item potent means that the data is safe to be replayed because initial data may get resent over the network, and so you don't want it to have side effects if it gets resent. Then, you simply call start, and as we're doing the connection bring up, all the attempts that we mentioned before, we will use that initial data if we can to send in TCP Fast Open. There is one other way I want to point out to use TCP Fast Open that doesn't require your application to send it's own data. If you're using TLS on top of TCP, the first message from TLS, the client hello, can actually be used as the TCP Fast Open initial data. If you want to just enable this and not provide your own Fast Open data, simply go into the TCP-specific options and mark that you want to enable Fast Open there, and it will automatically grab the first message from TLS to send out during connection establishment. There's another thing that you can do to optimize your connection establishment and save a roundtrip, and this is something that Stuart mentioned in the previous session, which we're calling Optimistic DNS. This allows you to use previously expired DNS answers that may have had a very short time to live, and try connecting to them while we do a new DNS query in parallel. So if the addresses that you had previously received that did expire are still valid, and you mark the expired DNS behavior to be allow. When you call start, we'll try connecting to those addresses first and not have to wait for the new DNS query to finish. This can shave off a lot of set-up time from your connection, but if your server has indeed moved addresses, because we're trying multiple different connection options, if that first one doesn't work, we will gracefully wait for the new DNS query to come back and try those addresses as well. So this is a very simple way that if it's appropriate for your server configuration, you can get a much faster connection establishment. The next area I want to talk about for performance is something that you don't actually need to do anything in your app to get. This is something that you get totally for free whenever you use URLSession or Network.framework connections, and this is user-space networking. So this is something that we introduced last year here at WWDC, and it's enabled on iOS and tvOS. This is where we're avoiding the socket layer entirely, and we've moved the transport stack into your app. So to give you an idea of what this is doing, I want to start with what the legacy model of the stack generally is. So let's say that you're receiving a packet off the network. It's the WiFi interface. That packet will come into the driver, will be sent into the TCP receive buffer within the kernel, and then when your application reads on a socket, that's going to do a context switch and copy the data up from the kernel into your app, and then generally if you're doing TLS, it will have to get another transformation to decrypt that data before you can actually send it up to the application. So how does this look when we do user space networking? So as you can see, the main change is that we've moved the transport stack, TCP and UDP, up into your app. So what does this give us? Now, when a packet comes in off the network, comes into the driver like before, but we move it into a memory mapped region that your application automatically can scoop packets out of without doing a copy, without doing extra contexts switch, and start processing the packets automatically. This way the only transformation we're doing is the decryption that we have to do anyway for TLS. This really can reduce the amount of CPU time that it takes to send and receive packets, especially for protocols like UDP in which you're going to be sending a lot of packets back and forth directly from your application. So to show how this works and the effect it can have, I want to show you a video that was taken using the same app that Eric showed you earlier to demonstrate UDP performance with user space networking. So in this example, we're going to have two videos running simultaneously. The device on the left is receiving a video stream from an application that was written using sockets. And the device on the right is going to be receiving exactly the same video stream from a device that has an app written using Network.framework so it can take advantage of the user space networking stack. And in this case, we're streaming the video. It's just raw frames. It's not compressed. It's not great quality or anything, but there's a lot of packets going back and forth. And we chose specifically for this demonstration to not lower the quality when we hit contention or when we couldn't send packets fast enough or to not drop anything but just to slow down if we had to. Now this is probably not what your app would do in real life, but it highlights exactly the difference in the performance between these two stacks. So let's see it right now. So there's exactly the same data, exactly the same frames being sent over as fast as they possibly can, across this network, and we see the one on the right is pretty easily outstripping the one on the left. And in fact, if you look at the difference, it's 30 percent less overhead that we're viewing on the receiver side only. And this is due to the huge difference that we see in the CPU percentage that it takes to send and receive UDP packets when you compare sockets and user space networking. Now, of course this is just one example. This is not going to be what every app is going to be like, because you're going to be compressing differently. You're going to be already trying to make your connections more efficient. But if you have an app that's generating live data, especially if you're using UDP to send and receive a lot of packets, I invite you to try using Network.framework within your app and run it through instruments. Measure the difference in CPU usage that you have when you're using Network.framework versus sockets, and I think you'll be really happy with what you see. So the last topic we want to talk about today is how we can solve the problems around network mobility, and this is a key area that we're trying to solve with Network.framework. And the first step of this is just making sure that we start connections gracefully. So we already mentioned this, but I want to recap a little bit. The waiting state is the key thing to handle network transitions when your connection is first coming up. It will indicate that there's a lack of connectivity or the connectivity changed while you were in the middle of doing DNS or TCP. We really encourage you please avoid using APIs like reachability to check the network state before you establish your connection. That will lead to race conditions and may not provide an accurate picture of what's actually happening in the connection. And if you need to make sure that your connection does not establish over a cellular network, don't check up front is the device currently on a cellular network, because that could change. Simply restrict the interface types that you want to use using the NWParameters. So once you've started your connection and you're in the ready state, there are a series of events that we will give you to let you know when the network has changed. The first one is called connection viability. So viability means that your connection is able to send and receive data out the interface it has a valid route. So to give a demonstration of this, let's say that you started your connection when the device was associated with a WiFi network. Then, your user walks into the elevator, they don't have a signal anymore. At this point, we will give you an event, letting you know that your connection is no longer viable. So what should you do at this point? Two things. We recommend that if it's appropriate for your app, you can let the user know that they currently have no connectivity. If they're trying to send and receive data, it's not going to work right now. But, don't necessarily tear down your connection. At this point, you don't have any better interface that you could use anyway, and that first WiFi interface may come back. Oftentimes, if you walk back out of an elevator onto the same WiFi network, your connection can resume right where you left off. So the other event that we give you is the better path notification. So let's take that same scenario in which you connected over the WiFi network. You walk out of a building let's say, and now you no longer have WiFi, but you do have the cellular network available to you. At this point, we'll let you know two things. First, that your connection is not viable like before, but we'll also let you know that there is now a better path available. If you connected again, you would be able to use the cellular network. And the advice here is to, if it's appropriate for your connection, attempt to migrate to a new connection, if you can resume the work that you were doing before. But only close the original connection once that new connection is fully ready. Again, the WiFi network may come back, or the connection over cellular may fail. And the last case I want to highlight here is a case in which you connect initially over the cellular network, and then the user walks into a building and now they have WiFi access. In this case, your connection, the original one, is totally fine. You're still viable, but you now also have a better path available. In this case, again, if you can migrate your connection, this is probably a good time to try to establish a new connection and move your data over. That will save the user their data bill. But, continue to use the original connection until you have the new one fully established. Just to show how this looks in code, we have the viability update handler that you can set in your connection, we'll give you a boolean back to let you know whenever you're viable or not, and a better path update handler to let you know when there's a better path available or is no longer available. And now the better solution to all of this to handle network mobility is something that we've talked about in previous years, which is multipath connections, Multipath TCP. So if you were able to on your server enable Multipath TCP and you can enable it on the client side with the multipathServiceType in your parameters, then your connection will automatically migrate between networks as they come and go. It's a great seamless experience that doesn't require any work in your application to handle. And this is also the same service type that's available in URLSession. A couple points I want to highlight here, specific to Network.framework. If you restrict the interface types that you allow to be used with your NWParameters that will apply to MPTCP, so you can still not want to use the cellular network with a multipath connection, and instead we'll just seamlessly migrate between different WiFi networks as they become available. Also, the connection viability handler that I mentioned before is slightly different from Multipath TCP because whenever we change a network, we'll automatically move for you, your connection is only not viable when you have no network available to you at all. So between waiting for connectivity, viability, better path, MPTCP, we really hope that all of the use cases in your apps for using tools like SC Network Reachability to check network changes manually have been replaced. However, we do recognize that there are some scenarios in which you still want to know what's the available network, when does it change. For that Network.framework offers a new API called the NWPathMonitor. So the Path Monitor instead of watching the reachability and trying to predict the reachability of a given host simply lets you know what is the current state of the interfaces on your device and when do they change. It allows you to iterate all of the interfaces that you can connect over in case you want to make a connection over each one, and it will let you know whenever those networks do change. So this can be very useful if you want to update your UI to let the user know, are they connected at all. And as Stuart mentioned in the previous session, there could be scenarios in which the user has a long form to fill out and they don't necessarily want to go and fill out something just to realize that there's no connectivity anyway. So use Network Path Monitor in any of these scenarios in which just having a waiting connection is not enough. So between all of these things, we'd really like to see people move off of reachability and handle network transitions more gracefully than ever before. So with that, I'd like to invite Josh back up to the state to let you know how you can get involved and start adopting Network.framework. Thank you, Tommy. So I've got a great new API for you that we think you're going to love. We'd like to talk about the things that you can do to start using it today, but first I want to talk about a few things that we'd like you to stop doing so we can really take advantage of the new technologies like user space networking. If you're on macOS, and you have a Network Kernel Extension, and there's something you're doing in that Network Kernel Extension that you can't do any other way, please get in touch with us right away. We need to provide you a better alternative because Network Kernel Extensions are not compatible with User Space Networking. We wanted to give you a heads-up that with URLSession, FTP and file URLs are no longer going to be supported for Proxy Automatic Configuration. Going forward, the only supported URL schemes will be HTTP and HTTPS. There are a number of APIs at the CoreFoundation layer that we would like you to stop using. They will be deprecated eventually. They are not yet marked as deprecated. These are CFStreamCreatePairWith, anything related to sockets as well as CFSocket. These cannot take advantage of a lot of the connection establishment that we've put in there with the new Network.framework, and they can't take advantage of the new User Space Networking. So we really want you to move off of these to take advantage of the incredibly robust connectivity improvements that you'll get with Network.framework and URLSession and the improved performance. There are some foundation APIs as well that we'd like you to move away from. If you're using any of these NSStream, NSNetService, or NSSocket for APIs, please move to Network.framework or URLSession. Finally, if you're using SCNetworkReachability, we feel that the Wait for Connectivity model is a much better model, so we'd really like you to move to that. And for those few cases where Wait for Connectivity isn't the right answer, NWPathMonitor is a much better solution going forward. So now that we've talked about some things we'd like you to stop doing, I want to focus on the things we really want to see you do. Going forward, the preferred APIs on our platforms for networking are URLSession and Network.framework. URLSession is really focused on HTTP, but Stream Task provides pretty simple access to TCP and TLS connections. If you need something more advanced, Network.framework gives you great support for TCP, TLS, UDP, DTLS. It handles listening for inbound connections as well as outbound connections, and we've got Path Monitor to handle some of the mobility stuff. Next steps, we really want to see you adopt these, adopt Network.framework and URLSession. Your customers are going to appreciate how much better your connections, how much more reliable your connections are established, and they'll appreciate the longer battery life from the better performance. While you're working on these, focus on how you're handling your sending and receiving to really optimize that performance. And take a lot of time to get that support for the viability and better route changes in there. It can make all the difference for providing a seamless networking experience. Now we know that Network.framework doesn't support UDP Multicast yet, so if you're doing UDP Multicast, we'd really like to understand your use cases so we can take those into account going forward. In addition, if you have any other questions or enhancement requests, we'd love to hear from you. Contact developer support or better yet, meet us in one of the labs. We have a lab after lunch at 2 p.m. and another one tomorrow morning at 9 a.m. For more information, see this URL. Don't forget the labs tomorrow morning and after lunch. Thank you so much and have a great WWDC.  Hello, and welcome to the session on implementing AutoFill Credential Provider Extensions. In this video, I'll first give an overview of the Password AutoFill feature and how it is improved in iOS 12. After that I'll go into detail about how Password Manager apps can now integrate with Password AutoFill using new APIs in iOS 12. And along the way, I will recommend a few best practices to take as you adopt these new APIs. First, let's talk about Password AutoFill. iOS 11 brought two major improvements to Password AutoFill. First, the most relevant credentials were displayed directly on the QuickType bar, so they're only one tap away, and second, iOS 11 brought password AutoFill to apps. This makes it super convenient to use credentials from iCloud keychain, whether they're needed on the web or in apps, like the shiny app you just saw. And new in iOS and tvOS 12, you can also use password AutoFill in Apple TV apps by selecting credentials to fill from an iOS device. These features are great for users of iCloud keychain, but some users rely on other password manager apps to store their credentials. To make it just as convenient for these users to access their safe credentials, iOS 12 allows password manager apps to participate in AutoFill for the same experience as iCloud Keychain. In iOS 12, there is a new UI for password AutoFill settings, which allows users to select an app to provide credentials to AutoFill, in addition to or instead of iCloud Keychain. Using the QuickType bar, the user can bring up a list of their credentials saved in the password manager. This UI is provided by an extension bundled with the password manager app. When a credential is selected, the extension hands it back to AutoFill, and the username and password are filled in the app. Of course, this also works with the QuickType bar suggestions as well. AutoFill can now surface the best credentials as defined by the app, so they're accessible with just one tap. When using these credentials, the app extension can optionally show its own UI to authenticate the user before filling the credential. This integration makes logging into apps even easier for users of password manager apps, as they no longer need to switch apps to copy their credentials. It also makes Password AutoFill available in more apps. Any app that supports AutoFill from iCloud Keychain will now work with Password Manager apps, without any additional work. With that overview, let's dive into how you can implement these capabilities in your password manager app. There are four main steps I'll cover. First, you'll need to configure your project to take advantage of some new APIs. This involves adding a capability to your app and an extension to your project. AutoFill will use this extension when it needs to consult your app or show its UI across the system. Next, your extension will need to support showing the user a list of their credentials to choose from when they open your extension from the QuickType bar. After that, if you want AutoFill to show your app's credentials in the QuickType bar, you will need to add support for this by telling the system about the credentials you want to show, and implementing another API in your extension to respond to users selecting those credentials. And finally, you may want to take advantage of an API that will allow you to present your extension's UI when users enable your Password Manager in Settings. Let's talk about these steps in more detail. First, you will need to make a few changes to your project. This starts with enabling AutoFill Credential Provider in your app's capabilities. This adds a required entitlement to your app, and links it to the new authentication services framework, which provides the APIs for Password Autofill integration. Next, you will need to add an AutoFill Credential Provider Extension target to your project. Xcode 10 includes a new template for this extension. The template will create a view controller class for you. A subclass of AS credential provider view controller. When AutoFill needs to invoke your extension, it will create an instance of this class and call certain methods on it, which your subclass will override. So once you've configured your project, the first thing to implement in your extension is the list of credentials that the user can bring up from the QuickType bar. Here is how this works. When the user is signing into an app, they can use the QuickType bar to bring up your credential list. At this point, AutoFill will launch your app extension and let it know where the user is logging in, so you can suggest the most relevant credentials. AutoFill will do this by preparing a list of AS credentials service identifier objects, representing the service the user is currently using. Your extension may receive multiple service identifiers, if AutoFill can determine multiple better [inaudible] to use in the current context. In apps, service identifiers are based on the app's associated domains. Apps that have adopted universal links hand-off or shared web credentials will have associated domains. For example, the Shiny app is associated with shiny.example.com, so AutoFill will provide your extension a service identifier of type domain for shiny.example.com. In Safari, the service identifiers are based on the URL of the current page the user is logging into. AutoFill will send the service identifiers to your extension by calling the prepared credential list for service identifiers method on your view controller. Here, your extension should set up its UI for displaying the user's credentials, and it can use the provided service identifiers to prioritize the most relevant ones. From here, two things can happen. If the user chooses to dismiss your extension, you tell the system about this by calling Cancel Request With Error on your view controller's extension context, and AutoFill will dismiss your extension. Otherwise, if the user selects a credential they want to use, your extension creates an AS password credential object based on the user's selection and then hands it to AutoFill by calling the complete request with selected credential method on the extension context. And AutoFill will use that credential to fill the username and password in the app. There are a few best practices the credential list should adhere to for the best user experience. First of all, be sure to include a button in your UI to cancel the request. The user may change their mind about signing into the service, or realize they don't have a credential saved, so you should support letting the user dismiss your extension without selecting a credential. Also, your credential list UI should make it possible to see all credentials, whether or not they match the service identifiers. In some cases, the user may need to choose a credential from a different domain. Allowing the user to access their entire set of credentials from the list, ensures your extension is always useful. And user authentication is completely up to your extension. If you need to authenticate the user, you should do so when the credential list is presented. And that is how you can implement a credential list in your extension so your app's credentials are available to use when signing in anywhere. Now we will make this even more convenient by allowing AutoFill to surface these credentials directly on the QuickType bar. I'll start with an overview of how this process works, describing the roles played by your code, the system, and the app where the user is signing in. To start with, your app needs to let AutoFill know ahead of time what credentials it wants to make available for the QuickType bar. Your app provides AutoFill a list of credential identities. The credential identity includes information about a credential, such as the username and the service, but not the password. When the user begins signing into an app, the app talks to AutoFill, and lets it know when a username or password field is focused. AutoFill then looks for appropriate credential identities to suggest for the app. It does this by searching through the credential identities that your app has already provided, so your extension doesn't need to be launched yet. If there are any matching credential identities to suggest, AutoFill displays them on the QuickType bar. These suggestions are rendered privately by the system, so the app isn't yet able to determine what credentials the user has saved for the app. When the user selects one of the suggestions, AutoFill launches your app extension to ask it for the full credential, including the password. It will tell your extension which credential identity the user chose, then your extension looks up the password belonging to the selected credential in your app's password database. At this point, the extension has the option to present its own UI before returning the password. This is useful for password manager apps that ask the user to enter a master password, or perform another type of authentication specific to the app. Once your extension has the password, it packages it in an AS password credential and hands it to AutoFill by completing the extension request. If the extension didn't show its own UI, AutoFill will perform appropriate authentication for the user. Depending on the device and the user's preference, this may be Face ID, Touch ID, Device Passcode, or None. If that authentication is successful, AutoFill will fill the username and the password in the app. There is a lot going on here, so I'm going to walk through the steps you need to take as a developer to support this workflow. The three things you need to do are provide AutoFill with the credential identities you want it to suggest to the user. Implement support in your extension to provide the passwords when those suggestions are selected, and display custom UI in your extension to authenticate the user before returning the credential, if your UX requires it. Once again, this step is optional. If you don't show custom UI for authentication, AutoFill will perform appropriate authentication for you. Credential identities are represented by instances of AS Password Credential Identity. This class contains all the information about a credential that AutoFill needs to know in order to determine where to offer it. This includes a service identifier, which tells AutoFill which apps or websites to suggest the credential on. The username of the credential, and optional record identifier string that you can use to correlate this identity to a record in your app's own database, and a rank parameter. If the user has more credentials for a particular service than the QuickType bar can show, you can use the rank parameter to mark certain credential identities as higher or lower priority. Credential identities having a higher rank value will be ordered before credential identities with lower ranks. These credential identities get saved to the Credential Identity Store, which is the database inside your app's container that you can modify using the AS Credential Identity Store class. AutoFill suggests credentials to the user by searching through your app's Credential Identity Store. The Credential Identity Store is secured with complete unless open data protection, so no operations can start while the device is locked. The system never syncs the Credential Identity Store to the Cloud or includes it in backups, so this information never leaves the device. Each app has its own Credential Identity Store, and only the app and its extensions can modify it. The store is only read by AutoFill for determining which credentials to suggest to users. And the Credential Identity Store can only be modified while your app's extension has been enabled by the user. If your extension is disabled, attempts to update the store will fail. And if the user disables your extension or deletes your app, the Credential Identity Store will be deleted. Your app should update its Credential Identity Store when it has new information about what credentials it can offer. As an example, let's say your app uses an online service to store the user's credentials. When the user signs in, your app would start retrieving the user's credentials. At this time, you would update the list of credential identities in the store, so the newly-fetched credentials could be suggested on the QuickType bar. As the user adds, removes, or modifies their credentials, your app updates the Credential Identity Store so it continues to accurately reflect this set of credentials that your app can provide. These updates might be because the user locally makes changes within your app, or perhaps because your app is synchronizing changes from other devices signed into the online service. Then, if the user were to sign out of the online service on the current device, your app would remove all the credential identities from the store, so the user doesn't continue to see suggestions for these credentials. In your code, you use the AS Credential Identity Store Class to interact with the Credential Identity Store. Using the Replace Credential Identities With and Remove All Credential Identities methods, you can replace or clear the list of credential identities that AutoFill will consider suggesting. When individual changes are made, these saved credential identities or removed credential identities methods allow you to add, update or remove credential identities without completely replacing the contents of the store. One important aspect of the system to understand is that the Credential Identity Store may be deleted at times that your app won't be able to predict. As a few examples, if the user disables your extension for AutoFill, then later re-enables it, the system will have cleared the store. If the system determines that your app provides credential identities, but consistently fails to provide the passwords when the user selects these credentials, the credential identity store may be deleted to prevent the user from seeing these stale credential suggestions. If the user restores their device from a backup where they were using your credential provider extension, the store won't contain any credential identities since it wasn't included in the backup. Your app should be able to handle these cases, and AS Credential Identity Store can help you detect these cases, so you can take the appropriate action when you need to update the store. You can use the Get State method to ask the system about the state of your app's Credential Identity Store, return it as an AS Credential Identity Store State Object. The first thing it tells you is whether or not the user has your app extension enabled. You should check this before attempting to update the credential identity store. If your extension is disabled, there is no point in trying to save or remove credential identities. The State also has a Supports Incremental Updates Property, which you can use to determine if the Identity Store is intact since the last time you've updated it. If you previously saved any credential identities to the store, this will return true, indicating you should use the incremental Save Credential Identities and Remove Credential Identities methods. Otherwise, if the Store hasn't been written to yet, perhaps because your app was just disabled and re-enabled, Supports Incremental Updates will return false. And you should populate the Identity Store by providing the full list of credential identities using the Replace Credential Identities With method. Once your app starts saving credential identities to the store, AutoFill can start suggesting your app's credentials in the QuickType bar. Next, you'll need to add support in your extension to provide the password when one of these credential suggestions is selected. When this happens, AutoFill will first launch your extension and ask it for the password without presenting your UI on screen. When it does this, AutoFill will call the Provide Credential Without User Interaction For method on your view controller, providing an AS Password Credential Identity, representing the credential being filled. In this method, you will look up the associated password belonging to the given credential and hand it back to AutoFill using the Complete Request With Selected Credential method. If your extension wants to have its UI presented at this point, cancel the extension request with the User Interaction Required error code in the domain AS Extension Error Domain. The system will then call the Prepare Interface To Provide Credential For method on your view controller, and present its UI. In this method, your extension sets up its UI for its workflow to provide the password. When the password is eventually available, you return the credential to AutoFill, also using the Complete Request With Selected Credential method. Once again, if your Extensions UI was presented, AutoFill won't perform any authentication before filling the returned credential. It is up to your extension to decide what type of authentication is needed. The most important thing to keep in mind when implementing this functionality is that your extension needs to respond to the initial non-UI request quickly, regardless of the results. Your UI hasn't been presented yet, so it's not obvious to the user that your extension is working in the background. If it takes a long time to return the password, the user may perceive the system, your app, or the service they're using as being unresponsive. This would be a poor user experience. And this is so important. If a few seconds pass, and your extension hasn't returned the password, requested to show its UI, or canceled with another error, AutoFill will cancel the extension without filling the credential. However, this timeout doesn't happen for debug builds, or when running on the simulator, so you can take your time to debug the extension without the system interrupting. When you're implementing support for displaying your app's credentials on the QuickType bar, it's essential that you keep the Credential Identity Store up to date, and in sync with the credentials your app knows about. If the store becomes out of sync with your app's data, the user might not see newly added credentials on the QuickType bar, or may continue to see credentials on the QuickType bar even after they've been deleted from your app. You should take advantage of AS Credential Identity Store's incremental update APIs. Replacing the entire list of credential identities every time any credential has changed, may become expensive the more credential identities you need to update. It's better for performance to incrementally save new credential identities or remove deleted ones as those changes are made, rather than re-writing the entire store. Keep in mind, when your extension is being called, the user is in the middle of using another app. Keep the interactions and your UI to a minimum, and only include what the user needs in order to user their passwords. If loading your password database involves expensive setup, avoid redoing the setup in the view load method of your view controller, and tearing it down later. The system may reuse your app extensions process if the user sequentially signs into multiple services using your extension. Consider using a singleton pattern, so any work done in one invocation of your extension can be reused the next time if it doesn't need to be repeated. And that wraps up how you can display credentials from your app in the QuickType bar. Finally, I'll discuss one more API your extension may find useful. When the user enables your app extension for Password AutoFill, you may have some setup that needs to be done before the user can get the best experience. For starters, if you support showing credentials in the QuickType bar, your app or extension will need to provide its credential identities to AutoFill first. But it may also be useful to show other settings at this point, perhaps to offer the user the ability to sign in to an online service to retrieve the passwords if they haven't already. Authentication services provides an API to support these work flows. When these are enabled to your extension, settings can launch your extension and present its UI, so you can let users configure it. To opt into this behavior, open the Info Property List for your app extension and add a new key under NS extension attributes. AS Credential Provider Extension shows configuration UI with the bullion value of yes. This is how the system will know to launch your extension when its enabled. Then, implement the Prepare Interface For Extension Configuration method in your view controller, and set up the appropriate UI for when your extension is first enabled. When your extension is done, call the Complete Extension Configuration Request method on your extension context, and settings will dismiss your UI. At this point, your extension is enabled. It has provided credential identities for AutoFill to suggest for the QuickType bar. It can provide the passwords when those suggestions are chosen, and it can show the user a list of all their credentials. You're now done integrating your app with Password AutoFill and the users can enjoy the convenience of AutoFilling Passwords saved into your app wherever they're needed. There are just a few more general best practices to consider while developing your extension. As discussed before, your principal view controller may be responsible for showing the UI for a diverse set of functionalities. To achieve this, we recommend using separate view controllers managed by your principal view controller. For example, you may want to have one view controller class for displaying the credential list. And another for authenticating the user when filling credentials. You can either present these view controllers from your principal view controller, or use view controller containment to embed their views. And if you prepare your interface by presenting view controllers, the presentation should be done without animation, since the presentation of your principal view controller is already animated. In general, extensions should be lightweight and ready to terminate when they're done, and this includes AutoFill Credential Provider Extensions. Your extension will be invoked to perform one particular task, and you shouldn't include any unnecessary work flows or user interactions beyond what is needed. Be aware that the system may terminate or suspend your extension for various reasons at any time. For example, the system will terminate AutoFill Credential Provider Extensions while they're in use if the user switches apps. And your extension will have a separate sign box from your main app, but it will still need to share data, such as the user's credentials. Use App Groups or Shared Keychains to share data between your app and its extensions. For a review about extension development in general, refer to the Creating Extensions for iOS and OS 10 part two session from WWDC 2014. Finally, you can use Safari if you need to debug your credential provider extension while testing filling credentials. To do this, first activate the extension scheme, select a target, and select Run. When you do this, Xcode will ask you to choose an app to host the extension. Choose Safari from the list, and click the Run button. Safari will then open, and you can navigate to a sign-in page where you want to test your extension. When you open your credential list, or select a credential from the QuickType bar, your extension will be launched, and Xcode will attach the debugger to it, so you can begin your debug session. For debugging your extension in the other cases, use the Attach to Process Item in Xcode's debug menu to start attaching the debugger. You can then manually open Settings to enable your extension if you want to test the settings UI, or you can open any app's login screen if you'd like to debug AutoFill there. In summary, iOS 12 enables Password Manager apps to integrate with Password AutoFill. Using APIs from the New Authentication Services framework, your credential provider extension can show the user a list of their credentials. Show credentials on the QuickType bar, and optionally provide a way for the user to configure the extension from settings. For more information, refer to the Apple Developer Page for this session. To learn more about the other improvements to password management in iOS 12, see the Automatic Strong Passwords and Security Code AutoFill session. And if you'd like to learn more about the Password AutoFill feature and associated domains, see the Introducing Password AutoFill for Apps session from WWDC 2017. Good afternoon. My name is Shane. I'm with the Darwin Runtime team. And I'd like to welcome you to measuring performance using logging. So we heard a lot about performance on Monday. Performance is one of those things that's key to a great user experience. People love it when their games and their apps are fast, dynamic, and responsive. But software is complex so that means that when your app is trying to do something, sometimes a ton of things can be going on and that means you can find some performance wins in some pretty unlikely places. But doing so, unearthing those of performance wins requires an understanding, sometimes a deep understanding of what it is your program is doing. It requires you to know when your code is executing exactly, how long a particular operation is taking. So this is one place where a good tool can make a real difference. And we know that building better tools, making them available to you, is one of the ways that we can help you be a more productive developer. So today I'm going to talk about one of those tools. Today, I'm going to talk about signposts. Signposts are a new member of the OSLog family. And we're making them available to you in macOS. We're making them available to you in iOs. And you can use them in Swift and in C, but the coolest thing is we've integrated them with Instruments. So that means Instruments can take the data that signposts produce and give you a deep understanding of what it is your program is doing. So first a little history. We introduced OSLog a couple of years ago. It's our modern take on a logging facility. It's our way of getting debugging information out of the system. And it was built with our goals of efficiency and privacy in mind. Here you can see an example of OSLog code where I've just created a simple log handle and posted a hello world to it. Signposts extend the OSLog API, but they do it for the performance use case. And that means they are conveying performance related information, and they're integrated with our developer tools and that means you can annotate your code with signposts and then pull up Instruments and see something like this. So Instruments is showing you this beautiful visualization of a timeline of what your program is doing and the signpost activity there. And then on the bottom there's that table with statistical aggregation and analysis of the signpost data, slicing and dicing to see what your program's behavior is really like. In this session, I'll talk about adopting signposts into your code and show you some of what they're capable of. And then we're going to demonstrate the new Instrument signpost visualization to give you an idea how signposts and Instruments work together. So let's start. I'm going to start with a really basic example. Imagine that this is your app. And what you're trying to investigate is the amount of time a particular part of the interface takes to refresh. And you know to do that you want to load some images and put them on the screen. So once again, an abstract, simple view of this app might be that you're doing the work to grab an asset. And after you've gotten them all, the interface is refreshed. What a signpost allows us to do is to mark the beginning and the end of a piece of work and then associate those two points in time, those two log events with each other. And they do it with an os signpost function call. There are two calls. One with .begin and one with .end. Here I've represented the begin with that arrow with the b underneath it. And I represented the end with the arrow with the e under it. And then we're going to relate those two points to each other to give you a sense of what the elapsed time is for that interval. All right. In code, there's this simple implementation of that algorithm where for each element in our interface, we're going to fetch that asset and that's the piece of operation that we're interested in measuring. So to incorporate signpost into this code base, we're going to simply import the module os.signpost that contains that functionality. And then because signposts are part of the OSLog functionality, we're going to create a log handle. Here, this log handle takes two arguments, a subsystem and a category. The subsystem is just probably the same throughout your project. It looks a lot like your bundle ID. And it represents the component or the piece of software, maybe the framework that you're working on. The category is used to relate -- to group related operations together or related signposts. And you'll see why that could be useful later in the session. Once we have that log handle, we're just going to make two calls to os signpost. One with .begin. One with .end. We're going to pass that log handle into those calls. And then for the third argument, we have a signpost name. The signpost name is a string literal that identifies the interval that identifies the operation that we're interested in measuring. That string literal is used to match up the begin point that we've annotated or that gets marked up with that os signpost begin called and the end point. So on our timeline, it just looks like this. At the beginning of each piece of work, we've dropped an os signpost. At the end of each piece of work, we've dropped an os signpost. And because those string literals at the begin and end call sites line up with each other, we can match those two together. But what if we're interested in also measuring the entire amount of time the whole operation, that whole refresh took? Well, in our code, we're just going to add another pair of os signpost begin and end calls. Pretty simple. And this time I've given it a different string literal, so a different signpost name. This time refresh panel to indicate that this is a separate interval, separate from the one inside the loop. In our timeline, we're just marking two additional signposts. And that matching string literal of refresh panel will let the system know that those two points are associated with each other. All right. It's not a super simple example. If your program ever does step one and then step two then step three in a sequential fashion then that would work. But in our systems, often we have a lot of work that happens asynchronously. Right. So instead of having step one, step two, step three, we're often kicking things off in sequence, right, and then letting them complete later. So that means that these operations can happen concurrently. They can overlap. In that case, we need to give some additional piece of information to the system in order for it to tell those signposts apart from each other. And to do that, so far we've only used that name. Right. That name will match up the end and the beginning point. So that string literal so far has identified intervals, but it hasn't given us a way to discriminate between overlapping intervals. To do that, we're going to add another piece of data to our signpost calls called a signpost ID. The signpost ID will tell the system that these are the same kind of operation but each one is different from each other. So if two operations overlap but they have different signpost IDs, the system will know that they're two different intervals. As long as you pass the same signpost ID at the begin call site and the end call site, those two signposts will be associated with each other. You can make signpost IDs with this constructor here that takes a log handle, but you can also make them with an object. This could be useful if you have some object that represents the work that you're trying to do and the same signpost ID will be generated as long as you use the same instance of that object. So this means you don't have to carry or store the signpost ID around. You can just use the object that's handy. Visually, you can think of signpost IDs as allowing us to pass a little bit of extra context to each signpost call which can relate the begin and end markers for a particular operation with each other. And this is important because not only can these operations overlap, but they often take differing amounts of time. Let's see this in our code example. So here's our code. I'm going to transform that synchronous fetch async call in to an asynchronous one. So here I'm just going to give it a completion handler. This is a closure that will run after the fetch asset is complete. And then I've also added a closure, a completion handler for running after all the assets have been fetched. In each case, I've moved that os signpost end call inside of a closure to indicate that that's when I want that marked period of time to end. Okay. So because we think that these intervals will overlap with each other, we're going to create new signpost IDs for each of them. Notice in the top example I've created one with the constructor taking a log handle. And the second one, I've made off that object that is being worked on, the element. And then I simply pass those signpost IDs into the call sites and we're done. You can think of signpost as being organized as a kind of classification or hierarchy. Right. All these operations are related together by the log handle meaning that log category. And then for each operation that we're interested in, we've given it a signpost name. Then because those signposts could overlap with each other, we've given them that signpost ID that tells the system that that's a specific instance of that interval. This interface was built specifically to be flexible so you control all the arguments into your begin site and your end site. You control that signpost name, the log handle you give it, and the ID. We've done this because as long as you can give the same arguments at the begin site and the end site, those two signposts will get matched with each other. That means your begin and end sites can be in separate functions. They can be associated with separate objects. They may even live in separate source files. We've done this because we want you to be able to adopt it into your code base. And so whatever entry and exit conventions you have, you can use these calls. So that's how to measure intervals with signposts. You may want to convey some additional information, some additional performance relevant information along with your signposts. And for that, we have a way to add metadata to signpost calls. So here's your basic signpost call. To that, we can add an additional string literal parameter. This allows you to add some context to your begin and end call sites. Perhaps you have multiple begin and exit points for a particular operation, but the string literal is also an OSLog format string. And that means I can use it to pass additional data into the signpost. So here, for example, I've used that %d to pass in four integers. But because it's an OSLog format string, I can also use it to pass many arguments of different types. So here I've passed in some floating-point numbers. And I've even used the format specifier to tell the system how much precision I want. You can pass dynamic strings in with the string literal formatter. And that'll let us pass in information that comes from a function call or comes from a user entered piece of information. And we reference that format string literal with a fixed amount of storage which means that you can feel free to make it as long and as human readable as you like. This human readable string is the same one that will be rendered up in the Instruments. So you can feel free to give it some context. I've given it here for the various arguments. And Instruments will be able to show that full rendered string, or it still has programmatic access to the data that's attached. In addition to metadata for those intervals, you may want to add individual points in time. That is, in addition to the begin signpost and the end signpost, you may have a signpost that's not tethered to a particular time interval but rather just some fixed moment. And for that, we have an os signpost with the event type. The os signpost with the event type call looks just like the same as the begin and end, this time with the event type. And it marks a single point in time. You could use this within the context of an interval or maybe because you want to track something that's independent of an interval like a user interaction. So for that fetch asset interval we're talking about, maybe you want to know when you've connected to the service that provides that asset. Or maybe you want to know when you've received a few bytes of it. You can use this to update the status or progress of a particular interval many times throughout that time of that interval. Or you might be tracking maybe a triggering event like maybe a user interface interaction like somebody has just swiped to update that interface. Although, if you're really investigating in a performance problem, they might be swiping a lot so this might be what you see instead. If you have signpost enabled, they're usually on by default, but I'd like to talk about conditionally turning them on and off. First I'd like to emphasize that we built signpost to be lightweight. That means we've done a lot of work to optimize them at emit time. We've done this through some compiler optimizations that make sure that work is done in front instead of runtime. We've also deferred a lot of our work so that they're done on the Instruments backend. And that means that while signposts are being emitted, they should take very few system resources. We've done this because we want to minimize the impact to whatever your code is running. And we've also done it because we want to make sure that even if you have very small time span, you can emit a lot of signposts to get some fine-grained measurements. But you may want to be able to turn your signposts off. Maybe you want to eliminate as much overhead as you can from a particular code path. Or you might have two categories of signposts, both of which are super-high volume and you really are only interested in debugging one or the other at a given point in time. Well, to do that we're going to take advantage of a feature of OSLog, the disabled log handle. So the disabled log handle is a simple handle. And what it does is every OSLog and os signpost call made against that handle will just turn into something very close to a no-op. In fact, if you adopt this in C, we'll even do the check for you in line and then we won't even evaluate the rest of the arguments. So you can just change this handle at runtime. Let me show you an example. So we're going to go back to the very first example code that we had. And you see that initialization of that log handle up top. Well, instead I'm going to make that initialization conditional. So I'm either going to assign it to the normal os log constructor or I'm going to assign it to that disabled log handle. If we take the first path, all the os signpost calls will work as I described, but if we take the second path, those os signpost calls will turn into near no-ops. So as I said before, notice that I didn't have to call any of my call -- I didn't have to change any of my call sites. I only had to change the initialization. And I made the initialization conditional on an environment variable. This is the kind of thing that you can set up in your Xcode scheme while you're debugging your program. Now I said you didn't have to change in the call sites, but maybe you have some functionality that is instrumentation specific. That is, it might be expensive but it might only be used for while debugging. So in that case, you can check a particular log handle to see if signposts are turned on for it with the signposts enabled property. The signposts enabled property can then be used to gate that additional operation. Okay. So all the examples that I've shown so far have been in Swift, but signposts are also available in C. All the functionality I've talked about so far is available: the long handles, emitting the three different kinds of signposts, and managing your signpost identifiers. For those of you who are interested in adopting in C, I encourage you to read the header doc. and header doc covers all this information that I have but from a C developer's perspective. All right. Now you've seen how to adopt signposts in your code. And maybe you have a mental model of what they represent. So I would love for you to see how signposts work in concert with Instruments. And for that, I'm going to turn it over for the rest of the session to my colleague, Chad. Thank you. All right. Thank you, Shane. Now today I want to show you and demonstrate for you three new important features in Instruments 10 to help you work with signpost data. The first is the new os signpost instrument. And that instrument allows you to record, visualize, and analyze all of the signpost activity in your application. The next feature is points of interest. I'll talk a little bit about what points of interest are and when you might want to emit one. And then I'm also going to show you the new custom instruments feature and how you can use it with os signposts to get a more, I guess, refined presentation of your signposts. So let's take a look at that in a demonstration. Okay. Now to start with, we're going to take a look at our example application first. And that is our Trailblazer application. This app is -- shows you the local hiking trails. And it basically downloads these beautiful images for you as we scroll. Now you'll notice that initially we have a white background and then the image comes in later and fills in. And this is a pretty common pattern in an application like this. And sometimes it's implemented with a future or a promise but this pattern -- as much as it helps with performance, it's also pretty difficult to profile. And the reason for that is because there are a lot of asynchronous activities going on. As the user scrolls, there are downloads that are in-flight at the same time. And if the user scrolls really quickly like this then the download may not complete before the image cell needs to be reused. And so then we have to cancel that download. If we fail to do that, then we end up with several downloads running in parallel that we didn't really want. So let's take a look at how we can use signposts to analyze the application of our Trailblazer. Now inside the trail cell, we have a method called startImageDownload. And this is invoked when we need to download that new image, and it's passed in the image name that should be downloaded. Now we have a download helper class here that we create an instance of an pass in the name and set ourself as the delegate so it'll call us back when it's downloaded. And in this case, since the downloader represents the concurrent activity that's going on, this asynchronous work, it's a great basis for a signpost ID. So we're going to create our signpost ID using our downloader object. Now to start our signposts, we're going to do an os signpost begin. And we're going to send it to our networking log handle so take a real quick look at our networking log handle. You see we're using our Trailblazer bundle ID and a category of networking. Now we're going to pass an image or, sorry, a signpost name of background image so that way we can see all of our background image downloads. And it will pass that signpost ID that we created. And we'll attach some metadata to begin to convey the name of the image that we are downloading. So then we'll start our download, and we'll set our property to track that. We have it currently runningDownloader. Now when that finishes, we'll get this didReceiveImage callback here. And we'll set our image view to the image that we received. And we'll call end on the signpost. And we'll use the exact same log handle, the same name, the same signpost ID but this time we're going to attach some end metadata to say finished with size. And you'll notice here that we've annotated this particular parameter with Xcode colon size-in-bytes. And what this does is it tells Xcode and Instruments that this argument should be treated as a size-in-bytes for both display and analysis. Now these are called engineering types. And they can be read about in the Instruments developer help guide which is under the help menu in Instruments. Now once we've completed our downloading, we can set that back to nil. Now there are two ways that we can finish a download. That was the success path. And then we have to consider the cancel path. So in prepare for reuse, if we currently have a running downloader, we're going to need to first cancel that downloader. So in that case, we're going to emit an end for the interval, and we're going to use that same logging handle, signpost name, signpost ID. And we're going to use cancelled as the end metadata to separate it from when we finish successfully. Now that's enough to actually do some profiling. So we're going to go over here to a product profile. And that will start up Instruments once we've finished building and installing. That will start up Instruments here. And we can create a new blank document. Then we can go to the library, and I can show you how to use that new os signpost instrument. So we have our new os signpost instrument here. And we'll just drag and drop that out into the trace. We'll make a little bit of room here for it and then we will press record. And I'll bring our iPhone back up here to the beginning. All right. So now we'll do some scrolling and then we'll also do some really, really quick scrolling. And then we'll let that settle down. Now we can go back to Instruments and see what kind of data we recorded. So I'm going to stop the recording. And now you'll notice here that, in the track view, we have a visualization of all of our background image intervals. Now that's the signpost name. Now if we hold down the option key and we zoom in, you can see there are intervals. And intervals are annotated with the start metadata and the end metadata. Now if we zoom back out and then take a look at the trace here again, we'll notice that we have no more than five images that are running downloads in parallel, which is a good thing. That means that our cancellation worked. And if we want to confirm that, we can come in here and you can see that a lot of these intervals have metadata that says that it was cancelled in the download. Now if you want to take -- if you want to look at a numerical -- or let's say you want to look at the durations of these intervals then you can come down here to the summary of intervals. And we see a breakdown by category and then by signpost name and then by the start message and then by the end message. So if we make this a little bit smaller, you can see that we made 93 image download requests. Of those, 12 were for location one. Of those 12, seven were cancelled and five finished with a size of 3.31 megabytes. Now if you look over here, these are the statistics about our durations. And you can see that the minimum and the average of the cancelled intervals is significantly smaller than when we finished with the full downloads. And that's exactly what you would expect to see in this pattern. Now if you want to see all of the cancelled events because you're interested in seeing those, you can put this -- focus arrow and it will take you to a list view where you can see all the places where location one had an end message of cancelled. And as we go through this, you'll see that the inspection head on the top of the trace will move forward to each one of those intervals. So you can track all the failure cases if that's what you're interested in. Now that's a great way to view the times of those intervals, the timing of those intervals. But what if you wanted to do an analysis of the metadata? What if you wanted to determine how many bytes of image data that we've downloaded over the network? Well, we've emitted metadata messages like finished with size and then the size. It would be great if we could total that argument up. So if you want to do that, you can come over here to the summary of metadata statistics. You can see that we have it broken down by the subsystem, the category, and then the format string and then below the format string, the arguments within that format string. And since our format string only has one, that's simply arg0. Now Instruments has totaled this up. And it knows that this is a size-in-bytes. And so it gives us a nice calculation of 80 megabytes. So we've downloaded 80 megabytes of image data total. Now you can see the different columns here. You've got it min, max, average, and standard deviation. So this is a great way to take a look at just statistical analysis of the values that you're conveying through your metadata. Now Shane mentioned that signposts were very lightweight and that is totally true except when you run Instruments the way I just ran Instruments. In what we call immediate mode, which is the default recording mode, Instruments is trying to show and record the data in near real time. And so when it goes into immediate mode recording, all the signposts have to be sent directly to Instruments. And we have to bypass all of those optimizations that you get from buffering in the operating system. Now with our signposts -- with our signposts application here, we're not really emitting enough intervals to really notice that overhead, but if you have a game engine and you want to emit thousands of signposts per second, that overhead will start to build up. So in order to work around that, what you can do is change the recording mode of Instruments before you take your recording. And the way you do that is by holding down on the record button and selecting recording options. And then in this section here for the global options, you can see that we have our immediate mode selected. And we can change that to last five second mode. Now this is often called windowed mode. And what it tells the operating system and the recording technology is that we don't need every single event. We just want the last five seconds worth. And when you do that, Instruments will step out of the way and let the operating system do what it does. Now this is a very common mode. We use this in system trace. We use this in metal system trace and the new game performance template. And so it's a very common way to look for stutters and hangs in your application. All right. So that is our os signpost instrument. Now let's talk about points of interest. Now if we come back to our Trailblazer application here, you notice that when I tap on a trail, it pushes a detail. If I go back and tap on a different trail, it'll push a different detail. Now it would be great if we could track every time these detail views come forward because then we can tell what our user is trying to do, and we can tell where our user is in the application. Now you could certainly do this with a signpost, but you'd have to drag in the os signpost instrument and record all of the signpost activity on the application. And it sort of dilutes how important these user navigation events are. So what we allow you to do is promote them to what are called points of interest. Now if I go to our code for the detail controller and we look at our viewDidAppear method, you can see that I'm posting -- I'm creating an os signpost event saying that a detail appeared and with the name of the detail. Now this is sent to a special log handle that we've created called points of interest. And the way that you create that is by creating a log handle with your subsystem identifier and the systems points of interest category. So this is a special category that Instruments will be looking for. And when it sees points here, it'll place them into the points of interest instrument. So if we come back here and take a time profile, you can see that we have our points of interest instrument automatically included. And if we do a recording and we do that same basic action, we go into the Matt Davis Trail and then we'll come back here to Skyline Trail and then we'll go back and we'll do one more for good measure. Now when you go back to Instruments, you can see that we have those points of interest prominently displayed. So you can see where your user was in the navigation of your app. And you can correlate this with other performance data. And so points of interest are really a way for you to pick and choose some of the most important points of interest in your application and make them available to every developer that's on your team or in your development community. And they can be seen right here in the points of interest. All right. So that is the points of interest instrument and how you create points of interest from signposts. Now another great feature of Instruments 10 is the ability for you to create custom instruments. And so to demonstrate what you can do with custom instruments in os signpost, we've created, as part of our project, a Trailblazer instruments package. Now I'm going to build and run that now. And you'll see when we do that, we start a separate copy of Instruments that has our just built package installed. And if we bring that version forward, we'll see that we now have a Trailblazer networking trace template. And if we choose that, we can see that we have a Trailblazer networking instrument in our trace document. And let's take a recording and see what the difference is between our points of interest or, I'm sorry, our os signpost and what this custom instrument can do. So we'll do the same type of thing. We'll do some basic downloading. And then we'll come back and we'll analyze our trace. Now the presentation here is significantly different. So let's zoom in and take a look at it. You'll notice here on the left, instead of breaking it down by signpost name, we've broken it down by the image being downloaded. So now we can see that image two was downloaded at this point and at this point. Now we've labeled each one with the size in megabytes of the download. And we've also colored them red if the download size is larger than 3 1/2 megabytes. So this is a custom graph that we created as part of our custom instrument. Now we've also defined some details down here. We've got a very simple list of our download events. And, again, you can navigate through the trace with those. And we also have -- let me see if I can get this back into focus here. We also have a summary for all the downloads. Very simple. We just want to do a count and a sum. And then we also have this cool thing called Timeslice. And in a Timeslice view, what we're trying to do is answer that question I was asking before of how many of these things are actually running in parallel? Well, if you want to take a look at the intervals running in parallel, you just scrub the inspection head over here through time, and you can see exactly what is intersecting that inspection head at any given point in time. So it's a great and a different way to look at your signpost data. Now if you are working with others on a project or you're part of the development community, using a custom instrument is a great way to take the signpost data and reshape it in a way that someone else can use and interpret without having to know the details of how your code works so they are a very important feature. Now the great news is that to create an instrument like that, the entire package definition is about 115 lines of XML and so custom instruments is very expressive and very powerful but also very easy. And that is the conclusion of our demo. So in today's session, we took a look at the signpost API, and we showed you how to use it to annotate interesting periods of activity and intervals inside your application. We showed you how to collect metadata and get that metadata into Instruments for visualization and analysis. And we showed you how to combine custom instruments in os signpost to create a more tailored presentation of your signpost data. Now all this really comes down to us being able to give you the kind of information that you need to help you tune the performance of your application. And so we're really excited to see how you use os signpost and Instruments together to improve the user experience of your application. That is the content for today. For more information, you can come see us in a lab, technology lab 8 at 3:00 today. And I also have session 410, creating custom instruments tomorrow where I'll be going over the details of how custom instruments works and show you how we created our Trailblazer networking instruments package. Thank you very much. Enjoy the rest of your conference.  Hi, everyone. I'm Vicki Murley and I'm the Engineering Manager for the MapKit JS Team. This is Session 212, Introducing MapKit JS. So, when iPhone came out more than ten years ago, it really changed the game, and it introduced us all to what we now kind of know as this app ecosystem. And it's funny to think about, because for so many of us, apps are such an integral part of our daily lives, but before there were apps, there were websites. If your company was founded before iPhone, your whole company may have started as a website, and even today, if you have an app, you might be using a website to reach a larger audience or even just a different audience of people. So, lots of developers out there, just like you, have a website and an app. And the WWDC site is a great example. We have this site and there's an accompanying app, which you're probably using to find your way around the conference center, as you're on the go, here this week. So, at Apple, we've been really lucky that we have been able to show Apple Maps on our own websites for a little while now. This is a page on the WWDC site, which is showing you other events around the convention center. If you've ever used Find My iPhone of iCloud.com, you've seen Apple Maps on a website. And if you've ever searched for a retail store, again, an example of Apple Maps on a website. So, most of you out there are probably used to using MapKit inside your apps, and today, we're making MapKit JS available to you for your websites. So, this week at WWDC, we're making a MapKit JS Beta available, and this is the same library that we have been using on our production websites. Every web mapping library out there, has a free tier of usage. Like some number of requests that you get for free, and MapKit JS Beta is no different. So, you -- as part of the MapKit JS Beta, you're getting 250,000 map initializations, and 25,000 service requests. That includes geocoding, search, search auto-complete, and directions. And often these free usage tiers are specified over some period of time, like per year, per month, per week. For the MapKit JS Beta, we're making this amount of free usage available to you per day. So, that's a good amount. And -- but if you need more for your particular use case or your business, you can contact us. There's a form online. Just fill it out and submit it, and we will receive that request. To use MapKit JS, you need a key, and you can get that by signing into your developer account, and go to the Certificates, ID's, and Profiles Panel. And you can get a key there, just like you would for any other service in your developer account. There's a limited number of keys available for the MapKit JS Beta, so I suggest you go and get one as soon as possible. Once you get a key and start using MapKit JS, I mean, I'm biased because I work on it, but I think there's a lot of things that you're going to really like about it. The first thing is, it lets you unify on one map provider, for both your app and your website. So, we've noticed lots of you are using Apple Maps inside your apps. Now, you'll be able to use Apple Maps everywhere. The MapKit JS APIs, are really inspired by the APIs that you're already using inside native MapKit, but they're using conventions that are familiar to web developers. So, they should be easy for you to adapt. Also, MapKit JS really brings the Apple Maps experience to web pages. It's localized and in 32 languages with full right to left support. It's accessible via keyboard access and with voiceover. It has the beautiful cartography that we all know and love in Apple Maps. And it has support for native gestures like Pinch to Zoom, Two-Finger Rotation, and also Shift, and Two Finger Scroll to zoom in. It also uses something that's called Adaptive Rendering Modes. So, what that means is, there's a couple different modes in which the map can be rendered. The first is Client-Side Rendering, and this is a full WebGL engine that is rendering the map on the client. And this allows us to do lots of things, like these very, smooth transitions as users pinch to zoom or shift two-finger scroll to zoom in. You'll see the labels fading in and out as we transition between zoom levels, just like on native. Also, Client-Side Rendering gives us full control over the labels on the map. So, we can enable features such as Rotation, where I'm rotating the map with two-fingers, but the labels are staying horizontal as I rotate. Finally, with CSR, we have the marker style annotations that you're used to seeing on iOS. And the title of this annotation is My Marker. If I select this annotation, the subtitle will appear, but it would overlap that label that is beneath it. So, with Client-Side Rendering, and full control over the labels, we can just collide that label out and remove it so that the labels on the map never overlap the text for your annotations. And even as users are zooming in and out, and the map labels are changing all over the place, the map labels are never going to overlap your annotation labels. In fact, if you have a draggable annotation and the user is picking it up and moving it all over the map, labels are never going to overlap your annotation labels. So, this is one of the rendering modes, Client-Side Rendering. But web code can run anywhere, and not all devices are created equal. So, for those times when your users may be running on a low performance configuration, maybe an old device, we have a different mode that we call Labels Only Client-Side Rendering. And what this is, it's a grid of image tiles that show the base map, with a layer of WebGL labels over the top of it. So, we can still give as many users as possible these great features like rotation and label collisions, in a high-performance way. In your users' configuration does not support WebGL at all, they get something that we call Server-Side Rendering, which is a grid of tiles, tile images, and in this case, the labels are actually part of the image. So, adaptive rendering modes are pretty cool because there is an ideal mode for every client configuration out there. And we choose the best mode for your user automatically. So, you don't have to decide whether to use the WebGL version, or the tile version, or whatever. We run a quick test and give that user the best mode for their configuration. In our experience so far, most users are getting Client-Side Rendering or Labels Only Client-Side Rendering. So, that's a quick introduction to MapKit JS. And now, to tell you more about using the API, I'm going to hand it off to Julien Quint. Thank you, Vicki. Let's get down to the technical details of using MapKit JS for your website. We will see how to set up a map. We will see how to navigate an annotate this map so that you can show your own data. And finally, we'll see how to enable rich interactions with additional services. To set up your map, you need to go through these four steps. First step is to import MapKit JS. Here you see a script tag that you can add somewhere on your page. And the URL of the MapKit JS script includes a version number. We follow the semantic versioning system where the first number is the major version number, which is updated if breaking changes are introduced. The second number is a minor version number, so when bugs are fixed, when new features are added or when performance improvements take place, we update this number. And finally, for urgent patches, we update the patch number. You can specify and X instead of a specific number for the minor or the patch version, and we recommend that you use this Version 5.0.X to get started so that you will get urgent bug fixes on our first public release. In order to be able to show a map on your website, you will need to tell your page -- to tell MapKit where to show that map. So, in order to do that, you can use an HTML element that will be the container for your map. So, in this case, I'm using a development which is usually a very good candidate for that. I give it an ID so that I can refer back to it easily. And one thing that is very important is to set its size, because the map doesn't have a size of its own, so it will just use the size of the elements that it is part of. So, if you have a development that I represented here with a gray background, a map will be shown inside that element. If your element size changes, then your map size will change as well. So, I have set up an element for my map. The next step is to initialize MapKit JS. So, we just saw that you need a key to be able to use MapKit JS, and in initializing MapKit, you get your authorization callback so that you will be able to use the services that MapKit JS provides. And most importantly, to show the actual map on your websites. And the last point is to create a map object, here using the Map Constructor from MapKit. I give it the idea of the container where the map will show. And this is what happens. We have a map showing on our sites. This is the different map. We didn't pass any other parameters, so it will show a different region and the different control. You can see the controls in the corners. You can see the Compass for rotation. The Zoom control. The Map Type Control on the top, etcetera. This was a pretty large area, but if you're on a page where you want to show only a very small map, these controls can use a lot of valuable space, so by default, when your map size is below a certain width or height, we will toggle these controls off automatically. On touch devices, the situation is a little bit different. We don't really need to show the -- some of the controls like the Zoom or the Compass, because we have gestures to change the rotation of the map or to zoom in. So again, we don't really need to use up that space by showing these controls, and we turn them off by default. Note that the Apple logo and the legal text are always present. These are the difference for the controls, but you can also set these control visibilities yourself. Some controls are so-called Adaptive Controls. This includes the Compass and the Scale. This means that depending on the platform, they will behave in a slightly different manner. So, on iOS, the Compass control is not shown by default, but if the map rotates, then we will show it indicate where North is pointing. And the Scale is an indicator of the units of distance on your map. And we show it only when the user is zooming in and out. So, here, we can see the -- when we have zooming and rotation, we can see the Compass appear and disappear in the bottom right corner. And the Scale appear and disappear in the top left corner. Of course, we can also select these controls to be always visible or always hidden. The rest of the controls have a binary state, so they can be hidden or set to be visible by setting a Boolean property. In this example, we show the controls that do not show by default on desktop, such as the User Location Control which appears in the bottom right of the window. Here, I've selected it. So now, the user location is tracked, and you can see the user location in the center of the map. And the Scale is shown at the top let, because I marked it to Always Visible. We can go further in configuring these controls. In order to match better the color scheme of your page, you can set the tint of the map. And this changes the color of the user controls. So, you can see, for instance, the -- all of the controls of the map. You can see the user control is now highlighted in red because I set this tint color to a red color. You can use any CSS color value here. The MapKit will use your [inaudible] language configuration, so that it will adapt itself to the language that your user expects. You can also set that language into the initialization code or you can even set it on the fly using the language property. So here, I've set it to Japanese so that I'm sure -- so that I can show the map, as well as the controls in Japanese. So, you can see the North indicator for instance in the Compass has been replaced, and the units are not in -- using the metric system which is more traditional in Japan, than using miles or years. If you set your language to a right to left language, such as Hebrew in this example, or Arabic, the controls are also mirrored to match your users' expectations. So, the controls are the way -- are some of the ways that your users can interact with the map, but again, also in turn directly. As I said, we can use touch events on touch enabled device. On this stuff, you can use the Track Pad for some of the gestures, and you can also use the mouse to click and pan the map, or double-tap for instance, to zoom in on the map. You can also disable these interactions directly, by setting the Zoom Enabled, Scroll Enabled, or Rotate Enabled property to be false. So, for instance, in the case where you have this very small map as I showed earlier, you might want to make sure that the user doesn't accidentally move the map around to be sure that it always shows the right area. And you would set these properties to false, which would make the map static. Now, that we know how to set up the map on our page, we want to make sure that we show some interesting content. So, we can do that by navigating around the world to show a specific area, but also to annotate the map to call attention to the region that we are showing. We'll see how to set the center in the region, how to add annotations to the map to mark locations, and how to cover the -- some areas of the map with overlays. So, starting with the center and the region of the map, again, this is the default map. And it shows the default region, which is zooming out at the minimum possible level. And centering the map on, zero, zero, latitude and longitude. You probably want to show a more specific area of the world. So, for this example, I want to focus on Rio de Janeiro in Brazil. And to do that, I will set the center of my map to the coordinate of Rio de Janeiro. So, you can see on the right, that the map center has moved to show Rio at is center. But setting the center of the map does not change the scale. So, at this scale, we are always still not very much in focus because we can see all of South America, for instance. So, let's zoom in a little bit. And in order to do that, I will set a region. So, a coordinate region is made up of a center, which is represented by these dots at the center of the map, as well as a latitude and longitude span. So here, I have represented the region as this dotted box. But we will not that the map -- the region that is set ultimately on your map, will be bigger than the region that you asked for because we need to make sure that it fits entirely within the bounds that you have set for your map. So, in this case, I need to add some padding, north and south of the region to be able to show the -- all of the region that was specified. So, in terms of putting the MapKit JS framework -- the center is set to a MapKit coordinator object which is a latitude and longitude pair. In the Coordinate Region, to set the region of the map, is an object that contains two member, a center which is also a coordinate and a coordinate span which is a latitude delta and a longitude delta there. How did I come up with the region or with the coordinates that I showed you in these screen shots? There are many ways to decide on regions and coordinates. You can look them up on Wikipedia and help put them in your code. You could get them from some database that has some geographical information. But one way that you can also get this information, is to use MapKit Services that includes Geocoding and Search. So, Geocoding lets you look at a place and will return your coordinate in a region. So, in these previous examples, I used the geocoding from MapKit JS to look at the coordinate in the region for Rio de Janeiro and this is what I set on my map. You can also use search to search for places. And it will return not only the coordinates of these places, but also bounding region that you can set on your map, to make sure that it encloses all of the results. Another way to quickly set a region on your map that is very convenient, is a method called Show Items. We will see now how to add annotations and overlays on your map, and by calling Show Items, we will make sure that we have a region that encloses these items so that they are all visible to your user. I will note that region changes may be animated, so that you can provide a nice animation. Or you can set the region instantly. Because these things, changing the regions, changing the center, etcetera, can be done from your code but can also be done by the user as a result of interactions you want to be able to react to these interactions. And we -- and the Map Object allows you to listen to user events such as region changes, when they begin, when they end, since they can be animated. And also, when gestures such as coding, zooming, rotations begin and end. We follow the model of DOM Event Handling, so you will have the familiar method such as Add Event Listener and Remove Event Listener, the name of the event, and you get past an event object with the parameters of the event. So, for instance, if I wanted to refresh my map with some new information every time the user has mapped to a different area, I can listen to region change and events, and that will let me know that the map has settled to a new place, and I can now use the region of that map to compute which information I wanted to show at this stage. Now, that we know how to set up a map, let's add some our own information on it. And we can do that with annotations and overlay. We'll start with annotations. And MapKit JS provides three kinds of annotations. The first one, you've had a quick peek at it, which is the Marker Annotation. You can customize annotations further than the -- what the marker annotation provides by using images, or even DOM Elements that will have completely customizable appearance. The marker annotations are the most useful annotations that MapKit JS provide as they also have lots of rich interaction built-in. And there's lots of styling options. So, they are analogous to the marker annotations that that you may have seen already on iOS. They have a built-in animation for selection and deselection. And they automatically collide out the underlying map labels as we've seen before. So here, when I select my annotation, the label for the exit of the station is hidden to make room for the subtitle. Marker annotations -- the appearance of marker annotation also adapts itself automatically to the rendering mode that has been selected by MapKit JS for your map. So, we've seen that if we have Client-Side Rendering Mode or Label Only Client-Side Rendering Mode, we are able to hide labels, so we can display the title and the subtitle of your annotations on the map. And this is what is shown on the left-hand side. If you have Server-Side Rendering, the labels are built into tiles and we cannot hide them. So, if you see the map on the right-hand side, we see that we have many more labels because none of them have been hidden. So, in order to be able to show our annotations legibly, we only show the annotation balloon and the labels are not shown on the map. We can still see the title and the subtitle of these annotations when we select them by using a callout bubble as in this example. And custom annotations, will also make use of these callout bubble to show the title and subtitle. Finally, the market annotations provide a lot of standing options. So, this is the default for market annotation. You create an annotation with a coordinate and some properties such as title and subtitle, and this is the basic default rendering. You have glyph image which is a pin shown in the balloon, and it comes up with a red color. You can change that color to any CSS color value. So, here, I've used a green color for my balloon. You can also change the color of the glyph. So, the glyph can only have one color. So, in this case, I switched from the default white to a yellow color. You can change the image of the glyph. So, from the default pin that is provided by MapKit JS, to any raster image that you can provide. So, in this case, I've provided this image. You can give several sources for your images for different pixel ratios. When the annotation is selected, it becomes bigger, so you have more room to show more details in your glyph. So, in this case, we can also specify a selected glyph image, which is different from the original one. Finally, instead of an image, you can change the glyph by using some text. We recommend that you stick to very, short strings such as one to three characters, and in this case, I'm just using the Letter M. When the glyph text and the glyph image are both specified, then the glyph text is shown. So, now that we know how to display marker annotations, we can go crazy and put a lot of annotations on our map. So, this is the result of doing searches for cafes and bars in Paris. And I've styled them slightly differently to distinguish them from one another. But the problem is that there is a very dense number of annotations in the same area. Fortunately, we have two ways in MapKit JS to deal with this sort of clutter. The first one is to set a display priority property on annotations. What this means is that when two annotations collide, the annotation with the highest display property, which is a number, will collide out the annotation with the lowest display priority. So, in this case, we can see that many annotations have been collided out. When you zoom in and out, these collisions run again, so that zooming in for instance, will let you discover more annotations. And when they have the same display priority, we will use the one that appears at the bottom of the screen as the more -- as the one with the highest priority. Another way to deal with clutter, is to use clustering instead of these display priorities. So, in this case, when two annotations that have the same clustering identifier, which is a property that can be set on marker annotations, and is a simple string, when two annotations that collide and have the same clustering identifier, they are replaced by a single annotation that will represent both of them, or more than two of them of course if more than two annotations collide. So, in this case, we see that some of the annotations can be shown by themselves because they have no neighbor, but in the dense areas in the center, we have clustered several annotations together. When a cluster is created, we use a marker annotation to represent it and we use the glyph to show how many annotations are in the cluster. You can also change the behavior -- not the behavior, the appearance of your clusters, by specifying a new style or a new kind of annotation. And now, we'll do a small demo to show these annotations in action. So, in this demo, which should appear here -- in this demo, we will use a map and the data that we will display is Bigfoot Sightings that Have Occurred in this Area. So, after this session, or maybe this weekend when you are done with the conference, you can maybe go explore the woods of Northern California and see if you can find Bigfoot. I have created a map which I have initialized in the manner that I've displayed in the previous slides. And what I have done, is I have gathered a list of Bigfoot annotations -- of Bigfoot sightings, sorry, including the location, the year in which they occurred, and some other information. So, what I will want to do is I will want to use these annotations, show them on the map, with marker annotation. So, let me start by creating a function to translate a sighting object which has all these properties, into a marker annotation. So, when I get a sighting, I want the coordinate of the sighting to be used as the coordinate of my marker annotation. And these sightings have several other properties, such as the year of the sighting, which I will display in the title. This comes from a database put together by the Bigfoot Field Researchers Organization. So, I will show the idea so that if you are interested in more details about that sighting, you can go look at that database and find even more information. And finally, I'm ready to create a marker annotation with that coordinate and these options that I've created. So, now that I know how to set up a marker annotation for a sighting, I will get the sightings that I've prepared. And with this list of sightings, I will create an annotation for each of them. I will then make sure that my sightings are shown on the map. My annotations are shown on the map. And I will use the Show Items method, which is very convenient because not only it ensures that the items are visible, but also that they are added to the map. And since I'm doing a webpage, I can also add some extra information on my page such as how many sightings there were in this area. So, let me make sure that I save and by reloading, I now see a map with 60 sightings on it. So, all of these annotations are sightings and we run into the problem that I've just highlighted which is that in this park area, I can see lots of sightings happening, and they all overlap each other. And even as I zoom in, I can see that there's still lots of clutter, so I will clean that up using the Display Priority Property on Annotations. So, in order to choose a display priority, what I will use is I will use a very important property of the sightings which is whether they were clear sightings. So, if someone really, really saw bigfoot, or maybe they just heard it or saw some traces of it. So, I have this clear flag which I will use to set my display priority. Display priorities a number and these priorities can be pretty arbitrary. So, we provide predefined values, such as required, meaning that your annotation must always be shown on the map, but also, high and low display priority. So, when the sighting is clear, I will set a high display priority. When it's not, I will set a low display priority. And finally, I will also encode that information as the color of my annotation so that the users can understand better why an annotation was shown or now. So, when it's brown, it's a clear annotation and when it's great, it was not so clear. So, let's see the difference that this makes. So, now you can see that the map is much cleaner because all of the collisions have been a result. You can see that some of the annotations are brown. So, these are the clear sightings. These are probably the ones that are the most interesting ones to visit first. But you can also see that if I zoom in on the map, then new sightings are being revealed. So, your users can explore the map and find out about where they can hope to see Bigfoot. I will conclude by this demo, by adding, another piece of information to my map. I will change the glyph to be either an icon of Bigfoot which I've prepared. So, this is a simple PNG image which I will use for the clear sightings, and in case the sighting is not so clear, I will make that more explicit by setting the glyph text to be a question mark. And now, what I can see, is that these questionable sightings are a question mark. And the Bigfoot sightings that are really clear, have the Bigfoot icon. So, this includes the first demo. Thank you. So, in this demo, we show how to create marker annotations from JavaScript objects, how to set the display priority of annotations to unclutter our map, and how to style these annotations with colored glyph image, glyph text, so that we can translate the information that we have onto the map. Sometimes, market annotation is very convenient, but may not be exactly what you want to use on your map. For instance, if you want to use the logo of your company and it has more than one color, then you cannot choose a glyph image because it's restricted to just one color. Sometimes the shape of the annotation is not really the one that you are going for, so in this case, you may provide images to represent your annotations instead. So, in this case, I have the small sticker look to mark places where -- which I have visited recently. So, I can use a raster image for these annotations. The title and subtitle are shown in a callout bubble, just like in Server-Side Rendering for marker annotations. And to create an image annotation, this is very similar to a marker annotation, but with the additional property that much be provided which is the URL for the images. And here, you can see two different URLs for two different pixel ratios. Another way that I could represent a notation is using the classic pin example. And pins usually come up in many different colors. And the problem is that if I want to provide a lot of different colors, I will need to provide a lot of different images. And this can quickly become unmanageable. So instead, what I can do is I can use a custom annotation. And in this case, instead of an image, I will use any DOM Element to represent my annotation. And these elements are created on demand for your annotations. So, let's see an example. Here, if I wanted to do just some color for any pin that shows up on my map, then I will create an annotation with a coordinate [inaudible]. And a third parameter which is a function that we will return a DOM Element for that annotation. So, in this case, I will use a Canvas Element. I create a Canvas Element. I get .Context. I drew a pin image in that Canvas. I changed the head of the pin to the color that I want, and I returned to Canvas. And this is the Canvas that is shown on the map for this annotation. So, these were the three kinds of annotations that you could use. But sometimes you want to show more than just a single location or a set of single locations. Sometimes you want to call -- to show complete areas on your map. And to do that, we have overlays and we have -- we provide three different kinds of overlays. And here are some examples of how you could use them. With a circle overlay, you can show all of the distances -- or the distances from a point. With a polyline overlay, we can show a route between two points. And you can highlight geographical areas such as a state, the boundaries of a country or a city, etcetera, using a polygon overlay. So, here is an example of a circle overlay. So, now I'm in Brussels and I've centered my map on the Manneken Pis. And this is right in the center of the city and this is a very nice place to walk around. So, by using concentric circles around my location of increasing radius, I can see walking distances. So, here I've created overlays with their coordinates, and the second parameter that you need to give to a circle overlay, is a radius expressed in meter. So here, every overlay has a radius starting from 400 meters, increasing by 400 meters. So, 400, 800, etcetera. So, I can see in the right, that there is a museum for comic strip art. That sounds interesting. And it looks like it could be between 1200 and then 1600 meters. So, about 10 to 15 minutes' walk. One important thing that I can do with overlays, is I can also style them. So here, to represent this style of overlay, I can use a MapKit Style Object, which has several properties such as the line width, the line depth, the stroke color which again, can be any CSS color. By default, circle overlays are filled with blue color, but in this case, I want them to be transparent. So, I set the field color to be null. I have decided that I want to go visit this comic strip museum, and MapKit JS as we will show, has services that can give you walking and driving directions between two points. So, in this case, I've asked for directions from where I was, the Manneken Pis, to where I want to go. I receive as a result, a route which I represent on this map using a polyline overlay. And a polyline overlay is a list of points. So, these are all coordinates linked to each other. I can style them by setting the Stroke Opacity for instance, or a thick line width to make sure that it shows up on my map. And the last example is a polygon overlay. This is very useful to do for instance data visualization. And here I have a map of the United States where every state is an overlay of its own. A polygon overlay is defined again by a list of points, but this -- in this case, the shape is closed and filled. You can even specify lists of lists of points to -- for more complex scenarios. So, for instance, if you have enclaves and exclaves, which is often the case with real geographical areas. Annotations and overlays can also have an extra piece of information added to them, which is this data property. So, in this example, I want to show the population of every state of the United States. I can show it as a color, which gives me a rough idea of the population. But I can also add some additional data so that if the user selects one of the overlays, so in this case, the State of Texas, I can show that the actual population is over 20 million people. And overlays are selectable just like annotations. To show overlays such as the route that I showed you or to show all of the states, we need a lot of data, we need a lot of points, so for the states, we had thousands and thousands of points. So, a convenient way to get that data into your program, is to use GeoJSON Import. So, if you have a GeoJSON data file, then you can import it automatically using MapKit JS. Annotations and overlays are created. So, in this example, this was a different data file, which contained a lot of UFO sightings. And this is just a detail of the map. So, you can see there are lots of these sightings. And all of these annotations and overlays are created automatically and can of course be customized using some delegate methods. Finally, these -- since these annotations and these overlays can be selected by users, you want to listen to events from them, as well as you could from the map. So, you can listen to selection and the selection events. And we've also seen earlier that annotations could be dragged, so you can listen to dragging events. And again, this uses the Add Event Listener Method. So, for instance, you can use Add Event Listener to listen to select events from the map, which will tell you when an annotation or an overlay has been selected. And we will see an example of that in the demo. The next section is Enabling Rich Interactions with Services, and for this section, I will give the mic to Melody. MapKit JS provides you with access to three services, Geocoding, Search, and Directions. I'll step through examples of each of those, but before that, I'll tell you what they have in common. You can use the services in a similar way, with four easy steps. You first create the service object, then you specify the request parameters and options, then you make the request, and finally, handle the asynchronous response via a callback function. So first, let's talk about the Geocoder. Here's an example of how you can use the Geocoder, and it has two methods: Lookup and Reverse Lookup. So, this is if have you an address or a place, and you want to find the coordinate, or if you have the opposite. You have a coordinate and you want to find the address. So, the first step is to create your object, which you can optionally provide at Gets User Location parameter. This will allow you to provide more context to the geocoder. This is useful in the case where there are places with the same name in multiple locations. For example, if you're using the geocoder here, you'll be most likely to return Brisbane, California instead of Brisbane, Australia. Next, you build the requests. Here, we're using this convention center. And then you can handle the response. As we mentioned before, the geocoder can easily be used to place marker annotations. So, here we're adding a marker annotation to the map. And then here we have an example of the reverse lookup where we have a coordinate, and we want to find the address for the place. So, that's the geocoder. Next up we have service context. So, as I mentioned before, you can provide a Gets User Location parameter, but you can also provide coordinate and region to search. This is useful to provide the most relevant results to your user. So, that's where places are in multiple locations with the same name, or you can provide your search results, closer to the user. Here's an example of how you can use the Search Service. It's an example for finding coffee nearby the user. So first, you create your service object. Then you build your request using query like coffee. You can also do something more specific like a specific name of a coffee shop that you have. And then you can handle the results, which can easily be displayed using annotation since sometimes you return multiple results. And you can use the coordinate as well as the title to populate the fields of annotation. And then finally, we simultaneously add and display the annotations using the Map Show Items method. So, this is useful if you have your full query that you want to send in the request, but if you have a Search Bar that you want to respond to user input, you can use the Search Autocomplete Method. With Search Autocomplete, you can provide a partial query to the user - to the service, and then save your user some keystrokes by providing suggestions which you can then use to complete a search. And the last service is directions. Similarly, to the other services, you create your directions object. You can choose to provide a language which will then return the route steps in that provided language. If you don't provide a language, the language provided to the MapKit initialization function will be used. And if that's not provided, the browser's preferred language will be used. Then you build your request using two required fields: the origin and the destination. These can be coordinates, addresses, or places. By default, the transportation type is by automobile. You can switch this using the transport enum to walking directions. We won't keep that, because that's a really far way to walk, but we will use Request Alternate Routes. You can use this to provide multiple options to your users. This way we can display different routes instead of the default of one optimal route. And then finally, you can display your results on the map. Here we have polyline overlays as you've seen earlier. And you also get a set of route steps as well as the distance and the travel time. So, those are the services. Now, that you're more familiar with them, Julien will come back up for a demo. Thank you, Melody. So, we will pick up where we left off and we will continue building our Bigfoot Finder. And there's one things that's really missing from this finder, is that even though it can tell us where we are, and it can tell us where Bigfoot is, it doesn't yet tell us how to get there. So, I'm going to add some directions from All Current Location, which is the McEnery Convention Center, to these Bigfoot locations. I will use the MapKit JS direction services to do that. I will use polyline overlays to draw them. But the first thing I need to do is I need to make sure that the user - I need to find a way so that the user can tell me which one of these sightings they want to see. So, well, this one is selected, so that's probably the one that the user wants to see, right? So, this is what we will do. We will use the Select Event Listener from the map to listen to annotation selections from the user. And when an annotation is selected, that means they might be interested in knowing how to see that specific location. So, we will request a route and draw it so that they can see how to get there. And then finally, we will let them also select one of the routes from the options that are returned by Direction Services and display more information about each of these routes. So, let's start by setting up our Event Listener. So, we listen to the select events from the map, which tells us when an annotation or an overlay is selected. So, this is the property of the event, object that I received. And if annotation has a value, then that means that this annotation was just selected. So, there is only one annotation overlay selected at a time, in MapKit. So, there is no ambiguity here. This is the one that we want to go to. So, I will show routes to that annotation. Showing our route means drawing it on the map. So, let's set up a polyline overlay to draw a route on the map. So, first off, I will start by defining a style that I will use for all my routes, setting some opacity and some line width to make sure that the route stands out. The default color is blue, which is fine for the moment, so that's what we will use. And just like we did a function that created annotations on demand for citing objects, here we'll create overlays on demand for routes. So, a route that is returned by the MapKit JS Direction Service, contains a lot of information including a path which is a list of steps that you can take to go from A to B. And each list of steps, itself contains a list of points. And if you remember to create a polyline overlay, I want a list of points. So, now that I have a list of - list of points, I will reduce it to a single list so that I will put all of - the path steps one after the other. This is the list of points that I want to create my polyline overlay. This is the style that I just defined above, that I want my overlay to show up as. And later, I will let the user select this overlay to see more information. So, I will keep around all of the route object, in the Data Properties so that I can display the distance of the estimated travel time. I know how to draw these overlays, so now I know - I need to request them from the service. So, here we go. This is a little bit longer but not so bad. I first need to create a directions object. I will just use the current language for the directions. So, the default is fine. So now, when I'm requesting a route to a specific annotation or rather, routes to a specific annotation, I will clean up my map first because maybe I've shown routes to a different place before. So, if I query the other list property of my map, it tells me which overlays are currently added to the map, and I can remove them by using the Remove Overlays Method. So, now I have a clean map, and I can display new routes. I'm building my request for the origin. I will hot code this place, the McEnery Convention Center. Again, I could put coordinates for instance, instead, but this is much more convenience. For the destination, I will use coordinate because I have them. They are the coordinate of the annotation that was given as a parameter. And I want to show all of the possible - not all of the possible but the several possible routes to this point. So, I make sure that My Request Alternate Routes Property's set to true. Now, that my request has been created, I can execute it on the Direction Services using the Route Method. Things can go wrong when you do these kind of queries. So, we must make sure that we have a way to handle errors when they occur. So, in this case, I will fall back to show a default route that I've created and it's just a straight line. So, if we see a straight line, then we know we're in trouble. But let's focus, let's be positive and focus on the cases where the request succeeds, and in this case, I will have a list of routes and I will create a new polyline overlay for every one of them. I will use my trusty Show Items Method on the map to add these routes and make sure that they are being shown by setting the region to enclose the routes. And let me introduce a couple more options in that method. The first one is Animate, because I wanted to make sure that there is a smooth animation from the current region of the map to the one where the routes are shown. And finally, I will also add some padding so that I can keep some of the context and not constrict the region too much around the routes. So, I'm now ready to request routes from the - this convention center to an annotation. So, let's select this one again. And now that my annotation has been selected, the request has been made. I see that I have three different results. So, they share a part of the route, but then they diverge in the end. And they start from our current location, the convention center. So, I think this is right. Yes, that looks right. I have got three results. So now, I might be interested in knowing more about what's the difference in travel time or distance. So, I can use selection to do that. But if I select a route, the only thing that happens is that my annotation has been deselected because there is only one selection at a time. The problem here is that there is no default behavior for overlays being selected. So, let's implement our own. What we will do is we will highlight these overlays and then display their information on the side bar. So, in order to highlight these overlays, I will create a new style for highlighting overlays. So, let's put it here, along with my old style. So, this style will keep the same opacity. We will make the line figure and we will make it purple so that it stands out from the other ones. But now, we need to apply this style to my overlay once it's selected. Fortunately, I already have an event handler for selections. And up to that point, we were only interested in listening to annotation being selected. But now we will just add a new -- we will add plans to this function. Oops. So now, if annotation is selected, I still do the same thing that I was doing, but if an overlay's selected, I will change its style to be the selected route style that I've just defined, so that it is highlighted. So, let's see this in action. We can use a different one. I'm feeling lucky. And now, we can see that we can select the routes to - from the list of routes that have - that has been provided. I haven't implemented showing the data yet. Let's see if I select another route. What happens is that there is nothing happening by default when an overlay's selected, but there is nothing happening by default when an overlay is deselected. So, I have to implement that part of the behavior as well. So, in this case, I will add a new event listener for deselect events. So, when an overlay is deselected, then I will switch its style to the default one. So, this should fix my issue. So, let's try again. I have a route here. And if I select again, I do the same place because here we have two overlays on top of each other. This will select the other overlay. And the deselection event handler is working, so, my previous highlighting has disappeared, and now, the new selection is shown correctly. So, this seems to be working pretty well. Here is another example. So again, I can select these routes. But now I want to know more about them. I want to know what are the driving directions, how long it takes to use one of the routes versus another, etc. So, this is very simple to do because all of the - everything that I need is already in place. I have saved data on my overlays, and I have Event List now for Selection and Deselection. So, when the overlay's selected, I will show the data in the sidebar. So, this is just classic. Now, we are really into web development land where we have a bunch of the time we went to -- format it into some HTML, so I won't go over the details. But the detail that we have is the one that is associated with the overlay. And it gets shown when the overlay's selected. And I will not make the same mistake about this this selection, so I will directly hide the data when the overlay's deselected so that I don't have several routes appearing at once. So, let's try again. Let's go back. I like this example. So again, I will select this route. And now that the route is selected, we can see on the right-hand side that we have some additional information. This is a quick name for the route. So, this is the route that goes from Mount Hamilton Road. It's 70 kilometers long and under ideal driving conditions, it should take me an hour and 32 minutes to reach there. And here are all the steps that I can follow to get to my destination. I could see what this one looks like. It's actually longer in distance but it should get me there faster. So, there we are. We have built a Bigfoot Finder and -- that lets us select where we want to go and tells us where we are going to go. Thank you. Quick recap. We've seen how to react to user events. So, in this case, selection and deselection. We've got driving directions from the MapKit JS Service. We've used polyline overlays to render these driving directions. And we've added our own behavior for the selection of overlays since there is no specific behavior provided by default. So, there is plenty more to talk about MapKit JS, but we only have one hour. So, I will stop here. But I hope that we have made a good case for MapKit JS, delivering a top-notch map experience for the web. That it lets you unify on a single map provider with a familiar and flexible API. So, if you have on your team people who have used the native MapKit API, this would feel right at home, because the same concepts apply. And web developers will be happy to see that the APIs are also tailored to match JavaScript. So, I will remind you that you need the MapKit JS Key to be able to use MapKit JS. So, please go to the developer portal, get your key, and try it out today. If you want to know more about MapKit JS, you can find the documentation and sample code at this URL. And if you want to know more about this session, you have this URL for the session itself. We have accompanying video that will take you step-by-step through getting and using a MapKit JS Key. And finally, we have labs happening on Wednesday morning and Friday morning, so you can come to talk to the whole MapKit JS team to ask us questions about MapKit JS. Thank you very much.  Hi, everyone. My name is Neil Desai, and I'm a watchOS Frameworks engineer, and I'm really excited to be here to talk about how to create some great audio apps for watchOS. And honestly, we've been waiting a little over a year to give this talk, so I'm really happy to be here, so let's just jump right in. In watchOS 4, we unveiled a revamped music and radio app. And in watchOS 5, we unveiled a brand-new podcast app. And we're finding that users absolutely love being able to play audio on the go or in the middle of a workout. And we're also finding that users really like to be able to remote control some playback that's occurring on another device. For example, your iPhone or HomePod. And I especially really like this feature. I love just being able to play some music out of my iPhone and then being able to control the volume, for instance, directly with my watch. That's a great feature. And today, with watchOS 5, we're going to show you how to build a great experience like you saw on the keynote like Audible was able to make and just like Pandora was able to make. And so they were able to use some of the new tools available in watchOS 5. And today, we're going to talk about how we can do the same sort of thing. And to do that, we're going to talk about four major themes. We're going to talk about our native controls and how we can now embed our system controls directly within our application. We're going to talk about, how do we get content over to the watch? And then, once we have that content, how do we actually play something back for our user? And lastly, we're going to go over the whole audio experience and how to build a really rich experience for our users that integrates well throughout the entire system. So let's talk about our native controls. In watchOS 5, we're unveiling two new controls that you can embed directly within your application. And the first is a Now Playing view which you can embed within your watchOS app. And this is really great for content that may not be yours, for instance. And you can also embed a volume control so you can build your own custom UI to allow your user to control the volume. And you might be wondering for that first view, that Now Playing view, when exactly would I ever want to allow my user to control some audio, but the content's not exactly mine? Well, let me give you an example. We do this ourselves in the workout app. So I'm in the middle of a HIIT workout. I swipe to the left. I get my power song, "Eye of the Tiger," and I'm a little bit of a dork, so I actually do listen to this song sometimes when I work out. I love it. And this is a great experience. You just swipe over, and you can allow your user to control the playback. And so if you're building a workout app, and you can do this, build the same sort of experience, just embedding the Now Playing view directly within your application. Let's talk about the characteristics of that Now Playing view. And the first thing I want to note is that the Digital Crown controls the volume. And because of that, you're going to want to place this view in a non-scrolling controller. So for instance, it's, in my app here, I've actually just pushed directly to a controller that has the Now Playing view in it. Or you could reload directly to a page that has this controller or place it in a page interface similar to the workout app. And this view is going to automatically switch the sources on behalf. So no matter if it's the iPhone or the watch that's providing the source of audio playback, this view's going to take care of it for you. You don't have to worry about that. And lastly, the Now Playing view, it's going to grab your application's tint color so it fits within the look and feel of your application. Now, to use this, let's see how it looks in Interface Builder. Here we are in Xcode, and I've selected my Object Library, and then I can just drag and drop my Now Playing view into a controller. Once my user navigates to that view, everything will be done for you. The system will take care of it all. And it really is just that simple. So that's our Now Playing view. Now, let's talk about our volume control. So the volume control can be set up to either control the iPhone's volume or the local watch's volume. And just like our Now Playing view, we're going to grab your application's tint color and apply it appropriately to this particular control. And in the case of when the user is actually scrolling with the Digital Crown, then, in that case, the volume control will be tinted to the system green color to match the rest of the system. So for the volume control, to actually decide when or not to control the local or iPhone's volume, there, in the Attributes Inspector is a check box for Controls Local Volume. So if you leave it unchecked, then the volume control will control the iPhone's content. And if you check it off, then the volume control will control the watch's content. And so if your application might have multiple scenarios where it could either remote control some content or play some content locally, then, in that case, you might have different controls with different volume controls within them, and you make those decisions at design time. All right, so those are really our system controls, so now let's talk about, how do we actually get content over to the device itself? So I've been building a museum guided tour type application. I want to allow my user to just tap in, listen to a guided tour while they're in the museum, and it's a great experience on the watch because I can just put my phone away. I can just have the entire experience on my wrist and enjoy the beautiful art around me. And let's say I had a, like a buffer that allowed my user to download the content on, when they're on their way to the museum. In that case, I would want to use URLSession and grab that directly from my server, all my content, and download it to the watch itself. So let's jump into some code and see how we could do that. So here we are. I have an IBAction which is tied to my WK Interface button. And the first thing you'll notice is I set my frontmost timeout extended property to be true. And we're going to talk a little bit more about frontmost later, but the thing I wanted to mention is that the user has made a user interaction, so it's fair to say that the user, next time they raise their wrist, would want to see our application. And because we're extending the frontmost timeout, then we're getting some additional priority by the system for our URLSessions. And again, I'll talk a little bit more about that later. And the user can, of course, put their wrist down at any point, so I want to make sure to use a backgroundSession. So that way, when my application goes into the background, my download is still occurring. And then, I can just download that task and appropriately resume it. And when you're building an experience like this, you want to indicate download progress and, of course, handle any errors. So let's look at how to do that. So here we are. I have my class, my DownloadManager, which is my SessionTaskDelegate and my URLSessionDownload Delegate. And in the first function, I can actually grab the total bytes written and divide it by the total bytes expected to write. So that way, I can get the progress of my particular download whenever URLSession calls me and gives me that information. And when I do this, I can just update progress for my user in my UI. And then, when the download completes, I can, of course, manage the file, and I might want to post a notification alerting my user that there's a playback opportunity. And, of course, I'm also going to be called into a didCompleteWithError, so I just want to clean up appropriately and handle any errors at that point, if there are any. And that's wonderful, URLSession, for when we can download directly to the watch, but sometimes the content might already exist on our iPhone. In that case, we can just use watch connectivity and transfer the file directly over to our watch. And to do so, we'll just use the transfer file API. And new in watchOS 5, you can get the progress of that transfer. So let's take a look. Here we are in code, and I see my WCSession FileTransfer. And new, there's a progress property that's available in watchOS 5 and iOS 12. And then, I can just get my fileTransfer, query for the progress, and update my view on iOS. And if you want to know some more about watch connectivity. There's some great sample code that's available online. Highly recommend you check it out, the simple watch connectivity sample code. And you might be wondering -- there's URLSession, there's watch connectivity -- which one do we use, exactly? And when does it make sense? Well, for URLSession, we want to use that when it's user initiated on the watch itself. And then, when we do that, we want a query the waits for connectivity property to know that we have a network connection at that point. And I just wanted to mention that if the iPhone is nearby, then requests are proxied through the iPhone when it's in range. And if you want to know some of the nuances of URLSession, there's a great talk from last year that's available online, Advances in Networking, Part 2. I definitely recommend checking it out. And then, on the flip side, there's watch connectivity, and that's really important when your user flow might be when it's initiated on the iPhone itself or in the case where you've already downloaded some content and there's no need to request it from your server again. And for both and especially for watch connectivity, you're going to want to set expectations for your user. Make sure to instruct them of when the download, or the transfer's most likely to complete. And that's going to be when the watch is on the magnetic charger. And for example, we do this ourselves. In the Apple Watch app in the Music pane, at the top, we actually show our user how many songs have actually been transferred over to the watch itself, and we even instruct our user, hey, music's going to sync when the Apple Watch is on its charger, so keep that in mind. All right, so we talked about getting content, and now that we have it, let's talk about actually playing something back. And on watchOS, we have a bunch of different tools we can use. And some of those audio tools are in WatchKit itself. So there's a presentMedia PlayerController with options, completion that you can use. And there's also WKAudioFile QueuePlayer. And so you can give it a list of items, and then the queue player will appropriately play back that content. So the presentMedia PlayerController option will present the system UI. So the controls are taken care of for you. There's not really much you have to do. And this is perfect for when it's short-form content and you don't really want to have to think too much about it. And WKAudioFIleQueuePlayer is great for when you have more long-form content that you want to play in the background. And this is more of a conveyer belt type approach where you have some items, you just want to hand it over to the system, and the system will play it on your behalf. And that's exactly what WKAudioFileQueuePlayer is for. And there's also AVFoundation at our disposal. So there's AVAudioPlayer and AVAudioEngine. And prior to watchOS 5, it was tied to the workout background mode, or when your application was foreground and screen on. For example, Nike Run Club app actually did this exact thing, where you're in the middle of a run, you want some mindfulness, you can play some guided meditation with Headspace. And prior to watchOS 5, background playback was, with AVFoundation, was really only possible for these types of workout apps. But new in watchOS 5, there's a brand-new audio playback background mode, which I'm really thrilled for. So let's talk about what we have now at our disposal. Kind of broadened our tool kit. And so we have AVFoundation, and it doesn't have to be tied to any other background mode, and we can now use just AVFoundation directly in the background. We also are exposing the ability to allow you to, allow your user to pick a different route. So maybe some different types of Bluetooth headphones. And new in watchOS 5, we're also exposing the MediaPlayer. framework, so you can provide some Now Playing information and also handle any media remote commands. And just like our own Music, Podcasts, and Radio app, the long-form content is restricted to Bluetooth routes, which really is just a fancy way of saying you can't play it through the speaker. So let's take it step by step and talk about how we actually get this background long-form content to play. So the first thing, you add audio to your info.plist background modes. And then, you just appropriately set up your session, and you're going to want to tell the system that you want to play this long-form content in the background. You want to allow your user to select a different route. And then, you're just going to want to call play and appropriately handle the playback when the user's in the middle of that playback session. So let's break that more, break that down more a little bit. And so in Xcode in the Project Settings, in the Capabilities tab, you just flip the switch for Background Mode, you check off Audio, and you're good to go to the next step. And so to set up your session, you're going to want to set your routeSharingPolicy to longform on your AVAudioSession. And then, you call the new activate withOptions completion method on AVAudioSession. And then, once it completes, you're going to call play. And in between when you first call the activate function and then when the completion comes back, what we call the route picker is going to get displayed on your behalf. And so this is what the route picker looks like. And so this is going to be showing up within your application, which is going to allow your user to pick whichever Bluetooth headphones route they want to have and control their playback. So let's talk a little bit more about that route picker. Again, all it is, you just call your asynchronous activate function, which is going to call and show your route picker, and then it's going to come back into your completion, and you handle that, and you just call play. And you're going to want to do this every time you want playback to occur. So that way, you can ensure that you have appropriate route to cause some playback to occur. All right, so let's jump into code and set up our session. And so we have our AVAudioSession, and then we're going to set our category to playback. And then, we're going to set our mode to default. And then, most importantly, we're going to set our route sharing policy to longform. And then, you want to activate your session, which is going to display the route picker, and then you're going to call play appropriately. And now, for that route picker, I wanted to call out that we really think of routes in two different ways. And the first way is our Apple wireless chip-based routes, which are commonly referred to as our W1 routes. So these are our AirPods or Beats Studio Wireless. And then, we just have our normal Bluetooth headphone routes. And it's important to make that distinction when we talk about the route picker itself. And so if the user has an active connected route, then the route picker's going to automatically select that route on your behalf. What's great is that the route picker might not even display to your user. You're just going to get called back in your completion immediately. And in the case where the user has an active W1 route but it's being used by the iPhone, then the system's going to grab that route from iPhone and bring it directly to your watch. And there's some cases where that won't happen. It's really just when your iPhone has more priority. So for example, say I'm in the middle of a phone call on my phone. Then, we're not going to take that route and put it on your watch because phone call's probably pretty important. And if there are no active routes, then the route picker will just appear on your behalf. All right, so that's our route picker, and now let's talk about, how do we actually get some playback to occur? Well, you just use the same AVAudioPlayer and AVAudioEngine. And there's a variety of different formats that are supported on watchOS. And what's great about this is if you have iOS code that uses these classes, then you could have a shared framework between your iOS and watchOS playback operations. And in the case of AVAudioEngine, you can actually play some DRM content in conjunction with AVAudioPlayer node. And so you can now play your own DRM content, decrypt it yourself, and play it back for your user. An especially important thing to note for watchOS is the power of the device itself, so only play audio when absolutely necessary, when the user actually wants the audio to occur. And for AVAudioEngine, the autoShutdownEnabled property is enabled by default, so if there are no active nodes on your AVAudioEngine, then we're going to automatically power down your AVAudioEngine when necessary. And so that's our playback. And then, we, let's talk about our MediaPlayer. framework. So we want to, of course, provide our Now Playing information to the system. And then, the Now Playing UI will update, and that's the system-wide notion of what's playing, and it's going to be updated based on the information you provide. So this is going to show up in the Now Playing app, and even inside apps such as Workout, or in the cases of any third-party apps that embed that Now Playing view. This will also show up there. And then, with the MediaPlayer. framework, we just want to handle the events, any type of media remote commands -- for example, a Next Track on AirPods, for instance. All right, let's jump into some code and see how we can provide that Now Playing information. So here we are. We have our MPNowPlaying InfoCenter. We just want to grab that. And then, we set up our nowPlayingInfo. And then, we have, we want to set our artwork, our MPMedia ItemArtwork. And it's not currently available in seed 1, but it's coming in seed 2. And in this case, the artwork that you provide will display on the Siri watch face. And then, we just set up, in this case, our title, album, artwork, and then we set it to our nowPlaying InfoCenter. And then, when we do something such as this, then our data will appear in the Now Playing app. And this is the actual Now Playing app. And I provided some, a song that I was currently listening to in my application. And when we do this, then, also, the Now Playing complication will also get updated. So in this case, the bottom shows the "Lights in the Sky" song that I was listening to. And then, for media remote commands, again, just use the MediaPlayer. framework. And now, in watchOS, you can handle those commands however you wish. So let's look at some code. So we just grab our MPRemoteCommandCenter. And then, instead of doing a Next Track, let's talk about how we can actually skip forward for our user. So I have this function. It's a enableSkipForwardCommand and, just by default, takes in 15 seconds. And I set my preferred intervals on my skip forward command. And then, I just add a target and selector, and then I just enable the command within my remote command center. If you want to know more about the media player framework and especially these media remote commands, I highly recommend checking out the MP remote command sample that's available online. And in this case, when I actually set my preferred intervals, then the system's Now Playing app will appropriately get adjusted. That Now Playing UI, wherever it is, will show the minus 15 seconds or plus 15 seconds, and that's all done on your behalf. All right, so that's the audio experience on watch, or that's our playback, so let's talk about our audio experience. So there's 4 major things I want to talk about first for the audio experience. There's our auto-launching feature where your app will automatically get launched. There's the frontmost app state and how we can take advantage of that in our application. There's our new interactive notifications. And how to integrate with Siri Shortcuts. And so we've been talking a lot about playback occurring on the watch itself, but I want to take a step back, and I just wanted to talk through that remote control case. So let's say I was playing some Apple Music on my iPhone, and automatically the Now Playing app will get displayed on my watch. And I love this feature. It feels like magic when I just look at my wrist and I can easily control my playback. I, it's the coolest thing. And in the case where it's our application that's actually causing the playback to occur on iPhone, then our app automatically gets launched on the watch. And we call this the auto-launch audio apps feature. And so whenever the Now Playing session is occurring on iPhone, then our Apple Watch app will be brought frontmost at that time. And we're going to stay frontmost for the duration of that session. So as long as the user is listening to content on iPhone, then our app will stay frontmost, unless the user decides to navigate away. And new in watchOS 5 is we're exposing a Now Playing session API where you're going to know when you're launched for this Now Playing session on iPhone, and so you can take your user directly to the appropriate view. And to do this, you're going to use the handleRemoteNow PlayingActivity on your WKExtension Delegate. So let's take a look at some code and see how we could use this. So here we are. I have my extension delegate, and then I can implement my handleRemoteNow PlayingActivity. And the first thing I want to check is, where is my user at that time? So I can just grab the visible interface controller, I can check where my user is, and if the user already is at my Now Playing UI, that's great. Let's leave them right there. But in the case they're not, we can just reload directly to our view. So that way, we have that same magical experience where I raise my wrists and I see the remote controls that I want to use. And in some cases, you might want to opt out of this feature if it doesn't make sense for your user. So just do the right thing. Make that decision. And let's say our app has nothing to do with our phone app, and that's an appropriate use of actually opting out of this capability. And so to do so, use the auto, the opt out of auto-launch feature on your WK extension's info.plist. And in the case you do opt out, then the Now Playing app will show instead. And now, let's talk about that frontmost app state, and let's give a little bit of a refresher of that. So here I have my app. It's foreground running. I'm looking at it. The screen is on, and this app is considered active and frontmost. But then, let's say I put my wrist down. In that case, the application is still considered frontmost because if I raise my wrist again, I'll see that application. And in that case, the application is just in the background. So with the frontmost app state, we get some enhanced capabilities from the system. Our background transfers from iPhones -- so say we were transferring our content from iPhone to watch itself, those background transfers, when they complete, the resumes will directly just wake up our app if we're in the frontmost app state. And the same thing in, with our URL session resumes. So if the URL session, our background session before was downloading and it completes, then our app will automatically get waked up and deliver the payload. And if we're frontmost, we're going to get called into, so we can decide whenever, if a notification appears, how we properly want to handle that notification. Maybe we want to keep our user in the app, or maybe we want to show the notification to our user. And in the frontmost app state, we can make that decision. And when you're in the frontmost app state, you can play some haptics. And if you want to know a little bit more about frontmost app state, there's the talk from last year, The Life of a watchOS App. It's a great talk. You might recognize the speaker. He's okay. And so you're going to stay frontmost for 2 minutes, but, if you recall, I extended my frontmost earlier. And so in that case, it's just going to be extended by default to 8 minutes. And so, again, this is great for when we were downloading that content from before, and so I can just extend my time out. And when we're actually playing back some audio, our app will be kept frontmost for the duration of that session, just like when we were actually remote controlling the audio when it was occurring on iPhone. And again, if the user ever navigates away, then you're no longer frontmost, but your background playback will still occur appropriately. And in that case, you can, of course, properly handle all of your background events. All right. So now, we're -- and when we are in that frontmost state, then we also can integrate properly with notification. So we could post, for example, when the download completes, a content available local notification. And we might want to have play as our primary action. And with our new interaction notifications on watchOS, we can allow our user to actually configure some playback right from within the notification and then be able to call play as the primary action, then get into your app and immediately cause some playback to occur. So for example, say I had a Salvador Dali exhibit. It's ready to go. I can then adjust the playback speed, immediately call play, that'll launch my app, and then I can play that great content at the appropriate rate for my user. If you want to know more about some interactive notifications, check out the What's New in watchOS talk given earlier, which is available online. It was a great talk, and you'll know a lot more about interactive notifications after watching it. All right, so do you remember this code from before where I actually posted that notification alerting our user of this type of playback opportunity? Well, let's say our app was frontmost at that time. In that case, when I got delivered the notification from frontmost in user notification, I'm going to get called into and will present notification with completion handler. And in that case, I can actually decide whether or not I want to still post that notification. Or maybe because my app's frontmost, I could just play a haptic and let my user know that, hey, raise your wrist, and there's some new content. I can update my view. And again, if you want to know some more about the frontmost notifications, check out the Life of a watchOS App. Great, so that's how we used our interactive notifications and the frontmost app state, but we also want to show up on the Siri watch face. And with audio apps, you can provide suggestions of audio to play on the Siri watch face using relevant shortcuts. So for example, that could be the next episode of your user's favorite podcast or the new music that you just synced over to the watch. For example, Audible was able to build the same experience where they can just get their user directly within their application from the Siri watch face and resume the playback of an audiobook. And whenever a user interaction is occurring, just wanted to highlight, think through your donations. Make sure to donate your INMedia PlaybackIntent to the system so that way the system knows what your user is commonly doing on the watch. And that way, you can become more relevant on the Siri watch face. And also, just think through your shortcut phrases like design appropriate phrases that your user could use within Siri to get your user directly within your application to allow for some playback to occur. And all in all, on the Siri watch face, design a great experience. You might have some glanceable information like, hey, there's an exhibit coming soon. Or you could have some tappable actions like Audible was able to make where, hey, we just want to play that audio tour and get our user directly in the app. And so for shortcuts, donate your INMediaPlaybackIntent. Use the relevant shortcut API to appear on the Siri watch face. And think through your shortcut phrases. And there's been some great talks this week about shortcuts, and I definitely wanted to highlight the Shortcuts on the Siri Watch Face talk. I saw that myself. It was a great talk, and I learned a lot. And so in summary, for the first time, you can embed your native controls directly from within your application. You can allow background playback to occur however you want and handle media remote commands however you want. And there are better ways that transfer audio assets to your watch and convey that type of progress to your users. So if you want some more information, feel free to go online or join us in the lab this afternoon. I'm really excited to see what type of audio apps are built on watchOS, and thanks, everyone. Have a great WWDC. Thanks.  Good morning. Good morning, everyone. How's it going. It's nice to see everyone today. Hopefully you've enjoyed the conference this week. We're really happy to be here to start off the last day. So, my name is Jon Dascola, and later I'll be joined by my colleague Heena Ko. We're both designers on the human interface team here at Apple. And we're going to talk about designing great notifications. So, this year in iOS 12, we introduced a lot of great feature to really enhance the notification experience, both for you and everyone that uses your apps. And we think these features are really powerful, and could create a meaningful, but also mindful experience for everyone. But, before we get into what's new, I think it would be helpful to take a trip down memory lane and see how the notification experience has evolved over the years, because knowing how we got here will help us make a better future. So, when iPhone launched, every alert was a blue box, model alert. Do you guys remember this? Every notification, every message, every invitation, it was an interruption that needed to be dealt with immediately. Nothing would be on the lock screen, so at times, it was pretty cumbersome when you'd unlock the phone, right? Whatever missed messages you received were backed up and would appear a single box at a time. So, you could either ignore them, one at a time, or if you wanted to reply, you launched into the app to take action. Then, in iOS 4, as more apps were sending push notifications, we started to queue them up on your lock screen. So, now people could see all of their missed notifications at a glance and choose the specific one they wanted to interact with. A swipe on that row would launch into the app but all the other ones would be dismissed from the lock screen. And they all went up to notification center. And that was a way to access all the other things you had missed. So, because so many notifications were being accumulated, this became a really useful place to go and to see all the other activity on your phone. Next, we introduced rich notifications. As the quantity of notifications was increasing, we wanted to increase their quality. Rich notifications are a great way to provide more context and information around each notification. That way you can see one, understand what it's for and get the extra information that you need to take action. And it all happens right in line. You don't need to launch into the app and lose the context of what you're doing. And that leads us to where we are today and all the work that we've done to enhance the notification experience in iOS 12. So, we're going to talk about some features that make the world better for you, as app developers. Ways to make your notifications more valuable, useful, and organized. And then, we'll talk about how those features can create a better experience for everyone. But before we get going, can we take a moment and be candid? Can we talk about the current state of notifications? Maybe not always the greatest. You know, people are getting a lot of interruptions. And they're not always for the right reasons, right. And we have way more apps than ever before. And they're sending more notifications than ever. And because people are seeing so many notifications, it's so important to make sure that we're creating a really great experience for everybody that uses your app. Because the best notifications, they're for connecting and communicating with people. I mean, that's what makes iPhone great, that real human connection. And then, you know, we have social notifications. I totally understand how these are important, right? People want to stay on top of their digital lives. Everyone likes their likes. And then there's even the occasional notification about new sneakers or something, that could be super fun and really useful. But not all of these things all of the time. Again, all things in moderation. So, as more apps are continuing to send more notifications and taking more of our attention, we really have to make sure we're doing the right thing. We need to remember that the best notifications are for connecting people and delivering meaningful information. Now, I know none of you here would ever do any of this, but there are some app developers that maybe go a little bit overboard. And this year, we've created a UI that makes it easier for people to control how their notifications are delivered. And there's even an option to turn off all the notifications from an unhelpful app. So, in iOS 12, when you receive a notification on the lock screen, you could swipe left and access some actions, and you'll notice this year, we've added a manage button. Tapping that brings in our newly designed quick tuning UI, which allows you to configure how the notifications from that app are being delivered. So, at the top, the first option is to deliver quietly. And these quiet notifications will be send directly into your notification center without interruption. It did not appear on the lock screen. It did not play a sound or haptic, or present as banners. And I think this is a really great option for an app that's sending you notifications that you want. Content that you're interested in, but you don't necessarily need to be interrupted by for every new post, right? So, you can access these notifications now on your own terms. But also, right here in the card is a button to turn off all notifications from that particular app. And we think having that shortcut is really handy. But having to use it, it's not a great experience for anyone. We don't want people to have to turn off notifications because they're annoyed or frustrated. And as developers you don't want to lose the privilege to reach out to those individuals, right? So, by the time we're done here, we want to make sure that you have all the tools and information you need to make sure you're sending only the most meaningful notifications that you can. Because if you send great notifications, you'll be, making everyone who uses your app happy. And if you've got happy people using your app, that's going to make you happy. And if you guys are all happy and they're happy, that's going to make Heena and I real happy today. And being happy is good, right? So, let's get into it and talk through the notification journey and how to create the best notification experience. We're going to start with the first run prompt, in that initial agreement and talk about all the best ways for you to make sure your notifications are allowed and delivered. Then we'll talk about the best ways to provide value with all the notifications that you're sending. Better ways to organize your content with notification grouping. And finally, how to make the most of rich notifications to create a really useful and holistic notification experience. So, let's get started and talk about that first prompt in asking for permission to send notifications. This is a familiar screen, right? It's your one and only opportunity to receive permission to send notifications. And it's an important moment. And you should remember that you're asking someone to make the difficult decision here. I mean put yourself in their shoes right. They just downloaded your app. They're excited to give it a run for the first time. And then all of the sudden, they're interrupted. And then they're asked make this decision about receiving notifications. How are they supposed to know, right? Especially if it's presented without any context and you're not letting them know why notifications are valuable, and if they've really had any time to experience your app. I mean why should they tap allow? So, this happens, right? They don't. And that was it. You had one shot, one opportunity to send notifications. And that was your only way to ask for permission. Well, in iOS 12, that's no longer the case. We've created a new feature that allows you to send notifications directly and only into notification center, without running that initial prompt. It's your choice. You can continue to show the prompt and ask for the privilege to send notifications to the lock screen with the risk of being denied. Or, you can choose to deliver your notifications quietly and directly in the notification center. So, they'll appear like this in your notification center list. And if we look a little more closely, you'll see the content within the notification platter is presented as it always is. Your information comes through as a normal notification. But at the bottom, we extend down the platter and we add some buttons. Now the prompt is integrated within a notification. It's less disruptive. It's a non-model experience. So, you won't be stopped when you're in the middle of using that app and being asked about notifications. You see the prompt right when you're looking at your notifications center. And the prompt now provides more information. It's presented with in an actual notification. So, I can see what sort of content the app will provide. And now, I'll have a better understanding of their purpose so I can better judge their quality, and be better informed to make that decision. So, let's look at some different categories of apps and talk about how they might go about requesting permission. So, we have social apps, news apps, games. And let's focus on a news app for a moment. Let's imagine we're creating a new news service from scratch and we're working out our notifications. Well, should we go to the directed notification center route? I mean that seems like a good idea, right? I mean there's a group of people out there that are interested in news. Maybe they're curious about what's going on in the world. But they don't feel the need to be interrupted for every single article that's posted. You know, especially if they've just downloaded the app and they're unsure of the content they might receive. I think the directed history route makes a lot of sense. But, you know I also see a very clear use case to ask for lock screen access too, right? People love breaking news. They want to be on top of what's going on in the world, as soon as it happens, right? I get it. So, what do we do? Well, if we start with the new world and talk about the cases that could be sent directly to notification center. So, if your app is sending content that can be consumed passively, and doesn't require critical or timely responses, then I think the directed notification center route is your best approach. It ensures that your content is delivered. It won't interrupt people. And most importantly, it gives them an opportunity to see your content and try your notifications before they have to commit to it. So, if our news app is sending longform articles, I think that makes sense, right? Or, social apps. I think they can deliver their likes and comments quietly. Games that are sending out promotional notification. Examples like that, I think they make sense to go direct to notification center. Now let's take a moment and talk about the traditional route. What sort of use cases make sense here? Well, if your apps need to deliver notification content immediately, if people need to see your notifications the moment that they're posted. Or your app requires an urgent response to notifications, ask for permission. You know, maybe that news app is breaking news. Or your social app has a big messaging component, or games where you need to see your friends online. If you've got an app like that, then go for it. I think that makes a ton of sense to ask for lock screen access. But if you do, there's a few things to keep in mind while you're designing that experience. Because if you do create a great experience here, I think it will greatly increase the probability that your notifications are going to be allowed. So, don't prompt or send this alert as soon as people launch your app for the first time. You know, give them a moment to experience your app, what you've built and what you have to offer. And find a place within your app to explain why your notifications are going to be valuable. Let them know why you're sending notifications. And more importantly, why they need to appear on the lock screen. And finally, that prompt, well post it at the right time. Present it when someone understands the reason why they'll be receiving notifications from your app. Like if you're a delivery service, wait for someone to complete their order and explain the notifications will update them on its progress. Or if you're a travel app, notifications will be for flight delays and gate changes, right. Because in either scenario, with the direct to notification center route or this one we want you to have the best opportunity to deliver your notifications. So, that's what's new when asking for permission to send out notifications. Remember, it's up to you. Both approaches are valuable and both should be considered. It's a decision that you're going to need based on your app and your app's particular content. So, now that you've taken all the right steps to make sure people will be receiving your notifications. Let's talk about how to make sure you're setting the best content in making notifications like extremely valuable for everyone. And why is it so important that we're sending out great notifications? It's because our attention is valuable. Our attention is precious. Right? Interrupting people, it's a real privilege. And we need to respect that. I mean you've all felt this right? You're trying to focus on something, you're in the zone. I mean maybe it's happened this week, you're updating all your apps to adopt these great new APIs and then all the sudden [ding]. You're interrupted. And you stop. And you lose focus. And when the wrong thing comes at the wrong time it's frustrating. So, you've got to be extra considerate when we're sending notifications. First off, we need to make sure we're sending great content and providing the highest quality of information. Everything you said in a notification has got to be meaningful. Every notification should provide a specific purpose. Each notification should have a specific message to communicate or a task to complete. Notifications are not just a reason to get people to launch into your app. So, Dark Sky, it's a really lovely weather app that sends particularly meaningful notifications. See, I think the interesting thing about the weather is you really need to know about the weather once it's different. You know, if there's a change in conditions. And that's when Dark Sky sends these notifications, right? Rain is starting soon. That's what I need to know. Just really smart. Door Dash is another great example. You get your notification when your food arrives. I mean everybody gets excited when their dinner is here, like what more do you need to know? It's perfect. And HQ does the right thing with their notifications. For a live game that requires you to be online at a specific time, a notification like this, it makes a ton of sense. So, what do they all have in common? Well, all those notifications, they served a clear purpose. They were presented for a specific reason with real information. They weren't just empty invitations to launch into your app. So, now that you're sending out really great content. It's important to consider when you're sending that content. So, be considerate when you deliver notifications. And reminders is great. You can choose whether you want your notification to be delivered by time, or by location. So, you get the appropriate content whenever you need it. And Headspace is great, they're one of my favorite apps. It's a meditation app that really helps with focus and clarity. So, I think it's appropriate that they let me choose when a reminder comes through. Because it would be terribly ironic if they were the ones sending me frustrating notifications at inopportune times throughout the day. Headspace is really great. And here, I love this screen with CNN, right. After you allow alerts, they ask you to choose the alert frequency. So, it's up to me to decide how often I would like to receive their content. It's such as smart way to manage and configure your notifications you know. Because as a user, now I have some expectations around what that notification experience will be. And here's a notifications from Duolingo. So, I was trying to learn a new language. I got a bit distracted here working on this presentation, so I missed a few sessions. And rather than continuing to send me notifications that I'm not interacting with, they decided to pause them. And I just think that's really considerate. Because I could totally understand the argument right, why they might want to send me more notifications at a time like this and really encourage me to get back and re-engage with the app. But with the new tuning features we announced, if I get annoyed by those notifications, I can easily turn them all off. So, for them, doing the right thing in the short term, while it might seem like the harder decision to make, I think it's going to pay off in the long run. So, in all cases, it's really important to consider how and when you're delivering notifications. And finally, we want to make sure your people are in control of the content that they're receiving. I mean there's a real trust relationship that's created when someone allows notifications from your app. They're making space for you to interrupt them with your content, right? And as we've discussed, it should be really valuable. But not all things have that same level of value or importance to all people. So, let's talk about giving everyone control on how their content is delivered, right? So, we've redesigned settings this year to more clearly visualize the different delivery methods of notifications. We hope that with more clear and graphic UI, it will make it easier for people to adjust how they're receiving your different notifications. And if you notice at the bottom here, we've also added a link to your third-party custom notification settings. And providing that link is even more important now because of our new tuning UI. Because if someone decides to tap turn off, we don't just immediately stop all the notifications from that app, but we present a confirmation step. So, in this action sheet, you could choose to turn off all notifications from the particular app, or tap a button, to go into that app's particular custom notification settings. And again, we want people to be in control of the types of content that they receive. So, while we hope that turn off everything button is a last resort, we want to encourage people to get into your app settings and customize the notification categories you have in a more granular way. So, with this extra visibility you know it's really important to make sure these settings pages are well-designed. So, let's look at a few examples. ESPN is an app with really detailed settings. And they've done a great job with their design. There's like a great sense of hierarchy and all of their content is tailored to my interests. And when I drill down and see the screen, they have specific details for each type of notification that you can receive. You know, basketball is different than baseball. Right. I have all the controls that I need to receive exactly the right notifications from ESPN. And "The New York Times" app is great. I have the options to configure the different categories or different sections that are alerting me. You know I might not find sports or politics helpful, but I can still receive breaking news and top stories. And I think this is really smart for "The New York Times" to have implemented. Because not everyone may appreciate interruptions every single category that they offer. So, for allowing this topic level control, it assures people to configure the experience to their liking without resorting to turning off all notifications from that app. So, it's really important, this set of control is supported in your app in a well-designed way. All right. So, those are the important things to remember when creating notifications. Now, you're going to be sending out some really valuable information. The next thing is to talk about notification grouping. And this is a new feature in iOS 12 to help everyone keep their notifications, you know all those great, and valuable content you'll be receiving, more organized. So, in iOS, we've always appreciated the convenience of seeing notifications on the lock screen. The chronological list is really helpful to organize. And it's great to see the content of notifications without having to interact with the device. You just pick up the phone and everything is there, it's all visible. But you know, there gets to be a point where that long chronological list starts to break down. Whenever there's a lot of different content coming on the screen, right? Especially multiple messages, or if you have group chats happening all at the same time, it's really difficult to follow. So, that's when we decided to start grouping notifications. Now, all your related content is organized together. A simple tap on each group expands it open and you can interact with each notification individually. And by default, notifications will group together by app. And for most cases, that make sense. But you know there's some circumstances when sorting them out in a more detailed way can be helpful. And messages is a perfect example. I don't think it would be the most useful thing to see every missed message from every conversation thread and group chat you're having all lumped together into a single group. So, to more clearly organize everything we create a new group for each conversation. And we call each individual group a thread. So, let's take a moment and talk about notification threads and what the best ways may be to organize your content. So, notification grouping uses the existing thread identifier API. This API was introduced as part of the notification content extensions. We expanded its use to create notification groups. The thread identifier can be any string that you want and the notifications with the same thread identifier are all grouped together. That's it. Super simple. So, threads make a lot of sense and you have separate conversations to group together. But what about other types of content? How should we handle grouping in those cases? Well, let's look at news. Each source is broken out into a separate thread. You can see there's a group from "The New York Times." A group for Quartz, and a group from "The Washington Post." And it's a really helpful way to find and organize content delivered from the news app. And let's look at another example. Podcasts. They really do the smart thing with their notifications. They create a special thread that groups together all of your new episodes. And what they do is they resisted the temptation to declare each individual Podcast a separate thread. Because remember, threading is about consolidating and organizing your content. So, when scrolling through your notification history, I think it's a much better experience to see all new Podcast episodes in a single group, rather than a bunch of discrete groups separated out and mixed together amongst all of your other missed notifications. Because, if there's a single group, a tap will expand it open and reveal all of the related content. Everything is together. Everything is organized. And everything is easy to find. So, while threads can be incredibly valuable, it's really important to not create too many. They should be used to highlight and distinguish meaningfully different types of content. So, remember it's okay to leave the default behavior of grouping all of your apps notifications together. Often that's going to be the best experience for people to find and interact with your content. All right, so that's notification grouping, and I think we're doing pretty good so far. You know, people have agreed to receive your notifications now. You're respecting their attention. You're sending good content at relevant times. If it makes sense for your app, you're grouping your content together by a few relevant threads. We're moving. Now, we need to make sure the rich notifications are rounding out the experience. And it's just so important, it's really important to create useful rich notifications. Because as I mentioned earlier, rich notifications are a way to provide more context and information to each notification. Now, we want people to take action on them, without losing the context of what they're doing. I say, like every notification should be like a little self-contained package of information to allow me to complete a specific task. I shouldn't have to launch into the app to find value in a notification. And photos is a great example. I can see that my friend added a new image to a shared photo stream. And with 3D touch, when I touch on it, I can see the big, full-sized version of the image that was added. And below there's a description. And with the quick action buttons, I can either like it, or leave a comment right in line. I don't need to leave the context of what I'm currently doing to take action on the notification. I tap like. I swipe it away and I'm back to the lock screen. Calendar is another great example. I see I have an invitation to an event on my lock screen. And I use 3D touch and press on it, I see my availability right in the notification. Again, I don't need to launch into Calendar to see my day. And then there's some great quick options below. I can accept the invitation right in line. So, if we look at messages, with our new notification grouping UI, my conversation with Heena is all grouped together. Now, when I press on the notification group, I'm seeing all of that group's content together. All of the individual platters that were stacked in the group are consolidated together in this view. I can read the entire thread right here. And now, in iOS 12, we've added interactivity to the rich notification views. So, you can double tap on a specific messages bubble and access our tap back UI. So, I tap like, and I can send that acknowledgment right back to Heena. So, as we mentioned, if you've created individual threads for your content, you can also have a threaded or consolidated rich notification. So, if we look at Podcasts. All of their new episodes are grouped together here on the lock screen. When I press on that stack, I get a single rich notification that summarizes each of the new Podcast episodes that have been released. There's a custom design with each show's artwork. The episode title and a short description. And because we can have discrete tap regions, there are separate play buttons for each episode. I mean, I just think this example is great. It checks all the boxes for making a great rich notification. There's detailed content, nice images, custom controls, and rich interactions. It's a great way to finish the notification experience. So, to summarize, when it comes to that first run experience, you have the question, did you ask for lock screen access, or deliver quietly in the notification center. Well, I think it depends on your content. Is it timely? Does it require an urgent response? Then go on, ask for permission to send notifications to the lock screen. If you're sending passive content that doesn't require an immediate response, then delivering directly to notification center could be the right approach. But either way it's your decision to make. This should be based on the needs of your app. Next, we need to make sure we're really providing value with our notification content and sending just like the highest quality notifications. So, they need to be meaningful content. We should be sending specific information. Notifications are not just a reason to launch into the app. And we've got to have a well-designed settings and configuration UI so people can easily tailor their notification experience from within your app. Notification grouping it's a great way to organize your content. So, by default, all notification from your app will group together. And you can use the threat identifier to create threads, if you need more nuanced grouping. But remember, only create threads when necessary. Don't overdo it. And finally, rich notifications should be created to provide that extra bit of content around the notification. Each notification should be a specific task to complete. You can add images, video, audio, and custom content. And now, interactivity to create a holistic notification experience. So, I mean that was a lot, right? That was a lot to take in, a lot to do, a lot to consider. But, it's not everything. There's still something else to consider. And for a lot of people it's the most important. And it's definitely the most personal part of the notification experience. The Apple Watch. So, I'd like to invite my fellow designer, Heena Ko here, to talk about making great notifications on the Apple Watch. Thank you. Thanks, Jon. Okay. So, we just heard about the importance of notifications on the phone. So, why think about notifications on Apple Watch? Well, we consider Apple Watch to be our most personal device. It states unlocked on your wrist, so you stay connected. But because it's so lightweight, you can stay focused on what you're doing. And particularly for Apple Watch, notifications are really great and incredibly effective. They're glanceable and the interactions are lightweight. And great notifications, they send you valuable and timely information. Or, they can encourage you to reach your health goals. And in some cases, they tell you critical information, like if you've had an elevated heart rate. Notifications are an essential part of the Apple Watch experience. In fact, we'd go as far as saying that notifications are the primary way people interact with apps on Apple Watch. There's another important reason why you may want to think about notifications on Apple Watch, and that's because they can be pushed to either device. You see, we coordinate alerts. So, we send them to the device that's most accessible to you. So, if your phone is locked, it's in your bag, in your pocket, which is a lot of the time, then that notification gets sent to the watch. So, you want to make sure that that notification looks great in both places. Okay, so here's a picture I took on a hike on Mount Tam the other week. It's really beautiful up there. While I was going on a hike without my phone, I was able to get this notification from Dark Sky about an upcoming thunderstorm. So, now, with Apple Watch Series 3 with cellular, notifications on the watch are more important than ever. I can go hours without my phone. And still receive notifications and stay connected. Okay, so you may ask, how do I make them look great on both devices? Well, we try to make it as easy as possible for you by giving you some stuff for free. So, let's take a look. Take this notification from one of my favorite Podcast Apps Castro. When that phone notification is pushed to the watch, some elements come with it with minimal work. Like, the image attachments, as well as the title, the body, and any relevant quick actions. So, here I'm able to add this Podcast app to my queue from my watch, so it's right in my phone when I want to listen to it. By simply adding a few additional elements, the watch's notification experience is so much better. Okay, so there are additional ways you can customize your watch notifications. You can add a sash color. You can add a background color. You can add images, icons and even inline video to make your notifications more visually rich. Okay, and now if you have a WatchKit app you can create interactive notifications. Interactive notifications are a new feature in WatchOS 5. They allow for more interactivity right in the notification, so you don't even have to launch the app. We're really excited about this one. Okay, so here's a notification from a fictitious ride sharing app. You guys know this. We all get these after every single ride. And occasionally I'll open the app and rate the ride right after the ride is over. But sometimes, actually a lot of times, I forget to do it. So, now in WatchOS 5 apps can create interactive notifications. Here's one from DiDi, I ride sharing app. So, they've included the ability to rate and pay right in the notification. I just have to rotate the digital crown, tap the stars, hit pay, and I'm done. So, this is a great example of an interface that clearly communicates the purpose of that notification, which is to encourage me to submit a rating after my ride. Pay By Phone is an app that allows you to pay for your parking spot remotely. It's really convenient when I'm still really far from my car and I need to extend my time. Here's a notification from them, letting me know that I only have 10 minutes left on my parking spot. It allows me to extend my time right in the long look. I just have to rotate the digital crown, and I tap steppers, and that's all I have to do to extend my time. So, this is a great example of a quick interaction. I was able to extend my parking time with just a couple of taps. So, amongst my friends, I'm oftentimes the person that chooses the restaurant and makes dinner reservations. And because of traffic and weather, you name it, people are always running late. So, here's an interactive notification from Yelp, letting me know that my table is ready. And these new notifications, they allow me to extend my checking time and in this one, for up to 45 minutes right in the notification. So, we don't need to give up our table if people are running late. Okay, so rich notifications are particularly good for moments of quick data input. Here's a notification from a fictitious medication reminder app. It's reminding me to take my medication before the end of the day. So, it not only reminds me to take my medication at the right time, but it also provides a range of ways to respond. I can say I took a single medication, or I can tap the take all button to say that I've taken both. So, this is really great, because this is something that I had to do every single day. Okay, so what do these notifications have in common? Well, they were informative. At the same time, they're succinct. They were visually rich. These used images, videos, and icons to make the notifications much visually richer. They're actionable. I was able to accomplish a tone of things without even having to open the app. And lastly, interactions on the watch are best when they're quick. We're going to make notifications richer, but we don't want to recreate the app experience. Okay, something you can do to make notifications even more effective is to get to know your audience. And tailor notifications to individuals. It can make a huge difference in how they experience your app. My Weather is an adorable app that sends me forecast notifications every morning and it's customized to my location. I really love receiving notifications from Wallet on my watch. It's especially great, when you're at the airport, and you're dealing with lots of luggage. Or you're at the store dealing with lots of groceries, they're also really handy when going to concerts. So, here's a concert ticket I got from Wallet. It arrived right when I got to the venue. It also contained a full-screen QR code that allowed me to enter the venue and I didn't even have to pull my phone out. This is a great example of customizing the timing of when I received a notification. It arrived right when I needed it and provided me with everything I needed for the event. Okay, so Qantas Airlines has really great interactive notifications, and they allow you to share your flight time with that clutch friend that's going to pick you up from the airport. Here, my friend Gabriel has just sent me his trip information through the Qantas app. It includes his ETA, as well as the option to set up a pickup reminder, which I'm totally going to use. Later in the evening, I'll receive the pickup reminder, along with a suggested time to leave. And it shows me exactly where I can pick him up, as well as the options for directions and it includes the option to send him a message letting him know that I'm going to be late. So, must know that I'm in LA. This is an excellent example of customizing notifications along an entire journey. They utilize time and location as well as just simply providing helpful tools to make sure people have a great experience from start to finish. Okay, so, at this point, everyone here should be a notification expert. But we covered a lot. So, let's do a quick recap. Okay the first run prompt. Notifications are oftentimes sensitive. But if they're not considering sending notifications directly to notification center. You won't be interrupting people and folks can read them on their own time. Providing value and sending great content. Remember notifications are about making human connections and conversation. They're also about delivering valuable information. Consider delivery. Consider providing ways for people to customizing notifications and incorporate things like time and location when sending them. Okay, notification grouping. So, iOS and WatchOS will group notifications by app by default. And most of the time, that should be totally okay. But consider threading related content to have discrete meaningful groupings. People are really excited about this one. Okay, rich notifications. Consider creating rich education so people can accomplish more right in the moment. And last, but not least, consider notifications on both devices. You'll be providing a great experience in any circumstance. So, the next time people hear this sound [ding], people will be delighted because you value their attention, and sent them something really great. Thank you guys. So, for more information about notifications, check out these related talks.  Hello. Hi, I'm really excited to be here to talk to you today about The Qualities of Great Design. My name is Lauren Strehlow, and I'm a design evangelist here at Apple. The evangelism team, our number one goal is to help developers just like you create great apps. And a big part of my job is actually to work with the design presenters who are putting on all of the awesome design session content that you see here at the conference. So when I started to work with potential presenters this year about their talks, this theme of quality kept coming up, but nobody was talking about it head on, so I decided to. But I went about it a little bit differently. I decided to talk to great designers from our developer community, such as the creative director at The Game Band. I also talked to the cofounders of the Layers Conference, which is actually happening right now across the street. And I talked to the VP of Design at Khan Academy. I also talked to some of my great colleagues here at Apple who work in a wide variety of design disciplines. They work in type design, sound design, motion, visual, and interaction design. These are all real people who I truly admire and respect, and I think they have created great apps and games that have tremendous caliber. So I conducted a series of interviews. I talked to 13 people, and I asked them a ton of questions to discover what quality means to them, what challenges they face, and what they do to strive for excellence in design. So before you think any of this is scripted, I actually didn't give anybody these questions in advance. So everything you're about to hear today is real reactions to these questions. I collected over 15 hours of interview footage, and I've distilled it down to the very essence of these responses. So the audio clips you're about to hear today represent what I discovered about quality in great design. So today, we're going to dig into interpretations of quality and how it influences our perceptions and design directions. We're also going to talk about its effect on people's lives. [laughs] Actually, this is really exciting because it turned out to be so much more than what I expected. We're also going to hear about aspirations from the designers that I interviewed, to hear what drives them, what goals they have when they're striving for quality, even if those goals seem unattainable. And finally, I want to share some techniques that were revealed while asking people about their experiences. And I hope that these techniques can help you approach the challenge of designing for a lot of people. So this session is all about the qualities of great design defined by designers. So let's kick things off, and get started, and hear a few answers to this one simple question: What is quality? Quality is nothing else than what we agree upon is good. If something is quality, it implies that there is nothing random about it. The number one thing is just that something with a lot of care and time went into it. It's one of those things that people can feel it when they feel it, and it's very hard for them to put their finger on it. All right. This one I can really relate to. I'm totally a feeler, and so this naturally led me to ask a follow-on question: What does quality feel like? Does it feel like somebody has thought of you already and all the things that you need are easy to get to and very understandable? I think that when I'm thinking about things that are quality or handling things that are quality, it's things that aren't painful to use in any way, which could be, like, mentally painful, physically painful, emotionally painful, things that don't make me feel uncomfortable, or dumb, or [laughs] or inconvenienced. If you just launch the app and you feel, as you use it, oh, well, this feels like state-of-the-art technology that I'm using. This feels like it's easier for me to get things done. I feel more productive. I feel like I'm able to achieve better results. All the concentration that you're building up goes to the task at hand, the thing that you actually want to do. And that to me feels like quality. If you're able to achieve that, and people can be very creative, and make beautiful things, or have special moments through a device -- for example, by taking really good pictures, or sharing pictures with friends, or finding the right music to play at the right moment, or, you know, doing fun stuff with FaceTime, and seeing people on the other side of the world that you haven't seen in a long time -- all of that is very special. And again, none of that is around the UI that you're doing. None of that is about what interaction you chose to do that stuff. That should all be obvious, and blatantly unspoken, and just completely in the background. Wow. I just love that, that quality isn't about the UI or interactions, that it's about the moment, the people. You know, these are the people that you're designing for, and you're helping them create memories and share those moments with others. That is what this is all about. So how do you design for that? Well, I actually asked that question. It's hard. [laughter] Yeah. I hope you weren't actually hoping for a real answer here. It's definitely very hard, and we have 55 minutes to dig into why. So we just heard a ton of answers to this question, what is quality? We heard that it is what we agree upon is good, that it's not random, it's something that shows a lot of care and time went into it, and it feels like somebody has thought of you already. It can also be not painful in any way, and it feels state of the art. Quality things make it easier to get things done. And finally, your concentration goes to the thing that you really want to do, like sharing that picture or playing your favorite song. So by asking about quality, it resulted in many different answers, but what's so great about that is that it reflects what each one of these people truly care about. And of course, people care about different things, so it makes complete sense that they all have different interpretations of what quality is when it applies to apps and games. So we have a lot to get to today, but I really want to dig into just a couple of these responses. I think they're all great, but let's just dig into this first one here. And we're going to listen to the clip one more time. If something is quality, it implies that there is nothing random about it. During this interview with Nicole, she explained that not random to her meant not slapped together. And she used this word "considered." Great designs are considered. They're organized, and they show a thought process has taken place. And what I found really interesting is that she said that that's really visible to people, that that visibility of quality in design actually came up in another interview when I was talking with Mike. I guess this is probably a little clich sounding, but it's often the little things that are the telltale signs about craftsmanship. Okay, we have to pause right there because we're about to talk about craft in a design session, and Mike already called it out that it's a little clich, but before we listen to the rest of this clip, I need you to imagine that you're in the audio booth with us. There's not going to be anything on screen, so I want you to really focus on the words that Mike is about to say. Okay, here we go. We're sitting in this room here, and there's these panels on the wall, and the panels have a little bit of a gap between them. And we can look at the width of the gap as you look from the top of the wall to the bottom of the wall. And if there's variable separation or variable width of that gap, you just get the sense that it wasn't well crafted. Right. So really, while Mike was talking about those variable widths in the gaps of the panels in the audio booth, you know, I understood that he's saying that those details are visible, and they matter a lot. And it's through consideration and being really deliberate, and thinking through every single detail thoughtfully, well, that shows. And so being really into your craft here is the point because the best things that we love are not random. They're not slapped together. They're intentional through every detail. And if you can do that, that's quality. So let's explore one more answer to the question, what is quality, and listen to the clip again. The number one thing is just that something with a lot of care and time went into it. This one in particular really resonated with me because when you're writing questions to interviews like, what is quality, you naturally end up asking yourself, and to me, quality is all about care. So when I heard care connected to quality not only from that clip we just heard from Travis, but it also came up in my interview with Gary. If it's quality, it's better than something that's not quality by virtue of someone caring about it. Right. I totally agree that caring makes things better. For example, I care about this presentation, so, naturally, it's just a little bit better than if I didn't care about it at all. So now, I feel like we should dig into, what is care? Okay, we're going to stick with a few interview clips from Gary so you can hear more of the full conversation. So I asked him, "How would you describe care?" Just that someone, excuse my French, gave a [beep]. [laughter] I'm not sure if that's a WWDC first, but it's simple. I like it. [laughs] Really, what he's saying here is that care is a motivator. So how do you look at care in design? I look at it from 2 lenses of care. Did I care enough to make it the best it could be? But do I also care about what your experience is? Be it you. Be it my mum. Be it my next-door neighbor. Be it anybody around the world. You have to kind of, sort of take yourself out of the equation, put yourself in their shoes so you can be sure that their experience is a quality experience. So you have to care enough to do that. It's not just about caring about your own sense of, "Oh, I like my design, and I cared enough to make it good for me." It's about it being good for as many people as possible or for the target audience, if it happens to be a more narrowly focused thing. Right. So when I heard this, I was just, I wanted to know how, immediately. How do you design that experience for others, and how do you know when you've made progress? I think sometimes we look at a tangled mess of problems that we need to solve, and if, at the end, you can look at the result and go, that is so much better, and if it came easy, it wouldn't be as fun, but the fact that you sweated over it, that you really worked hard at it, that it gave you sleepless nights, that you poured late nights and weekends into solving those problems, combined effort, team effort, the end result is so much more satisfying. I love it when we work hard and get there more than when ideas just come easily. I worry that we've actually missed something by not working hard. If it's too easy, we've become complacent, and I don't think you get anywhere rewarding without doing hard work. I really understood this point about working hard after Gary related this to mountain biking. We talked about this for a while, so I'm just going to summarize that conversation here for you today. Essentially, you have to work really hard to get to the top of the mountain, and then you can have a ton of fun on the descent. Now, it's not as much fun if you just started at the top and went down because it's really all of that time, and effort, and sweat that brings you to the top of the mountain that makes the view all that more beautiful and makes the ride down so much more fun. So in life, care naturally gives more time and effort because you have a passion to achieve your goals, and you'll do whatever is needed to meet your own expectations. And yeah, that's totally a lot of work, but that hard work has a payoff, and it's really satisfying. And the payoff, well, that's why you did it in the first place, right? You have a goal, and you make it happen. That's just awesome. So quality is all about care, and because you care, you're going to put in that time and effort to make things great. And that's just one answer to the question, what is quality? And because there are so many different interpretations of quality, well, that means it's just really hard to achieve. A little bit later, we're going to get into some techniques to hopefully help make the process of designing a little bit easier. But for now, let's just recap what we've learned. So far, just by asking a few simple questions, we've learned that quality means something different to different people, that it's not random -- it is crafted and considered. Quality is the result of time, effort, and care. And it is just hard to achieve, but that's okay. Quality has many different interpretations. What is yours? I think this is really important to ask yourself, what is quality, and what does it mean to you, and what does it mean to your app or game, because that's going to reveal what you truly care about, and that's going to help you focus on what you're working towards going forward. So let's just pause here for a moment, and we actually need to go back a bit. Why does quality matter in the first place? Why are we even talking about it here at WWDC? Well, what's the significance? Let's just ask, why is quality important? Quality impacts the world that we interact with in so many different ways all the time throughout our day. Yeah. Travis is totally right. Throughout our day, there are things that we eat, drink, wear, I mean, even the air we breathe. They all have some level of quality that impacts us. And we expect quality in our friendships, in our family, in our relationships. We seek out people who show us respect, who care about us and make us feel good. I mean, nobody keeps around poor-quality friends. We also expect quality from the products that we use and the things that we use every day. We expect them not to break. And if they do break, well, we tend to think that they're cheap and not well made, and we typically won't buy them again. So yeah, the concept of quality impacts our every day, and it's really in everything. While it's really wide reaching and broad, there seems to be something so special about quality, but what is it? What makes quality special? They say, like, oh, I cooked this with love, and it can just be eggs or whatever, but it does taste different, and I think Jessie and I like to think that whatever we do, that sort of love for what we're doing comes through. Right? I know these two ladies absolutely love what they do, and I know I can feel it when someone really loves something. And to Elaine's first point, you know, I think everyone here knows that Mom and Dad's food just tastes a little bit better because it's made with that care and love. So what really Elaine means by this is your intentions are felt. So if your intention is to create something great and you do everything you need to do to achieve that, people are going to know. They're going to feel it. And the reverse is true too. You know, if you rushed and you didn't really care about the look of settings in your app UI, well, it's going to impact the overall feeling that people have about your app. So throughout the course of conducting these interviews, I started to see the word "quality" everywhere, like quality coffee or quality meats. And even one night when I was working really late, and I ordered a pizza, and the pizza box said, "Our commitment is quality." [laughs] So I see the word everywhere now. It's really pervasive, from signs to badges, to labels. So I brought this up in a few of the interviews, not because I wanted to know what people thought of labeling, but, instead, to ask, why do we tend to be drawn towards things that are quality? Well, survival instinct at its root. On a more deep level, I guess, why do we desire quality? We desire quality because we want to live. And a good survival strategy for living is to put things into our body, to eat things, or to wear things, or to drive in things, or to live in things that are safe and have been well constructed so they aren't going to cause us bodily harm. And so I think purely on a survival level, quality is a very important concept for us. It's a very important attribute that we seek in everything. Mike explained a bit further during this interview. We were talking about this fictitious butcher shop, and if we were in this shop, quality would be communicated not only by the products itself, but also the shop being really clean and interacting with the really nice people that work there. Every facet of the entire experience would meet your expectations, [laughs] and it would also be connected to that proposition of quality. And that evokes trust. Trust comes from products upholding their promise, and that quality experience is far more important than any label because I am definitely not suggesting that you add a label to your apps because you [laughs] definitely don't need to. Every facet of your app experience, from the screenshots on the App Store to the description, to how you respond to customer reviews, all through to your app's interface, these experiences, these interactions that you have with people, that communicates the quality about your app or your game. And you don't want a label anyway. Why? In a way, it's like saying you're cool. Like, being cool doesn't involve saying you're cool. [laughs] And, you know, you only say that someone else is cool. It's only something which can be sort of determined by an outside party, in a way. And even then, there's a certain context where it feels more appropriate than others. Like, if we're talking about Thelonious Monk and we're just talking about jazz in the '50's, like, that's cool. That's the birth of cool, right. [laughs] That's where it's, like, totally appropriate to use that kind of term, but that's an earned coolness. And so quality has to be earned. I think Mike makes a really good point. So the significance of quality is massive. As we heard, it impacts our every day, it can be felt, and it evokes trust in the people that are using your app. All right. Now, everyone I talked to throughout these interviews, they all have a passion for excellence, so we're going to talk about four design aspirations, and these four aspirations really stood out to me because, well, they seemed just so incredibly elusive and hard to achieve. But these designers use them as motivation to strive for quality in their work, and these aspirations are to design things that are simple, stunning, timeless, and leave a positive impact on people's lives. So we're going to dig into each one of these to discover why they're important and also how they provide motivation to create great design. All right. Let's start with simplicity. This came up in response to this question: What makes a great app? It's simple. It doesn't try to do more than it needs to do. And what it does, it does it really well. I don't know about you, but I have heard that a lot. [laughs] So I'm sure you've heard it before too. But there's got to be a reason why that phrase is on repeat, and I was really thinking about this after the interview. You know, why should apps be simple, and why is that a good thing? Well, it finally clicked for me one day after work. I got home, I needed to cook some dinner, and I needed an app to help me with a recipe, to not make cooking any harder than it already is. And as soon as I was home, I remembered that I needed to go ahead and put, book pet care for my two dogs because I'm going on vacation after WWDC. Aw, aren't they cute? [laughs] Okay, we have to stop before I get distracted. We're talking about two tasks here, cooking dinner and pet care. Why do the apps that help me with those, why do they need to be simple? Well, it's because I don't go into them all the time, so they need to be instantly understandable, as if I already know how to use them. So familiar navigation and gestures really help with that. You know, the goal is launch the app, I totally get it, and I can get the task done. But why else should they be simple? Well, it could be a good day or a bad day, but I'm using these apps in real life, and that's where all of my mental energy should be, not wasted on a confusing app UI, interaction, or interface. Apps should not be a burden, and it's a lot easier to get back to real life when they're simple. So how would you describe simplicity in apps and games? It just has to work. Right. It just has to work for the people that are using it. But there's actually something else here. You don't want people to feel distracted. Simple, easy-to-understand apps that just work, well, they help keep people focused. I know for me, I can get really easily overwhelmed if something's out of place. It can just take me out of that experience, and that makes getting the task done even harder. I was actually talking to Jessie about this, and I asked her, what does she look for in great apps? Make my life feel a little bit easier or make my life feel a little bit nicer or more luxurious. I think that is just spot on. Make my life feel a little bit easier by being simple, focused, instantly understandable, and just do one thing really, really well. That is the goal. All right, this next aspiration is to be stunning, and this came up when I was talking to Caroline. I asked her, "What communicates great visual design?" Just that polish. Does it look great? Does it look stunning? And then, the other level of polish, on a different kind of app, like a game or something, to me is making it feel beautiful as an experience, like it feels like a work of art. Right, yes. So we talked a little bit further about this, and Caroline explained that what she means by polish is things lining up just the way you intend them to be, including how it feels. You know, swipe animations lining right up with those swipe gestures. And the best apps appear visually perfect, and that is just totally remarkable. And she also mentioned games. Games should absolutely aspire to be stunning. Games are a perfect escape from this world into a different one. And they accomplish this through stunning, immersive visuals. I mean, I love TV and movies, but that's a really passive escape. In games, well, they're an active one, and that's why I love them. But there is one essential thing that all games must do. They must teach you, the player, the rules. And this is really a first impression that people have of a lot of games, so it makes me really sad when games make a bad first impression because when I'm trying to learn, well, the rules feel slapped together, added on last minute, or it just doesn't feel a part of the game's world. So I decided to ask Sam, who's working on an upcoming game called Where Cards Fall, "How do you learn the rules of the game all while staying in the game's world?" With games in particular, there's a real joy that can come from discovering what they can do and learning for yourself. People tend to remember things the best when they experience them themselves, when they learn them through some sort of active discovery. So if we give them too much at the beginning, we're robbing them of that chance to reward their own curiosity. So we always want to make sure that they stay curious, that they stay interested throughout the entire experience, and that they have a lot of different moments where they can be rewarded for trying something. And when they try something and they discover something for the first time, they definitely won't forget it. But if we throw up like a tooltip or some way of just making sure that we as the game designers are comfortable knowing that they know something, there's a pretty good chance that they'll forget it later on since they didn't go through the process of learning it themselves. So when I go into a game for the first time, I want to learn how to navigate the world, if it takes place in a world, as quickly as possible or just how to understand the rules of the game as quickly as possible, but not so quickly that I'm kind of robbed of my curiosity to explore them a little bit further. So I tend to get a sense for what the game is, and if the game continues to surprise me, I'll make it through to the finish line. Wow, that was learning the rules in Where Cards Fall, and that definitely feels a part of the game's world, and I think everyone can learn from what Sam said about active discovery. And this is not just for games. Active discovery is so much better than reading tooltips or chasing a pointing arrow, and to Sam's point, being told isn't as much fun or, frankly, effective as discovering something for yourself. It's kind of like being told about a vacation versus being on that vacation yourself. It's way more fun, and you're just going to remember it better if you actually go through that experience. So whether you're creating an app or a game, aspire to be stunning. Create beautiful experiences, polish your visual design so it looks and feels just a part of your app or game's world. Okay, timeless. This has been on my mind since talking with Hugo, a sound designer. Most of you will recognize this. [ Ringtone ] I'm always looking for where that phone is. But what reminded me of this ringtone was this part of the interview. If we create a ringtone for the new iPhone, then we don't want it to sound dated after a year, of course. It should still be a great ringtone after 5 years, and maybe people will still remember it after 10 years. It was actually this section of the quote that really made me think of timelessness. You know, it makes sense for a ringtone not to want to sound dated, but this could also apply to your visual and interaction design too. Hugo used this word "durability" to describe quality during our interview, and I think that's really great. If we all aspire to create designs that are just a little bit longer lasting, it not only would save us time and effort now, but it would put us into the right mindset to provide something that we create with the opportunity to be timeless. This final aspiration really had an impact on me, and it came from my colleague Doug when I asked him, "What makes great app experiences?" When I think of, like, great app experiences and things that kind of provide something unique and valuable to users, I think a lot of it is the substantial kind of positive effect that it has on the user's life. Doug, you nailed it. Yes, absolutely, apps should provide something unique and valuable, but, actually, it was this part, the positive effect, that really resonated with me. It made me reassess the apps and games on my phone. After this interview, I literally scrolled through all of my apps, and I thought, which ones have a positive effect on my life? You know, wouldn't it be great if life just had a little more positivity? So next time you design, strive for simplicity because real life if where we need to be spending our energy. Aspire to be stunning because we all need a break from chaos, and it's really nice to escape and experience something beautiful. Design to be timeless. Think about how your designs can be good now but also years from now because we could all design to be a little less trendy and definitely more durable. And finally, think about how you could have a positive impact on people's lives. And, you know, this can manifest itself in really different ways depending on your app. It could be maybe that positivity is being really accurate, or maybe it's just being super entertaining. Maybe it's just making someone's life or day feel a little bit easier. I hope that you found some of these thoughts to be as inspiring as I did. I'm sure you're thinking now, though, well, sure, this has been mildly inspiring and somewhat educational, but what can I do? What can I actually take away and apply to my work? Well, I wanted to know that too. After talking with so many different people from different backgrounds, I wanted to know what they learned -- what they learned on the job through their experiences. So through a series of questions, I uncovered a few tips and techniques that these designers use regularly when designing. And something important to mention about design before we get started is, yes, design is hard because nobody sets out to create a bad app. Making great designs is what we all strive for, but let's be honest. We don't always achieve it, and that's okay because we keep trying and we learn a lot along the way. Okay, let's go ahead and hear this first technique, and it really surprised me. It came up when I was talking with Loic [phonetic], a type designer, and I asked him, "What helps you with your work?" So in the case of type design, for instance, we've developed all kinds of little drawing tricks. In type design especially, subjectiveness I feel can [inaudible] even more than in other aspects of design because we go to the depth of how the particular curve of, on the shoulder of a lowercase m is, for instance. And we can have a 10-minute discussion only on that. But the difficult part is describing what you see in words that the other person is going to understand the way you mean it, and that's very difficult when you're talking about shape. But one way we started doing that with some of my colleagues was basically redrawing the shape we're looking at with exaggerated features, where we basically kind of make a caricature of what we are seeing. So if I'm pushing what I'm seeing to the extreme of why I find it ugly or uncomfortable, it's because it feels like that to me. And through that drawing, usually the other people can start seeing that much more toned down but existing feature of the design that they weren't necessarily perceiving before because that's not what they were looking at. So it is about trying to make people see the world through your eyes and the other way around, like gaining the ability to see the world through their eyes, so that you can then find that third place that is actually the joint gaze that you have on the world. And that's the, that I think to me is the key of a successful collaboration in design is finding that place. Wow. Before we even dig into this technique, have you thought about type design? I mean, every single letter that you text, and email, and that you read, that's been designed, and type designers care so much about detail that they've invented ways to communicate about shape when there are no words. That's just so cool. So to Loic's point, when you're designing for others, it's really necessary to see the world through their eyes, and this technique helps achieve that. So draw caricatures and ask others to do the same. It's really the importance of putting yourself in others' shoes because you can better understand their world. And this is a great tool for communication if words are not doing it, and it'll help you understand what others are seeing. This next technique is going to be unveiled through a story. Actually, even better, a first job story. Because you learn a lot on your first job, and they have a huge impact. So I asked my colleague Doug what he learned at his first design job. One of the real kind of ahas for me was a question that I got at the Tech Museum from the Head of Design that was essentially, why is this good? Why is this thing that you just gave me good? And, you know, my initial reaction was, well, because I think this will be easy for people to do. And, you know, he responded, like, well, how do you know that? How do you know that this is going to be easier than the way that we do it now? And I kind of realized that I didn't know the answer to that. And then, I realized what he was really asking is, should we put all of this energy into making the changes that you're suggesting? How can we be sure that that's the right thing to do? And to answer that question, I opened up -- it might have been Microsoft Paint [laughs] -- and I drew a picture, and I took it out onto the museum floor, and basically asked visitors to give me their impression of what they thought they should do based on kind of how I had arranged things on the screen. So what Doug was talking about here, and he went on to explain, that it was this method of prototyping that helped him answer that question from his boss. Why should we put energy into your idea? So what Doug learned at his first design job was that he could not assume that his designs were going to work. And this actually also came up in my conversation with Gary. It wasn't just designed on paper, implemented in code, and sort of assumed that that was going to work. You can't just assume from your own experience and through chatting that something's going to work. You've really got to go out and try it. And I think this is the part that's really important not to overlook. You know, in this case, we are talking about trying designs in context out in the real world to vet solutions to problems, and trying solutions is something that Sam mentioned too. One of our ethos's is to solve a problem. Put it in front of players and see how they react. See if they're learning what they need to learn. And if we need to do more, we'll do more. It's impossible to rush the creative process of making a game because so much of what's special about the games that you remember are decisions that seem very obvious and very small but take a really, really long time to arrive at. And you can't hire more people to get there faster. You just have to sit with it and continue working on it until the best solutions arise. So Sam said something here that I want to expand on. More people are not going to fix a creative problem. Great artists are patient. Sometimes you just get writer's block and there's no amount of conversation that's going to fix it, so you have to wait and just let that creative process happen. And that's okay. It's a part of the process, and sometimes that's exactly what is needed to produce quality. So the theme really from these last 3 clips is to go out and try your designs in context with real people. Show people prototypes and learn from their feedback. This is a really essential technique to help work through your design decisions. So we can learn a lot from each other, so I wanted to know what advice would one designer give to another designer. Learn how to accept, and sort, and prioritize the feedback as it comes in, whether it's from yourself or from others. You will learn so much more quickly than if you are defensive and you just, like, buckle down. I personally struggle with this. I can almost not help becoming defensive when I hear feedback on a project I'm working on. It's like this instinctual reaction, especially if I just spent late nights and weekends working on something. But still, I know it's important, but I wanted to know why. Again, why is it important to be open to feedback? I think there's also just a reality, which is after you've used your own work for some amount of time, you naturally develop some blind spots. And that's where having a really great, collaborative team and people that you trust around you to help critique your work is a really important part of the design process. So you can get fresh eyes on stuff and trust that people have the good intentions and they're going to be able to give you great insights of how you can improve. Totally. I've been there. I've definitely been too close on a project and lost sight of a big picture or overlooked some detail. So by being open to feedback, it's really easy then to have someone else like your team or somebody that you asked a question to give you something, some piece of information that can help you improve your designs. And I really like that May-Li said this word, "trust," to trust the people around you. So be open to feedback. It makes everything better and just ups the overall quality of design in your work. So this next technique is more of a mindset, and it came up when I asked Cas, who works on platform patterns, "How do you approach designing for so many people?" Knowing what you don't know, in a way. What I mean is that if you're looking at these things or if you're designing for these things, obviously, you have ideas and ways of using a certain device or an app in your own way, in your own environment, in your own habits, and you might know a little bit about how your friends are using it, or how you've seen other people use it, or maybe feedback you're getting, but it's a really big world, and there's a lot of people out there, and they all have their own needs, their own habits, their own way of going through life. And so you have to be very mindful that how you think about things or your opinions are not always, first of all, correct ones, nor the only ones. I really like how Cas put this at the beginning -- know that you don't know. And he also used this word "modesty" through our conversation. To be unassuming when you're designing for so many people. And that is a really great headspace to be in when you're approaching design; to be modest and to be mindful. So how do you help keep people's experiences top of mind when you're designing? So in many ways, I just have to be skeptical and ask questions if the idea in front of me, is that going to work for someone in a wheelchair, for example? Is that going to work for someone that has never used this app before? Is it going to work for someone coming from a PC to a Mac? You just have to ask lots of questions, and it can just start people thinking about, oh, yeah, did I care enough to consider that aspect of it? Right. So this may seem really simple, but it's important to ask questions, especially from different perspectives. That's going to help you maintain quality throughout your work and keep those other people's experiences in mind. So design is a really collaborative process, and there's a lot that goes into making collaboration work. At one point or another, we're all going to start on a new project or a new team, so what helps make that process a little bit easier? I always like to set expectations upfront. I just say, hey, this is how I am. This is how I work. This is how I communicate. How do you work? How do you communicate? What's your preferred method of communication? And by the way, here's the goals that I think we are working toward. And here are the things that are important to me. And please share with me the same for you. And I'll always sort of say it with a caveat like, "We're going to get all this business out of the way, and we're going to speak very directly about it." I know for me, talking about communication can be awkward, but after I tried this technique, I learned that it helps avoid so much drama and confusion. So talk about communication, especially if you're working on teams. Work towards the same goal about creating something great. So really, these six techniques are all about caring about the other people and having tools to talk about it, and you have to care about other people and put yourself second. Be mindful of their lives and be modest about your own because we just learned you can't assume. You have to go out and try your designs through prototyping and also really remain open to feedback. That's essential to making progress in design because, as May-Li said, we all develop blind spots, and we're not only designing to make progress, but we want to get closer to reaching our own aspirations and our own goals. So these techniques are used by real designers that I talked to, and they help them go through the process of designing, and it makes their days a little bit easier, so I hope they can help you too. So after all of these interviews, I learned that quality is the result of time, effort, and care. It's considered, it's crafted, and it means something different to different people. I also learned that quality impacts our every day, that your intentions are truly felt. High quality evokes trust, and that's really important, as well as it has to be earned. It's also important to strive for simplicity, to aspire to be stunning, and design to be timeless. Just think about how your app or game can have a positive impact on people's lives. And finally, working towards quality is all about communication, collaboration, and understanding, especially understanding the people you are designing for. So really, quality is a principle. It is intentional, and it's something that you have to ask yourself, what does quality mean to you? And you need to know your interpretation to go forward, to put in that hard work that meets your expectations and hits your aspirations because, at the end of the day, you're not designing for you. You're designing for other people who are going to use your app or your game. So make great designs for them. Now, there are two important statements on quality in great design that stand alone. These are going to play back to back, and I hope each offers something different that you can take away from today. Sensing the human behind the experience I think is really an earmark of something great. A great design is going to be the product of tremendous amount of effort, and creativity, and skill, and [laughs] late nights. It's going to seem effortless, and it should hopefully recede in people's consciousness. And in a way, I think you're very aware of an interface which has been poorly designed, and you're not at all aware of an interface which has been well designed. So I talked to a lot of people, and I saved something special just for the end. Since I had the opportunity to talk to so many different designers, I had to ask them onw final question, and it's something that I've always wanted, wondered myself: What are designers' favorite colors? Oh, wow. My favorite color? I have to say-- Is there such a thing? I don't think so. I would have said glitter, but it's not a color. One of the things that I really love is Tahitian blue. But colors to me are like moods, and I do not have the same mood every day. I don't think I even have the same mood every hour, so. I really like white. Is it really a color? I think that's one of the reasons why I like it. My favorite color has been amber. I mean, lately, it's been green for some reason. I don't know why. I got to go back on this answer. Oh, it's actually Pantone 347. My favorite color is blue. Like a happy blue, and blue is, like, always such a color of responsibility, which is, like, so me. But I just think it's, like, soft and friendly. And food that's pink tastes good. I'm going to go with red. Throw out a hex value. I don't have a favorite color. Every color's great. [laughs] All right, maybe I'll just answer for this moment in time. I don't wear any green. My car's not green. But I just like the idea of green. And I can't pick one because I wouldn't be able to pick a single musical note, so I'll use all of them where it makes sense. I'll need to check the spec on that, I think. System blue, of course. Yeah. Special thanks to everyone who participated in these interviews. Thank you all so much. Have a great WWDC. Hello. Welcome to Testing Tips & Tricks. My name is Brian Croom. My colleague, Stuart, and I are really excited to share some great testing techniques with you that we have been learning recently. As the conference was approaching, we thought it would be really cool to have an app that we could use to find cool things to see and do around the area of the convention center. We've been building this app, giving it views for finding various point of interest around San Jose and listing how far they are away from you. Now, of course, we wanted to make sure we had a really great test suite for this app that we could run to give us confidence that our code was working properly and would keep working as we continue development. Today, we want to share four simple techniques with you that we found really helpful while writing tests for our app. Some strategies for testing networking code in your app, some tips for tests dealing with foundation notification objects, ways to take advantage of protocols when making mock objects in your tests, and a few techniques for keeping your tests running really fast. Let's start talking about networking. To allow for dynamic content updates, we've been building our app to load its data from a remote web server. Here are some things that we found useful when writing tests for networking code. But first, quick, a recap from last year. At WWDC 2017's Engineering Testability session, we discussed the pyramid model as a guide to how to structure a test suite, balancing thoroughness, understandability, and execution speed. In summary, an ideal test suite tends to be composed of a large percentage of focused unit tests, exercising individual classes and methods in your app. These are characterized by being simple to read, producing clear failure messages when we detect a problem, and by running very quickly, often in the order of hundreds or thousands of tests per minute. These are complemented by a smaller number of medium-sized integration tests that targeted discreet subsystem or cluster of classes in your app, checking that they worked together properly, each taking no more than a few seconds to run. And the suite is topped off by a handful of end-to-end system tests, most often taking the form of UI tests that exercise the app in a way very similar to how the end-user will do so on their devices, checking that all the pieces are hooked up properly and interact well with the underlying operating system and external resources. A test suite following this model can provide a comprehensive picture of how an app code's base is functioning. For testing the networking stack in this app, we really wanted to take the pyramid model to heart and use it as a guide for how to structure our test suite. Here, we see the high-level data flow involved in making a network request in the app and feeding the data into the UI. In an early prototype of the app, we had a method in our view controller that was doing all of this in a single place, and it looked like this. The method takes a parameter with the user's location and uses that to construct a URL for a service API endpoint with a location as query parameters. Then it uses Foundation's URLSession APIs to make a data task for a get request to that URL. Then the server responds, it would unwrap the data, decode it using foundation's JSONDecoder API, into an array of point of interest values, which is a struct that I declared elsewhere and conforms the decodable protocol. And it stores that into a property to drive a table view [inaudible] implementation, putting it onto the screen. Now, it's pretty remarkable that I was able to do all of this in just about 15 lines of code, leveraging the power of Swift and Foundation, but, by putting this together in the one method, I [inaudible] the maintainability and especially the testability of this code. Looking at the base of our testing pyramid, what we really want to be able to do here is write focus unit tests for each of these pieces of the flow. Let's first consider the request preparation and response parsing steps. In order to make this code more testable, we started by pulling it out of the view controller and made two methods on this dedicated PointsOfInterestRequest type, giving us two nicely decoupled methods that each take some values as input and transform them into some output values without any side effects. This makes it very straightforward for us to write a focused unit test for the code. Here we're testing the makeRequest method just by making a sample and put location, passing it into the method, and making some assertions about its return value. Similarly, we can test the response parsing by passing in some mock JSON and making assertions about the parsed result. One other thing to note about this test is that I'm taking advantage of XCTest support for test methods marked as throws, allowing me to use try in my test code without needing an explicit do catch block around it. Now, let's see the code for interacting with URL session. Here, again, we pull it out the view controller, made a APIRequest protocol with methods matching the signature of the methods from the request type that we just saw. And this is used by an APIRequestLoader class. That's initialized with a request type and a urlSession instance. This class has a loadAPIRequest method which uses that apiRequest value to generate a URL request. Feed that into the urlSession, and then use the apiRequest again to parse in your response. Now, we can continue write unit test for this method, but right now I actually want to move up the pyramid and look at a midlevel integration test that covers several pieces of this data flow. Another thing that I really want to also be able to test at this layer of my suite is that my interaction with the URLSession APIs is correct. It turns out that the foundation URL loading system provides a great hook for doing this. URLSession provides a high level API for apps to use to perform network requests. [Inaudible] objects like URLSession data tests that represent an inflight request. Behind the scenes though, there's another lower-level API URLProtocol which performs the underlying work of opening network connection, writing the request, and reading back a response. URLProtocol is designed to be subclassed giving an extensibility point for the URL loading system. Foundation provides built-in protocols subclasses for common protocols like HTTPS. But we can override these in our tests by providing a mock protocol that lets us make assertions about requests that are coming out and provide mock responses. URLProtocol communicates progress back to the system via the URLProtocolClient protocol. We can use this in this way. We make a MockURLProtocol class in our test bundle, overriding canInit request to indicate to the system that we're interested in any request that it offers us. Implement canonicalRequest for request, but the start loading and stop loading method's where most of the action happens. To give our tests a way to hook into this URLProtocol, we'll provide a closure property requestHandler for the test to set. When a URLSession task begins, the system will instantiate our URLProtocol subclass, giving it the URLRequest value and a URLProtocol client instance. Then it'll call our startLoading method, where we'll take our requestHandler to the test subsets and call it with a URLRequest as a parameter. We'll take what it returns and pass it back to the system, either as a URL response plus data, or as an error. If you want to do test request cancellation, we could do something similar in a stopLoading method implementation. With the stub protocol in hand, we can write our test. We set up by making an APIRequestLoader instance, configure it with a request type and a URLSession that's been configured to use our URLProtocol. In the test body, we set a requestHandler on the MockURLProtocol, making assertions about the request that's going out, then providing a stub response. Then we can call loadAPIRequest, waiting for the completion block to be called, making assertions about the parsed response. Couple of tests at this layer can give us a lot of confidence that our code is working together well and, also, that we're integrating properly with the system. For example, this test that we just saw would have failed if I had forgotten to call a resume on my data task. I'm sure I'm not the only one who's made that mistake. Finally, it can also be really valuable to include some system level end-to-end tests. Actually test a UI testing can be a great tool for this. To learn more about UI testing, refer to the UI testing in Xcode session from WWDC 2015. Now, a significant challenge that you encounter when you start to write true end-to-end tests is that when something goes wrong when you get a test failure, it can be really hard to know where to start looking for the source of the problem. One thing that we were doing in our test recently to help mitigate this was to set up a local instance of a mock server, interrupting our UI tests to make requests against that instead of the real server. This allowed our UI test to be much more reliable, because we had control over the data being fed back into the app. Now, while using a mock server can be really useful in this context, it is also good to have some tests making requests against the real server. One cool technique for doing this can be to have some tests in the unit testing bundle that call directly into your apps in that work in Stack and use that to direct requests against the real server. This provides a way of verifying that the server is accepting requests the way that your app is making them and that you're able to parse the server's responses without having to deal with the complications of testing your UI at the same time. So, to wrap up, we've seen an example of breaking code into smaller, independent pieces to facilitate unit testing. We've seen how URLProtocol can be used as a tool for mocking network requests, and we've discussed how the power of the pyramid can be used to help us structure a well-balanced test suite that'll give us good confidence in our code. Now, I want to call Stuart to the stage to talk about some more techniques. Thanks. Thanks, Brian. So, the first area I'd like to talk about is some best practices for testing notifications. And to clarify, by notification here, I'm talking about foundation-level notifications known as NSNotification and Objective-C. So, sometimes we need to test that a subject observes a notification, while other times we need to test that a subject posts a notification. Notifications are a one-to-many communication mechanism, and that means that when a single notification is posted, it may be sent to multiple recipients throughout your app or even in the framework code running in your app's process. So, because of this, it's important that we always test notifications in an isolated fashion to avoid unintended side effects, since that can lead to flaky, unreliable tests. So, let's look at an example of some code that has this problem. Here, I have the PointsOfInterest TableViewController from the app Brian and I are building. It shows a list of nearby places of interest in a table view, and whenever the app's location authorization changes, it may need to reload its data. So, it observes a notification called authChanged from our app's CurrentLocationProvider class. When it observes this notification, it reloads its data if necessary, and, then, for the purpose of this example, it sets a flag. This way, our test code can check that the flag to see if the notification was actually received. We can see here that it's using the default notification center to add the observer. Let's take a look at what a unit test for this code might look like. Here in our test for this class, we post the authChanged method notification to simulate it, and we post it to the default NotificationCenter, the same one that our view controller uses. Now, this test works, but it may have unknown side effects elsewhere in our app's code. It's common for some system notifications like UI applications appDidFinishLaunching notification to be observed by many layers and to have unknown side effects, or it could simply slow down our tests. So, we'd like to isolate this code better to test this. There's a technique we can use to better isolate these tests. To use it, we first have to recognize that NotificationCenter can have multiple instances. As you may note, it has a default instance as a class property, but it supports creating additional instances whenever necessary, and this is going to be key to isolating our tests. So, to apply this technique, we first have to create a new NotificationCenter, pass it to our subject and use it instead of the default instance. This is often referred to as dependency injection. So, let's take a look at using this in our view controller. Here, I have the original code that uses the default NotificationCenter, and I'll modify it to use a separate instance. I've added a new NotificationCenter property and a parameter in the initializer that sets it. And, instead of adding an observer to the default center, it uses this new property. I'll also add a default parameter value of .default to the initializer, and this avoids breaking any existing code in my app, since existing clients won't need to pass the new parameter, only our unit tests will. Now let's go back and update our tests. Here's the original test code, and now I've modified it to use a separate NotificationCenter. So, this shows how we can test that our subject observes a notification, but how do we test that our subject posts a notification? We'll use the same separate NotificationCenter trick again, but I'll also show how to make use of built-in expectation APIs to add a notification observer. So, here's another section of code from our app, the CurrentLocationProvider class. I'll talk more about this class later, but notice that it has this method for signaling to other classes in my app that the app's location authorization has changed by posting a notification. As with our view controller, it's currently hardcoding the default NotificationCenter. And here's a unit test I wrote for this class. It verifies that it posts a notification whenever the NotifyAuthChanged method is called, and we can see in the middle section here that this test uses the addObserver method to create a block-based observer, and then it removes that observer inside of the block. Now, one improvement that I can make to this test is to use the built-in XCTNSNotificationExpectation API to handle creating this NotificationCenter observer for us. And this is a nice improvement, and it allows us to delete several lines of code. But it still has the problem we saw before of using the default NotificationCenter implicitly, so let's go fix that. Here's our original code, and I'll apply the same technique we saw earlier of taking a separate NotificationCenter in our initializer, storing it, and using it instead of the default. Going back to our test code now, I'll modify it to pass a new NotificationCenter to our subject, but take a look at the expectation now. When our tests are expecting to receive a notification to a specific center, we can pass the NotificationCenter parameter to the initializer of the expectation. I'd also like to point out that the timeout of this expectation is 0, and that's because we actually expect it to already have been fulfilled by the time we wait on it. That's because the notification should have already been posted by the time the NotifyAuthChanged method returns. So, using this pair of techniques for testing notifications we can ensure that our tests remained fully isolated, and we've made the change without needing to modify an existing code in our app, since we specified that default parameter value. So, next, I'd like to talk about a frequent challenge when writing unit tests, interacting with external classes. So, while developing your app, you've probably run into situations where your class is talking to another class, either elsewhere in your app or provided by the SDK. And you found it difficult to write a test, because it's hard or even impossible to create that external class. This happens a lot, especially with APIs that aren't designed to be created directly, and it's even harder when those APIs have delegate methods that you need to test. I'd like to show how we can use protocols to solve this problem by mocking interaction with external classes but do so without making our tests less reliable. In our app, we have a CurrentLocationProvider class that uses CoreLocation. It creates a CLLocationManager and configures it in its initializer, setting its desired accuracy property and setting itself as the delegate. Here's the meat of this class. It's a method called checkCurrentLocation. It requests the current location and takes a completion block that returns whether that location is a point of interest. So, notice that we're calling the request location method on CLLocationManager, here. When we call this, it'll attempt to get the current location and eventually call a delegate method on our class. So, let's go look at that delegate method. We use an extension to conform to the CLLocationManagerDelegate protocol here, and we call a stored completion block. Okay, so let's try writing a unit test for this class. Here's one that I tried to write, and, if we read through it, we can see that it starts by creating a CurrentLocationProvider and checks that the desired accuracy and whether the delegate is set. So far, so good. But then things get tricky. We want to check the checkCurrentLocation method, since that's where our main logic lives, but we have a problem. We don't have a way to know when the request location method is called, since that's a method on CLLocationManager and not part our code. Another problem that we're likely to encounter in this test is that CoreLocation requires user authorization, and that shows a permission dialog on the device if it hasn't been granted before. This causes our tests to rely on device state. It makes them harder to maintain and, ultimately, more likely to fail. So, if you've had this problem in the past, you may have considered subclassing the external class and overriding any methods that you call on it. For example, we could try subclassing CLLocationManager here and overriding the RequestLocation method. And that may work at first, but it's risky. Some classes from the SDK aren't designed to be subclassed and may behave differently. Plus, we still have to call the superclass' initializer, and that's not code that we can override. But the main problem is that, if I ever modify my code to call another method on CLLocationManager, I'll have to remember to override that method on my testing subclass as well. If I rely on subclassing, the compiler won't notify me that I've started calling another method, and it's easy to forget and break my tests. So, I don't recommend this method, and instead to mock external types using protocols. So, let's walk through how to do that. Here's the original code, and the first step is to define a new protocol. I've named my new protocol LocationFetcher, and it includes the exact set of methods and properties that my code uses from CLLocationManager. The member names and types match exactly, and that allows me to create an empty extension on CLLocationManager that conforms to the protocol, since it already meets all the requirements. I'll then rename the LocationManager property to LocationFetcher, and I'll change its type to the LocationFetcher protocol. I'll also add a default parameter value to the initializer, just like I did earlier, to avoid breaking any existing app code. I need to make one small change to the checkCurrentLocation method to use the renamed property. And, finally, let's look at that delegate method. This part is a little trickier to handle, because the delegate expects the manager parameter to be a real CLLocationManager, and not my new protocol. So, things get a little more complicated when delegates are involved, but we can still apply protocols here. Let's take a look at how. I'll go back to LocationFetcher protocol that I defined earlier, and I'll rename that delegate property to LocationFetcherDelegate. And I'll change its type to a new protocol whose interface is nearly identical to CLLocationManagerDelegate, but I tweaked the method name, and I changed the type of the first parameter to LocationFetcher. Now I need to implement the LocationFetcherDelegate property in my extension now, since it no longer satisfies that requirement. And I'll implement the getter and the setter to use force casting to convert back and forth to CLLocationManagerDelegate, and I'll explain why I'm using force casting here in just a second. Then in my class' initializer, I need to replace the delegate property with locationFetcherDelegate. And the last step is to change the original extension to conform to the new mock delegate protocol, and that part's easy-- all I need to do is replace the protocol and the method signature. But I actually still need to conform to the old CLLocationManagerDelegate protocol too, and that's because the real CLLocationManager doesn't know about my mock delegate protocol. So, the trick here is to add back the extension which conforms to the real delegate protocol but have it call the equivalent locationFetcher delegate method above. And earlier, I mentioned that I used force casting in the delegate getter and setter, and that's to ensure that my class conforms to both of these protocols and that I haven't forgotten one or the other. So, over in my unit tests, I'll define a struct nested in my test class for mocking, which conforms to the locationFetcher protocol and fill out its requirements. Notice, in its RequestLocation method, it calls a block to get a fake location that I can customize in my tests, and then it invokes the delegate method, passing it that fake location. Now that I have all the pieces I need, I can write my test. I create a MockLocationFetcher struct and configure its handleRequestLocation block to provide a fake location. Then I create my CurrentLocationProvider, passing it the MockLocationFetcher. And, finally, I call checkCurrentLocation with a completion block. Inside the completion block, there's an assertion that checks that the location actually is a point of interest. So, it works. I can now mock my classes' usage of CLLocationManager without needing to create a real one. So, here, I've shown how to use protocols to mock interaction with an external class and its delegate. Now, that was a lot of steps. So, let's recap what we did. First, we defined a new protocol, representing the interface of the external class. This protocol needs to include all the methods and properties that we use on the external class, and, often, their declarations can match exactly. Next, we created an extension on the original external class, which declares conformance to the protocol. Then we replaced all our usage of the external class with our new protocol, and we added an initializer parameter so that we could set this type in our tests. We also talked about how to mock a delegate protocol, which is a common pattern in the SDKs. There were a few more steps involved here, but here's what we did. First, we defined a mock delegate protocol with similar method signatures as the protocol that we're mocking. But we replaced the real type with our mock protocol type. Then, in our original mock protocol, we renamed the delegate property, and we implemented that renamed property on our extension. So, although this approach may require more code than an alternative like subclassing, it'll be more reliable and less likely to break as I expand my code over time, since this way the compiler will enforce that any new methods I call for my code must be included in these new protocols. So, finally, I'd like to talk about test execution speed. When your tests take a long time to run, you're less likely to run them during development, or you might be tempted to skip the longest running ones. Our test suite helps us catch issues early, when fixing regression is easiest. So, we want to make sure our tests always run as fast as possible. Now, you might have encountered times in the past when you needed to artificially wait or sleep in your tests, because the code your testing is asynchronous or uses a timer. Delayed actions can be tricky, so we want to be sure to include them in our tests, but they can also slow things down a lot if we're not careful. So, I'd like to talk about some ways that we can avoid artificial delays in our tests, since they should never be necessary. Here's an example. In the points of interest app Brian and I are building, in the main UI, we have a strip at the bottom that shows the featured place. It basically loops through the top places nearby, rotating to show a new location every 10 seconds. Now, there are several ways I might implement this, but, here, I'm using the timer API for foundation. Let's look at a unit test that I might write for this class. It creates a FeaturedPlaceManager and stores its current place before calling the scheduleNextPlace method. Then it runs the run loop for 11 seconds. I added an extra second as a grace period. And, finally, it checks that the currentPlace changed at the end. Now, this isn't great, and it takes a really long time to run. To mitigate this, we could expose a property in our code to allow us to customize that timeout to something shorter, like 1 second. And here's what that kind of a code change might look like. Now, with this approach, we can reduce the delay in our tests down to one second. Now, this solution is better than the one we had before. Our tests will definitely run faster, but it still isn't ideal. Our code still has a delay, it's just shorter. And the real problem is that the code we're testing is still timing dependent, which means that, as we make the expected delay shorter and shorter, our tests may become less reliable, since they'll be more dependent on the CPU to schedule things predictably. And that's not always going to be true, especially for asynchronous code. So, let's take a look at a better approach. I recommend first identifying the delay mechanism. In my example, it was a timer, but you could also be using the asyncAfter API from DispatchQueue. We want to mock this mechanism in our tests so that we can invoke the delayed action immediately and bypass the delay. Here's our original code again, and let's start by looking at what this scheduledTimer method actually does. The scheduledTimer method actually does two things for us. It creates a timer, and then it adds that timer to the current run loop. Now, this API can be really convenient for creating a timer, but it would help us to make this code more testable if I actually break these two steps apart. Here, I've transformed the previous code from using scheduledTimer to instead create the timer first and then add it to the current runLoop second, which I have stored in a new property. Now, this code is equivalent to what we had before, but, once we break these two steps apart, we can see that runLoop is just another external class that this class interacts with. So, we can apply the mocking with protocols technique we discussed earlier here. To do that, we'll create a small protocol, containing this addTimer method. I've called this new protocol TimerScheduler, and it just has that one addTimer method, which matches the signature of the runLoop API. Now, back in my code, I need to replace the runLoop with the protocol that I just created. And in my tests, I don't want to use a real runLoop as my TimerScheduler. Instead, I want to create a mock scheduler that passes the timer to my tests. I'll do this by creating a new struct nested in my unit test class called MockTimerScheduler, conforming to the TimerScheduler protocol. It stores a block that will be called whenever it's told to add a timer. And with all the pieces in place, I can write my final unit test. First, I create a MockTimerScheduler and configure its handleAddTimer block. This block receives the timer. Once it's added to the scheduler, it records the timer's delay, and then it invokes the block by firing the timer to bypass the delay. Then, we create a FeaturedPlaceManager and give it our MockTimerScheduler. And, finally, we call scheduleNextPlace to start the test, and, voila, our tests no longer have any delay. They execute super fast, and they aren't timer dependent, so it'll be more reliable. And, as a bonus, I can now verify the amount of timer delay using this assertion at the bottom. And that's not something I was able to do in the previous test. So, like I said, the delay in our code is fully eliminated using this technique. We think this was a great way to test code that involves delayed actions, but, for the fastest overall execution speed in your tests, it's still preferable to structure the bulk of your tests to be direct and not need to mock delayed actions at all. For example, in our app, the action being delayed was changing to the next featured place. I probably only need one or two tests that show that the timer delay works properly. And, for the rest of the class, I can call the show next place method directly and not need to mock a timer scheduler at all. While we're on the topic of text execution speed, we had a couple of other tips to share. One area we've seen concerns the use of NSPredicateExpectations. We wanted to mention that these are not nearly as performant as other expectation classes, since they rely on polar rather than more direct callback mechanisms. They're mainly used in UI tests, where the conditions being evaluated are happening in another process. So, in your unit tests, we recommend more direct mechanisms, such as regular XCTestExpectations, NSNotification, or KVOExpectations. Another testing speed tip is to ensure that your app launches as quickly as possible. Now, most apps have to do some amount of setup work at launch time, and, although that work is necessary for regular app launches, when your app is being launched as a test runner, a lot of that work may be unnecessary. Things like loading view controllers, kicking off network requests, or configuring analytics packages-- these are all examples of things that are commonly unnecessary in unit testing scenarios. XCTest waits until your app delegates did finish launching method returns before beginning to run tests. So, if you profile and notice that app launch is taking a long time in your tests, then one tip is to detect when your app is launched as a test runner and avoid this work. One way to do this is to specify a custom environment variable or launch argument. Open the scheme editor, go to the test action on the left side, then to the arguments tab, and add either an environment variable or a launch argument. In this screenshot, I've added an environment variable named IS-UNIT-TESTING set to 1. Then, modify your app delegate's appDidFinishLaunching code to check for this condition, using code similar to this. Now, if you do this, be sure that the code you skip truly is nonessential for your unit test to function. So, to wrap up, Brian started by reminding us about the testing pyramid and how to have a balanced testing strategy in your app, showing several practical techniques for testing network operations. Then, I talked about isolating foundation notifications and using dependency injection. We offered a solution to one of the most common challenges when writing tests, interacting with external classes, even if they have a delegate. And we shared some tips for keeping your tests running fast and avoiding artificial delays. We really hope you'll find these tests useful and look for ways to apply them the next time you're writing tests. For more information, check out our session webpage at this link, and, in case you missed it, we hope you'll check out Wednesday's What's New in Testing session on video. Thanks so much, and I hope you had a great WWDC.  Hi, everybody. My name is Ari Weinstein, and I'm really excited to be here along with Willem Mattelaer to tell you about Siri Shortcuts. So, two years ago, we announced the first version of SiriKit, which enables your apps to work with Siri, and it has great domain-specific knowledge for things like sending messages, requesting rides, making payments, and more. But we know that you want to do even more with SiriKit, and so that's why, this year, we're so excited to announce Shortcuts. This central idea behind Shortcuts is that it lets you expose the key capabilities of your app to Siri, and this is really great, because it enables people to use the features of your apps in new ways and in new places. And it's a really powerful way to engage your users. So, exposing a shortcut opens up a ton of new possibilities for how people can use your apps, so let's explore all of the places where Shortcuts can be used. Shortcuts takes Siri's suggestions to the next level by surfacing what you want to do next, not just with your apps, but inside of your apps, and it does it by surfacing them in search at just the right time. And Siri can suggest shortcuts on the Watch, so you can use them right from your wrist on the Siri Watch Face. When Siri is confident that it's the exact right time for a Shortcut, it will be surfaced on the lock screen. And, when you tap on a shortcut, you can use it right in line with a rich custom UI specific to that app. And you can also add shortcuts to Siri, so you can use them with your voice just by asking Siri. And in Siri you'll see the same custom UI that you see in search and on the lock screen. And apps can provide a great voice experience in Siri by providing custom response dialog that Siri will read out loud to tell you things like how long it will take for your coffee to be ready. So, when the user adds a shortcut to Siri, they get to pick a custom personal phrase, so the user chooses what to say to Siri to invoke your shortcut. And, as a developer, you get to suggest what phrase to use, so, in this case, a suggestion is "coffee time." And, once a shortcut has been added to Siri, it works across all of your iOS devices, and on Apple Watch, and even on HomePod. So, we also have the new Shortcuts app, and with the new Shortcuts app, anyone can build their own shortcuts, and you do it just by dragging and dropping a series of steps together. And those steps can even include the shortcuts exposed by your app. So, today, we're going to talk about how to adopt shortcuts for your apps, and then we'll talk about how to optimize your shortcuts for great suggestions. And we'll also cover some privacy considerations that are important to keep in mind, and we'll talk about how to make great shortcuts for media playback. So, first, we'll talk about how to adopt the new Shortcut's APIs. There were three steps for creating a shortcut, and the first step is to define your shortcut. That means you have to decide what you want to expose as a shortcut and define each one so Siri knows everything that your app can do. Next, you'll need to donate your new shortcut. That means you need to tell the system every time the user does something in your app that you expose a shortcut for, which will let Siri learn when and where is the right time to suggest your shortcut. And the third step is to handle your shortcut. That means, when the user wants to go and use your shortcut in Siri, on the lock screen, in search, you need to be ready for your app or app extension to be invoked and be handed the shortcut to handle. So, before you define shortcuts, you'll need to decide what exactly it is that you want to expose. And you should start by thinking through what are the most important things that people like to do with your apps? Because those are the things that you might want to consider exposing shortcuts for. So, every shortcut that you expose should accelerate the user, to perform a key function of your app. That means it should take something that the user already wanted to do with your app and help them do it even faster. The acceleration that you offer should be substantial, so you shouldn't just expose a shortcut that does about the same thing as opening your app normally. If you expose a shortcut that doesn't provide very much acceleration to the user, it won't be suggested as frequently. And, next, every shortcut that you expose should be of persistent interest to the user, so that means it should be something that the user might want to do multiple times. It's not a good idea to expose a shortcut for something that the user might only want to use once or twice. And you should also expose only shortcuts that are executable at any time, because you can't rely on the user being in some particular state before your shortcut will be ready for use. So, once you've decided what shortcuts to expose, you're ready to check out the shortcuts' APIs. There were two APIs that we support for adopting shortcuts. The first is NSUserActivity. NSUserActivity is a lightweight way to represent the state of your app, and it integrates with other Apple features like Spotlight and Handoff. And the second is intents. Intents are a way of representing, in detail, a type of action that your app can perform, and Siri includes a bunch of great built-in intents that support a range of capabilities that apps can use to integrate with Siri. And, this year, we're introducing something really cool, which is the ability for you to define your own custom intents for use with Shortcuts. So, for every shortcut you expose, you'll need to decide whether it should be an NSUserActivity or an intent. So, let's talk about how to make that decision. Now, NSUserActivity is a great choice for building a shortcut. If you're just building a simple shortcut that opens something in your app, or if you're exposing a shortcut for something that you already index in Spotlight search, or that you already offer in NSUserActivity for, for example, for Handoff. But for the best Shortcuts experience, you'll want to adopt intents, and intents-based shortcuts are really cool, because they can run inline without launching your app. And they can include custom voice responses and custom UIs like we saw earlier. And they can also include granular parameter predictions, which Willem tell you a little bit about later. So, once you've decided how to adopt, you're ready to dive into the implementation, so let's do that now. First, let's go over how to adopt shortcuts with NSUserActivity, and the first step is to define your shortcut. For NSUserActivity, that means you need to declare a UserActivityType in your app's Info.plist file to register your activity type with the system. Next, you need to donate your shortcut. For NSUserActivity, that means every time the user is looking at the screen in your app that you want to provide a shortcut to, you should make an NSUserActivity object available. So, there's one key new thing here, which is the isEligibleForPrediction flag. Setting this on any user activity turns it into a shortcut, and what's really cool is if you have an existing user activity in your app, you just have to set this to true and that user activity will become a shortcut automatically. Now, note that the isEligibleForSearch flag has to be true in order for isEligibleForPrediction to have an effect. And, by the way, you might also want to consider the isEligibleForHandoff flag, which defaults to true. So, by default, all of your user activities will be able to be handed off between devices. Now, when you're creating your user activity, you want to make sure to include all of the information in your user info dictionary that you'll need to restore the activity later on. And the last step is to mark your activity current by attaching it to a UI viewController or a UI responder object that's onscreen. Now, the last step is to handle your shortcut, once you've defined and donated it. And every time the user uses an NSUserActivity from your app, it will be opened in your app. And so you need to handle it by implementing the app delegate method for handling NSUserActivity, which is called continueUserActivity. So, all you have to do is implement this method, check if the activity type matches the one that you registered, and, if it does, then you want to restore the state of your app to what it was when the user activity was saved. So, that's everything you need to do to get Siri to start suggesting shortcuts for your apps with NSUserActivity. Now, let's talk about how to expose Shortcuts through intents. The first step, again, is to define your shortcut. And so with intents, you'll need to start by deciding what type of intent you'd like to adopt. And, as you know, Siri includes many great built-in intents, like for messaging, workouts, lists, and more. And now we're introducing the ability to define custom intents in Xcode for Shortcuts. And so if the shortcut that you want to build corresponds to one of the default built-in SiriKit intents, then you should adopt that. But, otherwise, you can define your own. And whether you want to define your own or adopt an existing one to customize it, you want to get started by creating an intent definition file in Xcode. So, believe it or not, in my spare time, I've been working with a couple friends on a new app, and it's a soup delivery app, and it's called Soup Chef. It's the easiest way to order soup, and I'm really excited about the potential of hooking it up to Siri. So, let's step through how to use the new Intent Editor to build an intent for Soup Chef. I'm going to start by going to File, New File in Xcode, and I'll choose the SiriKit Intent Definition File type. And then I'll see the new Intent Editor. So, let's get started by clicking the plus button in the bottom left. Next, I want to give my intent a name. And, so in this case, I'm making an intent for ordering a soup, so I'm going to call it OrderSoup. Then I want to fill out my intent's metadata. So, let's look at that one step at a time. The first piece of metadata is category and defining a category helps Siri know how to talk about your intent and how to display it in the UI. So, for example, in this case, I'm choosing the Order category, and when I choose Order, Siri will say something like, "Okay, I ordered it," when I use it with Siri. And it'll display a button confirmation title like Order. So, we offer several different categories, and you should choose the one that matches your intent's purpose the most closely. Next, you want to fill out your intent's title and description, and that will be used in all the places where people can discover which shortcuts your app supports. And then there's the User confirmation required checkbox, which determines whether or not we'll ask the user for a confirmation before using your shortcut. So, Siri might say something like, "Are you ready to order with Soup Chef?" That's really great for my app, because I want to make sure that people aren't accidentally ordering soup [laughs]. So, then, you'll see the intense parameters. And these define all of the parameters that are passed to your shortcut. For example, in my case, I have two parameters. They are the list of items being ordered and the location to deliver to. Now, parameters support a short list of types, and those types include things like string, number, person, and location. But if you're building a parameter that represents an object that's in your app that's not on the short list of types then you can choose the custom type. Once you've defined your parameters, you'll want to look at shortcut types. These define all of the types of shortcuts that you'd like to be suggested to the user. Each type includes a particular combination of parameters that will be predicted, and formats for how to display the title and subtitle of each one, filling in the values for your parameters. So, if your shortcut can't be performed in the background because it needs to launch your app when you use it, you should uncheck the Supports background execution checkbox. Now, in some cases, one intent may have multiple shortcut types, where some support background execution and some don't. Now, in the case of Soup Chef, that's especially relevant, because we can support background execution when both the items and delivery location are predicted, because we have all we need to place an order. But if we add a shortcut type that only includes delivery location, Siri may predict that, and we won't actually have enough information to place an order. So, we'll need our app to open to ask the user what location to deliver to, and, in that case, we'll want to not support background execution for that shortcut type. Now, you should specify shortcut types for every variation of your shortcut that you think will be useful so that Siri can make the best predictions. And, for the best experience, you should-- on all the shortcut types, you can-- support background execution. Shortcut types that support background execution provide more acceleration to the user, and so they'll actually be suggested more frequently. You can provide up to 16 shortcut types for every intent that you define. Now, once your intent is defined, Xcode will automatically code generate an intent class and an intent handling protocol. In my case, we've generated the OrderSoupIntent class and an OrderSoupIntentHandling protocol with properties that correspond to what I just defined in my intent definition file. Because Xcode is doing code generation, it's really important to consider in which targets Xcode is doing this code generation, because you don't want to end up with duplicate instances of the same classes that conflict with each other. So, let's look at the targets in the target membership section of the inspector on my intent definition file. Your intent definition file should be included in every target in which your intent is used, so you should check the box under Target Membership. But you'll want to make sure that if you have a framework, you don't do code generation in multiple targets that are included from each other. So, if you have a framework, set the target membership to only generate intent classes for that framework by choosing intent classes and choosing no generated classes for every other target. But if you don't have any frameworks in your app because you haven't separated your app into frameworks yet, you should check the target for every single target. And that's how you define custom intents in Xcode. Once you've defined your custom intent, it's really easy to donate it. All you have to do is instantiate your intent object and then populate its parameters and create and donate an INInteraction object. Make sure to do this every time the user performs the equivalent of your shortcut in your app. In this case, every time the user orders soup, I want to donate this intent, because that's going to help Siri learn when is the best time to predict it? So, all that's left, now that we've defined our custom intent, is to handle it. And, just like with NSUserActivity, you should implement the continueUserActivity method in your app delegate in order to handle your intent. So, when an intent is opened in your app, it's passed in as an NSUserActivity object, whose activity type is the name of the intent class that you generated earlier. In this case, it's OrderSoupIntent. But if you only implement continueUserActivity, your shortcut will open your app every time. It won't run in the background, or work inline in Siri, or support things like custom voice responses. So, for the best experience, you want to create an extension to handle your shortcut in the background. And to do that, create a new target in your Xcode project and choose the Intents Extension template. Make your intent handler conform to the intent handling protocol. In this case, it's OrderSoupIntent handling and then implement these methods, which are confirm and handle. Now, note that unlike traditional SiriKit, you don't need to implement any resolve methods, because your intent is ready to go without any customization needed or any follow-ups from the user. So, in confirm, you should check all of the values of the properties of your intent to make sure that they're valid. And, if they're not, you should return an error code if you don't think you'll be able to handle that intent. And then you should handle, actually perform your shortcut. So, in this case, that means actually placing an order for the soup. And then you return a response object that indicates the outcome, such as success. Again, you should implement an intents extension for every shortcut that can run in the background, because then they'll run right in line inside of Siri, on the lock screen, in search, in the Shortcuts app, and on the Siri Watch Face, without having to launch your app. The most valuable shortcuts are the ones that run in the background, but you can also build a lot of great other shortcuts. And keep in mind that, even if you implement an intents extension, you should always implement continueUserActivity, because the user will have the opportunity to open the shortcut in your app from Siri, for example, by tapping on the card in Siri that shows your apps' custom UI. So, there's one more thing that I want to tell you about, which is INRelevantShortcut. Now, INRelevantShortcut is the way to expose shortcuts to the Siri Watch Face, and you do it just by providing INRelevantShortcut objects that include your intents or user activities. You can, optionally, include relevance information in your relevant shortcuts, which is a hint to the Siri Watch Face as to when your shortcuts will be relevant and when's the best time to show your shortcut? Now, the really cool thing about INRelevantShortcut is that it works even if you don't have a Watch app. So, you can actually expose relevant shortcuts from your iOS apps, and if they run in the background, they'll appear right on the Siri Watch Face. Okay, now that we've explored all the ways that Shortcuts can be used and how to adopt the APIs, I'm going to turn it over to my colleague Willem who's going to give you a great demo of how to adopt shortcuts in Xcode. Willem? Thanks, Ari. I'm so excited to be the first one to demo adopting Shortcuts. Before I dive into Xcode, let me show you the app we've been working on. As Ari mentioned, we've been working on a soup-ordering app called Soup Chef. Let me show it to you. So, this is Soup Chef. When I launch the app, I'm presented with my order history. Since I haven't ordered anything yet, this is still empty. I can place a new order by tapping the plus button. This presents me with the soup menu, where you can see the available soups I can order. Let's order a tomato soup. Next, I can specify the quantity and options for my soup. I'll order a single tomato soup with red pepper, and when I'm ready to order, I tap the Place Order button. I'm brought back to the order history where you can now see the order that I just placed. I can tap the order to see more details about a specific order. This view has an associated user activity. I think it would be pretty cool to suggest this to our users, since our users like to reminisce about the good soups they've ordered in the past [laughter]. And it would be great if this could be suggested to them. So, let's go to Xcode and see how we can do that. Here, I'm looking at the viewController for the order detail view. I'm creating a user activity, and I'm setting the requiredUserInfoKeys. I'm also making sure that it's EligibleForSearch. For it to be suggested, I also need to make it eligible for prediction. And that's it. Let's try it out. First, I need to make sure that I donate the user activity, so I'll go back to the view. That should be enough. Now we want to make sure that the donation actually happened, and, to do that, we added two new developer settings that make it super easy to see a recent donations in search and on the lock screen. To enable it, I go to Settings, and I scroll down to the developer section. At the bottom, there are two new toggles, Display Recent Shortcuts and Display Donations on Lock Screen. Let's enable both. I can now go back to the home screen and pull down to go to search, and I see the donation that I just did. Great. I can tap it, and the app is launched with that user activity, and I'm back to the order that I was just donated. Great. So, this is pretty good, but I think we can do a lot more. Since the main capability of our app is ordering soup, it would be great if this could be suggested to our users. And I want users to be able to do that without having to launch the app. So, I'll implement it using an intent. There isn't a built-in soup order intent, but now in iOS 12, I can create a custom intent, so I'll do that. And to start by creating an intent definition file, I'll go to File, View. File and select the SiriKit and then definition file. Click Next. Now I can give it a name. I'll keep it to Intents, and I'll put it in the Resources group. All right, and I'm ready. I click Create. I'm presented with our intent definition file and our new Intent Editor. Before I add the intent, I'll make sure to include the intent definition file in the right targets. As Ari said, we need to add it to all the targets where the intents are used. So, I'll add it to our shared framework, and, since we're using a shared framework, I don't want it to generate go for app target. So, in the dropdown, next to the app target, I'll select No generated classes. Now that that's done, we're ready to add our intent. I'll click the plus button in the lower left corner and select New Intent. I'll give it a name OrderSoup. Next, I'll select the category. In this case, it's an order intent. Fill out the title, OrderSoup, and the description, Order a soup from Soup Chef. Since this involves a real-world transaction, I want to use it to confirm this order before placing it, so I'll select User confirmation required. Next, let's define the parameters. I'm going to define three parameters, one for the soup, one for the quantity, and one for the options that the user selects. I'll start with soup. I click the plus button in the parameter section and fill out the name, soup. Since soup is a custom object of our app, I'm going to select the custom type. Next, I'll add quantity. I click plus again, fill out the name, and this is going to be an integer. Next, options. Again, options is a custom object of our app, so I'll use the custom type. And since the user can select multiple options, I'll also check the array checkbox. Finally, we need to define the shortcut types. At this point, I'm just going to define a single shortcut type that contains all the parameters. I'll click the plus button in the shortcut type section and select the parameters I want to include in the shortcut type, so I'll select soup, quantity and options. When I'm ready, I click the Add Shortcut Type button. I fill out the title Order, and then I'm going to insert the quantity and the soup with options, and I'll leave the subtitle empty, because I've already conveyed all the information I need to in the title. I'll also leave Support background execution checked, because I want the user to run this intent in the background. So, we just defined our first intent. Let's start using it. I'll start by adding two helper methods to our order object to make it easier to convert between it and the intent. I'll go to the order class and, at the bottom, I'll add an order extension. It contains a computer variable that returns an intent. In it, I create the intent. I set the soup quantity and options, and I return the intent. Extension also defines a new initializer that dates an intent. In it, I extract the soup, quantity, and options, and then I initialize the order with those values. Great, this will be very helpful. Next, we need to make sure that we donate the intent every time the user places an order. So, I'll go to the soup order data manager, and, in the placeOrder method, I'll add our donation logic. I create an INInteraction that contains the intent of the order, and then I just donate that interaction, and that's it. Finally, we need to handle the intent. I'll start by adding support in the app itself, so I'll go to the AppDelegate, and, in the continueUserActivity, I'll add support for it. I check if the activityType of the userActivity is equal to the class name of the intent that I want to handle. If that's the case, I extract the intent from the userActivity, and I generate an order with it. Finally, I present the order view. So, since we're launching the app, I'm assuming that the user doesn't want to immediately place the order but wants to make some customizations before placing it, so that's why I'm presenting the order view instead. And, finally, let's add support with an intents extension so the user can run this intent in the background. I'll need to add an intents extension first, so I'll go to File, View, Target and select the Intents Extension target. I'll give it a name, SoupChefIntents, and I click Finish. Since I've added a new target where I'll use the intents, I need to make sure that the intent definition file is included in that target. So, I go back to the intent definition file and add it to that target. Again, I don't want to generate code in that target, so, in the dropdown, I'll select No generated classes. I also want the extension to have access to the same data as the app, so I'll add it to the same app group. In the Project Settings, I select the intents target and in the capability step, I'll add it to the same app group. Great. Now, we're ready to implement the intent handler that was created with this target. First, I'll import our shared framework, SoupKit [laughs]. Since we're going to handle the OrderSoupIntent, this intent handler needs to conform to the OrderSoupIntentHandling protocol. This was generated as part of our intent. This protocol has one required method, the handle method. Let's implement it. In the handle method, I get an intent. I create an order from that intent, and, if that succeeds, I place the order, and I call the completion with an intent response with a success code. If I'm unable to create an order from the intent, I call completion with an intent response with a failure code. And that's it. I've just added support for a new intent in my app. Let's try it out. First, I'll need to donate the intent. So, I'll place another order. Click the plus button and this time I'm going to order a clam chowder with croutons, and I'll place the order. Now, if we go back to the home screen and pull down, I see my donated intent. Great. I can tap it, and I'm presented with a shortcut view and the order button. If I tap the order button, the order will be placed in the background, and it's ordered. I can go back to the app and see if it was actually ordered, and, yeah, there is a new order in my app. Great, so easy. I can also, instead of tapping the order button, tap the shortcut view, itself, which will launch the app with the shortcut, and, as I implemented it, it shows the order view, where I can customize the order. So, I'll add cheese, too. Who knows? And I'll place the order. Great. Finally, let's add the shortcut to Siri. I'll go to Settings, and I scroll up to the Siri and search. Here I can select the shortcut that I want to add to Siri. In this case, I'll add the order one clam chowder with croutons. When I tap the record button, I can say the phrase I want to associate with the shortcut. Soup time. Now, they got it. Let's take a step back and look at what we just did. We started by making an NSUserActivity that was already implemented in our app eligible for prediction. This allowed it to be suggested and is a really easy way to expose the content of your app. Next, we defined a custom intent. This is really the best way to represent a piece of key functionality of your app. In our case, that was ordering a soup. After defining the intent, we made sure that we donate the intent every time the user placed an order inside of the app. And, finally, we handled the intent, both in an extension to support background execution and in the app itself so the user can launch the app with the shortcut. So, now that we've seen how you can adopt shortcuts, let's take a look at how those shortcuts are suggested and what you can do to make sure your users see the best possible suggestions. Every time a user performs an action in your app and your app donates this, the system will look at the time, location, and other signals. For a time, we consider, among other things, the time of day and the day of the week. Well, for the location, we look at the overall location of the user and see if it's a significant location for that user. Let's see how the system now uses these to make a suggestion. Here, we'll just consider the time. Monday, at lunch, I order a tomato soup with croutons. That evening, I don't feel like croutons, so I order a tomato with red pepper instead. Next day at lunch, I, again, order a tomato soup with croutons. I keep doing this throughout the week, and, on Friday, at lunch, Siri might try to find a suggestion for me. It will look at my past behavior and try to find a pattern in it. And since it's lunchtime, and I usually order a tomato soup with croutons at lunchtime, Siri might notice this and suggest this to me, which is great. It's exactly what I wanted. So, this is all pretty high level, so let's take a look at how it actually works, starting with NSUserActivity. Imagine a user activity for the place order screen in Soup Chef. The user info dictionary could contain three keys, soup, quantity, and scroll position. The last one is there, so, in Handoff, you can bring the user back to the exact position they were last in. Let's take a look at how this can be suggested to the user. We start by donating a user activity where the soup is tomato, the quantity is 1, and the scroll position is 79 points. Next, we donate a similar user activity, but now scroll position is changed to 110 points. We keep doing this, and, at some point, Siri will try to find a suggestion again. It will look at past behavior and try to find a pattern of equal user activities. But since the scroll position is so inconsistent over time, it might not be able to find a suggestion. So, how can we fix this? We can use the requiredUserInfoKeys. RequiredUserInfoKeys is an existing property of a user activity. It represents the minimal amount of information that is necessary to restore the app to the state that the user activity represents. And for suggestions it will be used to determine which keys of the user info dictionary will be used to compare when looking for patterns. Let's apply this to a previous example. Now we specify that requiredUserInfoKeys are soup and quantity. Again, we donate the user activity where the soup is tomato, the quantity is 1, and the scroll position is 79 points. But now the scroll position will be ignored. Next, we donate something similar, and, again, the scroll position is ignored. We keep doing this, and, at some point, Siri will try suggestion again. Now, I will look back and try to find a pattern of equal user activities. And since it's no longer considering the scroll position, it might be able to say, "An NSUserActivity with soup tomato and quantity 1 is a good suggestion for this user." So, as you just saw, it's really important to specify the right set of keys as being required. Otherwise, your users might not get any suggestions at all. So, that's how it worked for user activity. Intents work similarly but offer you a lot more flexibility. The main signal for intents are the shortcut types that you define. Each shortcut type defines a combination of parameters that is valid for suggestion. It's similar to the requiredUserInfoKeys, but instead of having just one set, you can define multiple. Let's apply this to our Soup Chef app. Earlier, I defined an OrderSoupIntent with three parameters, soup, quantity, and options. At the time, I only defined a single shortcut type that combined all of these parameters, but, ideally, you would define more shortcut types, since that gives more options to the system to find patterns in your user's behavior. So, now I'll define three. One shortcut type that combines soup and quantity, one that combines soup and options, and one that combines all three parameters. Let's apply this to another example. Again, I start on Monday, at lunch, by ordering a tomato soup with croutons. The Soup Chef app donates this to the system, and the system will split us up into all the possible combinations based on the shortcut types that I just defined. That evening, I order a tomato soup with red pepper. Again, it gets donated, and it will be split up in all the possible combinations. Next day, at lunch, I order a tomato soup with croutons. It gets donated and split up. I keep doing this throughout the week, and, on Friday, at lunch, Siri might try to find a suggestion. It might see that I often order a single tomato soup. But since it's lunchtime, it also can see that, at lunchtime, usually order a tomato soup with croutons. Since that is a more specific shortcut, it will end up preferring to suggest that. So, I might get a suggestion to order a tomato soup with croutons. Great. So, that's how suggestions are made. Let's now take a look at how to make sure those suggestions are good, and it all starts from making good donations. A good donation should be something that is likely to be repeated. For user activity, there could be some content they might look at often. Or, for an intent, there could be an action the user would take regularly. You should make sure that the payload of what you're donating is consistent across all your donations, since that is what will be compared when looking for patterns. A good donation should also not include a timestamp, since that might not longer be relevant by the time this donation would be suggested. For instance, a shortcut that shows appointments for a specific day is probably not that useful, since, if you see that the next day or later, you're probably no longer interested in the meetings of that specific day. A shortcut with a relative time, however, is a lot more useful. You should also make sure that you donate exactly once for each user action, even if that user action involves multiple actions inside your app. Finally, selecting the correct parameters of your intent is also important. So, let's take a look at two possible types, starting with enums. You can define enums in your intent definition file next to your intents, and use it as the types of your parameters. We recommend that you use enums whenever the values for a parameter are bounded. For instance, if you would add a size parameter to a order soup intent, it might make sense to make that an enum, since the possible sizes are probably just small, medium, and large. Using an enum will lead to better suggestions and clearer titles and subtitles for your users. To learn more about how enums are used to generate titles and subtitles, I recommend that you watch the localization session. Another type you can use is the custom type. Using a custom type will result in an INObject and your generated intent code. An INObject combines an identifier with a display string. The identifier can be used to refer to an object internal to your app, while the display string contains the user readable representation of that object. That way both your users and your app always understands what the value of this parameter is. Finally, using an INObject also prevents the possible dependency between parameters. Let me illustrate that. There are two ways to represent the combination of an identifier with a display string. You could add two parameters to your intent, one for the identifier, one for the display string, or you could add a single parameter using the INObject. With the first approach, you have a dependency. Since the display string depends on the object as referenced by the identifier, we discourage having these dependencies as they could cause issues when we suggest this intent later. A good shortcut should also have an understandable title, subtitle, and image. It should represent what will happen when the user taps the shortcut, since it will be the only thing the user will see before interacting with it. And, of course, you want to test your shortcuts to make sure that they look right and that they behave as expected. There are a couple of ways to do that. As I showed you earlier, we added two new developer settings that allow you to see your recent donations on search and on the lock screen, instead of your regular Siri suggestions. By enabling these, you can see what your users will see when those donations would be suggested, and you can try interacting with them to make sure that they behave as expected. Another way to test your shortcuts is by adding them to Siri. An easy way to test them then is by editing the Xcode scheme to automatically invoke Siri without constantly having to say the phrase. In the scheme editor of an intents extension there is the Siri Intent Query field that you can use to provide the utterance to invoke Siri with. And, finally, you can create a custom shortcut in the Shortcuts app that uses your shortcut. This allows you to test the behavior of your shortcut when it's chained together with other shortcuts or steps from the Shortcuts app. So, now that we've seen what a good shortcut donation is and how does donation get suggested to your users, let's take a look at a couple of privacy considerations and how you can make sure your user are never upset by what is suggested to them. Your users expect that when they delete something from within your app that it's deleted for good. It's important to honor this so you maintain your users' trust and your so users aren't presented with suggestions for things that are not relevant to them anymore. So, if you donate shortcuts that contain information that the user can delete, you should make sure to delete those donations at the appropriate time. Let's take a look at how to delete donations, starting with NSUserActivity. There are two ways to delete a donated user activity. If you use Spotlight indexing and you set the relatedUniqueIdentifier, then deleting the content from Spotlight will automatically delete the donated user activity. Just set the relatedUniqueIdentifier on the contentAttributeSet to the identifier of the searchable item that it matches with. Then, when that searchable item would be deleted, it would automatically delete the user activity. If you don't use Spotlight indexing, you can use the persistent identifier property on NSUserActivity. This is a new property that you can set so you can keep track of your user activities and delete them at the appropriate time. To use it, just set the persistentIdentifier property before donating user activity. Then, when you want to delete it, you call deleteSavedUserActivities with the identifiers that you want to delete. You can also delete all your donated user activities. For instance, when your user logs out, you can do that by calling deleteAllSavedUserActivities. Intents have an existing API, which is similar to this new user activity API. Since you donate intents through INInteraction the leading and donated intent also happens through INInteraction. An INInteraction has an identifier and a group identifier property that you can set and then later use to delete one or more donated interactions. Just set the identifier and or group identifier before donating the interaction. Then, when you want to delete it, you call delete with an array of the identifiers that you want to delete. You can also delete all donated intents with a shared group identifier by calling delete with the group identifier. And, finally, just like NSUserActivity, there is a way to delete all your donated intents by calling deleteAll on INInteraction. Please make sure to delete your donations at the appropriate time. This will give your users the best possible suggestions and never make them wonder why they're seeing suggestions for things that are no longer relevant to them. So, now that we've looked at different things to consider when creating and donating shortcuts, let's end by briefly looking at what you can use to create the best possible media shortcuts. We created a new intent that works great with Shortcuts. This intent is called INPlayMediaIntent and allows you to create and donate shortcuts to play audio or video content. When handling this intent in your extension, you can choose to have it handled by your app in the background [inaudible]. This allows you to start audio playback straight from your app. Besides being suggested in search and on the lock screen, shortcuts with this intent will also be suggested in the playback controls on the lock screen when the user connects their headphones, making it even easier for them to start listening to your content. And, finally, shortcuts with this intent work great on the HomePod. Just add a play media shortcut to Siri on your iPhone and invoke it on your HomePod. The audio will start playing from your phone through HomePod. We also created a new API that allows you tell the system about new content they might be interested in. This is great for periodical content, where the content that you would want to be suggested to your users isn't something they've listened to or watched before. So, those are a couple of things that we have added to make great media shortcuts. Now, let's summarize what we just talked about. Shortcuts enable powerful, new experiences with your apps. It provides new ways to engage your users by exposing your app throughout the system in a variety of places. So, just search the lock screen, the Siri Watch Face on your Apple Watch, and in Siri itself. You can also use it in the new shortcuts app. You can adopt shortcuts by using NSUserActivity, which is a really easy way to expose the content of your app. Or you can use intents which provides you with a deeper integration in the system and can unlock entirely new experiences for your users. For more information, our session is 211, and you can find our page on developer.apple.com. You can also find us in the labs throughout the week. Thank you so much for coming. And we can't wait to see what shortcuts you'll create. Enjoy the rest of your conference.  Good morning. Welcome to our -- [ Audience Applause ] Thank you. Thank you. It's great to be here. Thanks everybody for coming. Welcome to our session on Introducing Podcast Analytics. I'm James Boggs. I manage the Business Team for Apple Podcasts and Siri Audio Briefs, and we've got a great session planned for you today. First, I'm going to share some details of our podcast business over the last year, including some very interesting and exciting milestones. Then I'm going to welcome Anne Wootton from our Engineering Team to the stage, and she's going to take us through some updates to Podcast app for iOS 12. After Anne, Alec Reitter will take us through a demo of Podcast Analytics, and then we will finish up with Anne sharing some requirements and taking a look at resources available to podcast makers. First off, an update on our podcast business. Well, we're just shy of the 13th Anniversary of the launch of iTunes 4.9. That's the moment when we took podcasts mainstream for the first time, and podcasts have come a long way since then. Our content catalogue has never been more entertaining, inspiring, and educational, and we've never reached more listeners around the world than we do right now. We're reaching them with some great programming like Sandra from Gimlet. This is a brand new program from one of the leading independent podcast networks out of New York, and it stars some really big talent, Kristen Wiig, Alia Shawkat, and Ethan Hawk. It's a really fun scripted comedy drama about the inner workings and quirks of a virtual assistant. Los Angeles-based Wondery is one of adventure capital backed network startups working in podcast today, and last year, Wondery collaborated with the LA Times to produce the thrilling true crime story of a charming but sociopathic con man, Dirty John. The Hollywood-style production value is powerful investigative journalism and remarkable storytelling, which is just riveting. It drove this show to the top of the charts in the US several of the markets around the world. If you haven't had a chance to check it out, I recommend it. It's a great binge. And of course, podcast fans everywhere are thrilled and enjoying Oprah's Super Soul Conversations. This is the audio instalment of the Queen of Talks iconic interview franchise, and we were overjoyed to have Oprah on Apple Podcast. These examples are just three of the more than 550,000 shows available today on Apple Podcast. That's quite a bit more content to listen to than the original 3,000 programs we launched with back in 2005. We're continually refreshing and managing our directory, automatically retiring shows which become technically unavailable or those that run afoul of our directory content guidelines, such as those with spammy content or shows seeking to manipulate our top charts. Don't do that. Just please don't do that. At the same time as we're managing the directory, we're accepting thousands of new submissions per week. That way, our catalogue is continually refreshed, and our users around the world can always find something great to listen to. Amongst those more than 550,000 active shows, we have over 18.5 million episodes of content. That's more than 8 million hours of programming. So, you'd better, better start listening, better start bingeing. As a reminder, Apple Podcast is available in 155 countries. That's not only for the distribution of existing shows but for the submission and acceptance of new programming from each one of those markets. The Podcast Directory serves an audience in all 155 of those countries with content in more than 100 languages. So, that means if you're a Korean native speaker, ex-pat, living in Toronto, you can find some great audio programming from home right in Apple Podcast, and we're particularly proud of this, particularly proud to have such a diverse catalogue and be one of the easiest ways to reach a global audience for audio programming that there is. The data from our beta version of Podcasts Analytics shows some remarkable listener affinity and engagement. We looked at several hundred of our recent top episodes, those with more than 7 days of playback data, and we found some really notable average consumption. Of course, this data is nonamized [phonetic] and aggregated and a sample, so no personally identifiable information is included, and it's not a comprehensive statistic but interesting nonetheless. It shows that once an episode is started, on average, about 80% of that episode content was consumed. This is great stuff and a real testament to the affinity and engagement the podcast audience has with this high quality content. Last fall, we successfully integrated more than 50,000 iTunes U collections into Apple Podcasts. This is content from some of the world's best educational institutions, including developing iOS 11 apps with SWF from Stanford's Paul Haggerty. Naturally, we're eager for the new instalment of this class series later this year. As part of Apple Podcast, teachers can now leverage the episode serial ordering support in Podcast app. They can reach a new broader audience of lifelong learners around the world, and it can distribute the content on tvOS for the first time. We're seeing some great use of modern podcast tags, which we introduced last year right here on stage at WWDC. Many podcasts include trailers to thematically introduce the feel and sound of their show. This is the best way really to launch a new podcast, and ESPN, for example, makes great use of trailers and, in addition, is amongst the chart shopping shows that uses seasons and bonus content. This allows Jody Avragon [phonetic] and the team to easily present the thematically different collections of episodes in each season. In addition to seasons and bonus content, 30 for 30 feeds their super fans with bonus episodes. These complement the full season episodes. A best practice regarding bonus content is to make sure that your bonus episodes are tagged appropriately to the full season episodes. Just want to ensure proper ordering of the content in Apple Podcasts. And of course, episode number, we're using this handy reference for your audience inside Podcast app. We're also making use of episode numbers that you provide in your RSS feed in Podcast Analytics. An innovative crop of venture-backed hosting startups has started to grow. Organizations such as Anchor, with some great new iOS-based creation tools, ART19, which is an easy to use solution for your enterprise podcast needs, and Simplecast, with brand new migration tools and a 2.0 version they just launched. These next generation mobile optimized podcast creation tools can help make your programming available to the world, and these folks join our lineup of long-standing hosting partners from around the world, including Libson, Megaphone, Omni Studios, and Wushka from Australia, Spreaker, and more. Check out our resources now for our full list of Apple podcast hosting partners. HomePod, which arrives soon in Canada, France, and Germany is an awesome podcast machine. Siri has been optimized for podcast playback, so you can just ask Siri to subscribe to or play your favorite show. We also offer audio news briefs. Just ask Siri to play me the news. We're proud to be distributing headline news bulletins from some of the best news organizations in the world, including NPR, the BBC, ABC Australia, ESPN, Bloomberg, and more. You can ask Siri for audio briefs and general news, sports, business, and even get an update on the goings on in music from our own Apple Music Team. We're thrilled to celebrate the incredible milestone of 50 billion episode downloads and streams on Apple Podcasts since the launch of the service in 2005. With over 10.5 billion episode downloads and streaming plays in 2016 and more than 13.7 billion last year, the growth trajectory of the platform is amazing. We're also thrilled that the shows that are driving all this consumption are of such a high quality, and it's a pleasure for us to be able to champion this content on top of our platform. We celebrate the podcasts that contribute to this 50 billion milestone, and one show in particular, "Stuff You Should Know" from Atlantis How Stuff Works Network, the first podcast, an Apple podcast to exceed 500 million downloads and streams. Wow! Congratulations to Chuck, Josh, Jerry, and the rest of the team. Well done! We're looking forward to the next half billion downloads and streams. So, with that business update, let me welcome Anne to the stage to take us through our app. [ Audience Applause ] Thanks, James. Thanks James, and hi. I'm Anne, and I run Podcasts Engineering at Apple. Before we dive much deeper into analytics, I want to highlight a few of the latest updates to the podcast app. As you probably heard in yesterday's keynote, we are thrilled to be launching podcasts for Apple Watch this fall. It was one of the most requested features for Apple Watch and for good reason. Between HomePod and Apple Watch, listening to podcasts has never been easier or more hands free. We're also really excited to share that we've enhanced our support for chapters in the podcast app. In the upcoming IRS release, we're extending full chapter support to MP3s. In addition to enabling podcasters to segment their episodes into shorter discrete chapters, this functionality also includes chapter art that displays as the play head advances across chapters. So, podcasters can supplement their audio with visual content. In the upcoming iOS release, we're also offering an improved experience for podcast notifications. The new podcast notifications are grouped across shows and episodes into a single expandable stack that reduces clutter on screen, and we also provide one-tap access to notification settings for all of your shows. That means if you only want to get notifications for your favorite show, 99% invisible, it's as simple as toggling the switches on this single settings page. So, last year, at WWDC, we announced that Apple would be launching Podcast Analytics for the first time. We were super excited to launch at the end of 2017, and Podcast Analytics is available in beta right now today to anyone with a podcast in our catalogue. We know that this is a service that was long desired and speaks to a critical need in the industry as evidence by work like the remote audio data spec being spearheaded by NPR. So, it's important to us that we provide this anonymous data back to publishers to help them better understand how their audiences are consuming their audio. And all of the data and analytics is based on the use of the update Apple podcasts app from iOS 11 and iTunes 12.7. We're collecting data only from those operating systems, Forward and the Analytic Stashboard includes listening data from HomePod, AppleTV, and will include Apple Watch listening data starting in launch of S5. As you know, privacy is really important to Apple, and we are only showing aggregate data of those users who have agreed to use the podcast app. It is anonymous, not personally identifiable, not tied to your Apple ID, and we only reflect the data points in aggregate that we've collected from at least five unique devices. And it couldn't be easier to get started. You don't need to write any code, you don't need to use an SDK, and it's totally free. If you have a podcast, all you have to do is submit your feed to Podcast Connect after you signed in, get it approved, get some listeners, then check your performance through the Podcast Analytics dashboard. Analytics reports the data from unique devices that engage with the Podcasts app. As my colleague, Alec, is about to show us from the Analytics Dashboard, publishers can access an overview that includes tables and charts with data-like unique device counts for a selected period of time, total time listened, time per device, average consumption, top countries, and more. So, some of you may be familiar with our podcast, The Very Hungry Tourists. They're back for season 2, and they even have some new cover art. Alec is going to take you on a deeper dive of how to get the most out of analytics. We're excited to continue providing key listening metrics for this industry, and we are listening to your feedback and continuing to improve the service. So, thank you. And with that, I'm going to hand it over to Alec to take us on a tour of Podcast Analytics. [ Audience Applause ] Thank you, Anne. So, podcasters, every day you're hard at work making great shows and episodes for people to enjoy all over the world. But how much do you know about your audience? Do you know how many listeners you have or how much they listen? Do you know where they are around the world? What are their favorite episodes? With Podcast Analytics, Apple offers you transparency into how your show is heard, so you can better understand your listeners and create the best show for your audience. My name is Alec Reitter, and I'm going to do a walkthrough and demo of Podcast Analytics. So, let's get started by heading to the login page. When you see this bright purple page, you know you've made it to the right place. Here, we're going to login with the Apple ID that we used to originally submit the podcast feed to the Apple Podcast catalogue. Now, if you have any trouble, such as you forget your password for your Apple ID, or you don't know which Apple ID to use, or if after you've logged in you don't see all the shows you expect to see, go to the top right corner of this page and click on that question mark icon. That will take you to Resources and Help, and there you can find Contact this Link, where people can give you the assistance you need. In this example, we're going to use John Appleseed's login credentials. So, let's login now and see what he sees when he enters Podcast Analytics. So, this is the show's page. Here, we see five shows for John's account. In this table, there are three metrics, and we're going to take a look at those in a minute, and all of the data in this table is driven by the date picker, which is in the top right corner. Right now, it's set to the last 60 days, and that is the default. So, let's take a look at the top row for The Very Hungry Tourists. Here, we can see the number of unique devices for the last 60 days. This answers the question, how many listeners do I have? Here we see 256,000 unique devices. Next to that, we can answer the question, how much do they listen? In this case, the total time listened is 327,000 total hours. Now, if we do a little math, we can find the average, which is the time per device of 1 hour and 17 minutes. So, this is a very popular show. It's doing really well. So, next, we're going to switch tabs from the show's tab to the episodes tab. Here, we can see a cross-section of episodes across all of the shows in John's account. Here, we see a few of the most recently released episodes. Now, this data table has a lot more information in it, and let's take a look at the top row. This is the most recent episode from The Very Hungry Tourists, starting on the right-hand side, we can see the name of the episode, then the name of the show, and now, the third column is the episode column. If you're making use of the new tags, like season, episode, and episode type, they'll show up here. Following that, we have the release date and then the duration. So, in this case, it's a 35-minute long episode. If, when you login, you don't see the right number in the duration column, it's because of what you have in your feed. We pulled this number straight from your feed. So, make sure it's accurate. Following the duration column, we see three columns that we've seen before, except this time, for this episode specifically. So, we have devices, total time listened, and time per device. And for the first time, we see average consumption. This is the last metric in the far right column. Average consumption is really great because it's really just another way of looking at time per device. So, in this case, it's 28 minute for this 35-minute long episode. That is 81%. What's nice about average consumption is it allows us to compare episodes of differing durations against each other to really get a sense of how healthy each episode is doing. And from here, let's click through on the Show Title and look at the show overview for The Very Hungry Tourists. So, here's the show overview. There's a lot going on here. Let's start at the top. Across the top, we have three headline numbers. These are the same numbers that we saw before in the shows table, but now we have a mini time series underneath these numbers to give you a sense of how things are doing. Maybe you can see, just looking at that little line, if your show is getting more or fewer listeners over time. Now, let's look beneath this row and take a look at the chart on the left here. This is the donut chart of total time listened. So, for all of the time spent listening to your show, were those devices subscribed or not subscribed? Here, we can see that 83% of that 328,000 total hours of listening was done by a subscribed device versus 17% not subscribed. So, these are pretty good numbers. But why might these numbers be different when you take a look at your data? Well, if you have a very small podcast, one with a very dedicated following, you might not have too many new listeners, but your subscribed count might be above 90, even 95%. Conversely, if you're doing a lot of marketing or promotion, or your show is growing rapidly, you're not subscribed count might be fairly high. Now, when somebody first starts to listen to your show, obviously, they're not going to be subscribed most of the time. But once they hear what you have and they hit that "Subscribe" button, that's when they end up in the other bucket. Now, let's take a look at the chart to the right. This is the Top Countries by Device. Here, we can see the top five countries for the show. As we can see, the United States comprises the majority of the listening at 78%. If we look at the fifth spot, we see Brazil. Now, what's neat about this is that Brazil wasn't on this top five before, but as Anne said earlier, this is season 2 of The Very Hungry Tourists, and on the first episode of this season, they actually went down to Brazil. They did some marketing and promotion, and it's starting to pay off. So, next, let's scroll down to the bottom of the page, and here we can see the ten most recently released episodes for the show. This is the same sort of data table that we saw before for episodes, but this time it's for a single show. What's nice about this is we can compare apples to apples and really see on an episode by episode basis how things are turning out. Let's go back up to the top, and we're going to switch from the Show Overview to add to the Trends tab. So, here we are in the Trends tab. This is a Time Series Chart where we can see day by day how many people are tuning into this show. And again, by default, we're looking at the last 60 days of data. In the top left corner, we see that headline number again of 256,000 unique devices, and down below the Time Series Chart we see a table with episode titles. We can choose up to five separate episodes to compare against one another in the chart above. So, let's first now talk about all of the different drop downs that control the chart and what we see in it. First up, we have a drop-down. This is the metric that we're going to chart. By default, we start off with Unique Devices, or you can also choose to have it render Total Time Listened. To the right of that, we have another drop-down. This is what I call the primary filter. By default here, we're looking at Episode as the primary filter, but you might want to choose Country or Subscribed Device. Now, whatever you choose for your primary filter, that's going to dictate the table beneath the chart. So, whatever is selected allows you to choose multiple entries to compare against one another. For example, we could choose Country, and we'll have countries in that table below. Whatever filters are not selected from this menu, they move over to the right-hand side of the page in their own drop-downs. For example, countries or subscription state [phonetic]. For each of these, you can choose up to one filter. Now, with that, that's enough talking through slides. Let's look at a demo, and I will take you back to that page but in a hands-on fashion. So, here we are. We're in that same view that we saw before of the Trends page. But now, we can see what happens when I interact with it. When I move my mouse cursor over the chart, we have this nice data legend with details about the specific point that I'm hovering over. In this case, I'm looking at May 18th, where 34,000 unique devices tuned in. And I can go down the page, and I'm going to select a few episodes to chart against each other. We get this nice colorful view, and as I mouse over, I can also again see the details of what I'm hovering over. In this case, I can see episode titles and their corresponding counts for those days. Now, I'm going to go change my primary filter, and I'm going switch it from Episode to Country. So, this updates the chart once again. This is the same actual graph that we saw the first time because now, again, we're looking at all countries. But I'm going to choose the top five countries and compare those against each other. Once again, we get this colorful chart. But in this particular case, because the United States represents so much of the listening for this show, it makes it difficult to see the other countries and how they stack up against each other. So, I'm actually going to go and deselect the United States. The chart adjusts, and we can really see with more detail how each country is doing. And now that I can, I'm going to show you the date picker. It's up here on the top right corner. By default, it sets at the last 60 days. You can switch that to weeks or a specific month, but we'll leave it at that for now. Now, there are lots of combinations that you can make here, but I'm going to move on after this, but I really encourage you to take a look at your shows and see how they're doing and really play around with this tool. I think it's a lot of fun to use. So, let's switch over to the Show Episodes tab, and in this view, we can see the top episodes or all episodes for the show, not just the top episodes. In this case, the show has 11, 11 episodes total. And now, because all episodes are together in this view, we have the opportunity now to answer some other questions like, what are the most popular episodes? One way to answer that is by sorting by the Unique Devices column. When I do that, I can see that, essentially, season 1, episode 4, Barcelona, Barbacoa, that is the most popular by devices, by unique devices. But maybe you think that it's actually total time listened that should be the determinant of popularity, so I'm going to sort by that instead, and we see that it's actually season 2, episode 1 that takes the top spot. Or for myself, I personally really like average consumption, so I'm going to sort by that, and I can see that at 89%, the trailer is actually the most popular episode, which makes sense because it's a short episode. So, now, we've actually seen all of the show level views. Now, I'd like to take you into an episode level view. So, I'm going to select, I'm going to click through on the title for the most recently released episode. Here's Broken Heirloom. So, it was just released on June 1st. Front and center, we see a timeline chart. This shows you the audience size over the duration of the episode; actually, at every five seconds, and when I hover over the chart, I can see a data legend again with details about the specific time in the episode, its total count, and its percentage of the max. And in the top left corner, we see this headline number for the total number of unique devices. Something to note about this chart, unlike all the other views on this, in Podcast Analytics, this chart is actually for all time, not just a specified date range. Now, what's really nice to see here is that, in this case, most of the people who started this episode actually hung out for the entire duration. This is a really healthy-looking episode. And beneath the chart, if I scroll down a little bit, we can see for the last 60 days the metrics for this show, for this episode, I should say. This gives you a more recent sense of how this episode is doing, not just all time. And at the very bottom, we see the top ten countries for this episode. But I'm going to scroll back up and cover two things that I breezed past before. There is a drop-down menu above that all-time devices number, and when I click on this, I can see that I can also choose Devices or Total Time Listened. Now, by choosing either of those, it will actually take us into a view which is a time series view, just like the Show Trends page except this time it's tied specifically to this episode. And now, the part of this graph that I like the best is actually the ability to play back the audio while seeing how the audience is reacting over the duration of the episode. And I can do this by going down to the bottom left corner of the graph and hitting this Play button. Hey there, Hunger Heads, and welcome back to another episode of The Very Hungry Tourists. I'm Maria Sanchez, Ph.D. And I'm Johnny Appleseed. In today's episode, we're headed to Brooklyn based Viny's Pizzeria. As we're here, even Johnny's massive wonder couldn't get in the way of our culinary adventure. I'm still blushing from that one. So, this is actually a very entertaining episode, so if you get a chance, you should definitely check it out. But like any basic player, you don't just have to start at the beginning. You can also click on the timeline to jump ahead to any specific spot in time, and if you want to fine tune, you can grab the play head and scrub around. So, I'm actually going to scrub to the beginning of what looks like an ad. And so, just to validate my assumptions, I'm going to move to just before and take a listen. Five years, you wouldn't believe it, but in the back there is a big window-- As the pizza goes in the oven, let's take a moment to hear about out great sponsors. Just say your hands and face are smothered with Tahini sauce. So, what do you do? Get a paper towel. Ah, too rough. Tissue? Too soft. What about napkins? That's a great idea. Napkins to you is a new drone delivery service bringing you napkins where and whenever you need them. Simply open the app, request a drop off, and the closest drone is deployed to your location. By typing in promo code "Hungry." You can have your very own napkins delivered for just $4.99 per month. Again, that's Napkins to You. So, that's sounds like a really entertaining ad, and it's too bad that some people decided to skip it, but if you actually do the math from before the ad to the bottom of the ad, it's only about a 14% drop, and it seems like it's a temporary drop. So, it's doing pretty well. So, with that, I want to switch back to the slide, and I just want to say that wraps up the walk through and demo of Podcast Analytics for you, and I hope that you were able to learn some things about how this tool works and maybe how it can help you better understand your audience. And Podcast Analytics is in beta, and we are very interested in your feedback. So, if you submit your feedback link in the top left corner, right next to the beta badge, we would love to hear what you think about the, about Podcast Analytics, and give us your feedback. Thank you. Now, let's bring it back to Anne. [ Audience Applause ] Thanks, Alec. That was awesome! So, that's a walk through Podcast Analytics, which, again, is available to anyone with a podcast in our catalogue. If you haven't used it, try it out. It'll also be at a lab immediately following this session. So, come find us. And that's where we are today, but really we've just begun. You guys are the reason that we did this, and we love hearing from you. So, keep it coming. For example, we know that many of you want an API or a subscription and download data or geographic demographics with more specificity. We love those suggestions. We hear you, and we're working on improving the product in the future. We are also going to be asking for some things rom you in the future, including new specifications around HTTPS, pubDates, consistent GUIDs, and cover art that meets minimum required resolution. So, for continued access to Analytics in the future, you will absolutely need to have those four things. We're also going to be releasing some guidelines around best practices for audio mastering to give you a sense of the ranges you should be using so that your podcast sounds the best it can on HomePod and within the context of the Apple Podcast catalogue. We'll be sharing these requirements with you all. We will put them in Podcast Resources at Help so you can refer back to them anytime. And speaking of which, last but not least, I want to remind you about our resources for podcasters. So, we have a Branding Guide for podcasters. We also have the Affiliate Program, and then Podcast Resources and Help, which you might remember from the top right corner of the purple screen that Alec showed you. That features the hosting partner list that James mentioned; some tutorial videos for analytics, and of course, the ability to contact us for help anytime. For more information, you can also visit this link or, like I said, come join us in the lab, the Apple Podcast Lab, immediately following this session at 10 a.m. Thanks so much. [ Audience Applause ]  So, good morning. Welcome to our session on What's New in ARKit 2. My name is Arsalan, and I am an engineer from ARKit team. Last year, we were really excited to give ARKit in your hands as part of iOS 11 update. ARKit has been deployed to hundreds of millions of devices, making iOS the biggest and most advanced AR platform. ARKit gives you simple to use interface for powerful set of features. We have been truly amazed by what you have created with ARKit so far. So let's see some examples from App Store. Civilisations is an AR app that brings historical artifacts in front of you. You can view them from every angle. You can also enable x-ray modes to have mode interaction. You can bring them in your backyard, and you can even bring them to life exactly they look like hundreds of years ago. So this is a great tool to browse historical artifacts. Boulevard AR is an all right app that lets you browse the work of arts in a way that's never been possible before. You can put them on ground or wall, and you can really go close to them, and you can see all the details. It's just a great way to tell you story of arts. ARKit is a fun way to educate everyone. Free reverse is an app that places immersive landscape in front of you. You can follow a flow of a river going through landscape and see communities and wild life. You can see how human activity impacts those communities and wildlife by constructions. So it's a great way to educate everyone about keeping the environment green and through sustainable development. So those were some of the examples. Do check a lot more examples from App Store. So some of you are new to ARKit, so let me give you a quick overview of what ARKit is. Tracking is the core component of ARKit. It gives you position and orientation of your device in physical world. It can also track objects such as human faces. Scene understanding enhances tracking by learning more attributes about the environment. So we can detect horizontal planes such as ground planes or tabletops. We can also detect vertical planes. So this lets you place your virtual objects in the scene. Scene understanding also learns about lighting conditions in the environment. So you can use lighting to accurately reflect the real environment in your virtual scene. So your objects don't look too bright or too dark. Rendering is what actually a user sees on the device and interacts with the augmented reality scene. So ARKit makes it very easy for you to integrate any rendering engine of your choice. ARKit offers built-in views for SceneKit and SpriteKit. In Xcode, we also have a [inaudible] template for you to quickly get started with your own augmented reality experience. Note also that Unity and Unreal have integrated full feature set of ARKit into their popular gaming engines. So you have all these rendering technologies available to you to get started with ARKit. So let's see, what is new this year in ARKit 2. Okay. So we have saving and loading maps that enables powerful new features of persistence and multiuser experiences. We are also giving you environment texturing so you can realistically render your augmented reality scene. ARKit can now track 2D images in real-time. We are not limited to 2D. We can also detect 3D objects in a scene. And last, we have some fun enhancements for face tracking. So let's start with saving and loading maps. Saving and loading maps is part of world tracking. World tracking gives you position and orientation of your device as our six degrees of freedom pose in real world. This lets you place objects in the scene such as this table and chair you can see in this video. World tracking also gives you accurate physical scale so you can place your objects up to the correct scale. So your objects don't look too big or too small. This can also be used to implement accurate measurements, such as the Measure app we saw yesterday. World tracking also gives you 3D feature points so you can, you can, you know some physical structure of the environment, and this can be used to perform [inaudible] to place objects in the scene. In iOS 11.3, we introduced relocalization. This feature lets you restore your tracking state after AR session was interrupted. So this could happen, for example, your app is backgrounded or you're using picture in picture mode on iPad. So relocalization works with a map that is continuously built by world tracking. So the more we move around the environment, the more it is able to extend and learn about different features of the environment. So this map was only available as long as your AR session was alive, but not we are giving this map available to you. In ARKit API, this map is given to you as ARWorldMap object. ARWorldMap represents mapping of physical 3D space, similar to what we see in this, on the visual, on the right. We also know that anchors are important points in physical space. So these are the places where you want to place your virtual objects. So we have also included plain anchors by default in ARWorldMap. Moreover, you can also add your custom anchors to this list since it is mutable list. So you can create your custom anchors in the scene and add them to World Map. For your visualization and debugging, World Map also give you raw feature points and extend, so you know the real physical space you just scanned. More importantly, World Map is a serializable object, so it can be serialized to any data stream of your choice, such as file on local system or to a shared network place. So this ARWorldMap object enable two powerful set of new experiences in ARKit. The first is persistence. So just to show you an example how it works, we have a user starting world tracking, and he places an object in the scene through ARKit hit testing. And before leaves the scene, he will save World Map on the device. So some point, sometime later, the user comes back, and he is able to load the same World Map, and he will find the same augmented reality experience. So he can repeat this experience as many times he wants, and he will find these objects on the table every time he will start his experience. So this is persistence in world tracking [applause]. Thank you. [applause] ARWorldMap also enables multiuser experiences. Now your augmented reality experience is not limited to a single device or single user. It can be shared with many users. One user can create World Map and share with one or more users. Note that World Map represents a single coordinate system in real world. So what it means that every user will share the same working space. They are able to experience the same augmented reality experience from different point of view. So this is a great new feature. You can use World Map to enable multiuser games, such as the one we saw yesterday. We can also use ARWorldMap to create multiuser shared educational experiences. Note that we are giving ARWorldMap object in your hands, so you are free to choose any technology to share with every user. For example, for sharing you can use air drop or multipeer connectivity that relies on local Bluetooth or WiFi connection. So it means that you don't really need an internet connection for this feature to work. :15 So let's see how ARKit API makes it very easy for you to retrieve and load World Map. On your AR session object, you will need to call get current world map at any point in time. This method comes with a completion handler in which it will return you and ARWorldMap object. Note also that it can also return and other in case World Map is not available. So it's important to handle this error in your application code. So once you have ARWorldMap, you can simply set initial World Map property in world tracking configuration and run your session. Note that this can be dynamically changed as well, so you can also reconfigure AR session by running a new configuration. So once AR session is started with ARWorldMap, it will follow the exact same behavior of relocalization that we introduced in iOS 11.3. It is important for your experience that relocalization works reliably. So it is good to, it is important to acquire good World Maps. Note that you can call get a current world map at any point in time. So, it's important to scan your physical space from multiple point of views. So tracking system can really learn about physical structure of the environment. The environment should be static and well textured so we can learn, extract more features of it and learn more about the environment. And also, it's important to have dense feature points on the map, so it can reliably relocalize. But you don't have to worry about all those points. In ARKit, API really makes things easier for you by giving you WorldMappingStatus on ARFrame. WorldMappingStatus is updated in every ARFrame and can be retrieved by WorldMappingStatus property. So let's see how this works. So when we start world tracking, WorldMappingStatus will be not available. As soon as we start scanning the physical space, it will be limited. The more we move in the physical world, world tracking will continue to extend the map. And if we have scanned enough physical world from current point of view, WorldMappingStatus will be mapped. So note that if you point away from a map physical space, WorldMappingStatus may go back to limited. So it will start to learn more about the new environment that we are starting to see. So how you can use WorldMappingStatus in your application code. Let's say you have an app that lets you share your World Map with another user, and you have a shared map button on your user interface. It's a good practice to disable this button when WorldMappingStatus is not available or limited. And when WorldMappingStatus is extending, you may want to show an activity indicator on UI. So this encourages your end user to continue moving in the physical world and continue scanning it and extending the map, because you need that for relocalization. Once WorldMappingStatus is fully mapped, you may enable your share map button and hide your activity indicator. So this will let your user to share the map. So let's see a demo of saving and loading World Map. Okay. So can we switch to AR 1. Okay. So for this demo, I have two apps. In one app I will retrieve and save World Map to a local file, and in my second app, I will load the same World Map to restore the same augmented reality experience. So let's start. So as you can see, WorldMappingStatus on top right corner. It was not available. As soon as I start to move in the environment, it is now extending my World Map. So if I continue to map and move in this environment, the WorldMappingStatus will go mapped. So it means that it has seen enough features from this point of view for relocalization to work. So it is a good time to retrieve and serialize World Map object. But let's make this augmented reality scene more interesting by placing a custom anchor. So through hit testing, I have created a custom anchor, and I am overlaying this object, basically it's a old TV. I think most of you may have seen in the past. So of course I can still continue mapping the world, and let's save the World Map. So when I saved my World Map, I could also show raw feature points that belongs to this World Map. So those blue dots that you see, they are all part of my World Map. And also as a good practice, I saved a screen shot of my point of view where I saved World Map. So now we have serialized World Map to our file. We can now restore the same augmented reality experience in another app. So let's try that. I will start this app from a different position, and you can see this is my world origin. It is defined on this side of the table, and my world tracking is now in relocalizing state. So this is the same opening relocalization behavior that we introduced in iOS 11.3. So let me point my device to the physical place where I created World Map. So as soon as I point to that same space, it restored my world origin back to where it was, and at the same time it also restored my custom anchor. So I have the exact same AR experience. Thank you. So now note that I can start this app as many times I want, and it will show me the same experience every time I start. So this is persistence. And of course, this can be shared with another device. So back to slides. So this was saving and loading map. It's a powerful new feature in ARKit 2 that enables persistence and multiuser shared experiences. In ARKit 2, we have faster initialization and plane detection. World tracking is now more robust, and we can detect planes in more difficult environments. Both horizontal and vertical planes have more accurate extent and boundaries, so it means that you can accurately place your objects in the scene. In iOS 11.3, we introduced continuous autofocus for your augmented reality experiences. IOS 12 comes with even more optimizations specifically for augmented reality experiences. We are also introducing 4 by 3 video formats in ARKit. Four by three is a -angle video format that greatly enhances your visualization on iPad because iPad also have 4 by 3 display aspect ratios. Note that 4 by 3 video format will be the default video format in ARKit 2. So all of these enhancements, they will be applied to all existing apps in the App Store except 4 by 3 video format. For that, you will have to build your app with the new STK. So coming back to improving end-user experience, we are introducing environment texturing. So this greatly enhances your rendering for end-user experiences. So let's say your designer have worked really hard to create these virtual objects for you, for your augmented reality scene. This looks really great, but you need to do more for your augmented reality scene. You need to have position and orientation correct in your AR scene so that object really looks like it is placed in the real world. It is also important to get the scale right so your object is not too big or not too small. So ARKit helps you by giving you the correct transform in world tracking. For realistic rendering, it is important to also consider lighting in the environment. ARKit gives you ambient light estimator that you can use in your rendering to correct the brightness of your objects. So your objects don't look too bright or too dark. They just blend into the environment. If you are placing your objects on physical surfaces such as horizontal planes, it is also important to add a shadow for the object. So this greatly improves human visual perception. They can really perceive that objection is on the surface. And last, in case of reflective objects, humans wants to see reflection of the environment from the surface of the virtual objects. So this is what environment texturing enables. So let's see how this object looks like in an augmented reality scene. So I created this scene yesterday evening when I was preparing for this presentation. So while eating those fruits, I also wanted to place this virtual object. And you can see, it is correct to scale, and you can see more importantly you can see a reflection of the environment in the object. On your right side of this object, you can see this yellow and orange reflection of those fruits on the right, and on the left, you can notice the green texture from the leaves. And in the middle, you can also see reflection of the surface of the bench. So this is enabled by environment texturing in ARKit 2. Thank you. So environment texturing gathers scene texture information. Usually it is represented as a cube map, but there are other representations as well. Environment texture or this cube map can be used as a reflection probe in your rendering engines. This reflection probe can apply this as texture information onto virtual objects, such as the one we saw in the last slide. So it greatly improves visualization of reflective objects. So let's see how this works in this short video clip. So ARKit, while running world tracking and scene understanding, continues to learn more about the environment. Using computer vision, it can extract textured information and start to fill this cube map. And this cube map is accurately placed in the scene. Note that this cube map is partially filled, and to set up reflection probes, we need to have a fully completed cube map. To have a fully completed cube map, you will need to scan your full physical space, something like a 360-degree scan you do with your panoramas. But this is not practical for end-users. So ARKit makes it very easy for you by automatically completing this cube map using advanced machine learning algorithms. Note also that all of this processing happens locally on your device in real-time. So once we have a cube map, we can set up reflection probe and as soon as we place virtual objects in the scene, they start to reflect the real environment. So this was a quick overview of how this environment texturing process works. Let's see how ARKit API makes it very easy for you to enable this feature. So all you have to do in your world tracking configuration to set environment texturing property to automatic and run the session. So this is as simple as this. AR session will automatically run this environment texturing process in the background and will give you environment texture as an environment probe anchor. AREnvironmentProbeAnchor is an extension of AR anchor, so it means it has a six degrees of freedom position and orientation transform. Moreover, it has a cube map in the form of metal texture. ARKit also gives you physical extent of the cube map. So this is area of the influence of the reflection probe, and it can be used by rendering agents to correct for parallels. So such as in case your object is moving in the scene, it will automatically adapt to new position and new texture will be reflected in the environment. Note that this follows same lifecycle as every other anchor such as AR plane anchor or AR image anchor. Furthermore, it is fully integrated into ARSCNView. So in case you are using SceneKit as your rendering technology, you just need to enable this feature in world tracking configuration. The rest is done automatically by ARSCNView. Note that for advanced use cases, you may want to place environment probe anchors manually in the scene. So for this, you will need to set environment texturing mode to manual, and then you can create environment probe anchors at your desired position and orientation and add them to AR session object. Note that this only enables you to place the probe anchors in the scene. AR session will automatically update its texture as soon as it gets more information about the environment. So you may use this mode in case your augmented reality scene has a single object. You don't want to overload system with too many environment probe anchors. So let's see a quick demo of environment texturing and see how we can realistically render augmented reality scene. So we can switch to AR 1. Okay. So for this demo, I am running world tracking configuration without environment texturing feature enabled. So as you can see on the bottom switch controller, it's just using ambient light estimate. And let's place the same object that we have seen before. And you can see it is, it looks okay. I mean you can see this on the table. You can see the shadow of it, and it looks like a really good AR scene. But what we are missing is that it does not reflect wooden surface of the table. So moreover, if I place something in the scene such as real fruit, we don't see a reflection of it in the virtual object. So let's enable environment texturing and see how it can realistically represent this texture. So as you can see, as soon as I enable environment texturing, the object started to reflect the wooden surface of the table as well as the texture from this banana. Thank you. So this greatly enhances you augmented reality scene. So it looks as real as possible, as if it is really on the table. Okay. Back to slides. So this was environment texturing. It's a powerful new feature in ARKit 2 that lets you create your augmented reality scene as realistic as possible. Now, to continue with the rest of the great new features, I will invite Reinhard on stage. It's working? Oh, okay, great. Good morning. My name is Reinhard, and I'm an engineer on the ARKit team. So next let's talk about image tracking. In iOS 11.3, we introduced image detection as part of world tracking. Image detection searches for known 2D images in the scene. The term detection here implies that these images are static and are therefore not supposed to move. Great examples for such images could be movie posters or paintings in a museum. ARKit will estimate the position and orientation of such an image in six degrees of freedom once an image has been detected. This pose can be used to trigger content in your rendered scene. As I mentioned earlier, all this is fully integrated world tracking. So all you need to do is set once in your property to set it up. In order to load images to be used for image detection, you made load them from file or use Xcode's asset catalog, which also gives you the detection quality for an image. So image detection is already great, but now in iOS 12, we can do better. So let's talk about image tracking. Image tracking is an extension to image detection with a big advantage that images no longer need to be static and may move. ARKit will now estimate the position and orientation for every frame at 60 frames per second. This allows you to accurately augment 2D images, say magazines, board games, or pretty much anything that features a real image. And ARKit can also track multiple images simultaneously. By default it only selects 1, but in cases, for example, the cover of a magazine, you may want to keep this set to 1, or in case of a double-page magazine inside a magazine, you want to set this to 2. And in ARKit 2 on iOS 12, we have a brand-new configuration called the AR Image Tracking Configuration that lets you do stand-alone image tracking. So let's see how to set it up. We start by loading a set of reference images, either from file or from the asset catalog. Once I'm done loading such a set of reference images, I use this to set up my session that can be of type world tracking by specifying its detection images property or of type ARImageTrackingConfiguration by specifying the tracking images one. Once I'm done setting up my configuration, I use this to run my session. And just as usual, once the session is running, I'll get an ARFrame at every update. And such and ARFrame will continue an object of type ARImageAnchor, once an image has been detected. Such an ARImageAnchor is now a trackable object. I can see this by conforming to the AR trackable protocol. This means it [inaudible] is tracked, which informs you about the tracking state of the image. It's true if it's tracked and false otherwise. It also informs about which image has been detected and where it is by giving me it's position and orientation as a 4 by 4 matrix. So in order to get such image anchors, it all starts by loading images. So that's good. Let's have a look at what good images could be. This image here could be found in a book for children, and in fact, it works great for image tracking. It has a lot of distinct visual features. It's we'll textured and shows really good contrast. On the other hand, an image like this, which could also be found in a textbook for kids, is not recommended. It has a lot of repetitive structures, uniform color regions, and a pretty narrow histogram once converted to gray scale. But you don't have to identify these statistics yourself as Xcode is here to help. So if I import these two images to Xcode, I see the sea life one without any warning, which means it's recommended, and the one about the three kids reading showing a warning icon, meaning it's not recommended. If I click this icon, I get a precise description why this image is not recommended to use image tracking. I get information about the histogram, uniform color regions, as well as the histogram. So once I'm done loading images, I'm left with two choices of configurations. First one is ARWorldTrackingConfiguration. So let's talk about that. When we use image tracking with world tracking, image anchors are represented in a world coordinates system. This means that image anchors optionally plane anchors. The camera and the world origin itself all appear in the same coordinate system. This makes their interaction very easy and intuitive, and what's new in iOS 12 now images that could previously only be detected can now be tracked. And we have a new configuration, the ARImageTrackingConfiguration, which performs stand-alone image tracking. This means it's independent from world tracking and does not rely on the motion sensor to perform the tracking. This means this configuration is not to initialize before it starts to identify images and could also succeed in scenarios in which world tracking fails, such as moving platform like an elevator or a train. I think in this case ARKit would estimate the position and orientation for every frame at 60 frames per second. And implementing this can be done in four simple lines of code. So all you need to do, I create a configuration type ARImageTrackingConfiguration and specify a set of images I'd like to track. In this case, I specified a photo of a cat, a dog, and a bird. I tell the configuration how many images I'd like to track. In this case, I specified this to be 2. In my use case, I imagined only 2 images will interact but not 3 at the same time. Note that if I'm tracking 2 images and a third comes into its view, it won't be tracked, but it will still get a detection update. And then I use this configuration to run my session. And as I mentioned earlier, you can also do this using world tracking by simply switching out these two lines. The only difference between image detection and tracking is the maximum number of tracking images. So if you have an app that uses image detection, you could simply add this, recompile, and your app may use tracking. So in order to show you how easy this really is, let's do a demo in Xcode. Can we go to AR 2? Yes. So for this demo I'd like to build a AR photo frame, and for this, I brought a photo of my cat from home. So let's build this using Xcode. So I started by creating a iOS app template using Xcode. As you can see by now it's pretty empty. Next, I need to specify which image I'd like to attach. For this I imported the photo of my cat, Daisy. Let's open her up here. That's my cat. I need to specify a name. I give it the name Daisy, which is the name of my cat, and I specify here the physical width of the image found in the real world, which is my photo frame. I also loaded a movie of my cat. So let's bring this all together. First, I will create a configuration, which will be a configuration of type ARImageTrackingConfiguration. I load a set of tracking images from the asset catalog by using the group name photos. This will contain only one image, which is the photo of my cat, Daisy. Next, I set up the configuration image tracking by specifying the tracking images property here, and I specify the max number of tracked images that we want. At this point, the app will already start an AR session and provide you with image anchors once an image has been detected. But let's add some content. I will load the video [inaudible] a AV player from the video by loading it from the resource panel. Now let's add it on top of the real image. So for this, I'm checking whether the anchor is of type image anchor, and I create an SCN plane having the same physical dimension as the image found in the scene. I assign the video player as the texture to my plane, and I start playing my video player. I create an SCN note from geometry, and I counter rotate to match the anchor's coordinate system. So that's it. This will run. Let's see it live. So once I bring the frame of my cat into the camera's view, the video starts playing, and I can see my cat interact. Since ARKit estimates position in real-time, I can move my device freely, or I can move the object. So I can really see that there's an update at every frame. Oh, oh, she just left. I guess it's the end of the demo, let's go back to the slides. So as you can see, it's really easy to use image tracking in ARKit. In fact, it's much harder to make a video of your cat. So image tracking is great at interacting with 2D objects, but we're not limited to plainer 2D objects, so let's talk next about object detection. Object detection can be used to detect known 3D objects in the scene. Just like image detection here, the term detection means that this object needs to be static and can therefore, or should therefore not move. Great examples of such objects could be exhibits in a museum, certain toys, or household items. And like image detection, objects need to be scanned first using an iOS app running ARKit. For this we offered the full source code of a full-featured iOS app that allows you to scan your own 3D objects. Such objects have a few properties such that they need to be well textured, rigid, and nonreflective. And they need to have roughly the size of a tabletop. ARKit can be used to estimate the position and orientation of such objects in six degrees of freedom. And all of this is fully integrated into world tracking. So all you need to do is set one single property to get started with object detection. So let's have a look how it can be set up. I load a set of AR reference images from file or from Xcode asset catalog. I will talk about the reference objects in a second. Once I'm done loading these reference objects, I used them to set up my configuration of type ARWorldTrackingConfiguration by specifying the detection objects property. When I'm done setting up my configuration, again I run my session with it. And just as image detection, once the AR session is running, I get an ARFrame with every update, and in this case, once an object has been detected in the scene, I will find an AR object anchor as part of my AR frame. Such an AR object is a simple subclass of AR anchor. So it comes with a transform, which represents its position and orientation and six degrees of freedom as well as it tells me which objects has been detected by giving me a reference to the AR reference object. And implementing this can be done with three simple lines of code. I create a configuration of type ARWorldTrackingConfiguration and specify a set of objects I'd like to detect. In this case, I envision to build a simple AR museum app by detecting an ancient bust and a clay pot. And I use this to run my session. So in fact, at the office, we build a very simple AR museum app, so let's have a look. So once this bust gets into view of my iOS app, I get a six degrees of freedom pose and can use this to show the scene, very simple infographics floating about the statue. In this case, we have simply added date of birth, the name of this Egyptian queen, which was Nefertiti, but you could add any content that your rendering engine allows you to use. In order to build this app, I had to scan the object first. So let's talk about object scanning. Object scanning extracts accumulated scene information from the world. This is very much related to plane estimation in which we use accumulated scene information to estimate the position of a horizontal or vertical plane. In this case, we use this information to gather information about the 3D object. In order to specify which area to look for the object, I just specify a transform and extend in the center. This is essentially a bounding box around the object just to define where it is in the scene. Extracted objects are fully supported by Xcode's asset catalog, so it makes it really easy to [inaudible] a new app and reuse them as many times as you want. And for scanning, we added a new configuration, the ARObjectScanningConfiguration. But you do not need to go ahead and implement your own scanning app as the full sample code is available for full-featured scanning app called Scanning and Detecting 3D Objects. So let's have a look how this app works. I start by creating a bounding box around the object of interest, in this case, the statue of Nefertiti. Note that the bounding box does not need to be really strict around the object. All we care is that the most important feature points are within its bounds. When I'm satisfied with the bounding box, I can click press scan, and we start scanning the object. I can see the progress going up and [inaudible] representation indicating how much of the object has been scanned in a spatial manner. Note that you do not have to scan the object from all sides. For example, if you know that a statue will be facing a wall in a museum, and there is no way that you could detect it from one specific viewpoint, you do not need to scan it from that side. Once you're satisfied with the scan, you can adjust the center of the extent which corresponds to the origin of the object. The only requirement here is that the center stays within the object's extent. And lastly, the scanning app lets you perform detection tests. So in this case, detection was successful from various viewpoints, which means it's a good scan. And our recommendation here is also to move the object to different location to test whether detection works with different texture and under different lighting conditions. Once you're done scanning, you will obtain an object of type ARReferenceObject, which we have seen earlier in the diagram. This object can be serialized to usually and AR object file extension type. It has a name, which will also be visible in your asset catalog as well as the center and the extent used for scanning it. And you will also get all the raw feature points found within the area when you performed your scan. So this was object detection. Keep in mind, before detection object, you need to scan them, but there is the full source code available for you to download it right now. So let's talk next about face tracking. When we released the iPhone X last year, we added robust face detection and tracking to ARKit. Here, ARKit estimates the position and orientation of a face for every frame at 60 frames per second. Here we can use the, this pose can be used to augment a user's face by adding masks, hats, or replace the full texture of a face. ARKit also provides you with a fitted triangle mesh coming to form of the ARFaceGeometry. This type, the ARFaceGeometry, contains all the information needed to render this facial mesh, and it comes in the form of all vertices, triangles, as well as detection coordinates. The main anchor type of face tracking is ARFaceAnchor, which contains all information needed to perform face tracking. And in order to render such geometry realistically, we added a directional light estimate. Here, ARKit uses your light as a light probe and estimates this ARDirectionLightEstimate, which consists of the light intensity, direction, as well as the color temperature. This estimate will be sufficient to make most apps already look great, but if your app has more sophisticated needs, we also provide the second-degree spherical harmonics coefficients that gather lighting conditions throughout the entire scene for you to make your content even look better. And ARKit can also track expressions in real-time. These expressions are so-called blend shapes, and there's 50 or more of them. Such a blend shape assume a value between 0 and 1. One means there's full activation. Zero means there is none. For example, the [inaudible] open coefficient will assume a value close to 1 if I open my mouth and a value close to 0 if I close it. And this is great to animate your own virtual character. This example here, I've used the jaw open and eye blink left and eye blink right to animate this really simple box face character. But it can do better than that. In fact, when we built an emoji, we used a handful more of such blend shapes. So all the blue bars you see moving here were used to get over the head post to map my facial expressions on the panda bear. Note that ARKit offers everything needed for you to animate your own character just like we did with an emoji. Thank you. So let's see what's new for face tracking in ARKit 2. We added gaze tracking that will track the left and the right eye both in six degrees of freedom. You will find these properties as members of ARFaceAnchor as well as a look-at point, this corresponds to reintersection of the two gaze directions. You may use this information to animate again your own character or of any other form of input to your app. And there's more. We added support for tongue, which comes in the form of a new blend shape. This blend shape will assume a value of 1 if my tongue is out 0 if not. Again, you could use this to animate your own character or use this as a form of input to your app. Thank you. So seeing myself sticking my tongue out over and over is a good time for summary. So, what's new in ARKit 2. Let's have a look. We've seen saving and loading maps, which are powerful new features for persistence and multiuser collaboration. World tracking enhancements simply shows better and fasting plane detection as well as new video formats. And with environment texturing, we can make the content really look as if it was really in the scene by gathering texture of the scene and applying it as a textured object. And with image tracking, with image tracking, we are now able to track 2D objects in the form of images. But ARKit can also detect 3D objects. And for face tracking, we have gaze and tongue. All of this is made available for you in the form of the building blocks of ARKit. In iOS 12, ARKit features five different configurations with two new additions, the ARImageTrackingConfiguration for stand-alone image tracking and the ARObjectScanningConfiguration. And there's a series of supplementary types used to interact with the AR session. The ARFrame, the ARCamera, for example. And this got two new additions, the ARReferenceObject for object detection and the ARWorldMap for persistence and multiuser. And the AR anchors, which represent positions in the real world, the anchor types. Got two new additions, the ARObjectAnchor and the AREnvironmentProbeAnchor. I'm really excited to see what you guys will build with all these building blocks in the ARKit available in iOS 12 as of today. There's another real cool session about integrating ARQuickLook into your own application to make your content look simply great. With this, thanks a lot, and enjoy the rest of the conference.  Good afternoon. Welcome to What's New in tvOS. My name is Hans Kim, and I'm one of the engineers on the tvOS team. I'm thrilled this afternoon to tell you about some of the new features in tvOS 12. We're going to talk about Password AutoFill, and we'll take a look at some new enhancements in tvOS's focus engine. And, we'll conclude with some exciting news about some new UI patterns on tvOS. We have a lot of ground to cover, and we have some great demos, so let's begin. And, to get us started, I'd like to invite my colleagues Alex and Conrad to tell us about helping your customers sign into your app. Thank you, Hans. Hello everyone. My name is Alex Sanciangco. And, today I am so excited to talk to you all about Password AutoFill. Signing in is a common experience that users have with apps. For instance, here's an app I've been working on called "shiny," that shows our users an adorable photo of their favorite dogs every day when they log in. In order to make logging in as smooth as possible, we've brought Password AutoFill to tvOS 12. With an experience just as familiar and easy as Password AutoFill on iOS. Let me show you what I mean. First, I'd like to show you continuity keyboard. As you might remember, continuity keyboard is an iOS feature that allows users to enter text on the Apple TV, using their iOS device. Let's see it in action. I'm going to navigate to that shiny app, and I'll select the email field. As I expect, I see the continuity keyboard notification appear, which has now been enhanced to offer Password AutoFill as well. So, I'll open that up now. And, I see that my username and password are suggested for the shiny app right there on the QuickType bar. And, I know that with one tap, it'll be automatically filled into the TV, and I'll be signed in. So, I'll do that now. Aw, look at that guy. So, I got the notification on my iPhone because my iPhone and Apple TV are on the same iCloud account. And, if you're a user of the TV remote and control center, or the TV remote app, the experience is similar there as well. But, we have something else brand new we want to show you today, that shows just how easy we've made signing in, even for a guest that might not have ever used your Apple TV before. And, to demonstrate that, I'd like to invite Conrad up to the stage. Thank you, Alex. Now, suppose that I'm Alex's cousin. I'm visiting for the week. Now, his puppy's pretty cute. But, I really want to sign in to shiny so I can see pictures of my own dog Max, or maybe his adorable sister, Min. Now, I've never used his Apple TV before, so I don't have any of the keyboard options available that he just discussed. Furthermore, since I use a strong password saved in iCloud keychain, I haven't memorized it. I expect that I'm going to have to look up my password, and then type it in character by character on the Apple TV. Well, let's see what happens. I'm going to go over here. I'm going to go to my phone. And, let's sign in. Oh, and even before I can open settings, or ask Siri to look up my password, my iPhone is already offering to help me AutoFill my password. So, I'm going to open up this notification. There we go. And, by entering a PIN I know that I am securely connecting to this Apple TV. Going to Touch ID. And, my shiny credential appears right at the top, with one tap I'm signed in. Super easy. Aw. Sorry, it's just-- it's Min. OK. Now one thing you might be wondering is how did that work without all the iPhones in the room showing a notification? I think this is really, really cool. The Siri remote is able to locate nearby iPhones on which to offer AutoFill. So, in this case, only my iPhone showed a notification. And that is Password AutoFill for tvOS. I'd now like to thank-- sorry, I'd like to invite Alex back up to the stage to show you just how easy it is to make this work flawlessly in your apps. Alex? Thank you, Conrad. Man, I love that demo. I just think that feature is so cool. Password AutoFill for tvOS is part of a suite of features that you might have heard about on Monday, meant to simplify the password experience. These features integrate into existing apps, without you having to make any big changes. And, there's a few things that you developers can do to ensure that Password AutoFill is as great as possible. Let's go over those briefly now. First, you'll want to ensure that the QuickType bar appears for your text fields. Next, you want to make sure that your app's credentials are suggested on the QuickType bar. And lastly, you want to enable a one-tap sign in experience. Now, let's look at how to do this in more detail. To ensure the QuickType bar appears for your text fields, you want to adopt UITextContentTypes. tvOS will automatically try to detect username and password fields, and you can help this process by explicitly marking your text fields. This is as easy as setting a single property on those text fields. Simply set, for your username text fields, it's textContentType to .username, and similarly for your password field, you'll set its textContentType to .password. Very simple, and this can be done in Interface Builder as well. So, after adopting the textContentTypes, here's what viewers would likely see in that continuity keyboard notification. And, this is good, because it gives users access to their passwords. But, it's behind that key icon. Too many taps away. What we really want the user to see is this: your app's credentials suggested right on the QuickType bar. The way you get this behavior is by adopting a technology called Associated Domains, which securely tells Password AutoFill which domains to suggest on the QuickType bar. Associated Domains is a powerful technology that enables other features, such as universal links and Handoff. It creates a secure relationship between the domain the user's accessing and the app that they have downloaded. Password AutoFill then uses this relationship to suggest the exact right credential, right on top of the QuickType bar. To learn how to adopt Associated Domains, I highly recommend checking out "Introducing Password AutoFill for Apps" from WWDC of last year. This talk contains a step-by-step guide for how to adopt Associated Domains for iOS apps, and the steps are identical for tvOS apps. So, after adopting Associated Domains, this is what the user will likely see on the shiny app after tapping your credential on the QuickType bar. And, this is awesome. The user has their username and passwords on their Apple TV without having to type a single character. But what we really want the user to see is this. Being automatically signed in and seeing their pupper of the day, because they've been signed in with one tap. You can get this behavior by implementing the preferredFocusEnvironments API. So, after filling in a password, a focus update occurs. And, Password AutoFill will perform the action of the focused button if one is focused. So, you should implement the preferredFocusEnvironments API, so you can provide your login button to the focus engine. Let's look at how you might implement this. Here I have a sample implementation of the preferredFocusEnvironments API, which returns an array of UIFocusEnvironment objects. First, we'll try to grab text out of our username and password fields. And, if we can, we'll just return that login button to the focus engine. If we weren't able to grab any text, that just means the user hasn't entered any yet. So, we'll return the username text field to be focused. Super simple. So, let's recap some of the major takeaways of Password AutoFill. For customers, Password AutoFill reduces the friction within your app by enabling a one-tap sign in experience. If any adoption is needed on your part for developers, this is super simple. You simply have to just adopt UITextContentTypes, adopt Associated Domains so that your apps credentials are surfaced to the QuickType bar. And, lastly, implement preferredFocusEnvironments to enable that one-tap sign in experience. And that's Password AutoFill for tvOS. We cannot wait for you and your users to get your hands on it. Next, I'd like to invite up Ada to talk about focus enhancements. Thanks, Alex. Hi, my name is Ada Turner, and I'm really excited to talk to you today about some enhancements that we've made to Focus in tvOS 12. Focus is a great way for users to interact with your apps on tvOS. It allows them to gracefully scroll through content and provides playful movement hinting as they interact with the touch surface of the Siri remote. Apps built with UIKit, SpriteKit, and SceneKit, all have native support for focus. However, apps that use alternative methods to render their content have not been able to directly support the focus engine. Today, I am pleased to announce that in tvOS 12, the focus engine, now supports apps regardless of how they are rendered. This means that apps written in frameworks such as Metal can now adopt focus directly into their interaction model. This is accomplished by allowing you to conform your own classes to focus protocols, even if they don't inherit from UIKit components. Now, what this means for your apps is they will get state management of what's currently focused. Geometric determination of where focus can and cannot move. Full accessibility support, and focus movement, hinting, and scrolling that feels just like native tvOS apps. Before we take a look at the new focus API's, let's briefly refresh ourselves on some existing focus components. First, we have UIFocusEnvironments. This protocol manages state where focus interactions can take place. It is notified when focus updates occur, and can influence focus update behavior. In UIKit, UIViewController is a great example of a focus environment. Next, we have UIFocusItem. This protocol inherits from FocusEnvironment, and adds the ability for focus items to actually be able to become focused. UIKit's UIView, and SpriteKit's SKNode are great examples of focus items. And, finally we have the UIFocusSystem, which provides the famously useful functionality of being able to register custom sounds to play during focus interactions. For more information on how to work with these components, and debug focus, I highly recommend that you watch "Focus Interaction in tvOS 11" from last year. And now, I'm really excited to introduce some new components to the focus engine. First, starting in tvOS 12, we are expanding the functionality of the focusSystem object. You may now retrieve a focusSystem for a given environment, and access the currently focused item off of the focusSystem. Next, we are introducing a new protocol, called UIFocusItemContainer, which provides geometric context to Focus items. A focusItemContainer is owned by a focusEnvironment, and can locate focus items within specific regions, allowing the focus engine to move focus to the best candidate. Next, we have a special type of focusItemContainer, called focusItemScrollableContainer, which adds support for automatically scrolling through content as focus is moved. And, finally, we now supply focusItems with movement hints, which contain raw values that you can use to create visual effects that suggest what direction focus is about to move in. Now, let's take a closer look at how we can form our own classes to these protocols. Let's start by implementing a custom focusEnvironment. In order for the focus engine to find your environment, and its child environments or items, you must supply a parent focus environment, and a focusItemContainer. For example, UIViewController might provide its parent viewController as its parentFocusEnvironment, and its view as its focusItemContainer. FocusEnvironment provides several methods you can use to control and react to focus updates. For example, preferredFocusEnvironments allows you to guide the focus system in selecting what item is focused after view initialization, or a programmatic focus update. I'd like to call your attention to two specific methods on the focusEnvironment. SetNeedsFocusUpdate and updateFocusIfNeeded. Your implementation of these methods must call to a specific method on the focusSystem. Next, let's implement a custom focusItemContainer. First, you will need to provide a coordinateSpace. UIView provides itself as a coordinateSpace. If your container is more abstract, you may return an existing coordinateSpace or implement your own. Next, you will need to implement focusItems in rect. This method must return any contained focusItems whose frames intersect with the provided rect. Note that the rect passed to this method is expressed in the coordinateSpace of the container, and the frames of each focusItem you return from this method must also be expressed in that coordinateSpace. Next, let's implement a custom FocusItem. Remember that this protocol inherits from FocusEnvironment, so you will need to implement all of those methods as well. In order for the focus engine to move focus onto your item, it must return "true" from canBecomeFocused. DidHintFocusMovement is an optional method that is called whenever the user moves their finger on the touch surface of the Siri remote. It provides the focusItem with a movement hint that contains raw values you can use to create an effect that indicates what direction focus is about to move in. Finally, you will need to provide a frame. As I said before, this frame must be expressed in the coordinateSpace of the containing focusItemContainer. For example, UIView expresses its frame in the coordinateSpace of its superview, which is also its containing focusItemContainer. Now, let's take a closer look at the focusMovementHint object. Movement direction is a vector, whose values range between negative 1, negative 1 and 1,1 representing how close focus is to moving in a particular direction. This value is tied to the path a user's finger creates on the touch surface of the Siri remote. Perspective, rotation, and translation are all values you can use to match tvOS's native interaction hinting. And, interactionTransform combines all three of these values into a single 3D transform. Next, let's look at how to implement a custom focusItemScrollableContainer. This is a special type of focusItemContainer, and by conforming to this protocol, your container signals to the focus engine that it supports scrolling. For example, UIScrollView conforms to this protocol. It provides three additional properties that allows the focus engine to manage its scrolling behavior. First, we have contentOffset, which is a read/write property representing how far the container has been scrolled. This property will automatically be set by the focus engine as focus is moved in order to keep the currently focused item on screen. Second, we have contentSize, which represents the total size of your scrollable content and third, we have visibleSize, which represents the onscreen size of your container. This property is analogous to bounds.size on the UIScrollView. It is important to remember that contentOffset will be set automatically, and it is your responsibility to update your rendered content as appropriate whenever this property is set. Now, let's talk about adding accessibility to our custom rendered apps. It's actually incredibly easy. By implementing focusItemContainers, focusItems in rect method, you are providing the focus engine with enough information to allow voiceover to assist your users in navigation. Remember to set accessibility labels and accessibility hints on your focusItems in order for voiceover to give your users the best experience. I highly recommend that you watch "What's New in Accessibility" from WWDC 2016 for a more in-depth look at how focus and voiceover work together in tvOS. And now, I'd like to invite my colleague, Paul, to give us a demo on how to create a focus-powered Metal app. Thanks, Ada. So, I'm working on the setting screen for a Metal-based game. At the bottom of the screen, there's some standard UI buttons. And, up at the top are some tiles for selecting levels. These tiles are actually rendered by the game engine itself. I'm sure you can tell by the incredible 3D graphics that you see up there. So, I want to be able to select the tiles using the remote. Previously, I would have had to handle events from the remote myself, and implement my own navigation, trying my best to match the feel of the focus engine. In tvOS 12, I can connect these tiles directly to the focus engine. So, let's go ahead and do that. The first thing I'm going to do is extend my LevelTile class to implement the UIFocusItem. This is what will allow it to become focused. There's a few methods here. I'm just going to direct your attention to a few at the top. It canBecomeFocused, I'm going to enter in "true." That's straightforward. For parentFocusEnvironment, I'm going to return the MetalView that renders these items. Finally in didUpdateFocus in context, I'm going to set the tile to draw itself in an active state when it becomes focused. Next, I need to tell the focus engine about these new items. To do that, I'm going to extend that MetalView that renders them. The view is already in the view hierarchy, and the focus engine already knows about it. So, it's a great place to hook in. And, because this is a UIView, it already conforms to UIFocusItemContainer. And, it provides itself as a coordinatespace. The only thing I have to do is override FocusItem in rect to return my level tiles, which of course are now UIFocusItems. And, I can get a performance win here, by only returning the tiles these frames intersect the path in search rect. Yikes. Let's take a look and see how that works. So, now you can see that the tiles are focusable, and the system even plays a standard sound when they become focused. I can even move focus in between my custom tiles, and the standard UI buttons down at the bottom. There's a problem here, though. The tiles extend offscreen. If I move focus offscreen, I can't see what I'm interacting with. Of course, what I want is for the tiles to move onscreen as they become focused. So, let's implement that. I'm going to extend that RenderView again, this time to implement UIFocusItemScrollableContainer. Now, the important thing here is to adjust my rendering by the contentOffset. The focus engine will set my contentOffset as focus moves to keep the currently focused item onscreen. Because this is a UIView, I'm going to also update by bounds.origin so that the coordinateSpace conversion continues to work correctly. See how that works? So, now you can see that as I focus a tile, it moves onscreen. If I keep going, I get nice, smooth scrolling with the same momentum-- thanks-- nice, smooth scrolling with the same momentum and animation as if this was a UIScrollView. So, this is looking pretty good, but I think we can do even better. What I really want is for these tiles to come alive when they're focused, just like the system elements. Let's go ahead and do that. Going to go back up to LevelTile. And, I'm going to implement an optional method, didHintFocusMovement. I'm going to take the suggested perspective, rotation, and translation values from the UIFocusMovementHint, and apply them when I render the focusTile. Let's see how that looks. Now, as I move my finger on the trackpad, the tile interacts just the way I'd expect. So, now I've added focus support to my custom Metal objects, and they feel just as familiar as if they were written using ULikeIT. Back to you, Ada. Thank you, Paul. Wow, that was a really Metal demo. With just a few lines of code, we were able to give our Metal user interface beautifully smooth focused movement and scrolling, and delightful interaction hinting, just like native tvOS apps. Now, let's recap all the awesome new focus features we learned about today. First, we learned how to implement custom focusEnvironments and focusItems, even if they don't inherit from UIKit components. Second, we learned how to use focusItemContainer, so that when users move focus through our apps, it feels just like native tvOS apps. Third, we learned how to use focusMovementHint to make our interface come alive as people interact with the Siri remote. Fourth, we learned how to use focusItemScrollableContainer to allow people to scroll through our apps content with that smooth, familiar, native feeling. And, finally, we learned how to give our apps full accessibility support simply by adopting these protocols, and providing accessibility labels and hints so that everyone can enjoy our apps. All of these awesome new features are available today in the Developer Beta, and I highly recommend that you download it, and find out just how easy it is to add focus support to your custom rendered apps. And now, I'd like to invite Hans back on the stage, to tell us about some awesome UI patterns on tvOS. Thank you, Ada. Focus interaction is integral in how we feel connected to the screen across the room. And, tvOS has many common UI patterns that make the most out of it. One such example is a label that animates its text when it's focused. Internally scrolling text, or marquee animation, is a useful technique in presenting variable length strings without altering the label's external geometry. It's also very effective in visually highlighting where your current focus is. This behavior is very widely used across tvOS, but there's not an easy way to do this. That is, until now. tvOS 12 makes it really easy. All you have to do is to set a new property on your label, enables MarqueeWhenAncestorFocused to "true." Then, when a view containing the label comes in focus, and the label contains a string, it's too long for the width. It will animate the string horizontally in a loop. We've used this API ourselves, and we think you will really like how simple it is to adopt the behavior. So, that's text scrolling in UI label. But, tvOS has many more idioms and patterns, such as an image and a label when come alive when in focus, an arbitrary view hierarchy that floats as one solid unit, buttons with customizable focus movement and content, and widgets for representing people. And, as you can see, these patterns are ubiquitous across tvOS and Apple's own apps. And, since they've been available in TVMLKit, we see them adopted in your apps as well. But, what if your app is based on UIKit? Well, we're really thrilled to share with you that tvOS 12 will make several of these available for your UIKit-based apps. And, we do this through a new lightweight framework, TVUIKit. The first four elements in TVUIKit are poster, caption button, card, and monogram. Let's take a look at each. Poster view is about images. And, TVPosterView is a composed view specializing in presenting an image, and a footer, which itself is composed of up to two labels. When a poster view is in focus, the image grows in size, and the labels move out of the way to give room. When it relinquishes focus, the image and labels come back to the normal layout. When you program an image to a TVPosterView, it works out just the right amount by which the image grows. And, TVPosterView is an ideal API for creating a UI like this. It's really easy. And, that's TVPosterView. Next, is caption button. Caption button is about call-to-action. And, TVCaptionButtonView is a composed view specializing in presenting a button-like content, and a footer, which itself is composed of up to two labels. The content view has a blurred background, and can either have an image, or a text. When a CaptionButtonView is in focus, it floats and unlike the PosterView, it increases its size only in the leading, top, and trailing directions. You can also limit the floating motion to horizontal only or vertical only. And, when you use multiple TVCaptionButtons and give them a consistent motion direction, you can create a sense of grouping. And, TVCaptionButton makes it really easy to do this. Next, is CardView. CardView is about custom views. And, TVCardView specializes in presenting arbitrarily composed view hierarchy. When a CardView is in focus, it's content view floats and all of its subviews move in unison as part of the floating content view. TVCardView is a great way to create a UI like this. It's really straightforward. Next, is monogram. MonogramView is for representing people. And, TVMonogramView has a circular content image, and a footer, which itself is composed of up to two labels. When you don't provide either a person name, or an image, TVMonogramView provides a generic silhouette. If you do provide a person name, TVMonogramView will create a monogram from the initials. And, of course, when you provide an image, it doesn't question it, we'll just use it. When TVMonogramView is in focus, the labels move out of the way for the image to grow in size. When you use a TVMonogramView, creating a UI like this becomes really straightforward. Now, you might have noticed that there's a consistent pattern among these four elements. Namely, there's a main content, and an optional header and footer. And, when this composition comes in focus, the header and footer move out of the way, and the content to grow in size. This common behavior is encapsulated in a base class, TVLockupView. Some of the things you can customize in a TVLockupView are the explicit content size. This is really helpful when you're laying out multiple LockupViews. Another one is the content-- the size by which the content grows when it's in focus. These are directional insets, so you can even specify different amount in all four directions. You may recall that the CaptionButtonView is actually using this. When you're providing your own content to the TVLockupView, you can take advantage of TVLockupViewComponent protocol. Whenever TVLockupView state changes, it will call updateAppearance forLockupViewState method on all of its subviews that implement it. This is your subviews opportunity to update its behavior, or customize the appearance based on the state. You can use TVLockupView to create your own widget that responds to focus interaction, or further customize the four special purpose subclasses we just discussed. So, that's TVLockupView and its subclasses. Finally, you may recall seeing something like this. It's simple, but its simplicity disguises just how difficult it is to implement this screen. TVUIKit makes it really easy to do this, and that's with TVDigitEntryViewController. TVDigitEntryViewController manages a fullscreen view that presents a title label, prompt label, a digit view, and the numeric keyboard. Among the things you can customize in a TVDigitEntryViewController are the number of digits, and whether the entry is secure or not. And, a completion handler that allows you to process the entered digits. Instead of just talking about it, I'd like to invite my colleague Marshall over to give us a demo. Thank you, Hans. My name's Marshall, and today we're going to take a look at how you can use TVDigitEntryViewController to collect numerical data from your users. So, I have an app here that we call Top Movies which allows me to watch my favorite content, but not all this content may be appropriate for everybody in my household, so what I'd like to do is protect it behind a PIN code, so I can restrict who's allowed to watch it. So, what we have here is a collection view, full of some TVPosterViews that Hans introduced. So, if we dive in to our collection views: didSelectItemAt indexPath. First thing we want to do is vend a TVDigitEntryViewController, and send appropriate title and prompt text, let the user know they should enter a 5-digit passcode. We set the number of digits to 5, and we set the isSecure entry to "true" since we are collecting a passcode. Next, we're going to implement the entryCompletionHandler. This returns a string, once the user has filled in the total number of digits in the digit view. So, here for now, since I'm working on the app, we're just going to check to see if it's all 1's. If it is, we're going to dismiss the view controller, and play the content. Otherwise, we're going to update the prompt text, let the user know that it was an invalid passcode, and we're going to call the clearEntry animated true. What this will do is clear out all the digits, and shake the digit view to let the user know that something went wrong. And then, finally, we're going to present the viewController. And, since this is just a viewController, we can use custom presentation styles. So here we're going to use the blurOverFullScreen, which was introduced in tvOS last year. Let's run this and see how it works. So, I'm going to select my movie. And, we see we get the blurOverFullScreen animation, and we get prompted to enter our passcode. Now, we know that it's all 1's right now, so let's see what happens if I enter all 2's. We see that we get a nice shake animation, and we update the text. Now, we go enter all 1's. We know that's the correct passcode, so we dismiss the viewController and we can play our content. Now, if the user wants to use-- they most likely want to use their own passcode, so what we're going to do is we're going to take this settings button here, and we're going to allow the user to set their own passcode. So, I've got my [inaudible] action down here. And, again, we're going to vend a TVDigitEntryViewController, and we're going to set the title text to let them know that we're collecting a passcode that will restrict which content can be watched, and again, set the number of digits to 5. Next, we're going to implement the entryCompletionHandler again. Now, I have an extra variable up here, a optional string called passcodeToVerify. What this is going to do, is it's going to hold the passcode the user enters the first time so we can verify it to make sure that they actually entered the correct passcode. So, we see that when our completionHandler gets called, we check to see if the passcodeToVerify is nil. If it is, this is the first time, so we're going to ask them to verify their passcode, and we're going to call the clearEntry animated, except this time false. We don't want to shake it, because they didn't do anything wrong, but we want to clear it so they can enter it again. Otherwise, if there is a value stored in the passcodeToVerify, we know they are verifying it. And, we check to make sure they're equal. If they are, we save the new passcode, and dismiss the viewController. Otherwise, we reset the prompt text back to what we originally asked, clear the entry, and then set the passcodeToVerify to nil so they can try again. And, finally we present our viewController. So, now we're going to go to Settings. It's going to ask us to please set a passcode. Let's go ahead and set it all 1's first. Now it's asked us to verify. But, this time I'm going to enter all 2's. Since we know that was incorrect. So, now let's do it correctly this time. We've got five 1's, we're going to verify our PIN code. And, we can save the PIN code so the user can use it when they need to. And, that's how you can implement TVDigitEntryViewController into your application. I'd like to invite Hans back up on stage. Thank you, Marshall. That was a great demo showing just how easy it is to adopt a common UI patterns, using TVUIKit. In addition, TVUIKit also has built-in support for localization, including right-to-left language support, and of course, accessibility. TVUIKit is available in the Developer Beta, so please download and check out the headers. And, as you use it in your app, we think it will really save you time and resources so you can instead, focus on what makes your app truly shine. So, that's TVUIKit. This afternoon we've just looked at a few areas where tvOS 12 can improve your app's user experience and performance. We learned about Password AutoFill, which makes it really easy for your customers to sign in to your app. And, it's especially more helpful if they have strong password. We've also saw how we expanded tvOS's focus engine to support all apps, regardless of how they are rendered. This is truly game-changing. And, finally we looked at TVUIKit, which makes it really easy to adopt common UI patterns on tvOS. We have a session page with all of this information and more. And, we'll be available at the tvOS labs, and Safari WebKit labs throughout the week. Please drop by and say hello, and bring your questions and code. Thanks for joining us this afternoon. Have a wonderful WWDC. Hello. Good morning, everyone. And welcome to this year's session on HTP Light Streaming. My name is Emil Andriescu. Today's talk is about measuring and optimizing HLS performance. First, let's reflect for a second on why we should care and why it is essential for application. Let's pretend it's Saturday night, you're in your favorite spot on the couch, you've skillfully browsed through all the reviews, title is set, popcorn is ready, and you eagerly tap play when this happens. Faced with this mysteriously and never-ending animation, you ask yourself, what could be worse. Let's face it, you know it, it's a playback error. But what do customers really expect from HTP Light Streaming? Well, they expect high-definition image, high fidelity sound, and instant media response when they tap play. Yet, streaming applications over the internet are always at the mercy of the network, so how do we reconcile? Well, HTP Light Streaming was designed to address this, that is to provide the best possible uninterrupted streaming experience in an unpredictable network environment. So why are we here? Well, there's more. Over time, HLS has evolved into a more dynamic ecosystem, supporting new offering features such as I-frame playback, new media formats, and of course new codecs. At the same time, we're constantly adding powerful iOS, tvOS, and macOS APIs such as you can tune and adjust playback to your target audience and provide a much richer user experience. Delivery patterns and transport protocols, they are also evolving, so it is important to look at your server side performance in connection to how content is being consumed, either on a mobile device or in the living room. Given all these options, how can you be sure that you are providing the best possible user experience to your audience? Well, the first step is to understand and quantify the user experience in conjunction to changes that you make to your content, application, or delivery. This is an area where we believe it is imperative to measure rather than to guess which configuration is optimal. What is this session about? First, we want to establish a common language for discussing streaming quality of service. Second, we want to discuss how to objectively measure your application streaming performance. Third, we want to help you identify and solve some of the problems that impair streaming quality of service. And finally, we want to get those master playlists right. This is because many of the problems and issues that we see with streaming quality are actually rooted in the authoring of the master playlist. Before going into detail, let's begin with a brief overview of an HLS playback session. As you'd expect, it begins with the download of a master playlist. Once the playlist is passed by AV Player, it knows what content it refers to. In this case, we have two bitrates, 1 megabit and 2 megabit. AV Player will pick one of these, will go ahead and download a media playlist together with additional artifacts such as keys, and then continue to download media segments until the buffer level is sufficient for playback. When that happens, the AV Player item will communicate a prediction of playability by setting the is playback likely to keep up property to true. If you've preset the AV Player Rate to 1, so you're using the Autoplay feature of a AV Player, the player will go ahead and start playback immediately. We call this Time Interval Startup Time. From this point on, the wall clock, also known as real time, and the player item time base will advance at the same speed, with one condition, which is that content must arrive at an equivalent or faster rate than that which AV Player is consuming. If that's not the case, AV Player will try and switch down to the 1 megabit here. If network still cannot keep up with real time at 1 megabit, well the buffer will eventually run dry, and AV Player has no choice here, it needs to stop playback, event which we call a stall. The player will remain in this state not only until data starts flowing again, but up until there's a sufficient level of buffer for the player item to trigger another positive playability prediction. After that, playback can continue normally. Now let's discuss about quantifying the user experience for such a session. We do that by defining a set of Key Performance Indicators or KPIs. We picked five of them that we believe are most representative for HTP Light Streaming. One question you may ask is how much time do my users spend waiting for playback to start. Is it one second? Is it five seconds or maybe 30 seconds? This is an essential point in terms of user experience. Further, playback stalls, like the one we just saw, they are disruptive to the user. We care both about how often they occur, but maybe more importantly, how long does it take to recover from a stall? Yet, the best strategy without knowledge of the future to not stall is to deliver content at the lowest available bitrate. But of course that's not what we want. We want to deliver the best audio and video quality while still not stalling. So there's a clear tradeoff between risk of stalling and media quality, right. For that, we need another measure of the overall media quality for a session. And finally, playback errors. We talked about that. They are more disruptive than stalls, right. What can we do to track playback errors? Okay. Let's begin with startup time. There are multiple APIs that you can use to obtain or compute startup time. First, don't use the AV Player status changing to ready to play. That doesn't tell you that playback is going to start. However, if you are using Autoplay, so you're setting the rate, the player's rate in advance, you can use the AV Player item status changing to ready to play or the AV Player item is playback likely to keep out changing to true. These are observable properties. When that happens, you know that playback is about to start, but there might be a few milliseconds before playback actually starts. So what we recommend is to either use the AV Player time control status changing to playing or to track the player item time base, and there's a notification that allows you to do that. AV Player relies on heuristics to avoid stalls, but we know it, sometimes they're unavoidable. You can monitor as stalls occur by registering to the AV Player item playback stall notification. The suggestion here is to count the occurrence of stalls. Of course, if you want to compare and aggregate stall behavior across sessions of different duration, then you need to normalize this. How do you do that? Well, we recommend that you use the total duration watched and compute the stall rate in terms of stalls per unit of time watched, such as stalls per hour. A stall of 30 seconds is much worse to the user than a stall of one second. This is why we also care about rebuffering time or stall duration. By measuring the time interval between playback stalled notification and when the player item time base changes back to 1, you can compute an accurate stall duration. Again, the total duration can be normalized using the duration watched of the session. Well, you might be wondering at this point, how do I compute the duration watched of a session? And the answer is, through the Access Log. Let's see how we do that. So this is a snippet of code. First, we get a reference to the Access Log from the player item. We iterate through the events in the Access Log, and we simply sum up each events duration watch. And there you have it. We computed a total duration watch for a session. And now you may be wondering, well what is this event? What is an event in the access log mean? Well, for that let's look at how AV Player Item Access Log works. So the AV Player Item Access Log provides a history of your session. It is initially null, but as playback occurs, you're going to receive an AV Player Item New Access Log Entry notification, and by that time, you'll have an Access Log. You'll see that events in the Access Log contain information on various areas such as the current variant URL, the current bitrate, duration watch, number of stalls, and so on. These values are initially by convention initialized to a negative value or null. As playback occurs, they are updated with actual measurement data and the actual variant URL that you're playing. There are two cases in which we will add new events to the Access Log, and that is variant switch, like in this case, or a playback seek. But before a new event is added, the old one becomes immutable, and then we add the new event. Now, keep in mind that while these values here are constantly updating as playback occurs, so the values in the last event, they are not observable properties. We also mentioned that we care about media quality. How do we compute that? A way to measure if the user is presented with the best possible media quality is of course to look at the video bitrate being served. Here we don't really care about the startup time or the stall duration, so let's remove those. So we're left with the playback state. In this example, we see that we played for a longer time on the 2 megabit variant and less time at 1 megabit. By time weighting each bitrate, we can obtain a single value of video quality that we can compare across sessions. We call this measure a Time-Weighted Indicated Bitrate, and computing it is just as simple as with the total duration. Once again, we get a reference to the Player Items Access Log. We iterate through the events in the log. We compute the time weight of each event with respect to the total duration watch we computed earlier, and finally, we sum up the weighted bitrate value. Now keep in mind that some of these properties may not be initialized, so do the appropriate checks in your code. Another event which you must absolutely track is of course playback failure. To do that, you observe the AV Player item status. If the value ever changes to false, it means AV Player encountered an unrecoverable error. A good way to transform this observation into a KPI? Well, one way to do it is to look at the percentage of failed sessions with respect to total sessions, but there might be other ways to do it. One thing I want to stress here is that not all errors in your stream may be fatal. Some may impact media quality while some might not even be perceivable by the user. But nonetheless, if there are errors, they convey that there is an issue with your stream. So how do I get more insights on the stream, right, what happened? And the answer is from the Player Item Error Log. The AV Player Item Error Log. The Error Log conveys failures with varying degrees of user impact. It works in a similar fashion as the Access Log except that events represent error rather than player access states. They cover various areas, such as delivery issues, network issues, content authoring errors, and so on. For instance, they can give you an insight on why a stall occurred, such as no response for a media file for about ten seconds. So we talked about startup time that you can track for every session. We encourage you to take a look at the distribution of startup times for your application. We also talked about stall occurrence and stall duration. We mentioned that Time-Weighted Indicated Bitrate is a good indication of experienced media quality across a session, and finally, you probably want to keep the percentage of failed sessions as low as possible. Keep in mind that not all KPIs are comparable across sessions. One example of that is that AV Player foundation uses the AV Player layer size on the screen to evaluate illegible variants for HLS. So for instance, if you've got 10 ATP content, it will probably not be displayed on a 200 pixel view, but it doesn't mean the user experienced poor image quality. What to do then? We recommend that you gather additional context information along with your streaming metrics. This will allow you to later partition your playback sessions in classes that make sense for your application. Sample code for this section is available on the Apple developer website as part of the HLS catalogue sample. Now, please let me welcome Zhenheng Li, who will talk to you about ways to improve HLS performance. Thank you. Thank you, Emil. Hello, everyone. My name is Zhenheng. We have discussed all the KPIs that our users care the most. In this part of talk, let's focus on ways to improve these APIs. We will look deeper in three areas. One, how to reduce the startup time. Two, how to investigate and avoid stalls. Three, how to investigate and avoid errors. Let's get started. So what can delay start of playback? Here is an example of the operations from the user clicks play until the video start to play back. The application create every asset and start inspection of the asset to find out durations and awardable media options of the asset. It takes a few round trip between the device and the content server to download the master playlist and [inaudible] playlist. After that, the application create AV Player and AV Player Item. Buffering starts. Oftentime, buffering is interrupted, content is encrypted. It takes a few round trips between the device and [inaudible] to fetch the decryption keys. Once the keys are fetched, buffering resumes. However, it may be interrupted again. Let's say the application offers a feature, resumes from the previously watched point. Application sets a sic time, a set [inaudible] time on the player on behalf of the user. Every player discard the existing buffer and start download from a new location. Segment 100. Again, it maybe interrupted. Users has a language preference setting in the application. She or he prefers Spanish audio. Thus, application sets media selection on the player item, existing audio buffer being discarded, player start downloading from a different language variant. In a few seconds later, player item notifies playback is like to keep up, application sets a rate. Playback starts, and it continues. All this time, user is waiting. So as we can see, it does take a few time-consuming operations to start up, run a trip between the device and the content server and the key servers. Round trip times between AV Player and applications, oftentimes these two sit at different processes. So how the application measures the time cost and startup time? It may measure the time spent between the API calls and the player and the Player Item status change notifications. Every player item also offers startup time in the Access Log. This time is measured by the AV Player item, represents the time for buffering only. It's measured from the start media downloading until the first playback is selected to keep up. So our user wants the video to start fast, in at most a few seconds. There are ways to achieve that. One option, we can move some operations to a different stage before the user clicks play. For example, AV Asset creation and inspection can be moved out. Key fetching can be moved out. Thus, when the users starts a video playback, there is less waiting time. So where do we move those operations to? While your user is viewing the video catalogue or video info, it's a good time to create an inspect AV Asset before the user decides to play. Now, last year we had introduced AV Content Key Session API. This new API decouples the media load from key fetching. It gives the application total control on key management. It offers ways to optimize key fetching, such as bundling up multiple key requests back to the key server. If you happen to adopt AV Content Key Session, spending a few hours of engineering hours, your user will notice a faster startup time. So we have moved the AV Asset creation and key fetching out of startup time. Now what's left is mainly the AV Player Item buffering time and the communication time from AV Player and your application. Oftentime, app may be able to avoid buffering, such as due to [inaudible] or due to media options. We can even try to reduce the round trip time between the player and the application. Thus the startup is further reduced. Let's take a look. When you create AV Player Item, if you know where your user is intending to start the playback, set the current time on player item. If you know what are the media options such as which language to download for playback, set that as well before you set the AV Player item onto the player. Same with the AV plyer. As soon as the user click play, set rate before the start downloading for the AV Player Item. Thus, the playback will start automatically as soon as Player Item has enough to play back. In summary, set up AV Player before buffering. Set AV Player rate before setting the player item onto the player. A lot of application offers a feature to allow the user choose multiple videos and play one after another, such as binge watching TV episodes. We have seen implementation such as one player and one player item per video. There's always a startup buffering time for each new video. You may reduce that buffering time for the new video by AV Queue Player. Create multiple player items, include them all on the play queue. While the player is playing the current item, when the media download finishes for the current item, player will start downloading for the next one while the current one is still playing. Thus, the next player item will start playback as soon as current event play to the end. So do use AV Queue Player to play multiple items and enqueue second AV Player Item well in advance. So what's left now? Buffering time. First, what determine network buffering time. Four factors. The choice of your variant, the content bitrate, your playlist target duration, and of course, last, the network bandwidth. Let's take a look a few examples of buffering time. First, it's a simple master playlist. It specifies an ATP video at about 5 mbps. Let's assume the network bandwidth is around 6 mbps. Our target duration is 10 seconds. In most of the cases, player item buffers one segment before it notifies playback it like to keep up. However, the same master playlist, almost the same network condition, the user may observe slower startup. The reason is, remember, the network bandwidths change, and the content bitrate also change. In this case, there are a few segments take longer to download. Thus, it takes longer to start. To solve this problem, offering a variant with lower bitrate may help. Player may decide to switch down and start up sooner. When all other information is absent, the first listed variant will be your startup variant. So in this example, same two variants. The lower bitrate is listed first with same network condition. Player will start faster, start up faster and also switch up pretty quickly given the network bandwidth is sufficient for playback. In summary, to reduce network buffering time, make a wise choice of initial variant. Lower content bitrate means shorter buffering time, but it is a tradeoff of video quality. If you are offering multiple media formats such as HDR and SDR videos or stereo audio and multiple-channel audios, make sure the initial variant for each media format are on similar level of bitrate so your user will have a similar experience regardless what kind of viewing setup they have. That's all about reduce startup time. Our video has started. Next, let's talk about stalls. To be really clear, stalls can happen, especially when the network bandwidth is really low. So in this part of talk, let's focus on how to investigate stalls and how to improve or avoid stalls. How the application investigate stalls. The application should be listening to the stall notification at all time. And the application should be also checking the AV Player status such as is playback likely to keep up. AV Player Item also offers Error Log and Access Logs. The application should be listening to an exam those logs when the stall happens. Next, let's take a look two stall examples. First, stall notification has been received by the application. The application should have received the Error Log as well. The error comments give you detailed information on what has happened. In this case, it says media file not received in 15 seconds. Application checks Access Log to find out what the AV Player was playing at the moment when the stall happened. It tells you detailed information such as the player was playing what content and such URI. The indicated bitrate is the content bitrate. In this case, 36 mbps, and that is a [inaudible] content. An observed bitrate is the current network bandwidth. In this case is 2.8 mbps. It's obviously due to the network bandwidth can't catch up with the content bitrate. So to deal with variable networks, remember to provide a full set of bitrate. Remember some of your users may have a slower network connection, or your user may be on the go, such as on cellular while viewing the video. If you're offering multiple video, multiple media formats, each codec combination needs it's own set of tiers. Not all stalls are due to network condition. Let's look at this one. Stall happened, Error Log tells you a different story this time. It says playlist file unchanged for two consecutive reads. If you check the Access Log at the time, player was playing live. They indicated the bitrate is rather low. The content is about 400K, and the network bandwidth is 3.7 mbps. This look like a content delivery issue. So to reduce or to avoid stalls due to content delivery, content server and CDN must deliver media files, segments, keys without any delay. Update live playlist at least every target duration. The CDN [inaudible] must be configured to deliver most recent playlist to avoid stale playlists. Synchronized discontinuity sequence number between playlist. Indicate server-side failure clearly using right HTTP status code. That's all about stall. What about error? How do we investigate errors? There are a few ways. We have Error Log and Access Log from AV Player Item. We also have error property from every player and player item that the application can observe. In addition, we have some media validation tools for you to detect the content issue. Let's look at them one by one. AV Player Item Error Log, they have talked a little bit about [inaudible] in this one. This type of Error Log is an indication that there is a problem with network or content format. However, they are not always fatal. When the error is indicated, playback may be perfectly fine at that moment. However, the application showed the check in the error comments to find out details, such as this one. We have seen it before, media file not received in 15 seconds. So it's an indication that your user may have observed or will observe stalls. Now next one is HTTP error, it says file not found. This an indication of a content delivery issue. The user may observe audio loss, video loss, or both. [inaudible] specified bandwidth for variant. Now this is an interesting one. It's an indication of a stall risk. However, the playback may be perfectly fine when the error is indicated. It means some of the segments bitrate is higher than what is specified in the master playlist. Last example, crypto format error, unsupported crypto format. This may be an indication of a failure, a playback failure. All this error message and a few more that are not talked about here are very helpful when we have AV Player and Player Item errors. Let's take a look. The application should be observing AV Player Item status and AV Player Item error property to find out this type of error. These errors are fatal errors. When the error indicated playback has been terminated already, so what should we do? How do we find out the cause? Here is example. The application is observing player item status when the status changed to failed. Application go off to check the AV Player error properties as well as the Error Log from the AV Player Item. Here is the error property from the player item. It provides some useful information. Error code from AV foundation error domain. It also provides some hint, go off and check the Error Log from AV Player Item. So corresponding AV Player Error Log gives you much more details. It tells you on this data and the time and what URI with what type of error. So in this case, it's unsupported crypto format. It also tells you what type of network interface the device was on when the error happens. Next type of error, HDCP. If you are offering content that requires HDCP protection, your application should be observing this long property name, property. It's output obscured due to insufficient external protection. The value of this property changes to two means three things. Current item requires external protection. Device does not meet the protection level. User will observe or is already observing video loss, like through [inaudible] for example. To avoid this issue, your master playlist should offer at least one variant that does not require HDCP for fallback. Remember, not all your users has the viewing setup that is HDCP capable. App user interface should reflect the property change to timely hint the user. A lot of playback issues are introduced by content authoring such as audio and video out of syncope or glitches while [inaudible] switching. In addition to the error investigation and handling that we have talked about, we would encourage you to use our media stream validator too, which is available on the developer website. That's all I want to talk about it today. Now let's welcome my colleague, Eryk Vershen, to talk about how to author the master playlist the correct way. Thank you. Thanks, Zhenheng. My name's Eryk Vershen. I'm an engineer working on HLS Tools. We've spoken about how to measure your performance and also how to address many of those concerns. However, one of the key elements to successful and error-free playback experience is to ensure that your master playlist is authored correctly. The master playlist is what allows the player to make intelligent decisions both before and during playback. So getting it right is critical. There we go. That's my advice. No, I'm just kidding. I think I need to give you a little more background to understand what Roger meant. We want you to put all of the encoding options you have into your master playlist and to describe them as completely as possible. Let's pretend you're asking me questions. This is the crucial question and the main thing you have to get right. Now, first you have to remember that just because a master playlist works doesn't mean it's right. I've actually seen master playlists that look remarkably like this. This is technically legal, and it's next to useless. I say, okay, well how about this one? It has a few more variants. Well, it's a little bit better, but it's still terrible. Can we even play this? What codec is it using? Is it HDR? Is it 60 fps? You need to tell us everything. We want you to tell us everything. For example, average bandwidth. Average bandwidth enables us to make better decisions about which variant to switch to. It's a better predictor or whether we'll be able to keep up with a stream. Codecs is what enables us to filter out things that we can't play, and resolution allows us to make good decisions about which variant to choose. Remember, we don't read the media playlists or the media segments until we have to. So you need to tell us things ahead of time in your master playlist. So here's a sample of a simple master playlist. This playlist allows the player to adapt to bandwidth changes and make good choices about which variant to use. Now, everything that we've done here is invisible to the user. It just makes the stream play better. Okay. Let's look at a common problem. Your stream plays, but you're not seeing any images in fast forward, or your not seeing a thumbnail in the scrubber bar. Here's the Apple TV scrubber bar. You can see how long your content is. You can see where you are in the content, where you want to go. Now, in order to get that thumbnail image, you need to give us an I-frame playlist, and the I-frame playlist is also what allows us to provide images in fast forward and reverse playback on your iPad or your iPhone. Now in order to talk about I-frame playlist, we first need to talk just for a moment about normal video. Now, here's a way of visualizing regular video segments in HLS. Each segment has content for a number of frames, so it has a duration in frames, and it has a particular average bitrate, and that bitrate varies from segment to segment. Now, because of compression techniques, most frames in a video can only be decoded relative to other frames. But I-frames, the I stands for intercoded frames, these are frames that are independently decodable, and they're the base frames that allow everything else to be decoded. Now, as I've shown you here, you might have more than one I-frame in a single segment, and the I-frames need not be in a, occur at regular intervals. An I-frame playlist is a playlist which just points to the I-frame content, that is only the I-frame data will be downloaded. And when we talk about the duration of an I-frame, we always mean the time from that I-frame till the next I-frame. Now, this particular set of I-frames, I've shown as extracted from my normal content, but you can also make what we call a high-density I-frame playlist. This isn't something just extracted from your normal content. Instead, you make it deliberately with more evenly spaced I-frames. This will allow us to work better. It allows us to give a much smoother result when you're fast forwarding. Now, here I'm showing you a master playlist without I-frame playlist added. Now, notice that the I-frame playlist has almost exactly the same tags as the normal playlist. The only difference is the I-frame playlist does not support the frame rate attribute because it doesn't make any sense in that context. Now, a good test for your I-frame playlist is to try and play it directly. That is, take the URI of your I-frame playlist and paste it into Safari. It should play at 1X speed, and you should see the I-frames displayed one after another in a slowly changing sequence. Now also I want to point out the difference in the bitrate. Notice that the I-frame bitrate is much lower than the normal bitrate. That should always be the case. Now, speaking of bitrates, we've defined how to compute the peak bitrate in the HLS specification. Make sure you do it that way. Otherwise, you may get that segment exceeds playlist, exceeds specified bandwidth error. Now, we're going to move away from video and talk about audio for a little bit. Now the most common question is how do I support multiple languages? Here's what the interface looks like. I've got a list of languages, and the user can select one. And here's a sample playlist. Now, notice what we did is we've added an audio tag, sorry, an audio attribute onto each of our video variants, and we've added a group, the media tags with group ID's. The group ID is simply a tag that allows you to associate the audio renditions with the video variants. So notice there are a number of differences between the two audio renditions. Just as with your variants, we want you to tell us as much as you can about your media. Now, there are two attributes that people tend to have trouble with on the media tags, and that's Default and Autoselect. Okay. So Autoselect says that the media selection code in the player is allowed to choose this rendition without any special input from the user. Most of the time, you want Autoselect set to yes. If you don't set this, the user's going to have to make an explicit choice to get that rendition. The default on the other hand is what to pick when the user has not given a preferred language. Generally this should be the original language of your video, and the default must be autoselectable because the system has to do the choosing. Now, this default has nothing to do with the default video variant. This is the default within the group of renditions. So, okay, great. I've got multiple language, but I'd really like to have some multichannel audio. I've got 5.1. Okay. Well the first thing to remember is not all devices can play multichannel audio. So you want to also provide the user with a stereo option. And you should think of this always as filling out a matrix. You need to have every format having every language. You may say, well, I don't have a multichannel original for my French. I don't have a 5.1 French. In that case what you should do is put stereo, your stereo content in that group instead. You need to have something in every slot of this matrix. So let's see a sample playlist again. This one is just like the previous example except I've changed the group ID, and remember that's perfectly fine because the group ID just serves to connect the audio renditions with the video variants. Now, here we had the multichannel group, and then I've set this up with French as stereo, so you can see how that's done. What you need to do is make sure that the codecs tag indicates all the different codecs that can occur within that rendition group. Now, notice that we had to duplicate our video variant. So now we've got two entries, one pointing to one audio group, and the other pointing to the other audio group. And you'll see this kind of duplication again in later slides. Well let's say rather than 5.1, I've got several audio bitrates. I've got some high bitrate audio, and I know I need to provide a low bitrate for some users. So in terms of the renditions, this is similar to what we had before. We still got a matrix. We want to fill it out with every language for every bitrate. And since these are both AAC, they're considered the same format. So if I also want to have another format, all I do is extend that matrix. Now, I want to mention that I've been saying language for convenience, but you should remember that it's the name attribute which is the unique attribute, not the language attribute. Now, in this playlist I'm not going to show you the media tags. I'm just going to show you the video variants with their audio group names. Now you want your low bitrate video associated with your low bitrate audio, and you want you high bitrate video associated with your high bitrate audio. And you'll always want to split this up like this. Don't do a situation where you have a complete set of video variants associated with your low bitrate audio and a complete set of variants associated with your high bitrate audio. Because if you do that, you can be at a high video bitrate and be bouncing between high and low audio bitrates. Now, here I've added in the AC3 content. Notice that again we had to duplicate our video variant entries, but they point to the same video playlist. Now, notice also that the bitrate on the video variants changes. Remember that's because the video, I'm sorry, the bitrate associated with the video variant is the bitrate of the video itself plus any associated renditions. Now, let's go back to the video for just a second because I want to have multiple video formats. I like to have HEVC, so I can have better quality at the same bitrate or I'd like to have Dolby Vision so I can have some HDR content. Again, we're kind of filling out a matrix. In this case, no matter which video format we choose, we want to end up with a reasonable set of variants. So the rows here are tiers based on quality, and we want to fill out the matrix with a variant in each tier in each format. Now, we don't have to necessarily fill out the higher portions of the tiers on our older formats. You can skimp a little bit there. But similar to audio, not every device supports things like Dolby Vision, so you want to provide an H.264 variant as a fallback. The main thing to remember is that in each column you want to have the bitrate form a nice progression. Now, this playlist has gotten a little too big to show on one slide, so I'm going to split it over three slides. This one shows you the H.264 variant. On this slide, we have the HEVC variant. Now, notice everything has a different video playlist that it's pointing to, and here's our Dolby Vision variant, and notice that everything here has had the same audio group. So, again, if we wanted to have multiple audio formats, we would need to replicate the video variants for each audio format. And again this wouldn't increase the number of video playlists we had to have. It would just increase the number of entries that we had in the playlist. Okay. We're almost done. Our last bit is about subtitles and closed captions. Now, you can probably guess how this works. Our variants need to point at the subtitle and closed caption groups that we're using. So we need to add an attribute to our video variant, and we need to describe the renditions. Now, notice that the closed caption rendition does not have a URI attribute. That tells the system that the closed caption data is found within the video content, not in a separate playlist. So, there you go, the right thing to do is to give us everything you've got. See, now you understand better what I meant. Okay. I'd like to quickly summarize the talk we've given today. Emil talked about key performance indicators, about how to get or compute the values and what they mean. And Zhenheng talked about ways to reduce startup time and how to go about resolving stalls and other errors. And I've talked about how to do master playlists. I'd like to briefly mention the HLS validation tools. They do identify many issues with master and media playlists, and it's worth your time to use them. As always, you can get more information from the WWDC app or the developer website. That's all we have today. Thanks very much for your attention and time.  Morning. Good morning. My name's Eric Hanson. I'm the Technology Evangelist for the Photos platform. And, I'm joined by three of my colleagues from the Photos engineering team today to talk to you about integrating your application with Photos on macOS. We have two key things that we're going to talk about. The first is a full update on the Photos Project Extension API that we introduced last year in macOS High Sierra. And then, we're going to get into interacting with your application via drag and drop. But first, let's talk about Photo Project Extensions. When iPhoto was first launched in 2002, Apple became one of the first companies to allow people to create beautiful books, and later cards and calendars. To date over 70 million photo books, cards, and calendars have been created using iPhoto and Photos. But, in that same amount of time, 16 years, we've seen this blossoming market. There is a tremendous breadth now of choice for users to create all sorts of things with their photos, both physical and digital. And, it's the observance of this amazing ecosystem that led us to the introduction of the Photo Project Extension API last year, to build on that ecosystem. We've seen some great extensions already. For example, Mimeo Photos. It lets you create these books, and cards, and calendars of exceptional quality. There's Whitewall, that lets you create gallery-quality framed prints from your photos. And, we have digital services like Wix.com, that make it very easy for you to create web photo albums to share with your friends and family. And, we're now seeing some new extensions on the horizon. Like this one, called Motif. It is a brand-new fully native experience, integrated directly into Photos, coming to you this summer. All of this choice, this whole ecosystem is a tremendous opportunity to you, as a developer. You can take the very rich metadata and the context and the image curation that we pass to your extension, and you can build on that, using the full stack of native frameworks on macOS, to create something really extraordinary, and new, that delights users around the world. And, it's for this reason that Apple is announcing we will be transitioning our entire print product business to this ecosystem in macOS Mojave. Project Extensions will be the way users will create books, cards, calendars, all sorts of things. Whatever you surprise us with with your extensions. And, by doing this, we're building a better Photos experience for everyone. So, let's talk about what's new in the Extensions API. First, with the UI. We now take the UI of your extension, and integrate it directly into the Photos app. So, the sidebar that you're familiar with is always available. Right? A user can be creating a project in one of your extensions, and can grab content from the sidebar, and just drag and drop it right onto your project. Or, they could pop up to the photo search, search for a photo they want to use, copy it, and paste it into the project that they're working on. We're also allowing you to integrate directly with the powerful editing tools of Photos. For example, you may want to allow a user to simply double-click on a photo to edit it. And, you can do that now, with a single line of API. You can invoke the actual full editor of Photos. We pushed the edit session to the top of the view stack, with that photo loaded. Let the user make any adjustments they want to make. And, we also give them access to all of the other assets that are used in that project. When they're done with their editing session, they simply hit the Done button, and they're returned to the project that they're working on. You get a notification that the library changed. You can respond to that, and update your UI accordingly. But, none of this would matter if it was hard to find the apps and the extensions that you create. And so, we're stepping up the game in the discoverability of your applications. The Photos app links to the Mac App Store in the Create menu. And now, with the brand-new Mac App Store, on macOS Mojave, we actually can link directly to a fully curated story that's targeted specifically at the extension experience. And, this is an evergreen story that will live on. It's something that we can update, and feature new experiences, and educate users on new ways to do things with their photos. And, when a user downloads an app from this story, they're downloading an app that contains an extension. And, historically, that may have led you, as a developer, to put some time into, kind of, educating users in your standalone app on how to use the extension in Photos. But, we think we can do something better. Wouldn't it be great if the user could download an app and immediately launch into the project creation experience in Photos? We're making that possible. We've added a custom URL scheme to Photos, where you can simply pass in your extension identifier, and optionally pass in a category. And, Photos will be brought to the back-- brought to the foreground, with the options of your extension displayed for the user. So, they're immediately taken into the creation experience as a first-launch experience. And, we think this is great. And, finally, when they create a project, we want those projects to live where they belong, right inside the user's Photo library. And so, that's exactly what we do. We have this Project Gallery in Photos that lets a user see all the things that they've created. And we now let the extension create a custom preview for each project. So, that preview can represent what the project really is. Take this photo mug for example. Not only can a user double-click on one of those projects to get back into your extension, to continue to work on it, they can create projects from other projects. They can simply select a project, go to the Create menu, create something new from it. And, we pass over everything we can to help your extension, kind of, start with some great context. But, in the case of the Apple projects, we can do a lot more. And so, we do. We send you photo-by-photo, page-by-page exactly how that was laid out. So that your extension can create a complete conversion experience from the Apple projects. And, we highly encourage you to do this. So, that's just a very high-level, from a UI perspective, of some of what's new. But, we'd like to take you a little deeper. And, to do that, I'd like to invite to the stage, my colleague from Photos engineering, Tobias Conradi. Tobias. Good morning. Thank you, Eric. Hello, my name is Tobias Conradi. I'm an engineer on the Photos team, and I will go into detail for some of the changes we did around Photos Project Extensions. First up, the Create menu. When we first introduced Photos Project Extensions in macOS High Sierra, we put all extensions as a flat list into the Create menu. But, as it turns out, in some cases, it's kind of hard to guess, just from the name of the extension what kind of projects you can create with that. We want to improve the user experience, so we are introducing project categories. The new Create menu in macOS Mojave looks something like this. We have categories, submenus in the Create menu. And, the categories we're introducing this year are Books, Calendars, Cards, Wall Decor, Prints, Slideshows, and for any extension that doesn't fit into these categories, Other. Now the user can go into the menu with a very specific intent. For example, I want to create wall decorations, and these [inaudible] all extensions that support projects in that category. How does the extension show up in the category? We have a new key in the extensions attribute dictionary in the extensions info.plist. It's called PHProjectCategory. And, the value for the key, is a list of the supported project categories. In this case, it's Wall Decor and Other. The next thing the user sees from the extension, is the [inaudible] sheet. You provide us with the data, and we display the project type descriptions you provide. The problem with the API we introduced last year is that it's kind of hard to provide up-to-date price information or current offers. That's why we're introducing a new, improved API to allow for dynamic updates, which would look like this. In addition to dynamic updates, we now also support display of a custom footer text at the bottom of the sheet, which you can use to display legal text if you have the requirement to do so. On an API level, the new dynamic API looks like this. We have the new method on the projectExtensionsController protocol, and we ask for a data source, instead of a list of product type descriptions. And, pass in the category from the menu in which the selection-- the extension was selected. And also, an invalidator object. Once you return the data source, we ask the data source for the project type descriptions, and for the optional footer text. Whenever your extension has the need to invalidate the information returned, you can use invalidator to invalidate the project type descriptions, or the footer text, and when necessary, photos will refetch the data from the data source, and display the up-to-date data in the UI. The next topic is the projectInfo. The projectInfo is structured, additional information about the contents of the project. And, it's structured into sections, and sectionContents which reflect creation level. And, the elements have basic layout hints, and also weights and scores for the assets. And, assets can contain important regions in the asset's regions of interest. This is just a quick reminder what the projectInfo looks like. If you want to know more about the projectInfo, I highly recommend last year's session, What's New in Photos API. The projectInfo is handed to your extension during begin project. And, whenever the user adds new assets to the project, the projectInfo will be outdated, because it's a static object. And, we want to solve the problem by introducing new API on the projectExtensionsContext to get updated project info, with the current state of the project. Let's take this as an example. We handed you this projectInfo during project creation, and then the user added some more assets to the project. You can then call the updatedProjectInfo method, with this projectInfo, and we will update any existing sections, and append a new section for any added assets. If the user adds some more assets, you can do the same again. Pass in this project info, we will update the existing sections, and append a new section for any added assets. Let's now go into a more detailed element of the projectInfo, the regions of interest. The regions of interest highlight important regions in the asset. For example, the faces of people. And, if the same person appears in multiple images, the region of interest identifier will be the same for the same person. Let's focus on Person B for a moment. In addition to the region of interest identifier, we have weight on the regions of interest, which represents the importance of the region of interest in the project. But, if you want to decide which image to pick as a representative image for a person, or for a region of interest, it's kind of hard to do with this API. That's why we're adding a new quality score, which represents the quality of the region of interest in the asset. On the left-hand side, the Person B is kind of out of focus and not really central to the image, while on the right-hand side, he's really in focus. That's why the quality score on the right-hand side is higher than on the left-hand side. On an API level, that looks like this. We have this alt weight, which is the importance of the region of interest in the context of the project. And then, the quality in the asset. I would now like to show you a quick demo, how you can use regions of interest in your extension to improve the experience, and also listen to a library notification to get notified whenever the assets or the project changes. And, use a new updateProjectInfo API. OK. I created a slideshow extension, and to show you how it looks like, I selected some assets in an album, and will create a new project with the extension. The extension has two views, one is the overview, which contains all assets of the project. And, the other is the playback modus, which plays the slideshow. As you might notice, the slideshow always zooms into the center of the image, which is kind of boring. And, I want to use regions of interest to always zoom into interesting regions in the photo. So, let's switch to Xcode to fix that. Here in my asset model, I have this property, preferredZoomRect, and it always returns the same rect, which is kind of boring. So, let's replace it with the code. What I want to do is to get all regions of interest from the asset element, and then sort them by their weight and quality. And, from the sorted list of regions of interest, we want to get the last element, and return its rect. If we now rerun the extension-- We always zoom into the most important region of interest in the asset. I think that looks way better. So, next, I want to add some more assets to the project. I grab an album in the sidebar and drag it over the extension. There's a plus next to the cursor, so it looks like the extension accepts the drag. I release the mouse and nothing happens. The assets were added to the project, but my extension doesn't list the change notification. And, doesn't know that assets were added to the project. So, let's switch back to Xcode and fix that. In my projectViewController, I have the begin and resumeProject method, which are a part of the projectExtensionController protocol. And, here I want to register for change observation. And, we add the same code in both methods. First, we get the PhotoKit object we're interested in. We're interested in additions of assets to the project, so we are getting a fetch result for all assets on the project, and then we are registering asset change observer for the library. In finished project, we [inaudible] registering. And, since Xcode complains that we don't implement the protocol, we add that. Change observer. And, down here-- We implement photoLibraryDidChange. PhotoLibraryDidChange is called whenever anything changes in the photo library, and we get a change instance as a method argument. We can ask the change engine instance for changeDetails of an object we're interested in. We're interested in the changes of the fetchAllAssets on the project, so we pass that fetchResult in. And, if change results are returned, we update our locally cached fetch result with the fetchResultsAfterChanges, and get the projectExtensionContext, and call updatedProjectInfo. And pass in the project info we have. Once the updatedProjectInfo is returned to us, we call the same setup code, but you could do something more sophisticated in here. Let's rerun the project again. OK. I can now drag the album from the sidebar, release it over the extension, and the assets are added to the project, and the extension listens to the photo library change observation, gets notified of the change, and we update our project info. So, as you just saw, with only a few steps, I was able to improve my extension by using the projectInfo, and the regions of interest in the projectInfo, and also registering asset change observer to get notified whenever anything changes in the photo lobby, and also update my project info. There are two things I want to highlight with integration. By default, Photos handles copy and paste of assets or albums from Photos into the extension. But, if your extension also wants to implement the paste section, which you should probably do for text and things like that, we need your help to know when your extension should handle the paste section, and when Photos should try to handle the paste section. So, please implement validateMenuItem, and if the menu action is a paste action, check if your extension can handle the current pasteboard contents, and if no, return false, and we will try to handle it as Photos. And, otherwise, if you can handle it, return true, and your photos will get the paste action. Something similar applies to drag and drop. Photos handles any drags of PhotoKit objects to the extension by default, but your extension can potentially interfere with that if you register for the wrong types. So, please be careful what types your extension is registering for, and only register for extension internal drags, or other drags you really want to handle. Be especially careful if you use WKWebView, because it registers for a bunch of drag types by default. That was everything about photo project extensions. Let me now hand over to my colleague Sanaa to talk about integrating with Photos-- interacting with Photos as a third-party application. Thank you. Thank you, Tobias. Hello, everyone. My name is Sanaa. I am a Photos engineer, and today with my colleague Joachim, we are going to talk about some best practices for receiving and providing images or videos between apps on macOS, using drag and drop. Drag and drop is one of the most intuitive and easiest way to move items from one place to another. But, sometimes, you might have encountered this situation instead. If this is something happening with your app, then you are sitting in the right session. So-- Let's take a step back to understand what's happening here. Drag and drop on macOS is using NSPasteboard, and when it's wrapped in with the pasteboard, reading and writing data happens on the main trade, for both the receiving and providing apps. In the past, since all local was stored on local disk, you could put the file URL's into the pasteboard. But, things have changed today. Images or videos might not be on disk if the user is using iCloud, so Photos, we need to download the full resolution item first, before putting the file URL into the pasteboard. Also, Photos respect the privacy settings of drag and drop, so if the user choose to save the location information, then Photos will export a new file which does not contain this metadata. Both downloading and exporting file takes time, and you don't want to do that on the main trade, since it's going to block your app UI. So, in order to fix this, we need an asynchronous API. In fact, we do have one, it's called file promises. File promise is a promise of-- that's a file of a certain type that does not exist yet on disk, will be written in a provided location. It also allows the sender to write files in the background. There are two ways to interact with file promises: receiving files by using NSFilePromiseReceiver and providing files by using NSFilePromiseProvider. Both of these modern API's have been introduced two years ago with macOS Sierra. So, let's start first with receiving file promises. As a general rule, apps supporting drag and drop should always accept both file URL's and file promises. And, I'll explain why. There are multiple apps providing file promises. Photos is using file promises when dragging an image, a video, or entire album, and since macOS Mojave, you can-- we added the ability to drag the people or memories. So, we are not the only app using file promises. Mail is using file promises when dragging a message to Finder, to save the entire email, including attachments. Safari is using file promises when dragging an image. And, Keynote is using file promises when dragging a selection of slides to create a new document containing those slides. So, if you want to receive files from those apps, or any app providing file promises, then your app needs to read file promises, and need to accept those files. So, let's see how we can do that by looking at some code. First, during setup, a view must register what types it'll accept by calling registerForDraggedTypes. And, in order to accept file promises, you can use the class property readableDraggedTypes on NSFilePromiseReceiver. Then, when performing the drag operation, and when enumerating or draggingItems, you should add support to NSFilePromiseReceiver, and make sure to handle it first. Because it's more likely to contain the highest quality representation. For each filePromiseReceiver, you call in the promise, and when the file is ready, then the reader block is called on the provided operationQueue, where you can handle the file. It's very important to provide a background operationQueue to not block the main trades while waiting for the file to be downloaded over-- to be downloaded, to be written by the source file. Because this process can take a long time, and you don't want-- especially if the file needs to be downloaded over a slow network. So, for a better user experience, you need to display a loading activity, and when the file is ready, then you can replace the UI with the real content. Here you will see an example of Mail, showing a placeholder UI while waiting for the image to be downloaded via file promises. So, that was receiving file promises. Now, let's see how we can provide file promises. And, you should consider implementing this in your app, if the data you want to send over drag and drop does not exist yet on disk. So, let's have a look on how we can do that by looking at the API. First, you will need to create an instance of NSFilePromiseProvider. You should create one instance for each promised file. And, before writing the filePromiseProvider object to the pasteboard, it must contain a file type, and a delegate. These delegate is doing the heavy lifting of writing files to disk. There are only three methods for NSFilePromiseProvider delegates. The first one is called by the drag destination, and returns the file name, not the full path. The second one return an operationQueue, and when-- an operationQueue where the file, it will be written. And, we highly recommend that you implement this optional method, and provide a background operationQueue, since otherwise, the main queue will be used otherwise. And, finally, writePromise to file is called, asking you to write file to disk. And, when you are done, remember to call the completion handler. So, that was receiving file promises-- providing file promises, and we have covered the API to-- that you can use to adapt your app to receive and provide file promises. And now, I'd like to invite Joachim on stage to show you all of that in a sample app. Thank you. Joachim? Thank you. Good morning. We're going to look at a simple beam generator app, which has a few issues with drag and drop. And we're going to dive into Xcode, and try to fix those issues together. So, here is my simple app. And, I can just grab a file I have here on desktop, and drop it onto its window. Just like that. On the upper right, I have a little button, where I can add a text box, and I can add some text to my image. I can select it, move it around, and when I'm ready, I can send it over iMessage, for example. So, while this image is great, I really want to use an image I found on the web. So, let's open up Safari, and try to drag this image onto our app. Unfortunately, this doesn't work. So, let's jump to Xcode, and see if we can fix that. Here I am in the main viewController of our app. And, we going to jump straight to view [inaudible]. And, have a look at what's going on. Right here, we're only registering for file URL's. So, let's fix that, and register for FilePromiseReceiver as well. Next, when handling the drag operation, we need to take care of FilePromiseReceiver as well. So. Let's jump to the next method right here. So, here is the list of supported classes, and we only have NSURL. So, we're going to add at the first index, NSFilePromiseReceiver. In the enumeration method here, we're going to add a new case. Where we're going to receive the promised files, and if we get a file URL, we're going to call the exact same method we will have called if we had just gotten the file URL over drag and drop. So, let's see if it works. So, here's the little app. Let's go back to Safari, take the image, and drop it onto the window. So, that was pretty easy. I only had to modify two existing methods in order to accept file promises. Now. Let's add some text. Like this. And, wouldn't it be great if I'm not quite ready to send out my meme to somebody, if I could just drag it onto the desktop to save it as an image file? As you can see, this doesn't work. So, let's jump back to Xcode and fix this as well. At the bottom of the class, we have this method which returns an object conforming to NSPasteboardWriting, and as you can see, we're just returning a simple NSImage here. So, we're going to replace it with a filePromiseProvider. So, we're creating a file provider, and we're going to provide a JPEG image. And, we're going to use this userInfo property to store the snapshotItem, which we're going to use later to write out the file to disk. As a next step, let's conform to NSFilePromiseProviderDelegate. Just like this. Jump back down. And, implement the three delegate methods you heard about just before. The first one returns the file name. For simplicity, we're just returning a static file name here. The second one returns an operationQueue. And, we happen to have one right here. And, the third method is actually going to write the file to disk. So, what's happening here, is that we're getting the snapshotObject out of the filePromiseProvider object, and we're going to use its JPEG representation to write the file to disk. So, let's have a look. This time, we're going to jump to Photos. Going to take this image, drag it into our app, add some text. And, drop it onto the Finder desktop. And, here we are. So, if you just follow the simple steps I just showed you, you can add support for file promises to your app, and improve the user experience. And, with that, I'd like to hand it back to Eric. Thank you. Great stuff. Great stuff. I hope you really enjoyed that. I just have a few words, just in summary. To, kind of, tie this all together. And, I'll start by saying, you know, photos really, really matter to all of us. Right? Millions and millions of people around the world take millions of photos every day. And, all of those photos find a home inside the Photos app. But, they're nothing if they can't be shared. If they can't be shown. If they can't be preserved into beautiful keepsakes. And so, all of these people capturing all of these images, they rely on you, the developer, to create amazing apps that let them do creative things with those images. So, if you only remember two things from this session today, please remember this. Support file promises. You saw it's really easy to have some very immediate interaction with the Photos app. Please, please do that. And, secondly, we strongly encourage you to explore all of the API that's available to you in the project extension experience within Photos. Going back to what I said at the beginning, this is a blossoming ecosystem. And, there's a tremendous opportunity to embrace it. And, collectively for us to create a better Photos experience for everybody. So, with that, please join us in the lab this afternoon. We have tons of engineers on staff to help you with any of your PhotoKit questions, Photos extensions, be it iOS, macOS, we really look forward to talking to you there. And, enjoy the rest of your WWDC. Thank you.  Hello! So welcome to the second session of Core ML. My name is Aseem, and I'm an engineer in the Core ML team. As you all know, Core ML is Apple's machine learning framework for on-device inference. And the one thing I really like about Core ML is that it's optimized on all Apple hardware. Over the last year, we have seen lots of amazing apps across all Apple platforms. So that's really exciting. And we are even more excited with the new features that we have this year. Now you can reduce the size of your app by a lot. You can make your app much faster by using the new batch-predict API. And you can really easily include cutting-edge research right in your app using customization. So that was a recap of the first session. And in case you missed it, I would highly encourage you to go back and check the slides. In this session, we are going to see how to actually make use of these features. More specifically, we'll walk through a few examples and show you that how in a few simple steps using Core ML Tools. You can reduce the size of the model, and you can include a custom feature in your model. Here's the agenda of the session. We'll start by a really quick update on the Core ML Tools ecosystem. And then we'll dive into a demo of our quantization and custom conversion. So let me start with the ecosystem. So how do you get an ML model? Well, the best thing is that if you, if you can, if you find it online, you just download it, right? Very good place to download your ML models is the Apple Machine Learning landing page. We have a few models there. Now let's say you want to train a model on your data set. In that case, you can use Create ML. This is a new framework that we have just launched this year, and you do not have to be a machine learning expert to use it. It's really easy to use. It's right there in Xcode. So go and give it a try. Now some of you are already familiar with the amazing machine learning tools that we have outside in the community. And for that, last year we had released Core ML Tools, a Python package. And along with that, we had released a few converters. Now there has been a lot of activity in this area over the last year. And this is how the picture looks now. So as you can see, there are many more converters out there. And you really do have a lot of choice to choose your training framework now. And all of these converters are built on top of Core ML Tools. Now, I do want to highlight a couple of different converters here. Last year, we collaborated with Google and released the TensorFlow converter. So that was exciting. As you know, TensorFlow is quite popular with researchers who try out new layers so we recently added support for custom layers into the converter. And TensorFlow recently released support for quantization during training and that's Core ML 2 supports quantization. This feature will be added soon to the converter. Another exciting partnership we had was with Facebook and Prisma. And this resulted in the ONNX converter. The nice thing about ONNX is that now you have access to a bunch of different training libraries that can all be converted to Core ML using the new ONNX converter. So that was a quick wrap-up of Core ML Tools ecosystem. Now to talk about quantization, I would like to invite my friend Sohaib on stage. Good morning, everyone. My name is Sohaib. I'm an engineer in the Core ML team. And today we're going to be taking a look at new quantization utilities in Core ML Tools 2.0. Core ML Tools 2.0 has support for the latest Core ML model format specification. It also has utilities which make it really easy for you to add flexible shapes and quantize in your own network machine learning models. Using these great new features in Core ML, you can not only reduce the size of your models. But also reduce the number of models in your app, reducing the footprint of your app. Now let's start off by taking a look at quantization. Core ML Tools supports post-training quantization. We start off with a Core ML neural network model which has 32-bit float weight parameters. And we use Core ML Tools to quantize the weights for this model. The resulting model is smaller in size. Now size reduction of the model is directly dependent on the number of bits we quantize our model to. Now, many of us may be wondering what exactly is quantization? And how can it reduce the size of my models? Let's step back and take a peek under the hood. Neural networks are composed of layers. And these layers can be thought of as mathematical functions. And these mathematical functions have parameters called weights. And these weights are usually stored as 32-bit floats. Now in our previous session, we took a look at ResNet50. A popular machine-learning model which is used for image classification amongst other things. Now this particular model has over 25 million weight parameters. So you can imagine, if you could somehow represent these param -- these parameters using a fewer number of bits, we can drastically reduce the size of this model. In fact, this process is called quantization. In quantization, we take the weights for our layers which [inaudible] to minimum and to maximum value and we map them to unsigned integers. Now for APIC quantization, we map these values from a range of 0 to 55. For 7-bit quantization, we map them from 0 to 127, all the way down to 1 bit. Where we map these weights as either zeros or ones. Since we're using fewer bits to represent the same information, we reduce the size of our model. Great. Now many of you may have noticed that we're mapping floats to integers. And you may have come to the conclusion that maybe there's some accuracy loss in this mapping. That's true. The rule of thumb is the lower the number of bits you quantize your model to, the more of a hit our model takes in terms of accuracy. And we'll get back to that in a bit. So that's an overview of quantization. But the question remains. How do we obtain this mapping? Well, there are many popular algorithms and techniques out there which help you to do this. And Core ML supports two of the most popular ones: linear quantization and lookup table quantization. Let's have a brief overview. Linear quantization is an algorithm in which you map these full parameters equally. The quantization is parametrized by a scale and by values. And these values are calculated based on the parameters of the layers that we're quantizing. Now and a really intuitive way to see how this mapping works is if we take a step back. And see how we would go back from our quantized weights which are at the bottom back to our original float weights. In linear quantization, we would simply multiply our quantized weights with the scale parameter and add the bias. The second quantization technique that Core ML supports is lookup table quantization. And this technique is exactly what it sounds like. We construct a lookup table. Now again it's helpful if we imagine how we would go back from our quantized weights back to our original weights. And in this case, the quantized weights are simply indices back into our lookup table. Now, if you notice, unlike linear quantization, we have the ability to move our quantized weights around. They don't have to be spaced out in a linear fashion. So to recap, Core ML Tools supports linear quantization and lookup table quantization where we start off with a full precision neural network model. And quantize the weights for that model using the utilities. Now you may be wondering well great, I can reduce the size of my model. But how do I figure out the parameters for my quantization? If I'm doing linear quantization, how do I figure out my scale and bias? If I'm doing lookup table quantization, how do I construct my lookup table? I'm here to tell you that you don't have to worry about any of that. All you do is decide on the number of bits you want to quantize your model to. And decide on the algorithm you want to use, and let Core ML Tools do the rest. In fact -- In fact, it's so simple to take a Core ML neural network model. And quantize it. Then we can do it in a few lines of Python code. But why stand here and talk about it when we can show you a demo? So for the purposes of this demo, I'm going to need a neural network in the Core ML model format. Now, as my colleague Aseem mentioned, a great place to find these models is on the Core ML machine learning home page. And I've gone ahead and downloaded one of the models from that page. So this model's called SqueezeNet. And let's go ahead and open it up. As we can see, this model is 5 megabytes in size. It has a input which is an image of 227 by 227 pixels. And it has two outputs. One of the outputs is the class label which is a string, and this is the most likely label for the, for the input image. And the second output is a mapping of strings to probabilities given that if we pass an image, it's going to be a list of probabilities of what that image may be. Now let's start quantizing this model. So the first thing I want to do is I want to get into a Python environment. Now a Jupyter Notebook is one such environment that I'm comfortable with. So I'm going to go ahead and open that up. Let's open up a new notebook and zoom in on that. Alright. So let's start off by importing Core ML Tools. Let's run that. Now the second thing I want to do is I want to import all the new quantization utilities that we have in Core ML Tools. And we do that by running this. And now we need to load up the model which we want to quantize. And we just saw the SqueezeNet model a minute okay. We're going to go ahead and get an instance of that model. Send this to my desktop. Great. Now to quantize this model, we just need to make one simple API call. And let's try a linear, quantizing this model using linear quantization. And its API is simply called quantize weights. And the first parameter we pass in is the original model which you just loaded up. The number of bits we want to quantize our model to. In this case, it's 8 bits. And the quantization algorithm we want to use. Let's try linear quantization. Now what's happening is that the utility is iterating over all of the layers of the linear networks. And is quantizing all the weights in those layers. And we're finished. Now, if you recall a few moments ago I mentioned that quantizing our model had an associated loss in accuracy. So we want to know how our quantized model stacks up to the original model. And the easiest way of doing this is taking some data, passing and getting inference on that data using our original model. And doing the same inference on the same data using our quantized model and comparing the predictions from that model. And seeing how well they agree. Core ML Tools has utilities which help you to do that. And we can do that by making this call which is called compare models. We pass in our full precision model, and we pass in our model which we had just quantized. And because this model is a simple image classifier which it only has one image inputs. We, we have a convenience utility. So we can just pass in a folder containing sample data images. Now on my desktop here, I have a folder with a set of images which are relevant for my application. So I'm going to go ahead and pass a path to this folder as my [inaudible] parameter. Great. So now we see we're analyzing all the images in that folder. We're running inference on the, we're using full prediction or full precision model. And we're running inference on our quantized model. And we're comparing our two predictions. So we seem to have finished that. And you can see our Top 1 Agreement is 94.8%. Not bad. Now what does this Top 1 Agreement mean? This means that when I pass in my original model, that image of a dog for example, and it predicted that this image was a dog. My quantized model did the same. And that happened over 98, 94.8% of the data set. So I can go ahead and use this model in my app. But I want to see if other quantization techniques work better on this model. As I mentioned, Core ML supports two types of quantization techniques. Linear quantization and lookup table quantization. So let's go ahead and try and quantize this model using lookup table quantization. Again, we pass in an original model, the number of bits we want to quantize our model to. And our quantization techniques. Oops, made a typo there. Let's go ahead and run this. Now, k-means is a simple clustering algorithm which approximates the distribution of our weights. And using this distribution, we can construct the lookup table for our weights. And what we're doing over here is that we're iterating over all the layers in the neural network. And we're quantizing and we're figuring out the lookup table for that particular layer. Now, if you're an expert and you know that your model, you know your model architecture and you know that k-means is not the algorithm for you, you have the flexibility of passing in your own custom function instead of this algorithm and the utility will use your custom function to actually construct the lookup table. So we finished quantizing this model again using the lookup table approach. And now let's see how well this model compares with our original model. So once again we call our compare model's API. We pass in our original model and we pass in our lookup table model. And again we pass in our sample data folder. Again, we run inference over all the images using both the original model and the quantized model. And we see this time we're getting a much better, little bit better Top 1 Agreement. Now for this model, we see that lookup table was the right way to go. But again, this is model-dependent and for other models, linear may be the way. So now that we're happy with this and we see that this is good enough for at least my application, let's go ahead and save this model out. We do that by causing or calling save. I'm going to give it the creative name of Quantized SqueezeNet. And there we go. We have a quantized model. So this was an original model. And we saw that it was 5 megabytes in size. Let's open up our quantized model. And the first thing we notice right off the bat is that this model is only 1.3 megabytes in size. So if you notice, all the details about, about our quantized model are the same as the original model. It still takes in an image input, and it still has two outputs. Now, if I had an app using this model, what I could do as we saw in the previous demo. Is we could just drag this quantized model into our app and start using that instead. And just like that, we reduce the size of our app. So that was quantization using Core ML Tools. To recap, we saw how easy it was to use Core ML Tools to quantize our model. Using a simple API, we provided our original model, the number of bits we wanted to quantize our model to, and the quantization algorithm we wanted to use. We also saw that Core ML Tools has utilities which help us to compare our quantized model to see how it performs against our original model. Now as we saw in the demo, there is a loss of accuracy associated with quantizing our model. And this loss of accuracy is highly model and data dependent. Some models work well or perform better than others after quantization. As a general rule of thumb again, the lower the number of bits we quantize our model to the more of a precision hit we take. Now in the demo we saw that we were able to use Core ML Tools to compare our quantized model and the original model using our Top 1 Agreement metric. But you have to figure out what the relevant metric for your model and your use case is and validate that your quantize model is acceptable. Now in a previous session, we took a look at a style transfer demo. And this network took in an input image, and the output for this network was a stylized image. Let's take a look at how this model performs at different levels of quantization. So on the top, top left here, your left. We see that original model is 30 -- is 32 bits and it's 6.7 megabytes in size. And our 8-bit linearly quantized model is only 1.7 megabits in size. And we see that the performance by visual inspection it's good enough for my style transfer demo. Now we can see that even down to 4 bits, we don't lose out much in the way of performance. I would even argue that for my app at least, the 3 bit will work fine as well. And we see at 2 bit, we start to see a lot of artifacts and this may not be the right model for us. And that was quantization using Core ML Tools. Now I'm going to hand it back to Aseem who's going to talk about custom conversion. Thank you. Thank you, Sohaib. So I want to talk about a feature that is essential to keep pace with the machine learning research that's happening around us. As you all know, the field of machine learning is expanding very rapidly. So it's very critical for us at Core ML to provide you with the necessary software tools to help with that. Now let's take an example. Let's say you are experimenting with a new model that that is not supported on Core ML. Or let's say you have a neural network that runs on Core ML but maybe there's a layer or two that Core ML does not have yet. In that case, you should still be able to use the power of Core ML, right? And the answer to that question is yes. And the feature of customization will help you there. In the next few minutes, I want to really focus on the specific use case of having a new neural network layer. And show you how you would convert it to Core ML and then how you would implement it in your app. So let's take a look at model conversion. So if you have used one of our converters, or even if you have not, it's a really simple API. It's just a call to one function. This is how it looks for the Keras converter. And it's very similar for say the ONNX converter or the TensorFlow converter. Now when you call this function, mostly everything goes right. But sometimes you might get an error message like this. It might say, "Hey, unsupported operation of such-and-such kind." Now if that happens to you, you only need to do a little bit more to get past this error. More specifically, such an error message is an indication that you should be using a custom layer. And before I show you what is the little bit of extra effort that you need to do to convert, let's look at a few examples where you would need to use a custom layer. So let's say you have an image classifier. This is how it looks in Xcode. So it will be high-level description of the model. If you look inside, it's very likely that it's a neural network. And it's very likely that it's a convolutional neural network. So it has a lot of layers, convolution, activation. Now it might happen that there's a new activation layer that comes up that Core ML does not support. And it's like at every machine learning conference, researchers are coming up with new layers all the time. So this is a very common scenario. Now if this happens, you only need to use a custom implementation of this new layer. And then you are good to go. So this is how the model will look like. The only difference is this dependency section at the bottom. Which would say that this model contains a description of this custom layer. Let's take a look at another example. Let's say we have a very simple digit classifier. Now I came across this research paper recently. It's called Spatial Transformer Network. And what it does is this. So it inserts a neural network after the digit that tries to localize the digit. And then it feeds it through a grid sampler layer which renders the digit again, but this time it has already focused on the digit. And then you pass it through your old classify method. Now we don't need to worry about the details here. But the point to note is that the portion in green is what Core ML supports. And the portion in red, which is this new grid sampler layer, is this new experimental layer that Core ML does not support. So I want to take an example of this particular model and show you how you would convert it using Core ML Tools. So let's go to demo. I hope it works on the first try. Back, oh yes. Okay. So let me close off these windows. Let me get, clear this. Clear the ML. Okay, so I'm also going to use Jupyter Notebook to show the demo. So I just navigate to the folder where I have my pre-trained network. So what you see here is that I have this spatial transformer dot [inaudible] file. This is a pre-trained Keras model. And if you are wondering if I did something special to get this model. Basically what I did was I could easily find an open source implementation of spatial transformer. I just exhibited that script in Keras, and I got this model. And along with this model, I also got this grid sampler layer Python script. Now this grid sampler layer that I'm talking about, it's also not supported on Keras natively. So the implementation that I got online used that Keras custom layer to implement the layer. So as you can see, the concept of customization is not unique to Core ML. In fact, it's very common in most machine learning frameworks. This is how people experiment in new layers. Okay, so so far, I just have a Keras model. And now I want to focus on how can I get a Core ML model? So I'll open -- there, let me launch a new Python notebook. So I'll start by importing this Keras model into my Python environment. Okay? So I import Keras, I import the, the custom layer that we have in Keras. And now I will load the model in Keras. Okay? So this is how you load model, Keras models. You give the part to the model and if there's a custom layer, you give a part to that. Okay. So we have the model now. Now let's convert this to Core ML. So I'm going to import Core ML Tools. Execute that. And now as I, as I showed you before that this is just a call to one function to convert it. So let me do that. That's my call. And I get an error as expected. Python likes to throw these huge error messages. But really what we're focused on is this last line. Let me -- So as we can see in this last line it says that hey, the layer or sampler is not supported. So now let's see what we need to do to get rid of that. Maybe I clear this all so that you can see. Okay. So now I change my converter call just a little bit so I have my Core ML model. And now I'm going to pass one additional argument. It's called custom conversion functions. And this will be a dictionary from the name of the layer to a function that I will define in a minute. And that I'm calling a good sampler. So let me take a step back and explain what is happening here. So as we know the way converter works is that it goes through each and every Keras layer. It will, if you look at the first layer. Then [inaudible] its parameters to Core ML. If you go to the second layer, then translate its parameters and so on. Now when it hits this custom layer, it doesn't know what to do. So this function that I'm passing here that convert this sampler is going to help my converter in doing that. And let me show you what this function looks like. So this is a function. There are a few lines of code, but all that it's doing is three things. First, it's giving a name of a class. So as we might have noticed, the implementation of the layer is not here. The implementation will come later in the app and it will be encapsulated in a class. And this is the name of the class that we'll later implement. So during conversion, we just need to specify this class name. That's it. And then there's the description which is a, which you should provide so that if anybody is, if somebody is looking at your model, they know what it has. And the third thing is basically translating any parameters that the Keras layer had to Core ML. For this particular layer, it has two parameters. The output height, and output weight. And I'm just translating it to Core ML. If your custom layer that does not have any parameters, then you load, then you do not need to do, do this. If your layer has lots of parameters, they can all go here, and they will all be encapsulated inside the Core ML model. So as you might have noticed that all I did here was very similar to how you would define a class, right? You give a class name. Maybe a description, maybe some parameters. So now let me execute this. And now we see that the converter went, conversion went fine. So let me this is behaving very weirdly for some reason. If you don't mind, I'm just going to delete this all. So let me visualize this model, and you can do that very simply using function in Core ML Tools. That's called visualize spec. And here you can see a visualization of the model. So as we can see, we have the [inaudible] and some layers there. And this is our custom layer. And if I click on this, I see the parameters that it has. So this is the name of the class that I gave. And this, and these are the parameters that I set. It's always a good idea to visualize your Core ML model before you drag and drop just to see if everything looks fine. Okay. This is the wrong notebook. Okay. And now I'll save out this model. And now let's take a look at this model. So let me close this. Okay. Let me actually let me navigate to the directory that I have. And here's my model. So if I click on it and see it in Xcode just to see how it looks. We can see that it has the custom description here. Okay. Let me go back to slides. So what we just saw was with a few simple lines, we could exhibit a convert a function to Core ML. And the process is pretty much the same if you are using the TensorFlow converter or the ONNX converter. So we have our model here on the left-hand side. The custom layer model with the parameters. Now when you drag and drop this model into Xcode, you will need to provide the implementation of the class. In a file say, for example, [inaudible]. And this is how it would look like. So you have your class, so you'll have the initializer function. So this would be just initializing any parameters that we had in the model. And then the main function in this class would be evaluate. This is where the actual implementation of whatever mathematical function the layer is supposed to perform will go here, in here. And then there's one more function called output shape or input shapes. This just specifies the size of the output area that the layer produces. This helps Core ML in allocating the buffer size at load time so that your app is more efficient at runtime. So we just saw how you would tackle a new layer in a neural network. There's a very similar concept to a custom layer, and it's called custom model. It has the same idea, but it's sort of more generic. So with a custom model, you can deal with any sort of network. It need not be a neural -- it need not be a neural network. And basically gives you just more flexibility overall. So let me summarize the session. We saw how much more rich is this ecosystem around Core ML Tools and that's great for you guys. Because now you have lot of choice to get Core ML models from. We saw how easy it was to quantize this, quantize Core ML model. And we saw that with a few lines of code, we could easily integrate a new custom layer in the model. You can find more information at our documentation page. And come to the labs and talk to us. Okay, thank you.  Hello, everyone, and welcome. My name is Betim Deva, and I'm an engineer on the Apple Music team, and I'm excited to share with you some updates that we have made to MusicKit. MusicKit was announced last year, and it enabled developers like yourselves bring customized music experiences within their apps. This year, we are announcing Apple Music on the web, which will allow you to bring the same kind of music experiences for your websites. We have a great web player ready for you to embed, and we are introducing MusicKit for the web, which will allow you to integrate Apple Music into your websites. We will talk more about Apple Music on the web in a little bit, but first, let's take a look at some exciting updates on MusicKit. It's been great to see how app developers have incorporated Apple Music into their apps. For instance, we all know and love making music videos with musical.ly. When you find new music in this app, you can easily add it to a playlist. Houdini makes joining Apple Music simple. It allows you to transfer your playlists to Apple Music. And Stationhead turns your playlists to radio stations. It allows you to broadcast your music and your voice to the world. And lastly, Ola is the biggest ride-hailing service in India. We have been partnering with them to bring the Apple Music experience within their fleet of cars. Since MusicKit was announced last year, we got a lot of feedback for providing a way to look up content using the International Standard Recording Code known as ISRC. I am happy to report that this has been added, and it will enable you to precisely match songs and music videos in the Apple Music catalog from virtually any other source. Another enhancement to MusicKit is the addition of iCloud Music Library APIs. Once the user has given authorization to your app, you can now provide them with ways to interact with their music library from within your app. These APIs enable your users to browse, search, and add content to their music library. You can also provide a way for your user to create playlists and add songs to an existing playlist, all from within your app. Last year, we gave you Apple Music APIs for our catalog. This year, we added the iCloud Music Library APIs. We think this is a great set of music APIs. If you have been using our existing iTunes Search API, we encourage you to migrate over to Apple Music APIs now. Now, I would like to invite my colleague DJ to the stage to talk about Apple Music on the web. Thank you. Thank you, Betim. This year, we're excited to be announcing MusicKit on the web. Last year, we talked all about the great ways for you to add Apple Music to your iOS apps. This year, we're bringing all of that same great functionality to the web. So let's talk about the components that make up MusicKit on the web. Last year, we added access to the entire Apple Music catalog with a REST API. This year, as Betim mentioned, we're launching new additions to that REST API that'll let you access the iCloud Music Library for the logged-in user. We're also launching the MusicKit JS library. Makes it really easy to use both of those REST APIs in the browser, and it gives your users the ability to play back full songs from the Apple Music catalog and from the iCloud Music Library right in their browser. Before we dig into MusicKit JS, I want to show you the Apple Music web player. A few weeks ago, we launched a new embeddable player. It works great both on the desktop and on mobile browsers. These embeddable players are a simple way for you to add full song playback from the Apple Music catalog right to your website. We have players for a single song, an album, or a whole playlist. You can find the embed code on the Apple Music preview pages or right within the embeddable player itself. All you have to do is copy and paste into your own website. These embeddable players are great for full song playback on third-party websites, and we've also enabled that functionality on the Apple Music preview pages. So the embeds are powered by MusicKit JS, which we're also releasing so you can build your own great music experiences. So let's talk about the functionality. First and most importantly, you'll be able to play back full songs from Apple Music in the browser. It works on all modern browsers without the need for any plug ins or external dependencies. You can use it on its own or with any JavaScript framework of your choice. So for playback, we need to authorize the user. We'll handle that for you to ensure that the user is an Apple Music subscriber. The JavaScript library is going to keep a queue of songs so you can play back a full album or a whole playlist. Once you have that queue of songs, you'll be able to control. You'll be able to play, pause, and skip to the previous or skip to the next track. And we're also going to keep track of the Now Playing metadata for the song that's currently playing or currently loading so you can render your UI off of it. So let's talk about how we would build a simple player for the web. We're going to include the JavaScript library. Then, you need to provide your developer token. You can generate this using your private key. We have a really great way to use declarative HTML markup. You can add some markup to your page. For example, buttons for sign in, sign out, or playback controls. We'll handle wiring up those elements to the appropriate functions in MusicKit. Then, we're going to add some play buttons. We'll use some content IDs or some URLs from the Apple Music catalog that'll let the users start playback. So let's take a look at the code for this. You're going to include the JavaScript library, and we're going to host this on our CDN for you. We're going to add some buttons for signing in and signing out. Some buttons to control playback. Here we have a play button, a pause button, and a next track button. We'll add some elements that we will render in metadata about what's currently playing. Here we can get the playlist name, the title, and the artist name, for example. And then, we have some elements that let you show the progress through the track that's currently playing -- the playback duration, the playback time, and the playback progress. So I'd like to bring up Jae Hess to the stage to show you how to use the MusicKit JS to build that simple player that we just talked about. Thanks, DJ. So we're going to get started here, and I have a basic HTML template on the right side in Xcode that's already linked to the MusicKit JS library. And on the left side, I have a Safari window open that's running a live reload server. This way, any changes I make to my markup will be reflected directly on the left-hand side. To get started with MusicKit JS, the first thing we need to do is configure it for our use. And with declarative markup, we have a very simple way to do that using meta tags. We can specify our application name as well as provide our developer token. Next, to get the functionality we want, we're going to put a button on the page for the user to click. And when this button is clicked, it's going to take a playlist from the Apple Music catalog, set a playback queue, and start playback in the browser. Here I'm feeding a content URL that I got from iTunes on the desktop for a specific playlist I'd like to embed. This playlist is the "Today at Apple Music" playlist. And so with that 1 simple line of code, we have full playback of the "Today at Apple" playlist in the browser. While this is great and it offers a really good solution, we probably want to add more controls and more functionality for the user to use. The first thing we want to do is allow the users of your application, which are subscribers of Apple Music, to log in and log out. And we'll do that by adding Authorize and Unauthorize buttons to the page. We use the Apple Music Authorize and Unauthorize attribute, and MusicKit JS manages the state of displaying those buttons for me. I've logged in previously, so you see that the state reflects, "Sign out of Apple Music." Next, we want to add some playback controls. And we can specify these using the Apple Music attribute, and we can specify things like skip to previous item, pause, play, skip to next item. We can add Now Playing information for the user to see the current playing item in the queue. And here I've specified the playlist artwork URL, and I can specify the height and width I would like that to render. And I also have access to any data attribute that's available on the media item. So I can specify things like playlist name, the title of the track, as well as the artist name. Underneath, I've specified that I would like the current playback time to be loaded into the HTML time element. So when the page reloaded, nothing's there for Now Playing. That's because we aren't playing anything, so let's fix that by adding buttons that would allow the user to queue up playlists. I'm going to paste in 16 playlists. And when the user-- Now, using the playback controls, I can pause playback. I can skip to the next item. I could even go back to that. Now, MusicKit JS offers great solutions with a declarative markup, but it also offers a full JavaScript API, so I'd like to bring DJ back up on stage to go over the JavaScript API with you. Thank you. Thank you, Jae. The declarative markup is a very straightforward way to add basic functionality. Let's talk about some more advanced usage. Here we're going to write some JavaScript to interact with MusicKit, and let's take a look at a couple examples. We can fetch metadata from the catalog by ID. We can search for content within the Apple Music catalog. We can browse the iCloud Music Library for the logged-in user, and search also works within a library scope. We can set and control that queue of songs directly. And we can react to playback events, and we drive our UI off of this. So we're going to look at some code. First, we're going to use the MusicKit getInstant method. MusicKit is a singleton because you can only play back a single song at a time, so we're going to assign this to that music variable, and we'll use that throughout the rest of these examples. You can look up a song by ID. The song method takes the ID, and then you need to supply a callback. And this is a promise. You'll see that promise model used throughout MusicKit. You then get a content object back, and this content object shows you the attributes of that song and any relationships -- for example, the artist and the album. As Betim talked about earlier, if you don't have a content ID, you can do the exact same lookup with a query filter for the ISRC. The objects that get returned are the same as the previous example. We have a batch API if you want to return a array of song objects with a single network call, and this also works for albums and playlists. We can let the user play back content from within their iCloud Music Library. So these APIs live under the library scope. You can get a list of songs, albums, and playlists. These APIs are paginated, so you can fetch just a subset [inaudible] manner, and you can unload more in as the user pages through or scrolls through your application. You could search the Apple Music catalog. Here we're going to search for just songs, and then the second example, we'll search for both songs and albums at the same time. You can also supply a limit to limit the number of results returned. You can do the same query, but scope it to within the user's iCloud Music Library. Here we're using the exact same search method with the same parameters, but we're doing it under the library scope. Next up, we have authorization. We're always going to handle authorization right before it's needed -- when playback's starting or when you're trying to access the iCloud Music Library for the current logged-in user. You could also trigger it yourself if you'd like. Perhaps you want to force that at the beginning when someone comes to your website. When the user presses a button, you can start playback. You do this using the setQueue method. Here we're supplying the idea of an album, and MusicKit will handle fetching the metadata and then setting that queue. If you have a content object that we saw previously, you can pass that in directly, and it will figure out what to do. setQueue returns a promise. When the queue is fully loaded, you can start playback. Here we're going to start playback automatically with music.play. Once those songs are playing, you might want to be able to control that. You can skip to the next or previous item. You can pause. You can present actions in your UI that will let a user quickly add those songs, albums, or playlists they're browsing to their own iCloud Music Library. We have an addToLibrary method here. You can do that same, you can do those same 3 calls in batch. Here we're adding those same 4 items as the top example, but we're going to do it in a single network call. So let's look at events next. MusicKit's going to fire events that you use to drive your UI from. We do this because MusicKit needs to own the audio element in order to do that full song playback. Here we're going to use addEventListener, which is going to be a very familiar pattern if you've done DOM scripting in the past. We're going to dive into just a couple of the events to give you an example. The mediaItem WillChange and the mediaItem DidChange events fire when the Now Playing item in the queue changes, when the first song you're playing transitions to the second song. You can use this to trigger an update to your UI that shows the song that's currently playing. When you actually begin playback, the playbackState changes events are fired. Here we have a will and a did change event again. For example, you can know when a song is loading, when it's playing, or when it's paused, and then you would update your UI appropriately. This event is going to give you the old state and then the new state that it's being transitioned to. As the song is playing, you'll get playback progress change events, and this event has the current playback progress as a percentage. You can use this to update a progress UI. And you're also going to listen for any media playback errors. The mediaPlaybackError event will fire. You can catch that and then present the appropriate error messaging to the user. So I'd like to bring up Jae back to the stage. He's going to take the player we just built, and we're going to show you how to use some of those JavaScript APIs to add search for the Apple Music catalog, search within the iCloud Music Library, and we're also going to use events to build a simple progress bar. Jae? Thanks, DJ. As DJ mentioned, we're going to take the previous example and we're going to extend it to add playlist search. Now, I've taken the, off stage, I've removed a lot of the stuff that we did from the previous example, but I've left some things in the page. I've also included an application JavaScript file that we'll be adding our JavaScript to in a moment, but I wanted to walk through quickly the HTML again. The major change we've done is we've added an input element for the user to search as well as 2 DOM containers that we can put those search results into. MusicKit JS doesn't offer an opinion on a library or a framework that you would be using with your application, so for this example, our CSS has been prebuilt off screen. We have some template helpers that we've built in a separate file as well. And we'll notice that our markup looks a little weird on the left-hand side. We got a couple of buttons that are magically floating, and the artwork is obviously empty. And this is because MusicKit JS assumes that if you're configuring it with the meta tags, that you want declarative markup enabled. When you don't provide that configuration, it assumes you're running JavaScript and you don't want declarative markup. But we provide a hook for you to re-enable that with JavaScript. We can configure MusicKit JS by listening to the MusicKit loaded event that's fired on the document. This lets us know that MusicKit JS is ready to be used and configured and playback can happen. We can configure our application name just like we did with the meta tags, but we now have an attribute that says declarativeMarkup that we can set to true, which lets MusicKit JS know that we're running in a mixed mode and we want those declarative markup features to be enabled. Next, I'm going to add a search handler. This is straight JavaScript. It's not specific to MusicKit JS, but I did want to walk through that when the user presses Enter on the search box is when we're going to fire our callback to do the search. And we can implement that by using the search method off of MusicKit's API property. This will perform the search against the Apple Music API, and we can specify any query parameters that the Apple Music API accepts with a configuration object. So we can specify we want to search, we want to limit the search to playlists and that we want our results to be limited to 8. We have our custom templating, but I want to highlight these 3 lines, which is where we tell our MusicKit instance to take the artwork that the user has clicked on, and we want to set the playback queue to that playlist ID. That returns a promise where we can call music.play, which will start playback for the user. And then, we have our custom rendering as well. I perform a search for "a list." We'll see 8 catalog results from the Apple Music catalog for A list playlists. It would be nice if the same search could apply to my personal iCloud library as well. And we can do that in almost the exact same manner, except the search method is off of the library property instead of the API property. This lets MusicKit JS know you're looking for the cloud library. Set playlists and set the same for 8. And then, the code looks exactly the same with setting the queue and starting playback. If I perform that same search, we'll see my iCloud Music Library results as well as the catalog library results. And I can click, and we get playback of the cloud library item as well. Looking at the Now Playing screen, or the Now Playing information at the bottom makes me realize that it's missing a progress indicator to let me know how far through the current playing track we are. And we can add something like that by simply listening for the playbackProgress DidChange event that MusicKit JS provides. Here we add an event listener to the MusicKit instance, listen for playbackProgress DidChange, and have a custom renderer to render progress when we get that event. So we can look for A list again. And this time, when I play that song, we see our custom progress bar rendering. Now, there's a lot more we could get into with MusicKit JS, but I'd like to bring DJ back up to kind of summarize what we've gone over today. You can now bring great music experiences to your websites. You can use the declarative markup to quickly add simple functionality or you can use the JavaScript APIs directly to build a more advanced application. We're looking forward to seeing what you can build with it. We have some great documentation available, and we have a lab coming up right after this session in Technology Lab 3. Happy to answer any questions you have. Thank you.  In this session we are going to tell you all about Quick Look. Welcome to previews from the ground up. My name is Raffael and I'm going to present together with Maxime, both software engineers at Apple. We are going to start with a brief overview of what Quick Look is. Then we're going to show you how to adopt the Quick Look Preview Controller in your application. And then we're going to explain how to provide custom Quick Look previews and custom thumbnails for your own file formats. Let's jump right into it, what's Quick Look? Quick Look is all about previewing documents, it allows you to present documents without the hassle. More precisely, Quick Look is an iOS framework that provides multiple core features to you as a developer. It allows you to preview documents of several commonly used file formats, to provide custom previews for files of your very own file format, and to provide thumbnails to the system for files of your own custom file format. Quick Look is in fact already used by many of Apple's first party applications. For example, in files to present your documents that you have in the Cloud. Or in Mail and Notes to preview your attachments. And in Messages to show your pictures and videos that you've sent. So who is this session for? Well if you want to know how to present documents in your application without having to implement all the common features the users are used to you're in the right session. You also might own a custom file format which iOS doesn't support natively and you want to make sure that your file format is properly handled by iOS with custom thumbnails and previews stay here if you want to learn more about this. And no matter if you're a newcomer to iOS or if you already have an app we're going to guide you step-by-step. By the way, if you haven't checked it out yet you should definitely take a look at last year's session, Building Great Document-based Apps in iOS in which we have introduced the UA document browser view controller and also have covered the basics of Quick Look preview and thumbnail extensions briefly. There's also going to be a document browser one-on-one session this year which we highly encourage you to attend if you're interested in file handling on iOS. Quick Look has been around for quite a while already. It's been introduced with the iOS 4 SDK. The main class you're interested in is to [inaudible] preview controller which is a view controller to preview documents. It uses the classic data source and delegate pattern that you already know from your UIKit and other frameworks. And presenting Quick Look with a beautiful [inaudible] transition couldn't be easier with the API that we give you. Why would you want to use Quick Look? Well if you have a similar use case to files, mail or the other apps we've shown you before Quick Look is the perfect choice for you. Quick Look comes with everything you need out of the box. It supports the commonly used file types and it provides that native user experience that your users are used to from other first party iOS apps. We made sure Quick Look has a create performance on all devices, so that all the animations and gestures are fluid. And you might be dealing with files from untrusted sources, but don't worry we got your back. Quick Look comes with a sophisticated security model that keeps your application safe. Let's take a look at the supported file types. As you can see all the common types are in here. Media files such as images, audio files and videos, documents such as PDFs and office files, but also zip archives. Previewing a zip archive lets you look into the contents of the archive and view the contained files individually and share them for example. Now if your custom file format is missing in this list don't worry. Last year we introduced a new extension point for implementing Quick Look preview extensions. Preview extensions allow you as a developer to create a preview for files of your very own custom file format. We will cover this in the second part of the session. And this year with iOS 12 we have also added support for the new AR file format USDC, so with just a few lines of code you get a fully immersive augmented reality experience in your application. Okay, now let's talk about user experience. For each of the file types shown before we made sure that Quick Look provides the best possible preview on your mobile devices. For example, when previewing images you're able to zoom into the images conveniently. You're also able to close Quick Look with a swipe down gesture. In PDF documents you get a sidebar with thumbnails for easier navigation. In spreadsheets you can switch through the individual pages of the document. And in media files we show a scrubber that allows you to go back and forth and so on. You get the idea. So when you choose Quick Look your users get all the gestures they are used to from other applications. So what are some common use cases for using Quick Look? For example, you might want to let the users zoom into photos or you want to be able to flick through a collection of photos left and right. You want to play audio or video files without having to deal with lower-level frameworks. Or you want to present for example a scrollable and zoomable user agreement PDF to the user. Use Quick Look for this. But Quick Look shouldn't be misused, here's when not to use Quick Look. Quick Look is all about previewing but doesn't provide editing features. So if you need to provide functionality such as image editing, PDF management or movie trimming you need to choose a different approach. Similarly, if you need more advanced playback control for your videos Quick Look is probably not the best choice, you can use AVPlayer instead. Quick Look is meant to be presented full screen. If you simply want to present an image, a video or a document embedded in your layout together with other views surrounding it you probably want to use UAImageView, AVPlayer, WKWebView or other kind of views instead. Also note that it's not supported to customize the view hierarchy of Quick Look. Please don't try to add views on top of the previews the navigational toolbar, this is simply not supported. So now that we've introduced Quick Look, let's take a look at how to actually make use of our APIs. If you want to present documents to the user as a first step you instantiate a new QLPreviewController which is basically a UIViewController. Next, in order to tell the preview controller which documents to preview, you need to assign a data source to it. The data source is an object that conforms to the QLPreviewController data source protocol. We'll take a look at this in a second. And as a third and last step, all you have to do is present the view controller. Let's take a closer look at how the data source works. Your data source has to conform to the QLPreviewController data source protocol. This is a protocol that requires two methods. First, you need to return the number of items that your preview controller should preview. It should provide more than one item the users will be able to swipe left and right to flick through the documents. Next, Quick Look then will ask you for one or more QLPreviewItems depending on what number you've previously returned. Okay what's a QLPreviewItem? It's a protocol that acts as an abstraction for your documents. For Quick Look a preview item is basically the URL, where to find the document on disk and optionally a title that Quick Look will show in the navigation bar. The class representing documents in your app needs to conform to the QLPreviewItem protocol. But by the way, NSURL already conforms to the QLPreviewItem protocol, so if you don't need custom titles you can simply return NSURL instances in your data source without implementing the protocol yourself. Let's take a look at an example. In this scenario our view controller that presents Quick Look acts as the data source of the QLPreviewController. Therefore, we let it conform to the QLPreviewController data source in an extension and we implement the two methods. Here we are using an area of file URLs that acts as the model. We return the number of URLs that we have in the number of preview items in controller method. And then each URL respectively referenced by the index in the previewController controller previewItemAt index method. Okay we've set up our preview controller and it knows what files to preview, so we are ready to actually present the view controller. There are two ways for doing that. You can either present the preview controller modally on top of the current context or you push the preview controller into a UINavigationController. Both ways work great and you need to decide what fits best in your situation. To present it modally you do what you do with any view controller you call present. Here's an example of what a modal presentation can look like. If you have a UINavigationController and prefer to push the preview controller instead you use pushViewController. This is what it looks like. The QLPreviewController class provides a few more methods that I would like to highlight. First, if you want to know if Quick Look is capable of previewing a certain document use the canPreview item class method, this will return true if the document matches any of the supported default file types or if there is a preview extension available that can handle the file. The data source we've already covered. Reload data reloads the list of documents your preview controller should present. If this list ever changes while the preview controller is currently presented use this method to trigger a reload. If you need to obtain the index of the currently previewed item in your list of preview items remember the user can swipe left and right to switch to a different one, use the current previewItemIndex variable. This is both a getter and a setter, so if you want Quick Look to start with a particular item when it's being presented set the index to the right value. Last but not least, preview controllers also have a delegate. Let's take a look at the QLPreviewController delegate protocol now. If you assign a delegate to your preview controller you get more ways to control how Quick Look behaves. It also lets you react to Quick Look presentation events. Note that all of the methods in the QLPreviewController delegate are optional. You don't have to implement them and in fact you don't even have to assign a delegate at all. The protocol hosts a bunch of methods you can implement. The first two methods lets you react to the event that Quick Look is about to be dismissed by the user and also that the dismissing is now complete. You might want to use these methods to update the view controller that is presenting Quick Look right now. Next, the preview controller should open URL for item method allows you to prevent Quick Look to follow a link contained in previewed documents. With this method you can for example prevent the user from leaving your app when tapping a URL or a phone number contained in a PDF. And last but not least, as promised before the QLPreviewController delegate protocol also provides API to get a smooth zoom animation when Quick Look is presented and dismissed. With these methods a thumbnail that is on screen in your user interface can transition into a full Quick Look preview when tapping it. Imagine an email attachment or a photo inside a conversation view which you can tap to show it full screen with Quick Look. We made it super easy for you to get this cross fade zoom animation in your application. If you implement one of the two approaches in your delegate Quick Look will ask for the information that is needed for performing the zoom. This is basically the rectangular area the animation should start from when presenting or end in when dismissing Quick Look. Either you provide the frame and the image of the thumbnail or and this is the preferred way of doing it, you use our mode and method in which you simply return the thumbnail as a view. With this approach you don't have to worry about coordinates Quick Look will do all the heavy lifting for you. And this is what it can look like. Note how the thumbnail transitions into the full-screen Quick Look preview. All right now it's time for a demo in which we would like to show you what we've just covered. Okay, here's an iOS project and instead of talking about it let's just press the play button to see what we've already prepared and what we are dealing with here. We are calling this Wildlife Explorer and it's a very simple app. It basically displays a grid of photos and each of these photos that we see here acts as a thumbnail for what's behind it once you tap it. For example, tapping the elephant should show a large photo of the elephant in full screen and tapping the giraffe should preview an entire PDF about all sorts of giraffes. Right now we can't do anything else than just looking at it, tapping won't do anything yet so let's fix this. Now this application is fairly simply structured. For displaying the grid of photos we use a CollectionViewController. When initializing it we obtain a list of URLs of documents that are bundled with the application like the elephant we've just seen. We use that list to initialize what we call a document data source. This object serves as the model for the collection view in the implementation of the UICollectionViewDataSource protocol. Here we return the number of documents we have just gathered and then create a cell for each of the documents with a thumbnail as the image of the cell respectively. And that's pretty much the core of the application so far. Now let's add what we've talked about before. When tapping a cell we would like to show Quick Look with the right document. We've already made sure that we have a method that is called when the user has just tapped a cell. To show Quick Look we create a new QLPreviewController in here and configure it. We create a new controller, we assign the document data source to it as the data source and then make sure Quick Look shows the correct preview when it's presented by setting the right CurrentPreviewItemIndex. Great, now Xcode is already complaining about our new code, it doesn't know QLPreviewController yet. We need to import Quick Look first and therefore we return to the top of the file and import Quick Look. Done. The next thing Xcode complains about is the data source. The document data source that we assigned to the new preview controller doesn't seem to conform to the QLPreviewController data source protocol yet which we have talked about earlier. To fix this let's switch to the implementation of the document data source. The document data source is a very simple kind of object, it simply owns an area of URLs so far. We need to add the QLPreviewController data source protocol to the list of protocols that this class implements. Next, we implement the missing methods. As you can see the way we use the document data source here is very similar to the way we use it for the collection view. In the numberOfPreviewItems method we return the number of URLs that we have gathered before from our documents folder. And then Quick Look asks for the individual QLPreviewItems and we return the right URL referenced by the index. Great, the data source has been properly set up so we can go ahead and actually present the preview controller. Therefore, we return to the didTapCollectionCell method and add the missing method call. Let's check out how this looks like. All right we are back in grid and now when tapping a cell Quick Look appears displaying the right document in full screen. Note how easy it is to present images and even PDFs in full screen with all of the gestures that you are used to. We can zoom into the document with two fingers, we can select text, and even copy it. And once we are done we can use a pinch gesture to dismiss Quick Look again. So what we are using here to present Quick Look is a modal presentation style. As I've explained earlier, we can also present Quick Look with a push animation. Therefore, we simply tell the application's navigation controller to present the preview controller by pushing it on top of the current one. Let's take a look. As you can see, this is a different style of presenting Quick Look and it works just as fine as presenting it modally. Okay Quick Look works and now the last thing I would like to show you is how to use the Quick Look delegate in order to get a beautiful zoom animation when tapping a thumbnail in our grid. First, we switch back to the modal presentation style which is needed for the zoom transition to work. Then we need to assign a delegate to the preview controller and in this case, we assign self. However, self doesn't conform to the QLPreviewController delegate yet, so let's add it to the list of protocols. And now as the last step we implement the magic method which tells Quick Look which view to use as the source for the zoom animation when it is about to be presented. As you can see, we are using the image view of the most recently tapped cell which we keep a reference of and return it to Quick Look. That's literally all we have to do to make the zoom animation work. Quick Look will call this method when it's presented and when it's dismissed. Let's try it out. Now when we tap a thumbnail notice how the thumbnail is animating to the full-screen preview. Also observe no matter how we dismiss the preview it beautifully transitions back to the right origin. Great, that's it for the demo, back to the slides. And now I would like to hand it over to Maxime who is going to tell you all about previews and thumbnail extensions. Thank you Raffael. So as you have seen, Quick Look makes it super easy to add a powerful previewing feature to your app. But there is more to talk about. The first thing I want to show you is how to extend Quick Look's previewing capabilities by providing a preview for your own file format. We have listed the file types that Quick Look supports natively before. However, since iOS 11 this list can be extended by bundling a preview extension with your iOS app which allows you as a developer to provide previews for your custom file formats. You should provide a preview if you're in a custom fil format and you'd like this one to be previewed by Quick Look just like any other native type. File types that are meant to be shared are usually good types to create an extension for, especially if you want your users to preview the content easily after receiving a file in an app like Mail, Messages or in a Note for instance. And then everything works together and your application that is using a QLPreviewController with a file that is not natively supported will benefit from your preview extension. Here is an example of what you can achieve thanks to a preview extension. We have built a sample app that can be downloaded from Apple's website that allows one to create customized and interact with Particles. Since we wanted to share our cool Particles to other users we created a new file format, the particles file format. Each file represents a particle system and you can configure all sorts of [inaudible] systems. Now let's say you share a particles file with your friends. When previewing them in messages or as an email attachment for instance [inaudible] get to see is a blank screen, that is certainly not what we want. But by implementing your preview extension for our particles file format it will replace that blank screen with a beautifully rendered three-dimensional interactive particle system. How good is that? I am sure you now want to learn how you can make your own preview extension for your file formats. First, you will need to add a new target in your existing application project. To do so you will have to select the Quick Look Preview Extension template in Xcode. After that several files will be generated for you by Xcode. The PreviewViewController class with a basic implementation. This is where you will have to add code for your preview. The storyboard that has the PreviewViewController as its entry point. And an Info.plist that's the first thing you will want to edit. Let's take a closer look at it. To get started the attribute we are the most interested in is named QLSupportedContentTypes. This is the one that is highlighted on the screen shot. This is an array in which you list all the file types for which your extension can provide previews. Please note that you can only provide previews for UTIs you own and export. When Quick Look can't natively preview a file type it will make use of the content types added to this array and select an extension that can handle previewing it. So make sure to list all file types [inaudible] extension support. Let's see how we achieve this with our Particles app. As you can see Particles declares and exports a new particles file type. Its identifier is [inaudible] example.applesample code.particles.particles. Since we wanted our extension to support particles previews we added this identifier to the QLSupportedContentTypes of its Info.plist. By the way, if you're interested in UTIs you should check out the Documents Manager session of this year. When a new preview has to be generated for a file type [inaudible] Quick Look will create a new PreviewViewController instance and calls it preparePreviewOfFile at URL completionHandler method. This method is part of the QLPreviewingController protocol and is mandatory to provide previews for a file. Quick Look will display [inaudible] while waiting for your extension to be ready to display the preview. All you have to take care of is loading the contents of your preview and calling the completion handler as soon as your extension is ready. All right that's it for the preview extensions, let's go one step further. Similarly to previews you can also provide thumbnails for files of your own file format. Let's talk about Quick Look thumbnail extensions. A thumbnail is an image of a limited size that represents the contents of a file. There is throughout iOS and macOS to allow users to identify files officially without having to open them to the file contents. For instance you can see on the screenshot how iOS makes uses of them in the file tap. iOS can generate thumbnails for different file types, images, videos, PDFs, text files, and USDZ files. Now you may have noticed the blank icons on the left. This is because these are particles files and iOS doesn't know how to generate thumbnails for this file natively. But don't worry we are going to show you how to create a thumbnail extension that you can use to improve this situation for your custom file types easily. Look at how great these thumbnails look for particles files thanks to the thumbnail extension we have added to our app. Thumbnails provided by this extension appear in the Files app, as well as in any UIDocumentBrowswer ViewController-based app. They also appear in the Quick Look list, that you can see when tapping the list button when previewing multiple files in Quick Look. If you have custom file types that the user can share and interact with you will likely want to provide a certain extension with your app. Let's now see how you can do so. Creating a thumbnail extension is as just creating a preview extension. To do so add a new target in your existing application project and select the Thumbnail Extension template. Xcode will generate for you two files, the ThumbnailProvider class with a basic implementation. This is where you will have to add the code that takes care of generating thumbnails. And an Info.plist this is just like for the preview extension, the first thing you will have to edit after creating your extension. As in the Info.plist of the preview extensions the Info.plist of the thumbnail extensions have a QLSupportedContentTypes array that needs to be filled with the content types that your extension supports. So make sure to include in QLSupportedContentTypes all the content types for which your extension can generate thumbnails. After setting up your extension you will be able to start implementing your QLThumbnailProvider subclass. You have two ways to provide a thumbnail for a file. You can [inaudible] use it CoreGraphics or UIKit [inaudible] techniques or you can return an image file URL. You will have to override the provideThumbnail for request handler method in your QLThumbnailProvider subclass. We extensively covered this part of the Quick Look API in your session for WWDC 2017, Building Great Document-Based Apps in iOS 11. So if you'd like to provide thumbnails for your custom file types I highly encourage you to check it out. In a nutshell, you will need to make use of the parameters contained in the QLFileThumbnailRequest of the method. The URL of the file, the maximum and minimum sizes of the thumbnail, and its scale. For each thumbnail request the API expects you to create a QLThumbnailReply object. This object will have to take care of generating the thumbnail. You will have to provide it to Quick Look through the completion handler of the method. All right let's see all of this in action it's demo time. Let's start by taking a look at how things look like after installing the Particles app without any Quick Look extension. All right we don't have any file yet, let's create a new one by pressing the + button. What you can see here is the particle editor. Let's create a file particle system, we change the color a bit, and save the document. Well as you can see we don't get to see the beautiful particle system we just designed. [Inaudible] of that by adding a thumbnail extension to our application. Now let's save the file to a [inaudible] and see how the preview looks like in Quick Look. All right let's open the file format. All we see is a blank screen, we can do better, let's improve this too. Let's switch to Xcode and implement a preview and a thumbnail extension. We already have an existing project containing the extension setup. We are going to use these as a starting point and add the two extensions. As mentioned previously we need the Info.plist of our extensions to be configured so that the system knows we are able to provide previews and thumbnails for particles files. So Particles app defines its own file format in the exported UTIs section of the application target. We need to configure the extensions to use that UTI in the Info.plist. As you can see the QLSupportedContentTypes of the Info.plist of the preview extension does contain the identifier of the UTI of the particles file format. And this is also the case of the Info.plist of the thumbnail extension. So the only thing left to do is to actually implement these extensions. Let's start with the preview extension. As mentioned in the presentation we need to implement this method, preparePreviewOfFile at URL completionHandler. Our main app Particles already has a view controller class which [inaudible] the particle system on screen. We are going to reduce [inaudible] controller in our extension. We are going to create a helper method that will take care of loading such a view controller and adding it to the view hierarchy. As you can see present particleViewController for a document simply creates a particleViewController and passes it to the document it received so that the particleViewController can render the particles that the document represents. We still need to call this method from preparePreviewOfFile at URL, so let's do it. We first create a document that is a subclass of [inaudible] document. We open it and once it is open and usable we call our helper method that will displace a particleViewController. Finally, we may not forget to close the completion handler to notify Quick Look that our view controller is loaded and ready to appear on screen. Our preview extension is now ready. Let's implement the thumbnail extension. All we have to do here is to implement the provideThumbnail for request handler method. So request provides several properties that you need to consider when rendering a thumbnail. In this example we will make use of the file URL as the maximum size of the thumbnail. Note that there are also a scale and minimum size properties, but in the case of our particle system we don't use them. We then create a drawing block that we are going to provide later [inaudible] completion block. In this case, we call the helper method that will take care of drawing the thumbnail, drawThumbnail for fileURL, contextSize. We are going to implement this helper method in a second. You may have noticed that our drawing block returns a Boolean, this flag indicates if the thumbnail was successfully drawn or not once this block will be used to generate the thumbnail. Let's now see how we draw our particle thumbnails. To draw the thumbnails we make use of the URL of the file and of the size of the context we will draw into. In this case, we all know the maximum size of the thumbnail request since we can generate particle thumbnails of any size. Our method first creates a document that represents the file and attempts to open it. If this fails it returns false to indicate that it could not generate a thumbnail. After opening the file it then creates a particleViewController that will be used to render the particles file. Then it takes a snapshot of the particleViewController and uses it to draw the thumbnail. We close the document before returning true to indicate that we successfully have generated a thumbnail. Now that we have [inaudible] to generate thumbnails we need to create our [inaudible] reply. We created out of the context size, which in this case is the maximum size and the drawing block. Also we have our reply object we provided through the completion handler [inaudible] parameter, which in this case is new since we always add them to draw thumbnails for particles documents. If [inaudible] while drawing it the thumbnail block success value will indicate it, so the thumbnail can be discounted later. We are finally all set, let's run this code and see how this looks like. Wow. We now see a beautiful thumbnail for our fire particles file. This looks so much better than before. Now let's check our preview extension. Okay. So we are in Note and Quick Look is still showing the blank screen from the previous time we tried to previewing your file. Let's dismiss Quick Look and present it again. We now see our great particles preview, note how easy it was to make our custom file format [inaudible] in iOS. And that's it for the demo. So what have we learned today? First, we have shown you how easy it is to add powerful previewing capabilities to your application by using the QLPreviewController of Quick Look. Then we have taken a look at Quick Look's extension points. Preview extension allows you to have your custom files preview by Quick Look just like any native file type. While the thumbnail extension is used to provide thumbnails of your own file types to iOS when needed. Together these extensions make your file formats first-class citizens in iOS. And that's it for this Quick Look session. If you would like to have more information you can check out Apple's website. Thank you for your attention. Good morning, welcome to What's New in LLVM. I'm Jim Grosbach, your friendly neighborhood pointy hair boss. I'm here to tell you a little bit of background about the LLVM project before we dive into the deep technical details of all the exciting new things that we have for you today. Start off, LLVM is more than just the compiler, it is the background for the Clang compiler, for the C family of languages that we all use every day, but it also powers the Static Analyzer, the sanitizers, the LLDB debugger, and is the optimization code generation framework underneath the GPU shader compilers for all of Apple's mobile platforms. In addition to this, it also powers one additional little project that you may have heard of from time to time called Swift. And like Swift LLVM is an open source project. We all operate under the watchful eye of our LLVM wyvern here, he's normally a very friendly fellow, though I do have to caution you he gets a little bit cranky if you call him a dragon so don't do that. As an open source project LLVM is a partnership, we work with industry partners, academics, researchers and hobbyists from all over the world and in different parts of the industry and many more all over the place. This is really fantastic, we work together to build the greatest tools that we possibly can to move technology forward. And if you ever have a compiler itch that you would like to scratch we would like to invite you to participate with us and go to the LLVM website here at llvm.org or you can come talk to us later today later today in the LLVM labs and many of our compiler engineers from Apple will be there and I'm sure will be more than happy to talk your ear off about anything and everything compiler related you've ever wanted to know. So for today we have a great set of things that we want to share with you. We have updates on automated reference counting that makes it even easier for the compiler to help you with your memory management. We have new diagnostics in Xcode 10 and new checks in the Static Analyzer to help catch bugs in your project sooner at build time to improve the quality of your code. We have compiler features that improve security, both of Apple's platforms and of your apps. And new features to allow you to take advantage of all of the really great new things on the hardware architectures to get the performance that we all want out of our platforms and architectures. So with that I would like to invite my colleague Alex up to talk about ARC. Alex. Thank you, Jim. Automatic reference counting has greatly simplified Objective-C program since we introduced it a couple of years ago. A couple of restrictions made it harder to migrate from the old manual retain release mode over to ARC. I'm happy to say that we've now lifted one such restriction. Xcode 10 has support for ARC object pointer fields in C structures. Let's take a look at an example let's say we'd like to write a food ordering app and we'd like to create a data structure which represents a menu item. In Xcode 9 and earlier it would have been impossible for us to actually use a C structure with ARC object pointer fields, so we would have had to use a C, an Objective-C class here. Xcode 10 now allows us to actually create a C structure that has ARC object pointer fields. Let's keep going and keep writing our food ordering app. Let's create a function that orders free food for us. In the function let's create a variable item of type menu item with a price of zero. Then let's pass this item into another function that actually orders the food for us. When the item is created the compiler has to synthesize code which retains the ARC object pointer fields in the item. The code comments on the slide demonstrate the code that the compiler synthesizes. This code ensures the name and the price of the item are not released prematurely before the item is actually used. Now at the end of the function item goes out of scope and is deallocated from the stack so the compiler has to synthesize code which releases the ARC object pointer fields in the item. This ensures that the name and the price are not leaked when the item is released. Previously it was possible to use Objective-C object pointer fields when using manual retained release mode, but you had to write the retains and releases yourself. With ARC the compiler hides all of this complexity for you and synthesizes code that retains and releases the fields. So the compiler is really your friend here and it does the correct job of managing memory for variables on the stack and also for fields in other structures, and also instance variables inside Objective-C classes. But there is one place we have to put in a little bit of extra work to support structures with ARC object pointer fields and that place is heap. Let's go back to our structure, let's say you would like to allocate an array of menu items on the heap. Now if this was an Objective-C interface we could have used an NSArray here, but it's not so let's use malloc and free. Now this code actually has two issues. First issue, the memory is not zero initialized when it's allocated, which means that their pointers will be invalid which will cause undesired runtime behavior for your program at runtime. The second issue is that the ARC object pointer fields are not cleared before the memory is deallocated which will cause runtime memory leaks in your program. Now to fix the first issue you can replace the call to malloc with a call to calloc. This will ensure that your memory is zero initialized, which will remove all of those nasty unexpected runtime issues. To fix the second issue you can write a loop before it's allocated in your memory to clear out all of the ARC object pointer fields in your items. This will ensure that the name and the price in the items are not leaked when the items are freed. Now this is an exciting new feature and if any of you were put off from migrates over to ARC because of lack of features like that I hope that support from ARC object pointer fields in Xcode 10 will help you reconsider your choice. Now let's take a look at Objective-C pointers and structures in general and see where and how can the structures be used in different language modes in Xcode 10. So in Xcode 10 you can use structures that have Objective-C object pointer fields across different language modes. For example, you can use the same structure in C Objective-C or even Objective-C++. And it will work correctly even when you're compiling your code in ARC or in the manual retain release mode. In Xcode 10 we actually unified the Objective-C++ ABI between calls to functions that took in or returned structures that had ARC object pointer fields in Objective-C++. And this was done through an ABI change in Xcode 10 and ABI change affects functions in Objective C++ which return or take in a structure by value that has ARC object pointer fields and no special member functions like constructors or destructors. Now if you are not sure what this means for you or whether your code is affected by this ABI change please take a look at Xcode's release notes where we describe in more details the effects and the impact of this ABI change. Now there is one caveat when it comes to the ARC object pointer fields and C structures, they're not supported in Swift. So if you try to use a structure that has ARC object pointer fields from Swift you will just get a compilation error because the structure will not be found. In addition to new features like support for ARC object pointer fields Xcode 10 comes with a lot of new compiler diagnostics. We actually have over a hundred new warnings in Xcode 10 and today I'd like to talk about two of them. The first warning might be of interest to those of you who have mixed Swift and Objective-C code. So as you know Swift code can be imported into Objective-C and Xcode allows you to do that by generating a header file that describes the Swift interface using Objective-C declarations. And you can import this header file into your own Objective-C code to get access to the underlying Swift declarations. Now let's get more specific and let's talk about how Swift's closure parameters are important to Objective-C. So right now on the screen you see an example of a Swift protocol called Executor. This protocol defines a function member called performOperation which takes in a closure parameter called handler. Now in Swift closure parameters are non-escaping by default, which means that they should not be retained or called after the function returns. Now it can be easy for the program and to forget that this contract exists when conforming to the executive protocol in Objective-C. For example, as you see right now on the slide we have a dispatch Executor interface in Objective-C and conforms to the Executor protocol, so it provides the performOperation method which takes in the handler block parameter that corresponds to Swift's handler closure parameter. But just by looking at the Objective-C code we have no way of knowing whether the handler parameter can escape or not. Xcode 10 now provides a warning that helps us to remember that this parameter is actually non-escaping. To fix this this warning you can annotate your block parameter with the NS NOESCAPE annotation. You should also annotate the implementation of the method or the parameter in the implementation of the method with NS NOESCAPE annotation. Now the NS NOESCAPE annotation is simply a reminder for you the programmer to ensure that you don't store or call the handler block after they perform operation method returns. So it's there for you to help you remember that there is this contract that exists between your Swift and Objective-C code. Now the second warning might be of interest to those of you who work with more low-level code and who care about the way that structures are laid out in memory. Let's take a look at one structure. So in C structures have to follow strict layout and alignment rules. In this particular structure that you see right now on the slide the compiler has to insert a 2-byte pattern between the second and the third field of the structure. Sometimes you might want to relax these rules and the compiler provides a pragma pack directive that you can use to control the layout and the alignment of your structures. Now in this example we use the pragma pack push, 1 directive to remove this fixated layout and to ensure that our structure is tightly packed. This can be useful when serializing your structures or when transferring your structures over the network. Now pragma pack is typically used with a push and a pop directive, but it can be easy for the programmer to forget to insert the pop into the code. Xcode 10 will now warn about code that doesn't have a corresponding pragma pack pop directive and to point you to the location of the push. So to fix this warning you should take a look at the location of your push directive and insert the pop directive at the corresponding location in your code. So in our case we can insert the pop directly after the packed structure. Once we do that the new layout rules will apply only to the packed structure so they won't affect any other structures in our program. These two new warnings that I mentioned are enabled by default in Xcode 10 and they are there to help you write more correct and more robust code. And to talk more about more correct and more robust code I'd like to invite George up on stage who will talk about the new static analyzing improvements in Xcode 10. George. Thanks Alex, so I would like to tell you about some of the improvements we have done for Xcode 10 for the Clang Static Analyzer. So the Clang Static Analyzer is a great tool for finding HK hard-to-reproduce bugs in your program. And not only the Static Analyzer finds the bug for you it also displays the visualization in Xcode of the paths which [inaudible] the bug. So here nil is added to NSMutableArray which can cause a crash later on. And Static Analyzer shows you the path for this crash so you can see how the application can be fixed. And I would like to tell you about three of the new improvements we have done. Firstly, we have a new check for detecting Grand Central Dispatch anti-patterning, which can cause poor performance and hangs of your replication. Secondly, we have a new check for detecting a misuse of autoreleasing variables inside autorelease pools which can cause crashes with [inaudible]. And finally, we have improved performance and visualizations for the Clang Static Analyzer. So let's start with a new check for detecting Grand Central Dispatch anti-pattern. So many APIs on our platforms are asynchronous, but sometimes developers would like to use them in a synchronous way for one reason or another. Maybe because their code is already running on the background queue or maybe because the function cannot proceed at all until the required value is available. And the tempting solution there is to use a semaphore to ensure synchronization. So that's what's happening in this example, so here there is an SXPC object self.connection and we use its property remoteObjectProxy to call, to get the current task name asynchronously from a different process. And then we wait on a semaphore which is signal to inside the callback. And that helps to ensure that by the time the function returns the task name is available. So this approach works but has known performance implications. So the main problem here is when you wait using a semaphore on some asynchronous process you might be waiting on a queue with a much lower priority than yours costing prior inversion which [inaudible] performance and cause hangs. And moreover using a semaphore in such a way also spawns useless threads which further degrades the performance. And to help you address this issue now Static Analyzer warns on such cases helping to see where the issue occurs. Now let's see how the issue can be fixed. In the best-case scenario there is a synchronous API available which can be used in stat. So for an SXPC connection there is an [inaudible] API synchronousRemoteObjectProxy which when used in start eliminates the need for the semaphore and runs much foster. Alternatively, if no such synchronous API is available you could restructure your application to use continuations in stat and just calls the required function inside the callback. So this check is not enabled by default but we encourage you to enable it in build settings in order to make sure no such problem securing your application and it runs as fast as possible. Now let's talk about the second check for detecting the autoreleasing variables outliving the lifetime of the autorelease pool. So the autoreleasing qualifier specifies that the value has to be released once the control exits the autorelease pool. So here we have an example where we create an error variable inside the autorelease pool and once the control is outside of the autorelease pool the variable is released and subsequently destroyed. And autoreleasing pools are a useful feature of Objective-C to help contain the big memory footprint of your applications and to ensure that thumbprints are destroyed where necessary. However, it can cause unexpected crashes and they're even more unexpected because you don't even need to write the word autoreleasing in your application to have those crashes. So for instance, there is a validation function here and it takes in out parameter NSError. And out parameters are actually autoreleasing in Objective-C under ARC by default. So when we write to this out parameter inside the autorelease pool and then the function exits the error value is actually released. And then if the caller tries to read the value of this error variable they might crash with use-after-free. That pattern is already hard to detect, but it actually gets even worse when you don't even control the part of the application which has the autorelease pool. So here is a similar function which [inaudible] and out parameter error and then it calls an enumerateObjectsUsingBlock which is a popular foundation API which calls a block on every element of a collection. However enumerateObjectsUsingBlock actually calls [inaudible] given block inside the autorelease pool of return. So a similar problem occurs here that when we create an error value inside the block and write it to the out parameter it will actually get released by the time the control reaches out of enumerateObjectsUsingBlock. And then when the caller tries to read it they also can crash with the use-after-free. And previously we have introduced the compiler warning which warns when an implicitly autoreleasing out parameter is captured in the block. And the compiler warning suggested to make such parameters explicitly autoreleasing. But we have noticed that such issue kept occurring, so in Xcode 10 we introduced a more powerful Clang Static Analyzer warning which knows which APIs call the provided block inside the autorelease pool and warns about such cases. So now let's see how this issue can be fixed. And the simplest fix here is just to introduce a strong local variable and then when you're inside the block write a value into the strong variable in stat. And then only copy to the out parameter once the control is outside of the block and you know it's not inside the autorelease pool and it's safe to write into the autoreleasing variable. And finally, we also have improved performance and visualizations of the Clang Static Analyzer. So in Xcode 10 we have improved the analyzer to explore your program in a more efficient way so now it finds up to 15% more bugs during the same analysis time. And not only it finds more bugs the bug report it now generates tend to be smaller and more understandable. And what I mean by that is sometimes in Xcode 10 you would get examples which have a lot of steps and a lot of arrows and which would be somewhat hard to comprehend. And in many of those examples in your version of Xcode we give you a much smaller error path which is much easier to see and you can see the issue much faster. So in order to use Static Analyzer on your projects you can use Product, Analyze or you can even enable Analyze During Build to make sure no analyzer issue gets unnoticed. So I encourage you to use the Static Analyzer, it's a great tool to find your bugs before users do. And now my colleague Ahmed will talk about low-level improvements. Thank you George. So as Alex and George told you, we have lots of warnings and Static Analyzer checks in the compiler, but you also have the sanitizer and all of these tools help you find lots of bugs, including security bugs. So I'm sure you all have lots of tests and use all these great tools to find all the bugs in these tests. But for some of the most egregious security bugs we want to make sure that they don't happen in release builds if somehow they snuck past all the testing. So for those we have mitigations in the code generator that are always emitted even in release builds. So I'm Ahmed, I work on the code generator and today I'm going to tell you about a new mitigation in Xcode 10. So to see how that works we need to understand how the stack works. So here I have a simple C function called dlog and I use it to print a string that I'm passed into a dlog bug. So in this case it's called with a string hello. And the way this works is we need to allocate some memory to keep track of this call. So we allocate that into a region called the stack. So the stack grows down towards the null pointer or address zero. So when we do our dlog hello call this allocates what's called the stack frame and the stack frame contains things like the return address so that we know to go back to main. But it also contains other things like parameters and local variables. So for instance if I have a log file [inaudible] local variable that lives in the stack frame. So now if I try to make another function call to this dlog file function that in turn will allocate its own stack frame. And when it's done it's going to deallocate the stack frame and return back to the caller. So now let's look at this stack frame in more details. So let's say I change my function to have a local buffer, so it's a 4 bytes character array. And I'm trying to prepare my debug string by first doing a strcpy of the string that I'm passed into that buffer. So this does the obvious copy by [inaudible], so it does H-E-L-L. But then there's a problem at this point we already wrote 4 bytes and that we already exhausted all 4 bytes available in our buffer. So if we keep going which is what strcpy will do then we're going to override the return address and this is a big security problem. So if an attacker controls the string that I'm copying which is not that hard, then it can control the return address. If it can control the return address then they control basically what the program does next, so it's a big security problem. So if you had a test that caught this and you ran the address sanitizer, then you will have had an easy way to fix this. And really what I should have done here is strncpy that knows about the size or even better use a higher-level API like NSString or [inaudible] string. But still sometimes these bugs can survive into release builds and we avoid these by using what's called the Stack Protector. So the Stack Protector changes the layout of the stack frame to add a new field the canary so that when we do our write we have a little bit of code right before the return of the function that checks whether the canary is still valid. So if we keep writing in strcpy we're going to override the canary first and then we're going to check the canary first before returning and that's going to abort. So we turned ad potentially exploitable security vulnerability into a reliable crash and that's not good for an attacker. So this is what's called the Stack Protector. It defects certain kinds of stack buffer overflows, which is the attack that we just saw. And it's already enabled by default in many versions of Xcode. So next I'm going to talk about a trickier case where we introduced a new mitigation. So let's say I took my function, again my dlog function and I changed the buffer so that now it's a variable length array. And the length comes from a parameter called len. So let's say len in a specific call is something big like 15,000, so now the stack frame has to be at least 15,000 bytes long. But memory is not all immediately available, so memory is split into pages and the stack grows only when necessary. So for instance, when we try to access by 10,000 of the buffer that's in the next page of the stack that's not yet available so it's going to do a page fault in the CPU that talks to the opening system, the operating system sees that we have the right to grow the stack, and it grows it and we can continue writing. So this all happens under the hood. But say an attacker controls the length and it makes it huge, big enough that it spans many pages. So now there's a new problem, the memory is not infinite so if we keep allocating in this stack eventually we'll hit another region of memory that's already allocated and usually that's the heap. And when we do that then we're going to clash with the heap, with whatever is already used in there, so that's usually things like malloc and new. So if we try to see what would happen with our strcpy example then we will try to write the bytes one by one. So we do H-E-L, etcetera. And from the standpoint of the CPU, the code that's generated and the operating system this is all fine because we're just writing into a page that's already available and allocated. But it really isn't because this is part of the heap, this is not part of our local stack allocated array. So when we do our writes we're actually overriding some completely unrelated piece of information like I don't know a Boolean that checks whether we should check a password. So this is another important security flaw. So this is something that we mitigated with a new feature and the future works by emitting some new codes at the entry of the function that checks whether it's okay to have the stack frame. So it asks the operating system above the maximum size of the stack and if you try to make an allocation that's bigger than that then it actually aborts. And again, this turns a potentially exploitable security bug into a reliable crash and that's no good for an attacker. So this is Stack Checking, it detects something that you might have heard of called Stack Clash and it's enabled by default in Xcode 10. So next I want to talk about a new set of features we added in Xcode 10 and that's support for new extension, sect extensions. So as you all know we have lots of great Apple devices and one of the great things about Xcode is that with just a few build settings you can target your code for each of these devices. And so under the hood in macOS, iOS, watchOS, etcetera we tweak every OS so that it uses everything that's available on a specific piece of hardware. So it guarantees maximum performance no matter where we run. And so if you an app with extremely high-performance requirements that's something that you might want to do as well. So we have three features to talk about that are available in the iMac Pro and the iPhone 8 Plus and X. And let's start with the iMac Pro. So the iMac Pro has the Intel Xeon CPU which has a set of new features called AVX-512. So AVX-512 is a set of new instructions with vector registers. And these provide benefits over X86-64, so in X86-64 we can only assume that we have 128-bit vectors available, so that's guaranteed on any Mac ever that's Intel powered. Now it happens that any new Mac today has more than that, but the iMac Pro is the first that has 512-bit registers. And with the Auto-Vectorizer that's enabled in the Xcode Clang this is great because it means that we can have many more elements in the vector. So this can greatly improve throughputs. But there are other benefits with AVX-512, so for instance we not only have bigger vectors we also have more of them. So on X86-64 we only have 16 now we have 32, so this is a lot of data to process. And even if for some reason the auto-vectorizer is not able to make use of these vectors then we still have ore skill registers or even for code that just does float or double. There are lots of performance benefits in AVX-512. So let's look at how we can exploit it in my compute [inaudible] expensive function. So the first thing I'm going to do is to keep around my existing function because that's going to be the fallback that I have that runs on all Macs. Next, I can try to specialize my function. So one way to do that is using the target attributes. And that tells the compiler that it's okay to assume that this function has AVX-512, so it only runs on an iMac Pro. So if you use simd.h, for instance the simd float4 128-bit vector type then now we might have better performance than the AVX-512 version using the same code. And if you use the even larger vector types, so for instance simd float16, then now you have much better performance than the AVX-512 version where the 512-bit vector is actually native. And if you go all the way down to X86 intrinsics, then now you can start using the new AVX-512 variance, as well as the M512 types. So if you want to specialize larger units of codes, so not just individual functions but files, targets, libraries, then you can use the new AVX-512 value of the additional vector extensions build setting. So when you do that there are some things to keep in mind and if you're familiar with AVX-1 and AVX-2 these are very similar issues. So you can only pass large vectors, so 256 bits and up from and to AVX-512 functions. So the ABI is different from the generic and a specialized variance, so you cannot pass them between those. Additionally, these vectors are large and they're large enough that their natural alignment is too big for what's guaranteed by things like malloc. So you have to take that into account when allocating these anywhere other than the stack. And so in general all of these things are things that we are already go through lots of things in the opening system. So for instance, if you can at all use accelerate.framework and it's much easier to do so because we already specialized all the functions for every single microarchitecture. So this is AVX-512. Now we also have new features on the iPhone 8, 8 Plus and X. So one of the first feature is ARM v8.1 Atomics and that's thanks to one of the great things about the iPhone X and that's the A11 Bionic chip. So the A11 Bionic chip has one great new feature compared to the A10 which is its support for six CPUs, six cores running all at the same time and that's a first in iOS. And since you have more cores than you probably have more threads all at the same time and with more threads you might need more synchronization to make these threads cooperate. And that's implemented using atomics. So the A11 chip also introduces a new family of atomic instructions that are better optimized for the new extra cores. So let's look at how that works. So the way atomics work is through a small sequence of codes. So suppose I have a thread and it's trying to access main memory, so it has an atomic shared variable in there and it's just trying to increment it. So under the hood the code generator will emit a small sequence of codes that first takes exclusive excess of a cache line and that's a small region of memory that contains completely this atomic variable. Now that we have exclusive access we can load from the variable, then we can do our increment on the temporary loaded value and store the result back. And we know that this is safe because we have exclusive access, so no other thread could have changed the value while we're computing our temporary results. But now suppose another thread does access either the same variable or another variable in the same cache line. So both are going to try to have exclusive access over this variable and that is not possible, that's what it means to be exclusive. So both of them are going to fail their exclusive access and they're going to have to try again until one of them succeeds. And this is not ideal for performance. So in ARM v8.1 which is the architecture in the A10 CPU we have new instructions that do this all in a single step and in some cases, that can greatly improve performance. So again, this is something that you can specialize code for using the per function specialization or for entire targets. And this is something that's only really useful when you have your own C11 or C++ 11 atomics. So in general, it's much easier to use the higher-level libraries like GCD or PThread or os unfair lock, etcetera. So these are already tweaked for ARM v8.1, but they also cooperate to the operating system to have even better performance. So another feature in the A11 CPU is 16-bit floating points. So you are all familiar with the two standard floating point types, so we have double which is 64 bits and float which is 32 bits. So on A11 we also have the 16-bit float16, this has much less range and precision so it's not as useful for as many cases. But in some cases like machine learning or when you're trying to talk to GPU via Metal this is great because it's smaller and it's faster to compute. And that's even more true if you put them in vectors where you can put more of them in the same ARM vector. So this is also something that you can specialize code for and in general something to keep in mind with all of these features is that they're not available everywhere. So when you want to use them you have to always make sure that they're actually dynamically available on the device you're running and you can do that using sysctlbyname. And so in general we already do all this in system framework, so it's much easier to just rely on those. So these are three new instruction set extensions, we have on the iMac Pro AVX-512 and on iPhone X, 8, and 8 Plus we have Atomics and 16-bit floating points. So that's just part of all the new features in Xcode. So from ARC object pointers in C Structs to the improved static analyzer there are lots of great things in Xcode 10. And there are also some things that we didn't even talk about like for instance, over a hundred new warnings and support for C++ 17 standard library function. So if you want to learn more we have the video and the slides available on the website soon. And if you're here at the conference come join us at the lab this afternoon. Thank you.  Thank you. My name is Dan Omachi, I'm an engineer in Apple's Metal Ecosystem Team. Now my main role on that team is to help developers learn Metal and have a great experience using it. But prior to this I worked on Apple's OpenGL and OpenGL ES frameworks for many years. And even before joining Apple I worked on a couple of other OpenGL implementations. So I'm really proud of what we've accomplished with OpenGL over the past years, but I'm also really excited about the direction Metal's taking us. Today my colleague Sukanya Sudugu and I are going to give you a brief introduction to Metal and provide some guidance about how you can easily port your OpenGL apps over. Now this is particularly important this year because as announced in yesterday's state of the union OpenGL, openGL ES and OpenCL are deprecated. Now existing and soon-to-be launched apps can still use OpenGL ES on tvOS and iOS12 and OpenGL and OpenCL on macOS 10.14 Mojave and this will remain so for some time. However, new projects should target Metal from their inception. So if you're not already familiar with Metal, then it's time to get started. Before you rip out all of your OpenGL code and start porting it all to Metal you've got a few options to consider. Apple offers several high-level frameworks that do rendering, different kinds of rendering, SpriteKit for 2D games, SceneKit for some 3D, Core Image for great effects. Also, each of the major third-party game engines already use Metal as their primary renderer. But you may decide that these approaches just aren't for you and the path forward is to do it yourself in Metal, that's what we'll talk about today. So let's start out with Metal design and some fundamental concepts. So OpenGL was originally designed 25 years ago and its core reflects the origins of hardware accelerated 3D graphics. So updates with new versions and extensions have really served us well by exposing new GPU features and performance techniques. However, there are still some fundamental design choices that no longer apply to current hardware. The OpenGL pipeline was originally an entirely fixed function and although today OpenGL supports a programmable pipeline it doesn't neatly match that of modern GPUs. Also, asynchronous processing is not a core feature of the API, there are a number of features which allude to the fact that the GPU works in parallel with your app but much of this is implicit. And 25 years ago only the most expensive workstations and servers had multiple cores. So designing an efficient multithreading model wasn't a priority. So back in 2014, it was clear to us that a new application interface for the GPU was necessary. Obviously, we needed an interface that could efficiently harness the GPU's power. To do that the app needed the CPU out of the way so the GPU could do its thing. But when an app needs the CPU for rendering it needs to be able to use all of the CPU, including its multiple cores. If the interface needed to be predictable to prevent developers from falling into hidden pitfalls in the API every call should have an obvious consequence and few operations should be performed implicitly. Part of this predictability would be the need to control where resources were placed and if and when synchronization was necessary. Now Apple had a few other requirements. We wanted this interface to be approachable, it needed to have the right balance between low-level control and clear usage because the easier it is for you to understand something the more effectively and efficiently you can use it. And of course, we designed Metal to map well to all modern GPUs. In particular, we knew that hardware coming down the pipeline needed software that could push it. This included the Apple designed GPU of the A11 bionic. So how is Metal different than OpenGL? Well you perform the most expensive operations less often. Objects are designed in such a way that API state can be translated and prepackaged into GPU commands at creation. This way, there's much less cost to use them later on when actually rendering. The graphics pipeline with Vertex and fragment shaders also reflects the modern GPU. So it's less costly to translate the API's configuration to GP commands. This doesn't mean the pipeline is entirely different or requires some whole new way of thinking. The main difference in Metal is that the grouping of the pipeline stages into objects is more efficient, not that the pipeline itself is changed. Metal also has a clear model for multithreaded execution. There are a number of different ways for applications to drive the GPU from different threads, but for each there are explicit but unconstraining rules to do this. Additionally, the most commonly used objects like textures, buffers and shaders can safely and efficiently be used across threads. Metal does not need to perform any expensive internal logging operations to prevent these objects from getting into some invalid state. Finally, the execution model reflects what really goes on between software and the GPU. Many of the implicit operations performed by OpenGL are explicitly performed by an application using Metal. This allows you to make intelligent decisions as to when and how your app uses the GPU. It also means there's less software between your application and the GPU. Most Metal calls go directly to the GPU driver and that driver also needs to do less processing to translate from API calls to GPU commands. So let's dive in and see how this works. All GL calls are performed on an OpenGL context. The context does a ton of things, it tracks API state, it manages OpenGL objects and other memory, and translates and submits GPU commands. Metal splits all of these jobs of an OpenGL context into a number of smaller objects. The first object that creates is a Metal device which is an abstract representation of a physical GPU. The device creates objects such as textures, buffers and pipeline objects which contain shaders. It also creates a key object called a Metal command queue. The command queue really only has one job, to create a sequence of command buffers. Here we've created one command buffer and a command buffer is simply a list of GPU commands that your app will fill and send to the GPU for execution. Typically your app will create a single command buffer per frame, however, there are some situations where it makes sense to create two or three in a frame. But an app doesn't write these commands directly to the command buffer, instead it creates a Metal command encoder. The command encoder translates API calls into GPU instructions and writes them to the command buffer. After a series of commands have been encoded your app will end encoding and release the encoder object. There are actually a couple different kind of encoders and your app can create new encoders from the command buffer to encode more commands. Now it's important to note that up until now on this diagram the GPU hasn't done any work. Metal has created objects and encoded commands all with the CPU. It's only after your app has finished encoding commands and committed the command buffer that the GPU begins to work and executes those commands. You can also encode commands to multiple command buffers in parallel on separate threads. Now I've already spoken about how the command queue produces a sequence of command buffers and that you create a command encoder from a command buffer. Let's take a closer look at these encoder objects used to write to our command buffers. There are three main types. There's the Render Command Encoder whose commands resemble that of an OpenGL command string where you set state, bind objects, and issue draw calls. There's the Blit Command Encoder with which you can issue texture and buffer copy operations. You can also use it to generate Mipmap for textures and transfer pixel data from buffers similar to how glReadPixels operates with OpenGL's pixel buffer objects. And then there's the Compute Command Encoder which allows you to dispatch commute kernels. The Render Command Encoder is probably the encoder you'll become most familiar with as it's responsible for the majority of operations that a graphic centric app needs. The set of commands encoded into a Render Command Encoder is often referred to as a Render Pass. In a Render Pass you set up render objects for the graphics pipeline and issue draw commands with those objects. This includes operations similar to glDrawArrays and glDrawElements and other OpenGL draw commands. Each Render Command Encoder is strongly associated with a set of render targets. And the render target is simply a texture that can be drawn to. This includes color, depth and stencil textures, as well as multi-sample textures. You specify a set of render targets when you create a render command encoder from a command buffer. And all draw commands are directed to these targets for the lifetime of the encoder. To direct commands to a new set of targets you end encoding with a current render command encoder and create a new encoder with new targets. This creates a very clear delineation between commands directed to different sets of render targets. Let's talk a little bit about the object you'll use for rendering. Metal has a number of similar objects to OpenGL. There are textures, buffers, samplers and pipeline state objects which resemble OpenGL's program objects, and also depth stencil state objects which don't really exists in OpenGL but are just the containers for the same depth and stencil state that OpenGL's got. One significant difference between OpenGL and Metal objects are how they are created and managed. The objects are all created from a device object so they're associated with a single GPU. You set most of an object state when you create it by specifying properties in a descriptor object. All states set when the object is created is fixed and immutable. Although the state of textures and buffers are fixed upon creation, data contained by these objects can be changed. Metal can do all the expensive setup for an object once when it's created. OpenGL's mutable model makes it completely possible that if your app touches a little piece of state the recompilation of that object may occur. And even if your app doesn't change an object state OpenGL needs to check a hierarchy of flags to confirm this before drawing. So with this immutable model Metal never needs to check for object state changes make draw calls much faster. Additionally, objects can be efficiently used across threads, Metal never needs to lock an object to prevent it from getting into an invalid state from a change on another thread. So let's talk about porting. Here is the typical phase, here are typical phases of a rendering application. You build the app compiling source and bundling your assets. Start up and initializing it. Loading the assets and initializing objects that will persist for the lifetime of your application. And you repeatedly render setting up state, issuing many draw calls, and presenting frame after frame. I'll talk about developing for Metal in the earlier stages of the application's lifetime, including building shaders offline and creating object's initialization. And Sukanya will describe how to port your per frame rendering code. So let's start out with application build time which is where shaders are typically compiled. We'll begin with a look at the shading language. The Metal shading language is based on C++. So just like C++ you can create classes, templates, structures, define enums and namespaces. Like GLSL there are built-in vector and matrix types, and numerous built-in functions and operators commonly used for graphics. And there are classes to specify sampler state and operate on textures. The best way to describe the language is just to show you. So here is a vertex and fragment shader pair. The vertex keyword at the function at the top specifies that that function is a vertex shader, likewise for the fragment keyword for the function at the bottom. Note that they have custom names unlike GLSL where every shader is just called main. This is important because it allows you to build large libraries of unique shaders using names to indicate what they do. Anything passed from your application is an argument to one of these shaders. There aren't any loose variables outside of functions as there are with GLSL. As you can see, there are these bracketed symbols next to each parameter. These are attribute specifiers which extend upon the C++ language to indicate special variables. So this vertex ID attribute indicates that this VID parameter should contain the index of the current vertex when this vertex executes. It's used down here in two places to index into an array of vertices. These parameters here with a buffer attribute specifier these indicate that these variables are filled by buffer's objects set in the Metal API. I'll talk more about how the Metal API relates to these in just a minute. But first let's take a closer look at some of the types used here. Now these are all custom types that you would define. The vertices parameter here is defined as a pointer to a vertex type and its definition is right here. The structure has two members, a model position member and a texture coordinate member. It defines the layout and memory of each vertex in a vertex array passed into this vertex shader. Let's look at this vertex output type returned by our vertex shader. Its definition here specifies what's passed down for the rasterization stage and eventually to the fragment program. The four-component floating-point member named clipPos has this position attribute specifier next to it. This indicates that this member will serve as the output position of our vertex shader. This texCoord member which doesn't have an attribute specifier defaults to a variable that will be interpolated with the texture coordinate values of other vertices defining the rasterized triangle much like a varying in GLSL. Let's take a look at some of the parameters of our fragment shader. So we're using this vertexOutput struct that we just looked at down here as an input to our fragment shader. And it gives us the interpolated texture coordinate that we constructed in the vertex shader and we use it here to sample from a texture. In our application code which is written in Objective-C we use a Render Command Encoder to map objects to shader parameters. The index argument with each of these calls are similar to OpenGL's attribute indices, vertex attribute indices, and textured units. They specify indices which map to indices in our shader parameters. So by calling the encoder's set fragment buffer method with an index of three we map this my uniform buffer object to the uniform's parameter of our encoder, of our shader. We make similar calls to set our texture and sampler objects. Now I want to talk about one library that's incredibly useful for shader development. This is called SIMD. Now SIMD is actually a library that's separate from Metal and is used in many orthogonal frameworks. But it was built with Metal in mind. SIMD defines vector and matrix types commonly used to implement graphics algorithms, so this includes three and four component vector types and three by three and four by four matrix types. One of the nicest features of this library is that you can use it to share code between your application code and your shading language code. This is really useful because you can define the layout of data passed from your application to your shaders with structures using these types. Here's how it works. You create a structure with data you'd like to pass from your application to your shaders. So for instance, if you want to pass down a model view projection matrix to the and the position of the SIMD primary light source you can use SIMD types in your structure to do this. You put the structure's definition and a header which you would include in both your application code and Metal files. This way the data layout used by your shaders matches what you've set in your code because they're using the same types. This makes bugs due to layout mismatches much less common. One of the key ways in which Metal achieves its efficiency is by doing work earlier and less frequently. So you'll use Xcode to compile Metal shader files when you build your application. This runs a front-end compilation performing string parsing, shader analysis, and some basic optimizations. It converts your code into a binary intermediate representation that is usable on any hardware. Actual GPU machine code isn't built until your app runs on the user's system. But this removes half the compiled time needed when your app runs. The built intermediate representation binary is archived into a Metal library file. Xcode will compile all the Metal shader source files into a default Metal library, placing it into your app bundle for retrieval at runtime. In addition to having Xcode built shaders you also can build shaders during your app's runtime. Some OpenGL apps construct shaders at runtime by concatenating strings or running a source code generator of some kind. For those gathering all of the shaders at build time can be a challenge. But for bring up purposes or if built time compilation just isn't possible for your app Metal is able to compile shaders from source at runtime just like OpenGL. However, there are a number of disadvantages to this, most obviously you give up on the performance savings of keeping this work off the user's system. You won't see any shader compilation errors at built time so you'll only find out about them when you run your app. Finally, if you compile at runtime you can't include headers in your shaders, so you can't share types between your shaders or your application code. If you use runtime share compilation to get your app up and running quickly I strongly encourage you to spend some time getting your shaders to build within Xcode. This way you can benefit from not only the runtime performance savings but also these conveniences in shader development. Let's move onto the steps your app will need to take to initialize your Metal renderer, so this includes creating a number of objects. I'll start out with the devices and command queues and then I'll describe how to create objects for your assets, including textures, buffers and pipeline objects. So devices and queues. These are the first objects you'll create. The device is an abstract representation of a GPU. It's responsible for creating objects that are used for rendering including these textures, buffers and pipeline objects. In iOS since there's only ever one GPU there's only one device you can get, but on macOS systems can have multiple devices since there may be both an integrated and discrete GPU and even multiple eGPUs. Usually though, getting the default device is sufficient for most applications. And getting this device is really simple, you call MTLCreateSystem DefaultDevice. One of the first things you'll do with this device is create a command queue. The queue is used mostly in your applications render loop to obtain command buffers each frame, but you want to create the command queue at initialization. Typically, a single queue is sufficient but more complex apps that execute many tasks in parallel may need multiple queues. Creating a queue is also really simple, you call newCommandQueue with the device you just created. And once you've got your queue you can get command buffers for your render loop. Let's move on building your render objects. I'm going to talk about creating three types of objects used for rendering, textures, buffers and pipelines. Let's start with textures. As mentioned earlier, you create render objects from a device object. You'll use a descriptor object to create most of these. Descriptors are really simple objects without any true methods. They only contain properties needed for object setup. So for example, to create a texture you'd use a texture descriptor. You specify the properties that you'd like the created texture to have. So things like the type of texture, 2D, 3D, cubeMap. The texture's dimensions and the number of Mipmaps you'd like it to have. And the pixel format of data in the texture. Once you've set the desired values for each of the properties you call a method on the device to create a usable texture object. This also allocates the memory backing the texture image. Once you've created the object you no longer need the descriptor as it's only used for object creation. And properties you set in the descriptor are locked and can no longer be changed. However, the contents of the texture image can be modified. So I want to talk about one property you'll set in the texture descriptor and you'll also use when creating a buffer. This is called the storage mode. As mentioned, when you create a texture object Metal allocates memory for it right then and there. The storage mode property tells Metal in which pool memory it should be allocated. The simplest option shared storage mode gives both the CPU and the GPU access. For buffers this means you get a pointer to the memory backing the object. Protectors, this means you can call certain easy-to-use methods to set and retrieve data from the texture. You can use private storage mode which only gives the GPU access to the data, this allows the Metal to apply some optimizations it would be unable to if the CPU also had access to that data. But only the GPU can directly fill the contents of this type of texture. You can indirectly fill data from the CPU using a blit encoder to copy from a second intermediate resource using shared storage. On devices with dedicated video memory setting the resource to use private storage allocates it in video memory only. Finally, on macOS you can use a third storage mode called managed storage. This allows both the GPU and CPU to access the object data. And on systems with dedicated video memory Metal may create a mirrored memory backing for efficient access by both processors. Because of this, explicit calls are necessary to ensure data is synchronized for GPU and CPU access. Here's an example of creating a texture object. First you create a texture descriptor and set a number of properties in it, including the storage mode. And then we create a texture using the device. To fill a texture's image data we calculate the bytes per row and just like with GLText [inaudible] we specify a region to load. And then we call the texture object's replace region method which copies the data into the texture from a pointer we supply. Now there are a few notable differences between OpenGL and Metal texture objects. While OpenGL does have a sampler object they are optional. Wrap modes and filters can be set in the texture's object itself in OpenGL. In Metal you need to create a separate sampler object or specify sampler parameters in your shaders. Texture image data is not flipped in Metal. OpenGL uses a bottom-left origin while Metal uses a top-left origin. So make sure you're aware of the difference when loading your textures. Desktop OpenGL also performs conversions when the data supplied is in a different format than the internal format. However Metal similar to OpenGL ES does not, so you'll need to make sure your assets are already in the proper format or implement paths to perform conversions. Let's move on to buffers. Metal uses buffers for all unstructured data. They're really similar to OpenGL's vertex, element and uniform buffers. So if you're already using these buffers in OpenGL you'll have an easier time with your port. Creating a buffer is pretty simple with a device you call newBufferWithLength specifying the size of the buffer you want. You also specify a storage mode. You load a buffer through the contents property, which is simply a void pointer to the memory backing the buffer. To Metal's core the data is completely unstructured, so it's up to you to define the layout in your application and share your code. Here we're casting the contents pointer to a structure which uses some of the SIMD types I mentioned earlier. We set the members of the structure which fills in the buffer's data using the layout or application as is defined with the structure's definition. As mentioned earlier, you can share a structure's definition between your application and share code which ensures the layouts match between these two components. A common mistake is a mismatch between buffer data and how it's interpreted in the shader due to an assumption or due to assumptions about alignment. For instance, you might assume that the shading language's float3 type consumes only 12 bytes. Three 4 byte components is 12 bytes right. Well if you pack your data that way you'll run into trouble because the shading language actually interprets a float3 as consuming 16 bytes. That's because the types are kept as stricter alignments for CPU and GPU optimal CPU and GPU usage. Similar problems often occur with three by three matrices. If you really want to tack, if you really want to pack data tightly there are pack types available in the Metal shading language. After passing a pack data in a buffer your shader code would need to pack data, would you need to pass that packed data to a regular vector before it can perform operations on that data. So when you're bringing up your application I recommend using the most convenient storage modes. This makes it more easy to access the data in those resources. So in iOS create all textures and buffers with StorageModeShared. On macOS you can't use StorageModeShared with textures, but StorageModeManaged does make image data access easier, although private storage is most optimal. You can use StorageModeShared with buffers on macOS, but be careful with this. If you mix data that both the CPU and GPU needs access to with data only the GPU needs access to it can be difficult to detangle these two types of data later on and put them in separate memory pools. Now that I've described a little bit about creating textures and buffer objects directly I'd like to mention the MetalKit framework which can easily create textures and vertex buffers from common file formats. There's a texture loading class and some functionality to create Metal buffer backed meshes loaded by Model I/O. Let's talk about shaders and the render pipeline objects that contain them. You create one of these pipeline state objects using a render pipeline descriptor object. The object contains a vertex and fragment shader pair. It also specifies the layout of vertices feeding the vertex shader. And also blend state and the pixel formats of render targets that the fragment and shader can render to. Just like with the texture object you call method on the device which will produce an immutable render state pipeline object. And also like a texture object you only need the descriptor to create that object. Here's some code showing how to build these pipeline state objects. Before you create the pipeline, you'll need to get the shaders out of a Metal library. This line here loads the default Metal library that we built in Xcode. For this library you can obtain your vertex and fragment shader using their function names. You create a render pipeline descriptor object where you set these shading functions and also specify render target pixel formats. Finally, you use the device with the descriptor object we've set up to produce a pipeline state object. The most significant difference between the OpenGL and Metal graphics pipelines is that while an OpenGL program object contains just a vertex and fragment shader pair Metal pipeline objects also include a vertex layout, blend state, and render target pixel format. So you need to know all of these things before you build a pipeline. Having these extra pieces of state in the pipeline enables Metal to fully compile shaders into GPU machine code upon object creation. This is something that's not possible with OpenGL's program objects. Now you'll need to build a system that allows you to create pipelines when you initialize your app. It helps if you can choose a canonical vertex layout and a limited set of render targets. This reduces the combinations of state your app uses and the number of pipeline objects needed. However, not everyone knows up front which pipelines their app will need. A common first step in porting an app to Metal is to keep a dictionary of pipelines you've already made. As your app encounters new combinations of state it can build and store pipelines using the descriptors as keys. Keep in mind this solution I mentioned is kind of a hack and you want to avoid shipping your app with it. This is because creating any of these objects is expensive. When you create a pipeline object the GPU compiler kicks in and translates and optimizes the shader's binary intermediate representation to machine code. If your app creates these just-in-time during its render loop you're likely to see stalls and a reduced frame rate for a short period of time. Allocating memory for textures and buffers is also an expensive operation, that's not even accounting for the processing required to fill in these resources with data. However, once you've created these objects using them in your render loop requires very little CPU over [inaudible] since so much is done up front. With that I'd like to hand it over to Sukanya who will talk about porting your render loop. Good evening everyone, I am Sukanya Sudugu, GPU software engineer at Apple. I am super excited to share some of our porting experience with you, so let's continue porting. Dan already talked about application setup, so by now you would have built all your shaders and also created all persistent objects needed for frame generation. So now let's dive into your application's render loop which is the code that runs for every frame. With Metal you will explicitly manage the asynchronous nature of GPU's execution in your render loop. So this includes obtaining and submitting command buffers to GPU with the list of GPU commands. And updating your resources in such a way that allows the GPU to asynchronously read from it which you have written in your render loop. And encoding render passes by creating render command encoders. And finally, presenting your frames to the display. So let's first talk about this command buffer object. One of the key changes from OpenGL is that Metal provides explicit control over command buffers, which means it's up to you to create these command buffers and you can decide when to commit them to GPU for its execution. So the straightforward thing to do is to create one command buffer which will render your whole frame. And once your application is up and running and if you perhaps see some idle time on GPU, then you might want to consider splitting your frame across multiple command buffers. And then the GPU can get started executing one command buffer while CPU is encoding the others. Alternatively, splitting your frame encoding across multiple threads requires you to create one command buffer per thread. But keep in mind that there is some additional CPU cost associated with each command buffer. So you should be using as few as possible. Metal also provides an API to register a completion callback, which will be involved when GPU completes executing this command buffer. So let me show you all these APIs in action. So you will have created command queue in your application initialization method. Now in render loop you will use it to obtain command buffer by calling its command buffer method. Now you will encode commands into this command buffer. I'll talk more about this encoder shortly. But when you are finished encoding you will call commit method so that this command buffer will be submitted to GPU for its execution. Now you have two options for what to do while this command buffer is being executed on GPU. You can wait by calling this waitUntilCompleted method which is similar to glFinish in OpenGL. This method will synchronously wait and only returns when all the commands in the command buffer has been executed by the GPU. Alternatively, before you commit this command buffer to GPU you can add a completion handler, which will be invoked when GPU is done executing this command buffer. This allows CPU and GPU to execute in parallel. Also this allows us to track and efficiently update shared resources whose data is changing every frame. Since we are here let's talk more about these resource updates and see how we can make them efficient using this completion handler. So almost all applications will be pushing new data to GPU every frame. For example, new shaded uniforms needed for animations. In Metal CPU can write to this shared resources anytime even if GP is accessing the same memory at the same time. Metal does not protect you against these data-races. This compares to OpenGL which by default prevents these data-races either by waiting for the GPU workloads to finish or by doing extra copies, which may not be optimal for your application. With Metal you can implement any optimal synchronization strategy which suits best for your applications. So in most cases, best approach is to multi-buffer your shared resources so that CPU and GPU are never trying to access the same buffer simultaneously. So let's see an example, you have an OpenGL application and you just ported it to Metal and it is using single buffer for these dynamic data updates and you did not implement any synchronization method yet in your application, then here's what will happen. Your application will write to that buffer while generating commands for the first frame and then submits this command buffer to GPU. Then GPU when it is executing this command buffer will try to read from this buffer. And while you're updating the same buffer for the next frame GPU is still reading from this buffer. So this is clearly a race condition and the results are undefined. There are a few potential fixes which you can implement in your application. The simplest is to call the command buffer's waitUntilCompleted method after every frame. But this waits for the GPU to finish reading before you override this buffer for the next frame. But as you can see, here this is a very poor utilization of both CPU and GPU, so this is not something you want to do in your shipping code. But it is very useful to get your Metal application up and running. And even to detect that if your application really indeed has these kind of data conflicts. The efficient way to synchronize your resource updates is to use multiple buffers. So in this case we will use three buffers to triple buffer our dynamic data updates. So in frame 1 we will write to a buffer and then the GPU will read from it. And in frame 2 we will write to another buffer avoiding this race condition. Similarly, in frame 3 we will write the third buffer. But now we have used up all three buffers and exhausted our buffer pool. We need to wait for the GPU to finish the frame 1 so that we can reuse it for frame 4. Now this completion handler will come handy, it will let us know when GPU has done executing this frame. So when frame 1 is done we can reuse its buffer for frame 4 and so on. So let's look at a sample. For triple buffer implementation outside your render loop first we will create a FIFO queue of three buffers. And we also need a frameBoundarySemaphore which is initialized to start value as 3. So this says semaphore will be signaled at each frame boundary, that is when GPU finishes executing a frame allowing the CPU to reuse its buffer. Also initialize buffer index to point to the current frame's buffer. And in our render loop before we write to a buffer we need to first ensure that its corresponding frame has completed its execution on the GPU. So at the beginning of the frame we wait on our frameBoundarySemaphore. Once the semaphore is signaled indicating that the current frame has completed its execution on GPU now it's safe to grab its buffer and reuse for new frame data. And we will now encode some commands to bind this buffer to GPU. And before we commit this command buffer to GPU we will add a completion handler to this command buffer for this frame. After adding completion handler we will commit this command buffer to GPU. Now once GPU finishes executing this frame our completion handler will be invoked which will then signal this frame semaphore. This allows CPU to reuse its buffer for new frame encoding. So this was a simple triple buffer implementation which you can adopt for handling any dynamic data resource updates. Now that we have our command buffer and also, we have handled our resource updates let's talk more about render pass encoders which you will use to encode your draw calls. As Dan mentioned earlier, command encoders translate API calls into GPU hardware commands in a command buffer. I will be talking about render command encoders which provides APIs for typical graphics, operations like setting your pipelines, texture buffer objects, and also issuing the draw calls. So when creating your encoder you need to first set the render targets. So here is a render pass descriptor where you will set the render targets and then ask the command buffer to create a new encoder for this render pass. And now you can use this encoder to encode your draw calls. One key change from OpenGL is in Metal once the encoder is created you cannot change its render targets because GPU performs much better when it sees a largest span of draw calls which are rendering to the same set of render targets. So Metal API reflects this by giving an explicit start and end to a render pass. Now all the draws issued by this encoder will be rendering to these render targets. When you want to draw to a different set of render targets then you end this render pass and start a new one. And here is where we are creating a render pass descriptor and binding color and depth attachments. Now you can use that descriptor and create a render command encoder. Now all the draws issued by this encoder will be rendering to these targets. Additionally, in Metal you can also provide load and store actions for these attachments for optional GPU bandwidth usage. So these load and store actions allow you to control how the texture contents will be handled before and after a rendered pass. So here we have color and depth attachments for a render pass. If it specified the load action as clear for both of these render targets the GPU will first clear their contents. Then the GPU will execute commands in this encoder rendering to these render targets. Now you can specify the store actions to be performed at the end of the render pass. Here for the color buffer since the store action is store it will preserve the contents of this color buffer at the end of the render pass. And for the depth attachment, since we said store action as don't care it will discard its contents. So these texture load and stores are expensive GPU operations that consumes memory bandwidth. So choose clear and don't care wherever possible. And here is how you set your action. In this example we specify clear as a load action and set the clear color. And here we specify store action as store. Similarly, you can specify load and store action for each of the render targets you set in this render pass descriptor. So that's how we configured our render targets. Now you can create the encoder from this descriptor and we can start encoding draw calls. But before that I want to show you some OpenGL commands and they will show you their Metal equivalents. So this is a typical OpenGL draw sequence. In this segment first we bind a frame buffer which will set all the render targets. Then we bind our program which contains vertex and fragment shader. And we bind vertex buffer containing vertex data. And then we bind another buffer containing all the uniforms. And then we'll set a texture to sample from. And finally, we draw. So here is the Metal equivalent, it's a few more lines of code since it's explicit but it's actually very similar to OpenGL. First, we create a render command encoder using render pass descriptor which has our render targets. Then we set a pipeline object which has our shaders. Then we set a buffer for accessing our vertex shader and this happens to contain the vertices we will draw with. Note that the Metal doesn't distinguish between buffers containing uniforms or those containing vertices. So we will use the same API to set uniform buffer. So here the first call will give buffer access to vertex shader while the second call will give access to the fragment shader. Next, we set a texture for our fragment shader to sample from. And then we draw. And once we are done encoding commands we will indicate this to Metal by calling endEncoding on the encoder object. So this was a simple example of a metal render pass. Now that we have a complete render pass let's see how to get these renderings onto the display. With OpenGL you present your rendered frame onto the screen but you don't explicitly manage the system's render buffers also known as drawables. But with Metal to get your content displayed on the screen first you need to obtain the special textures called drawables from the system. So you can use a MetalKit view which will provide you a drawable texture for each frame. And once you obtain this drawable you can encode render passes and render to these drawables just like you would render to any other texture. However, unlike other textures now you can present this drawable to your view. So let me show you a code sample. First, I want to mention that these drawables are limited shade system resources, so you should hold on to them as briefly as possible. This means that you should encode all your offscreen render passes before you acquire a drawable. Then you can create a MetalKit view for either a fully populated render pass descriptor which you can directly use to create encoders, which is what you see here or you can ask for a specific texture using view.currentDrawable property. And then you can build your own render pass descriptor around it. Once you are finished encoding your frame you can call the command buffer's present drawable method which puts your drawable onto the screen when GPU finishes executing this command buffer. So now once GPU is done executing this frame you should see your frame on the display. So that was our quick tour to get your renderings onto the display with Metal. But I have a few more tips to share. It is possible to port one render pass at a time into Metal. The IOSurface and CVPixelBuffer APIs can be used to create a texture which can be shared between OpenGL and Metal. This will allow you to render to a texture in OpenGL and then read from it in Metal or vice versa. So some of the applications can leverage this to port incrementally, especially with plugin architectures which are built around OpenGL. So there is a sample code linked to this session showing exactly on how to do this. Now that you have a Metal application you can adopt all new Metal features. If your application is CPU bound then multithreading your CPU encoding work can help. You can encode multiple command buffers simultaneously on separate threads. You can even split single command buffer encoding across multiple threads using parallel render command encoder. Also, GPUs compute by planned processing APIs is built right into Metal. You can use GPU in whole new ways. To start with instead of CPU you can leverage compute pipeline to generate graphics GPU data. By letting GPU to generate its own data you're not only reducing the CPU utilization and synchronization points now you are freeing up the data bandwidth to GPU. Now with this high bandwidth compute processor you can implement many more complex algorithms like these. Metal supports many more great features which you can adopt in your application to improve its performance. In addition, as part of your porting process we highly encourage to use our Metal powerful debugging and optimization tools which are integrated right into Xcode. So Metal has a great API validation layer that will print detailed information about any improper API usage and also suggests some potential fixes. This is a GPU debugger which gives you a capability to step through your Metal calls and view how your frame is being rendered step-by-step. You can view all the resources associated with your frame. It also provides the GPU expert tips based on your resource usage. For this year we have also added shader debugging capability where you can debug your shader functions just like other functions. We also added a shader profiler which showing a rich set of performance metrics related to your shaders. And also, a dependency viewer which allows you to evaluate dependencies between the render passes and you can potentially merge any redundant render passes to improve your application's performance. And finally, the Metal system trace tool in instruments will give you a complete view of your application's behavior and performance so we highly recommend using these tools to ease your porting effort. Wrapping it up, OpenGL and OpenCL are deprecated. They are still around but we are discouraging their future use. It is time to adopt Metal. We have full suite of developer tools, a lot of experience helping many teams successfully go through this effort and come out with substantially improved performance. I'm hoping the information we shared today will give you smooth porting experience. And we are very happy to talk to you about your applications and how to get them up and running with Metal. So we will be available tomorrow afternoon at OpenGL to Metal porting lab and there are many upcoming sessions on Metal. I recommend attending these sessions to learn more about how to debug and optimize your applications with Metal. With that I thank you all for attending and I hope you enjoy the rest of your week.  Good afternoon, and welcome to WWDC. I know I'm competing now with the coffee and cookies, but I don't know if the coffee is gluten free and the cookies are actually caffeine free, so stick with me. My name is Frank Doepke and I'm going to talk about some interesting stuff that you can do with Computer Vision using Core ML and Division Framework. So, what are we going to talk about today? First, you might have heard that we have something special for you in terms of custom image classification. Then we're going to do some object recognition. And last but not least, I'm going to raise your level of Vision awareness by diving into some of the fundamentals. Now, Custom Image Classification, we've seen the advantages already in some of the presentations earlier, and we see like what we can do with flowers and fruits and I like flowers and fruits as much as everybody else. Sorry, Lizzie, but I thought we'd do something a bit more technical here. So, the idea is, what can we do if we create a shop and we are all geeks, so let's build a shop where we can build robots. So, there are some parts that we need to identify. And I thought it would be great if I have an app that actually can help customers in my store identify what these objects are. So, we've got to train a customer classifier for that. Then, once we have our classifier, we build an iOS app around that, that we can actually run on all devices. And when going through this, I'm going to go through some of the common pitfalls when you actually want to do any of this stuff and try to guide you through that. Let's start with our training. So, how do we train? We use of course, Create ML. The first step of course we have to do, is we have to take pictures. Then we put them in folders and we use the folder names as our classification labels. Now, the biggest question that everybody has, "How much data do I need?" The first thing is, well, we need a minimum of about 10 images per category, but that's on the low side. You definitely want to have more. The more, the better, actually your classifier will perform better. Another thing to look out for is highly imbalanced data sets. What do I mean by that? When a data set has like thousands of images in one category, and only ten in the other one, this model will not train really well. So, you want to have like an equal distribution between most of your categories. Another thing that we actually introduce, is augmentation. Augmentation will help you to make this model more robust, but it doesn't really replace the variety. So, you still want to have lots of images of your objects that you want to classify. But, with the augmentation, what we're going to do is, we take an image and we perturb it. So, we have noised it, we blur it, we rotate it, flip it, so it looks different to the classifier actually when we train it. Let's look a little bit under the hood of how our training actually works. You might have already heard it, the term, transfer learning. And this is what we're going to use in Create ML when we train our classifier. So, we start with a pretrained model, and that's where all the heavy lifting actually happens. These models train normally for weeks, and with millions of images, and that is the first starting point that you need to actually work with this. Out of this model, we can use this as a feature extractor. This gives us a feature vector which is pretty much a numerical description of what we have in our image. Now, you bring in your data and we train the set, what we call the last layer, which is the real classifier, on your label data and out comes your custom model. Now, I mentioned already this large, first pretrained model. And we have something new in Vision for this [inaudible]. This is what we call the Vision FeaturePrint for Scenes. It's available through Create ML and it allows you to train an image classifier. It has been trained on a very large data set, and it is capable of categorizing over a thousand categories. That's a pretty good distribution that you can actually use [inaudible]. And we've already used it. Over the last few years, through some of the user [inaudible] pictures that you've seen in photos, have been actually using this model underneath. We're also going to continuously improve on that model, but there's a small caveat that I would like to kind of highlight here. When we come out with a new version of that model, you will not necessarily automatically get the benefits unless you retrain [inaudible] new model. So, if you start developing with this, this year, and you want to you know, take advantage of whatever we come out over the next years, hold onto your data sets so you can actually retrain [inaudible]. A few more things about our feature extractor. It's already on the device, and that was kind of an important decision for us, because it makes the disc footprint for your models significantly smaller. So, let's compare a little bit. So, I chose some common, you know, available models that we use today. The first thing would be Resnet. So, if I train my classifier on top of Resnet, how big is my model? Ninety-eight megabytes. If I use Squeezenet, so Squeezenet is a much smaller model. It's not capable of differentiating as many categories, and that's 5 megabytes. So, it's about saving there, but it will not be as versatile. Now, how about Vision? It's less than a megabyte in most cases. The other thing of course why we believe that this is a good choice for use, it's already optimized. And we know a few things about our hardware or GPUs and CPUS and we really optimized a lot on that model so that it performs best on our devices. So, how do we train [inaudible]? We start with some labeled images, and bring them into Create ML, and Create ML knows how to extract [inaudible] Vision Feature Print. It trains our classifier and that classifier is all what [inaudible] will go into our Core ML model. That's why it is so small. Now, when it comes time that I actually want to analyze an image, all I have to do is use my image and model, and now in Vision or in Core ML, it knows another [inaudible] again how to train -- sorry. Not train. In this case, use our Vision Feature Print and we'll [inaudible] the classification. So, that was everything we needed to know about the training. But, I said there was some caveats that you want to kind of look at when we deal with the app. So, first thing, we only want to actually run our classifier when we really have to. Classifiers are deep convolutional networks that are pretty computational intensive. So, when we run those, it will definitely use up kind of you know, some electrons running on the CPU and GPU. So, you don't want to use this unless you really have to. In the example that I'm going to show in my demo laters, I really only want to classify if actually the person looks really at an item and not when just a camera moves around. So, I'm asking the question, "Am I holding still?" and then I'm going to run my classifier. How do I do this? By using Vision, I can use Registration. Registration means I can take two images and align them with each other. And you're going to tell me, "Okay, if you shift it by this amount of pixels, this is actually in how they would actually match." This is a pretty cheap and fast algorithm, and it will tell me if I hold the camera still or if anything is moving in front of the camera. I used VN Translational Image Registration Request. I know that is a mouthful. But that will give me all this information. So, to visualize this first, let's look at the little video. What I'm doing in this video is I show basically a little yellow line that shows me how basically my camera has moved or the Registration requests have moved over the last couple of frames. So, what out for that yellow line and see if it's long, then I have moved the camera around quite a bit, and when I'm holding it still, it should actually be a very small line. So, you see the camera is moving, and now I'm focusing on this, and it gets very short. So, that's just a good idea. It's like, "Okay, now I'm holding still. Now, I want to run my classifier." Next thing to keep in mind is, have a backup plan. It's always good to have a backup plan. Classifications can be wrong. And what that means, even if my classification actually has a high confidence, I need to kind of plan for sometimes that it doesn't work quite correctly. The one thing that I did in my example here, as you will see later on, I have something wherefore which I don't have a physical object. So, how do I solve that? In my example, I'm using our backward detector that we have in the Vision Framework to read some data backward label to identify this. Alright, enough of slides. Who wants to see the demo? Okay, what you see on the right-hand side of the screen is my device. I'm going to start now my little robot shop application. And when you see I'm moving around, there's nothing happening. When I hold still, and I point it at something, I should see a yellow line and then voila, yes step is a stepper motor. Okay? Let's see what else do we have here on this table? That is my micro controller. That's a stepper motor driver. We can also like you know, pick something up and hold it. Yes, this is a closed loop belt. What do we have here? Lead screw. And as I said, you can also look at the barcode here, if I get my cable [inaudible] long enough. And that is my training course. Hey, Frank? Of course, for that I didn't-- Frank? What's going on? Frank, yes, I'm going to need you to add another robot part to your demo. This is Brett. That's my manager. That'd be great. As usual, management, last minute requests. I'll make sure you get another copy of that memo. I'm not going to come in on Saturday for this. Alright, well what do we have here? We have a [inaudible] motor. Alright, let's see. It might just work. Let me try this. Do I get away with that? No, it can't really read this object. Perhaps so? It's -- no, it's not a stepper motor. So, that's a bug. I guess we need to fix that. Who wants to fix this? Who wants to fix this? Alright. So, what I have to do now is I have to take some pictures of the aforementioned, [inaudible] motor. So, I need to go to the studio and you know, set up the lights, or I use my favorite camera that I already have here. Now, let's see. We're going to take a bunch of pictures of our [inaudible] motor. And it's kind of important to just kind of vary it and don't have anything else really in the frame. So, I'm going for a [inaudible]. I need to have at least ten different images. Good choices. I always just like to put it on a different background. And make sure that we get a few captured here. Perhaps I'm going to hold it a little bit in my hand. Okay, so we have now a number of images. Now, I'm going to go over to my Mac and actually show you how to actually do then the training work for that. Okay, I'm bringing up my image capture application and let me for a moment just hide my [inaudible]. If I now look in my Finder, you can actually see I have my training set, which I used already earlier to train and model that I've been using in my application. And I need to create now a new folder, and let's call that Servo. And from Image Capture, I can now simply take all the pictures that I just captured and drag them into my Servo. Alright, so now we have that added. Now, I need to train my model again since my manager just ruined what I've done earlier. Okay. I used a simple scripting playground here. Not the UI, just because well, this might be something I want to later on incorporate as a build step into my application. So, I'm pointing it simply at the data set that we just added our folder to, and I'm just simply going to train my classifier, and in the end, write out my model. So, what's going to happen now as you can see, we're off to the races. It's going to go through all these images that we've already put into our folders and extracts the scene print from that. It does all the scaling down that has to happen and will then in the end, train and model based on that. So, it's a pretty complex task, but you see for you, it's really just one line of code, and in the end, you should get out of it, a model that you can actually use in your application. Let's see as it just finishes. We're almost there. And voila, we have our model. Now, that model, I've already referenced in my robot shop application. This is what we see here now. As you can see, this is my image classifier. It's 148 kilobytes. That's smaller than the little startup screen that I actually have. So, one thing I want to highlight here already, and we're going to go into that a little bit later. So, this image, that I need to pass into this has to be of a-- a color image and a 299 by 299 pixels. Strange layout but this is actually what a lot of these classifiers will do. Alright. So, now I have hopefully a model that will understand it. Now, I need to go into my sophisticated product database which is just a key list. And I'm going to add my Servo to that. So, I'm going to rename this one here. This is Servo. I'm giving it a label. This is actually what we're going to see. This is a servo motor and let's say this is a motor that goes swish, swish. Very technical. Alright. Let's see if this works. I'm going to run my application now. That's -- okay. Let's try it. There's our servo motor. Just to put this in perspective. This was a world first that you saw. A classifier being trained live on stage, from photos, all the way into the final application. I was a bit sweating about this demo [laughter]. Thank you. Now we've seen how it works. There's a few things I want to highlight actually when we actually go through the code. So, I promise, we're going to live code here a little bit. Okay, let me take everything away that we don't need to look at right now. And make this a bit bigger. Okay, so, how did I solve all this? So, we started, we actually created a Sequence Request Handler. This is the one that I'm going to use for my registration work, just as Sergei already explained in the earlier session, this is good for tracking objects. I will create my request, put them into one array, and now what do you see here for the registration? Just keeping like the last 15 registration results, and then I'm going to do some analysis on that and see if actually I'm holding still. I'm going to keep one buffer that I'm holding onto while I'm analyzing this. This is actually when I run my classification. And since this can be a longer running task, I'm actually going to run this on a separate queue. Alright, here's some code that I actually just used to open. This is actually like the little panel that we saw. But the important part is actually, "So, how do I setup my Vision task?" So, I'm going to do two tasks. I'm going to do a barcode request and I'm going to do my classification request. So, I set up my barcode request. And in my completion handler, I simple look at, "Do I get something back?" and also just since I'm only expecting one barcode, I only look at the very first one. Can I decode it? If I get a string out of it, I use that actually to look up -- that's actually how it worked with my barcodes to see then -- oh, yes, that is my training [inaudible]. Alright, so I add that as one of the requests that I want to run. Now, I'm setting up my classification. So, in this case, what I've done, I used my classifier and I'm loading simply this from my bundle and create my model from there. Now, I'm not using the code completion from Core ML in this case, because this is the only line of Core ML that I'm actually using my whole application, and it allows me to do my custom kind of error handling. But, you can choose also to use the code completion already from Core ML. Both are absolutely valid. Now, I create my Vision model from that. My Vision Core ML Model, and my request. And again, simply when the request returns, meaning I'm executing my completion handler. I simply look, "What kind of specifications did I get back?" And then I set this threshold here. Now, this is one that I empirically set against a confidence goal. I'm using 0.98. So, a 98% confidence that this is actually correct. Why am I doing that? That allows me to filter out actually when I'm looking at something, and maybe not quite sure what that is. Maybe we'll see that in a moment, actually what that means. So now, I have all my requests. When it comes to the time that I actually want to execute them, I created a little function here that actually mean, "analyze on the current image." So, when it's time to analyze it, I get my device orientation, which is important to know how am I holding my phone. I create an image request handler on that buffer that we currently want to process. And asynchronously, I let it perform its work. That's all I have to do basically for actually doing the processing with Core ML and barcode reading. Now, a few things, just okay. How do I do the scene stability part? So, I have a queue that I can reset. I can add my points into that. And then simply I had created a function that allows me to look basically through the queue of points that I've recorded. And then setting like, "Well, if they all sum together, only show a distance of like 20 pixels." Again, that's an empirical value that I chose. Then I know the scene is stable. So, I'm holding stable. Nothing is moving in front of my camera. And then comes our part of catch the output. So, this is the call that actually AV Foundation calls [inaudible] buffers from the camera. I'm making sure that I you know, hold onto the previous pixel buffer because that's what I'm going to compare against for my registration, and swap those out after I'm done with that. So, I create my Translational Image Registration Request with my current buffer. And then on the Sequence Request Handler, I simply perform my request. Now, I get my observations back. I can check if they are all okay. And add them into my array. And last but not least, I check if the scene's stable. Then I bring up my little, yellow box which is [inaudible] detection overlay. I know this is my currently analyzed buffer. And simply ask it to form its analysis on that. The one thing that you saw that I did at the end of the asynchronous call, in the currently analyzed buffer, I released that buffer. And you see here that I check if there is a buffer currently being used. Now, that allows me to make sure that I'm only really working on one buffer, and I'm not constantly queueing up more and more buffers while they're still running in the background, because that will starve the camera from frames. Alright, so when we run this, there's a few things I want to highlight actually. So, let me bring up Number 1, our console here a little bit on the bottom. And you can actually see, when I'm running this at first, and so, I'm going to run this right now here. Hopefully you should actually see something. You see that the confidence scores are pretty low because I'm actually [inaudible] over the [inaudible]. It's not really sure what I'm really looking at. The moment actually I point it at something that it should identify, boom, our confidence score goes really high and that's actually how I'm sure that this is really the object I want to show. Now, another thing I wanted to demo, let's look actually what happens in terms of the CPU. Alright, so right now, I'm not doing anything. I'm just showing my screen. So, when I'm just moving the camera around, scene is not stable, I'm using about 22% of the CPU. Now, if I hold it stable and actually the classifier runs, you see how the CPU goes up. And that's why I always recommend only run these tasks when you really need. Alright, that was a lot to take in. Let's go back to the slides and recap a little bit what we have just seen to now. So, go right to the slides. Okay, recap. First thing, how did we achieve our scene stability? We used a sequence request handler together with the VN Translational Image Registration Request, to compare against the previous frame. Out of that, we get our translation as of terms of an alignment transform which tells me the X and Y of like how the previous frame has shifted to [inaudible] the current one. Then we talked about that we want to only analyze the scene when it's stable. And for that, we created our VN Image Request Handler off the current buffer. And we passed in together both the barcode detection and the classification. So, that allows Vision to do its optimization underneath the covers and can actually perform much faster than if you would run them as separate requests. Next was the part about thinking about how many buffers do I hold in flight? And that's why I say manage your buffers. Some Vision requests, like these convolutional networks, can take a bit longer. And these longer running tasks are better to perform on a background queue, so that [inaudible] or whatever you do in the camera, can actually continuously running. But to do this particularly with the camera, you do not want to continuously queue up the buffers coming from the camera. So, you want to drop busy buffers. In this case, I said I only work with one. That's actually in my use case scenario works pretty well. So, I have a queue of 1, and that's why I simply held onto one buffer and check, as long as that one is running, I'm not queuing new buffers up. Once I'm done with it, I can reset and work on the next buffer. Now, you might ask, "Why am I using Vision when I can run this model in Core ML? It's a Core ML model." Well, there is one thing why this is important to actually use Vision. Let's go back and look what we saw when we looked at our model. It was the strange number of 299 by 299 pixels. Now, this is simply how this model is trained. This is what it wants to ingest. But our camera gives us something like 640 by 480 or larger resolutions if you want. Now, Vision is going to do all the work by taking these [inaudible] buffers as they come from the camera, converts it into RGB and scales it down and you don't have to write any code for that. That makes it much easier to drive these Core ML models for image requests through Vision. So, that was image classification. Next, we talk about object recognition. Now, a little warning. In this demo, actually a live croissant might actually get hurt on stage. So, for the squeamish ones, please look away. So, what we're using for our object recognition is a model that is based on this YOLO technique, You Only Look Once. That is a pretty fast-running model that allows us to get the bounding boxes off objects and a label for that. And it finds multiple of them in the screen. As you see in the screenshots. The advantage of those is that I get actually like where they are, but I won't get as many classifications as I can do with like our overall image classifier. The training is also a little bit more involved, and with that, I would like to actually refer you to the Turi Create session that was, I believe, yesterday, where they actually showed you how to train these kind of models. These models are also a little bit bigger. So, how does this look? Let's go over to our demo. Robot shop is closed. Time for breakfast. Alright. Let me bring up my quick template here and I have my new little application. It is my Breakfast Finder. And what do we see? Oh, we have a croissant, we have a bagel, and we can identify the banana. And see, they can all be kind of like in the frame. I'm going to detect them. So, some mention in these cooking shows, normally they show you how to do it, but then pull the prebaked stuff out of the oven. Well, this model, actually has been baked way before this croissant has been baked, and I can prove this. It's fresh. And still a croissant. Alright. It's fresh but still, got to chew. Let's look quickly how this looks in the code. So, what did I do differently? [Inaudible] actually in the terms of like setting up my request. All I have to do is use my Core ML model, just as I did in the previous example, create my Core ML request, and afterwards, I'm looking actually at simply, "How do I draw my results?" Now this is where we have something new to make this [inaudible] a little bit easier. And when we look at all of this, we get a new object that is the [inaudible] recognized -- Recognized Object Observation, and out of that, I get my bounding box, and my observation of like the labels. Now, that's one thing I would like to show you here. Let's run our application from here and I put a break point. Okay. Alright, we are now on our break point. So that I only look at the first label. So, what we are doing when we actually process these results, I'm going to take this, let's try object observation, labels. So, what you actually see is that I get more than one back. I get my bagel, my banana, coffee -- I didn't bring any coffee today. Sorry about that. And croissant, egg, and waffle. Now, they are sorted in the order of like the highest confidence on the top. Usually, this is the one that you're interested in. That's why I'm taking the shortcut here and just looking at the first one. But you always get all of the classifications back in terms of an array of the ones that we actually support in the model. Alright. That was our Breakfast Finder. Let's go back to the slides, and this time, I'm pushing the button. Good. So, we made this possible through a new API and that is our VN Recognized Object Observation. It comes automatically when we perform a Core ML model request, and if that model is actually using an object detector as a space. Like in this example, it is based on the YOLO based models. Now, you might say, "Well, I could have already run YOLO like last year. There were a bunch of articles that I saw on the web." But look at how much code they actually had to write to take the output of this model, to then put it into something that you can use. And here, we only had a few lines of code. So, it makes YOLO models really, really easy to use now. Let's go once more over this in the code. So, I create my model. From the model, I create my request. And in my completion handler, I can simply look at the area of objects, because we saw we can get multiple objects back. I get my labels from that, my bounding box, and I can show my Breakfast Finder. Now, there's one more thing I would like to highlight in this example. You saw how these boxes were a little bit jittering around because I was running the detector frame by frame, by frame, by frame. Tracking can often be a better choice here. Why? Tracking is much faster even in terms of like running, than actually these models would run. So, redetection takes more time than actually running a tracking request. I can use the tracker basically if I want to follow now an object on the screen because it's a lighter algorithm. It runs faster. And on top of it, we have temporal smoothing so that these boxes will not jitter around anymore and if you see some of our tracking examples, they actually move nicely and smoothly across the screen. If you want to learn more about tracking, the previous session from my colleague Sergei, actually talks about how to do all the implementation work of that. Alright, last but not least, let's enhance our Vision mastery and go into some of our fundamentals. Few things are important to know when dealing with the Vision framework. First and foremost, and this is a common source of problems, image orientation. Now, not all of our Vision algorithms are orientation agnostic. You might have heard early that we have a new face detector that is orientation agnostic. But the previous one was not. This means we need to know what is the upright position of the image? And it can be deceiving because if you look at it and preview on the final, my image looks upright, but that is not how it is stored on disk. There is something that tells us how the device is oriented, and this is called the EXIF Orientation. So, if an image is captured, that's normally in the sensor orientation, with the EXIF, we know what is actually upright and if you pass an URL into Vision as our input, Vision is actually going to do all that work basically for you and actually read this EXIF information from the file. But like when -- as we showed in the demos earlier, if I use my live capture feed, I need to actually pass this information in. So, I have to look at what does my orientation from my UI device current orientation and convert this [inaudible] to CG Image Property Orientation because we need it in the form of an EXIF orientation. Next, let's talk a little bit about our coordinate system. For Vision, the origin is always in the lower, left corner. And all processing is done in the up right -- if the image would be in an upright position, hence the orientation is important. All our processing is really done in a normalized coordinate space, except the registration where we actually need to know how many pixels [inaudible]. So, normalized coordinates means our coordinates go from zero, zero, to 1,1, in the upper right corner. Now what you see here is to that performed face and landmark detection request. And you will see that I get the bounding box for my face, and the landmarks are actually reported in relative coordinates to that bounding box. If you need to go back into the image coordinate space, we have utility functions and VNUtils like the VN Image [inaudible] from normalized way to convert back and forth between those coordinates. Next, let's talk about confidence scores. We touched a little bit on this already during our robot shot example. A lot of our algorithms can express how confident they are in their results. And that is kind of an important part to know when I want to analyze later on what I get out of these results. So, if I have a low confidence of zero, or do I have a high confidence of 1? Clicker. Here we go. Alright. Unfortunately, not all algorithms will have the same scale in terms of like how they report their confidence scores. For instance, if we look at our text detector, it pretty much always returns a confidence score of 1 because if it doesn't think there's text, it's not going to return the bounding box in the first place. But as we saw, the classifiers have a very large range actually of what this confidence score could be. Let's look at a few examples. In my first example, I used an image from our robot shop example and I ran my own model on it. And sure enough, it had a very high confidence, this is a stepper motor. Now, on this next examples, I'm going to use some of the models that we have in our model gallery. So, don't get me wrong. I don't want to compare the quality of the models. It's about like actually what did confidence they return and what actually want to do with this. So, what did this tell us basically when we want to classify this image? Well, it's not that bad, but it's really sure about it either. The confidence score of 0.395 is not particularly high, but yes, it has a sand part, there's some beach involved. So, that's usable basically as a result when I want to search for it, but might I label the image with that? It's probably questionable. Let's look at this next example. Girl on a scooter. What did the classifier do with this? Well, I'm not sure she's so happy to be called a sweet potato. Let's look at one more example. So, here's a screen shot of my code. What does the classifier do with that? It thinks it's a website. Computers are so dumb. So, some conclusions about our confidence scores. Does 1.0 always mean it's a 100% correct? Not necessarily. It will fill the criteria of the algorithm, but our perception, as we saw particularly with the sweet potato is quite different. So, when you create an application that wants to take advantage of that, please keep that in mind. Think of it. If you would write a medical application and saying, "Oh, you have cancer," that might be a very strong argument where you want to probably soften that a little bit depending on like how sure you can really be on the results. So, there are two techniques that you can use for this. As we saw in the robot shot example, I used a threshold on the confidence score because I really label the image and I you know, when it's actually filtering everything out that had a low confidence score. If on the other hand, I want to create a search application, I might actually use some of the images that I had and still show them, probably on the bottom of the search because there's still a valid choice, perhaps in the search results. As usual, we find some more information on our website. And we have our lab tomorrow at 3 p.m. Please stop by. Ask your questions. We're there to help you. And with that, I would first of all like to thank you all for these great application that you create with our technology. I'm looking forward to see what you can do with this. And thank you all for coming to WWDC. Have a great rest of the show. Thank you.  Hello everyone. My name is John Wilander. I am an engineer on the Safari and WebKit team and I'm here to present to you today how to secure web content. Or, as I like to call it, take the Swede's advice. So, you might be asking yourself, web content, that's a pretty broad term. Is this session really for me? Yes, it is for you. To start off, apps use tons of web content. It may be used for ads, login flows, splash screens. You might be using Safari View Controller to-- for parts of your app. You might have whole parts of your UI rendered in a web view. You might also have companion web apps that are supposed to be rendered in a web browser and that are sharing content or providing a joint experience across apps and web apps. So, really, yes, the session is for you. And security is important. You want to stay in business? You want to provide the best customer experience. You want to be ahead of your competition. You want to get this right so that when they don't they are the ones to fall, you stay in the market. OK. What are we going to cover today? We're going to have a look at first securing your transports. This is just a brief mentioning from me about basics that we need to do before we do the rest. If you don't secure your transports, most other bets are off. Then we'll go into a look at cross-origin lockdown. This is basically me showing you a bunch of technologies that are available to you in WebKit and in browser engines across the line that can help you get into more secure defaults for your web content. We would like to opt in for you, but we can't test your sites for you or your web content for you, so we need you to opt in. And then, as a motivation, if you feel, well, why should I care about these security mechanisms, I'll actually show you some attack scenarios and map them back to the security technologies so that you understand where they come into play and how they can defend your web content. So, let's head right in to securing your transports. This one should be familiar to you. So, this is about moving to HTTPS and WSS. WSS is for web sockets. Now, just the fact that HTTP and WS, the plain text equivalents, still exist is not an excuse to keep using them. You need to move to secure transports because that's what's going to ensure that the content that you are rendering in a browser or in your app comes from the server you expect it to come from and no one has fiddled around with the content in transport. So, moving to HTTPS for your main content. There are a couple of technologies that can help you get there. First on your to-do list-- except for just moving to HTTPS in general-- is strict transport security or HSTS. This is an HTTP response header you can send to the rendering engine, such as WebKit, and tell it, hey, my domain should always be loaded over HTTPS. If there's ever an HTTP request from me, just automatically upgrade it for me to HTTPS. Never making plain text connections to my server, please. And this is also a thing you can tell the rendering engine for how long it should remember this. So, typically at least half a year ahead in time. Now, you might also be loading content off of other servers, servers that are not under your control, not your domain name. There's also a header you can send to the browser engine called upgrade insecure requests, also an HTTP response header, which tells the browser, hey, go ahead and upgrade all those other links to content too, even if it's not from my server and even if they have not set strict transport security. So, those two things that will automatically upgrade to secure transports. Another thing you need to do is make sure that you mark your cookies secure. This is an attribute you add in your set cookie header, semi-colon secure. This means that you're telling the network stack this cookie should only ever be sent over a secure transport. If there for some reason is a plain text request for my server or to my server, don't send the cookie. And, since you should be using secure transports, all of your cookies should be marked secure. Final-- finally on your to-do list here for secure transports is specific for apps. You might have heard about app transport security or ATS. It is the default behavior for iOS apps, meaning that they should be using only secure transports for network traffic. Now, there is an opt out you can have in your info p list to say, hey, you know, for web content, I'd still like to do insecure requests. Don't do that. You should be saying no in this info p list and specifically for arbitrary web content loads. OK. Now we've secured the transport. Let's move on. We're now going to look at cross-origin lockdown. The reason why I call it lockdown is this is really you opting into saner, safer, more secure defaults for your web content. We're locking it down to more or less what it should have been all along and then you open up the specific things you want to do. So, let's move in and look at what are cross-origin loads anyway. What are we going to lock down? Well, web technologies have this powerful feature where you can weave together content from different servers and different origins. Different domain names, if you so will. An example of this is images. You can load images from any server on the web into your web content and that's a cross-origin load. You can also load scripts from other servers. Interestingly, those scripts actually execute with the same powers and privileges as your own scripts, so you need to be really careful what scripts you load off of other servers. They more or less own your web content. They are executing with the same powers as you. And, as a third example, iframes. You can embed full pages from other servers. Cross-origin, meaning from a different domain name. These are three examples of cross-origin loads. Now, we've had a security mechanisms-- security mechanism on the web for over 20 years that has provided basic protection for these cross-origin loads and it's called the same-origin policy. And it differentiates-- here you've got these examples with real domain names. So, you've got the example for loading the image, you've got a CDN where you loaded the script, and you may have a social widget, an iframe, coming from social.example here. And, just to mention, when I say .example it's just me being sure that I'm not talking about real websites here. If it's easier for you, you can think of it as .com or .org. Anyway, these are the cross-origin loads and the same-origin policy keeps track of where did I load this image from or where did I load this script from, where did I load this iframe from, so that it can provide basic protection. That basic protection means that your page in this example cannot reach into that iframe from social.example and read its bytes, read its contents, or write to its contents. The same-origin policy is guarding, saying this part of the page actually came from a different server, you're not allowed to just go straight in there and change things or read what the user happened to enter into that form, for instance. An easier way to think of the same-origin policy is just matching two pages, two tabs in a browser. One tab shouldn't be able to just go into another tab and see what the user's doing over there. It's the same-origin policy effectively doing its job there too. So, the lockdown. How do we lock this down properly? We've got the same-origin policy, but it's not good enough. So, you need to opt in to some more defaults. Starting out with subresource integrity-- I'll go through them and I'll mention them here. This is just a simple change in the markup for your script tags. The second part is content security policy. This is providing a full-on security policy for your whole page where you can say, hey, I only want to load scripts from over here. I don't want to have any frames from other servers, and so on. We'll look at that. And the third category here are simple server configurations. These are HTTP response headers that you send out telling the browser engine, hey, I want more secure defaults, I don't want these old legacy APIs that you've been supporting because I don't need them. Please help me here. So, let's dive into it and look at subresource integrity. Now, this is a perfectly plausible way of loading that framework from the CDN, your content delivery network. And that script tag is going to just contain the URL to fetch that script, right? As I mentioned, you're now relying on that CDN to provide you with the right script that's going to make your web content work the way it's supposed to. But actually, if they decide to, you know, bump the version or if something goes wrong on that server, they're serving you-- they're sending you the wrong script, your web content may be crippled. It may-- something may go wrong. It might not work the way it's supposed to. You know what script you're expecting, but what you're sending out when you're just sending a script tag like this-- looking like this-- is just telling the browser whatever you get back in JavaScript, execute it. A much better way is to say, no, I want to make sure it's the script I expect. And-- but you can use the integrity attribute in your script tags. In there you provide a check sum, in this case with the algorithm sha256, saying this is the check sum of the script I expect to get from my CDN, and only if I get something that matches this check sum, then you execute it. So, the browser engine will go through and make sure to compare that check sum before executing the code. Well, what happens if it doesn't match? What if they bumped the version or something went wrong and I got the wrong script? Now my web content doesn't work anymore. Well, you add a check. In this case, you've loaded or tried to load a framework. You checked did the framework load OK, do I have that object in my dom now. If not, go load it from my own server which I am in control of and I don't make changes to willy-nilly. That may reduce performance, but at least you have a still functioning web content or website. OK. That's sub-resource integrity. Let's move on to content security policy. I mentioned this is like setting a policy for your page, like what should be allowed here, and I actually think of it as an architecture policy. If you're a software engineer, you want to keep track of your dependencies. What am I allowed to do here? Who-- what can I load and what can't I load? So, this is again an HTTP response header. So, when you're sending out a page from your server, you add this header to the response, saying, hey, I'm going to run with a content security policy. I'm going to be in control of the content on my page. Then you start out by saying the default source for loading any kind of content from remote servers is myself. Only load from my server by default. Now you've locked it down completely and if there is any kind of content load in the page trying to pull something in from a server that's not yours, it will just be denied because WebKit will deny that because it doesn't match the policy you've set. And, as you see, I made a comment there. No inline. That is also part of the default behavior. Once you set it this way, you're also saying no inline scripting. Only load scripts from files. That's why I keep thinking of it as an architecture policy because you're separating logic into files, separating from markup, and separating from styling, which are also in files. CSS files. OK. Now, you wanted to load that script from cdn.example and if you just go with a default source self, then that's going to be locked down. Right? You can't do remote script loading. Well, there's a whitelisting mechanism in the content security policy. You just say script source, hey, let's open up for cdn.example because I actually want to load script from there. Now you can load from your own server and cdn.example, but nowhere else. Well, you might want to have that social media plugin, little widget in there in an iframe, well, you can add a frame source directive saying I want to be able to load iframes from social.example, but no one else gets to have an iframe on my page. Again, you're in control of that. And there's even a directive for the reverse case when your content is being loaded in an iframe on someone else's page. In this example, news.example is loading your content in an iframe. You can send out the directive-- the last directive here in content security policy frame ancestors. This is telling the browser, hey, check whosever framing me and all the way up to the top page and they all have to be on my list of what I allow. So, in this case, your web content has said news.example, I have a business deal with them, they're allowed to have my content in an iframe. OK. That's quite a lot, so let's just review this line by line quickly so we know what it's saying. Content security policy, you start out default source self. Lock everything down. Everything that's going to be loaded with a source attribute, like loaded from a server, needs to be from my origin, my domain. Then you open up. For script, you add what domain names do I allow scripts from. You say where do I allow myself to load frames from. You whitelist them. And, finally, if you're ever going to be in an iframe yourself under someone else's page, you put them in that white list for frame ancestors. OK. That's the content security policy. It has more knobs to turn and we'll also see in a couple of the attack scenarios a few flavors of these things. Moving on. We're going to have a look at cookies. Cookies are-- they actually were called magic cookies in the beginning of the web, and they really are magic in that we use them for so many things. And perhaps the most sensitive thing is they authenticate users. Once you have logged in with your credentials, you get a cookie that holds your session and that cookie, if it's moved or its stolen and moved to another browser engine in many cases can impersonate that user. So, you really need to protect your cookies. As I already mentioned, you should mark them all secure so that they never leak in plain text requests. But there are more things to do, right? So, looking at Http Only cookies. This is fairly old technology. I would guess this has been around for at least 15 years, the ability to say HTTP only. You might be familiar with the web API document.cookie. This is a way for JavaScript to read and write cookies. JavaScript is powerful. And, as I mentioned, if you load JavaScript from other servers, they have access to all of your user's cookies through this API. You should not be offering them that much power. Instead, you should protect your cookies, especially your authentication cookies that are effectively a way to log in as a user by marking them Http Only. This means that-- this cookie-- and this is the set cookie header you are seeing here where we are sending HTTP only, which means don't expose this cookie in the document.cookie API. So, JavaScript can now not see this cookie and of course not fiddle with it, steal it, or manipulate it. So, this is something you should be doing. We also have another thing, new technology as of the betas, SameSite cookies. Again, you are setting this on the set cookie header that you-- when you're setting the cookie in the browser. See here I've got the same site attribute equals strict. There's also a lax version of this, but what this is basically saying is this particular cookie, only send it when I am the page owner, when I am the main page, not when I'm embedded. When I own the whole user experience, I have the whole web view, then send this cookie. So, in the embedded case, if your web content is being embedded by someone you don't trust, the SameSite cookie will not be sent and your server can detect this. Aha! I didn't get that SameSite cookie, so either this user is not logged in or I'm being embedded and I'm not going to allow this thing to happen were that, for instance, a sensitive transaction. So, that's getting in control of being embedded. SameSite cookies. OK. Cross-origin lockdown. We're now going to look at really brand new technology. WebKit is first implementing these two new response headers and they're all about restricting who gets to load your resources. Cross-origin resource policy. We've already mentioned these two types of content that are allowed to be loaded cross-origin, images, and scripts. And they've been around since the origins of the web and therefore there has never really been a mechanism to say, hey, I actually don't want any other sites to load my scripts or load my images. Could I just only load them myself please? No, we've had this thing on the web where anyone can load images and scripts from any server. And now we're providing you with a control to tell the browser, hey, I don't want that. And it's called the cross-origin resource policy. In this particular example here, I'm giving it the directive same, meaning only my own site can load this. Only when my domain name is the main page domain and I'm pulling in this image, then load the image into the web content process. Same thing for script here. So, when someone else comes along and tries to cross-origin load your image or your script and you're sending this response header, those things won't be loaded. So, that puts you in control. And this is new technology. Finally, in the cross-origin lockdown, we've got the cross-origin window policy. Now we're really getting into the weeds of old web APIs. You might be familiar with the ability to open someone else's page-- or for that matter your own page-- in a new window. Not like a regular link navigation you click and you may open a new tab or navigate in the same tab, instead you're opening a full-on new window. Back in the days, we even saw this in a form of ugly pop-overs or pop-ups and those kind of things, but it's the old API window.open. And it has some weird things going on. So, if your content is opened by someone else, it looks to you as if, whoa, the user just went to my site, I got my SameSite cookie, everything looks fine, but the opener maintains a handle and kind of owns your window and has certain APIs to control that window that the opener can even navigate your window outside of your control. And with the new response header cross-origin window policy, you finally get to say no to this. Most sites are not in the practice of using this API and this kind of communication where the opener owns the window. Like, it's a very rare thing to see on the web, but it's there and it works that way by default. By sending this header, in this case deny, you're saying that handle should not be there. It's OK to open me, but you don't get to control me from that point. There is also a way added in this-- I'm not showing it here-- but you can say allow post message. You might want to be able to talk between these two windows without giving the opener control of your window and then you can say allow post message. OK. That's also new technology and now we've gone through the cross-origin lockdown. As you can see, that's not too hard. It's a set of response headers. It's the content security policy where you whitelist things, and it's the integrity attribute for script tags that's more or less what we're talking about here. I'll give you some motivation. We're going to look at some attack scenarios. What are you defending against and how to these security technologies fit in with these attacks? We're going to look at cross-origin attacks, speculative execution attacks-- some people may have heard about Spectre-- and we're going to look at window control attacks. Let's dive into the cross-origin ones. We're going to talk about cross-site scripting, when something goes bad with your CDN. And that one you kind of could tell that I was going for, right? And cross-site request forgeries. Cross-site scripting. Imagine you have built a messaging app. You have it as a native app with a web view for rich formatting, for instance, and you may have a companion web app that one can go or your users can go to in a regular web browser, and you can send messages to each other and maybe with very rich formatting. Most of your users are, of course, nice. Nice people sending each other nice messages. Some of them are angry sending angry messages, but every now and then comes along a malicious user who might send a message looking like this. It starts out nice, hello, but then it contains markup. Now, if you have made a mistake somewhere in the chain, in the native apps web view or on the website, where this will actually be interpreted as real markup instead of a message, suddenly the attacker, the malicious user here, is able to run scripts in the recipient's browser engine. In this case, trying to steal the cookies of that user. But remember, the scripts are running with full power. They can take over your whole UI, make requests on your-- on the victim user's behalf. Cross-site scripting, which this is an example of, is a bad thing. You don't want it to happen to you. Luckily, you have marked your authentication cookies Http Only, right? So, ha, they're not available to that malicious script. Now, Http Only cookies don't protect you against cross-site scripting. That script is still running. Http Only cookies protect your cookies. So, you need to do something more and what did you do? Content security policy. You started out with default source self, right? Saying, eh, only I get to run script on my page please. And that means by default no inline scripting and that whole class of bugs just goes away. Now there is no way to inject script into your web pages and make them execute because you set the policy for the page. OK. What could happen if your CDN gets compromised? This is bad and let's just hope it doesn't happen, but it can happen. So, you're loading the framework off of that CDN and you really rely on this because it's executing with the same powers as your own scripts. Now, what if they've been compromised and are redirecting that script request to evil.example, loading attack code. Now suddenly you thought you got a framework-- maybe they were nice enough to bundle in the framework, but they're also doing some nasty things. So, what did you do? Well, you whitelisted script source in your content security policy, so the browser will just refuse to execute any code coming from evil.example. OK. Oh, now the attacker needs to be a little bit more sneaky. He or she is probably going to change the script on the CDN server instead of redirecting it to the evil.example server. So-- and it's on your white list, so you're going to load and execute that script. But, you've got subresource integrity to save you. You are in control of the integrity of that script coming from a server that you don't own and if it doesn't match the check sum, you're going to load it off of your own server instead. So, that attack is also foiled. OK. That's compromised CDN. Final of the cross-origin attack, the cross-site request forgeries. This really ties into the last word, forgeries. These are-- this is the attacker trying to forge a request to look like something one of your users is trying to do, where they're actually not. So, we're back at the messaging app and you have this forum where the user is supposed to be able to send messages. And, frankly, when it comes down to it, it's going to be HTTP requests. Probably in HTTP post when that message comes into the server and look at the cookies, everything looks good, and then you'll send that message off to its recipient, right? Well, now comes along the malicious attacker that of course has phished one of your users, meaning sent a link and lured that user onto the attacker's site, evil.example, and is showing some really, really fascinating stats on cats. That makes the victim stay around for a little while at least. I mean you've got to check these stats out. What the victim user doesn't realize is that there's a hidden resource load with your web content sending a message on the victim user's behalf-- which of course automatically adds the cookies, that's just how the web works, right? If a request goes out on the wire, cookies are added, and to your servers this looks like I guess this user wants to send this message. That could be spam, that could be sending links for malicious software. It could be breaking up with a partner-- ah, OK, they won't go that far, but malicious software, we'll stay there. So, you don't want this to happen. Luckily, you have used same-site cookies. They will not be sent in the embedded case, which means that your server will detect I'm being embedded here. I don't allow the user to send messages from an iframe under some other page. I need to own the UI when I do this sensitive thing, such as sending messages, I'm just going to deny this request. This, again, puts you in control. OK. We have now covered cross-origin attacks and looked at how some of those security mechanisms help us. We're going to move on to something that's really exotic and new; Spectre attacks or speculative execution attacks. What I'm going to cover here is what speculative execution is, how it can be turned into an attack, and then we'll look at how to defend against it. So, now we're deep down in a CPU. It's executing code. It has reached a conditional. It's going to do something conditionally. It's asking itself, hey, I've got an array index here, x. Is this index OK? Is this in bounds or out of bounds for my array read? Because if it's out of bounds, I should do some error handling here, but if it's in bound I should go ahead and load that data in the array. Now, modern CPUs learn over time if they see the same code path over and over again that, hey, x is always in bound. It's always OK to load this array. Why don't I go ahead and speculatively do that, before I know whether it's OK or not? And this is how it works. It does this code-- takes this code path speculatively, loads that data, and then when the final answer comes in, in this case, oops, no, this is one of the rare cases where x was not OK, you shouldn't have been loading that data, it backs it out and takes the right code path. How is this OK? Well, it's OK because the speculative path is not committed. It is only done in advance so that we can take advantage of, oh, I'm already done with all of that work when the final answer to the original question comes in. And that's speculative execution. How can this be turned into an attack? It turns out that cache effects that are a result of the speculative execution can be monitored by malicious code and then they can leak that data read that was never supposed to happen. It was out of bounds, that load should have not happened, but it effects caches and by measuring caches the-- an attacker can leak that data that was not supposed to be loaded. OK. How does this map to web content? Remember the same-origin policy? This thing that has been protecting us for 20 years that makes sure that the main frame from one origin cannot reach into and read the bytes of some other frame, maybe your embedded content. With speculative execution attacks, we can no longer rely on the same-origin policy. There is now an ability-- if you can run scripts and you're in the same process, the same web content process as some other content, you can read that content through speculative execution attacks. OK. This is a big challenge for the web. I can tell you, I've been sitting in meetings with the other browser vendors. We are working super hard to try to fix this by default for the web, but we need your help. If you can opt into a bunch of things-- we've already looked at the defense mechanisms, right? If you can opt into these, you can help us by telling us, hey, I have sensitive content. I'm willing to have better defaults. You don't have to support these legacy APIs for me. Then we can be much more aggressive in fighting speculative execution attacks on your behalf. So, let's look at how this maps. The basic thing you're going to try to do here to fight speculative execution attacks is making sure your web content never ends up in the same web content process as a frame from evil.example. If they have a frame evil.example, they can execute code in your-- in the same process that your web content resides in and read your bytes. So, how do we make sure this never happens? OK. We've got these things. First, WKWebView. Now we're looking at apps using WebKit as a framework. I'm looking here at Safari, which is an example of an app that has been using WKWebView and we sometimes refer to it as Modern WebKit, has been using it for many, many years. And it provides excellent advantages. For instance, these three tabs, the evil.example, your web content, and webkit.org are all rendered in separate web content processes. Further, WKWebView provides you with a separate process for networking. This is where cookies get added and HTTP headers are parsed and et cetera. Now, speculative execution attacks all rely on being in the same web content process and doing the speculation thing to try to read things that the attacker wasn't supposed to be able to read. Well, if you have separated things into different processes, this attack doesn't work, right? You cannot do speculative execution attacks across process boundaries, so this is already a defense here and of course that's super important that the web content process cannot reach into the network process and do speculative execution attacks there. But if you are still using UIWebView, which, by the way, we are deprecating as of the betas, you're not in a very good place. Now, if you have evil.example and you have your web content and you've got the network stack and then you put this all in your app, from the view of a speculative execution attack, this all blends together. It's all the same process space. There are no guards for speculative execution attack in this scenario. You need to get off of UIWebView. If you move to WKWebView, you'll instead get this. Out of process, separated, badness happening in the evil.example content process cannot affect the rest. OK. We've moved to WKWebView. Let's look at how content security policy can help you. You might have an embedded social widget from social.example, but then you might have an injection attack, maybe it's the messaging thing again where someone can send markup in a message and you accidentally render it and they pull in an iframe from evil.example. Well, you may also have an ad there that you want to have there and then the ad network gets compromised and redirects to evil.example and then pulls in an iframe. And, remember, you never want to be in a web content process together with a frame from evil.example. So, what have you done? You have deployed content security policy and you've specified from where you allow frames and please don't allow evil.example. This is how you protect accidentally getting a frame in your process from evil.example. So, the reverse. Can evil.example pull your web content into the evil process? Of course. So, how do you protect against that? We are back at the content security policy and now we see frame ancestors. And this is what I referred to earlier with a slightly different flavor. You can actually say none and this means never iframe me. This content should never be in an iframe from someone else. This is my content. Only I get to render this thing. So, this-- if you place this, that-- the evil.example page cannot pull in your content in an iframe. And, of course, we do that blocking in the network process. That's important, right, because a speculative execution attack can only happen in the same process space, so we cut it off before it enters where evil.example can execute JavaScript. OK. HttpOnly cookies, do they really map to speculative execution attacks? Yes, they do. They're even more important here than to try to fight cross-site scripting. Why? Well, remember that old API document.cookie is the way to look at cookies from JavaScript? Now that we don't-- we can't rely on the same-origin policy anymore, evil.example can reach into an iframe from your content and actually read the cookies. Super bad. But if you mark them HttpOnly, WebKit will make sure to keep them in the network process. We don't need to move them into the web content process because they never need to be exposed in the document.cookie API. So, this automatically protects the cookies against speculative execution attacks. SameSite cookies. This is basically giving your server control of the embedded case. Again, evil.example has decide to try to pull off a speculative execution attack against your content. Loaded an iframe with your content, but the SameSite cookie doesn't get sent, so your server will know that, hey, I'm being embedded here. I'm not going to allow this. So, you're going to deny this whole resource load. Server side. And that way not end up in evil.example's process. Finally, to fight these attacks, cross-origin resource policy. Now, this was the new header I told you about where you can tell the web browser engine that, hey, images, scripts, and other things from my server, only I get to load them. So, if you say-- send this header for your images and your scripts on your server, we will make sure to block them from being loaded by evil.example and we'll do that block in the network process. Again, giving you that process separation so that a speculative execution attack cannot read your bytes. OK. We have the final attack category to have a look at and this is a pretty brief one. Window control attacks. So, we're going to look malicious window navigation, sometimes referred to as tabnapping, and then the defense. Again, we're back here at evil.example and instead of embedding your content, thus the window.open opens your content in a new window and you don't get to know. You don't get to decide, yeah, OK, someone opened my web page. And the SameSite cookie may be sent here. You might think everything's good, but evil.example could wait until your user loses focus of that page of yours and then use the handle to navigate it to a fraud page that looks like your page and asks the user to please log in again, of course stealing the credentials and sending them off to the attacker. This is one of the things that can happen if you leave the control up to the opener. Instead, you deploy the cross-origin window policy, you deny this thing, and there will be no handler for the attacker page to navigate your window with. So, that's the final defense in attack. It's time to take action. We're going to review what we've been looking at here, just so that you know what you need to go back to your office or your home and start working on. First, we have to secure these transports. Move to HTTPS and WSS. You should have secure cookies. You should also mark them HttpOnly. If you take the Swede's advice, just make all of your cookies HttpOnly and secure. And you need to migrate off of UIWebView to WKWebview. These are the basics. Then we have the defense mechanisms we've been going through. Content security policy will help you here with cross-site scripting and speculative execution attacks. HttpOnly cookies will provide you with some kind of protection against cross-site scripting, meaning the scripts can't steal the cookies, but it will definitely help you against speculative execution attacks because HttpOnly cookies are kept in the network process. Subresource integrity. This is the way you fight against a compromised CDN. Someone's executing code on your page that you never wanted to be there. SameSite cookies, this is a way to fight forged requests done invisibly by some phishing page to your server. You will know that because the SameSite cookie won't be sent when you're embedded and so you don't accept the request. It also happens against speculative execution attacks because if an evil page is trying to pull in your resources to be able to leverage an attack against them, again, SameSite cookies won't be sent and you can deny the load. Then we get into these new technologies, the cross-origin resource policy. This is the way you can say that images, scripts, and other cross-origin loads, deny them, I only want to load this on my page. And, finally, controlling windows is making sure that that handle goes away if someone else opens your web content in their own window. These are fairly easy to adopt, especially for you. You know your sites, you know your web content, you know your apps, you know how they're supposed to work. If we would just turn this on by default for all of you, it would probably break a few things, so you need to opt into this. And by opting in means you should check that the security works-- you can try to pull off an attack against your own content, make sure that, yeah, my content security policy is defending me, good, and of course test that the functionality that should be there is there. We have a blog. The WebKit project is an open source project. We have a blog where we talk about these technologies and other things. This is where we'll update you on those two last response headers. The cross-origin resource policy and the cross-origin window policy. Because we're still discussing those with the other browser vendors, trying to make sure that they're standard so that the same response headers work across all the browsers. And so there might be slight name changes or added attributes that we still don't have here. Also, it's so brand new that you actually won't have access to this until seed two or the public seed. We don't have full support for these in the developer seed yet. Quick shout out to these other sessions. Please come see us, including me, at the Safari, WebKit, and Password AutoFill Lab tomorrow at 2:00. You can come there and discuss with me or my co-workers how to deploy these technologies and make them work the best for you. And there's also a session on Friday that's more general on what's new in Safari and WebKit. Thank you.  Good afternoon, everyone! Welcome to Behind the Scenes of the Xcode Build Process. My name is Jake Petroules, and I'm an engineer on the Xcode Build System Team. And today we're going to be exploring the Xcode build process. I'll start by telling you all about Xcode 10's new build system which is written from scratch in Swift. And provides improved performance and reliability. We'll answer questions like what exactly happens when you press Command B? How is the build process structured? And how does Xcode use the information in your project file to determine how to model and orchestrate the build process? Next we'll venture into the compiler realm. And find out about Clang and Swift builds your source code into object files. We'll show how headers and modules work. See how the compilers find declarations in your code, and how the Swift compilation model fundamentally differs from that of C, C++ and Objective-C. Finally, we'll land at the linker which performs one of the final steps in the build process. We'll explain what symbols are and how they relate to your source code. And how the linker takes the object files produced by the compilers and glues them together to produce the final executable for your application or framework. By the way, we're going to use a small sample app called PetWall as a running example throughout this talk. It's just a small iOS app that displays photos of our pets. So let's first understand what the build process is and how it works when building a typical app like PetWall in Xcode. You can see here we've got an app target, a framework, a bunch of different source code files in Swift and Objective-C. And maybe this looks a bit like your own project. So when you build an app, there's a number of steps involved to go from the source code and resources in your project to the package that you're shipping to customers or uploading to the App Store for distribution. You have to compile and link source code. Copy and process resources like headers, asset catalogues and storyboards. And finally code sign and maybe even do some custom work in a shell script or a make file like building API documentation for your framework or running code linting and validation tools. Now most of these tasks in the build process are performed by running command line tools. Like Clang, LD, AC tool, IB tool, Code sign, and so on. These tools have to be executed with a very specific set of arguments and in a particular order based on the configuration of your Xcode project. So what the build system does for you is to automate the orchestration and execution of these tasks each time you perform a build. And since there can be tens of thousands of tasks or even more involved in a build process with a complex graph of interdependencies. It's definitely not something you want to be manually typing into the terminal 110 times a day. Let the build system do that for you. Now I mentioned that the tasks in the build process are executed in a particular order. Let's talk about how that ordering is determined and why it's important. The order in which build tasks are executed is determined from the dependency information that is the tasks, the inputs that a task consumes, and the outputs that it produces. For example, a compilation task consumes a source code file like PetController.m as input and produces an object file like PetController.o as output. Similarly, a linker task consumes a number of object files produced by the compiler in previous tasks. And produces and executable or library output. Like the PetWall executable that will go into our .app bundle. And hopefully you're starting to see a bit of a pattern emerge here. You can see how the dependency information flows through this graph structure which ultimately informs the order of execution. Now if you look at the compilation tasks in the graph as sort of like lanes of traffic. You can see hoe the compilation tasks are totally independent in their own lanes and can therefore run in parallel. And because the linker task takes everything else's input, we know that has to come last. So the build system uses dependency information to determine the order in which tasks should be run and which tasks can be run in parallel and we call this dependency order. Now that we've covered what the build process is, let's go into a little more detail on how it works. What happens when you press build? So the first step is for the build system to take the build description, your Xcode project file. Parse it, take into account all the files in your project, your targets and the dependency relationships. Your build settings, and turn it into a tree-like structure called a directed graph. And this represents all the dependencies between the input and output files in your project and the tasks that will be executed to process them. Next the low-level execution engine processes this graph, looks at the dependency specifications and figures out which tasks to execute. The sequence or order in which they must be run and which tasks can be run in parallel. Then proceeds to execute them. And by the way, our low-level build execution engine for the new build system is called llbuild. And it's open source and developed on GitHub. If you're interested in build systems development, feel free to check it out and see how it works. We'll have a link to this and other open source components related to the build process at the end of the talk. Okay, let's talk about discovered dependencies. Now since you can never have too much dependency information, the build system might actually discover more information during the task execution process. For example, when Clang compiles an Objective-C file, it produces and object file as you'd expect. But it can also produce another file that contains a listing of which header files were included by that source file. Then the next time you build, the build system uses the information from this file to make sure that it recompiles the source file if you change any of the header files that it includes. And you can see the dependency path through PetController.h, PetController.d, .n, all the way to the .o file. Now we've been talking a lot about how the build system's main job is to execute tasks. And of course the bigger your project, the longer the build process will take. So you don't want to run all of these tasks every single time you build. Instead, the build system might only execute a subset of the tasks on the graph. Depending on the changes you've made to your project since the previous build. We refer to this as an incremental build and having accurate dependency information is very important in order for incremental builds to work correctly and efficiently. Now we talked about how changes affect the build system, and how they relate to incremental builds. So how does the build system actually detect changes? Each task in the build process has an associate signature which is the sort of hash that's computed from various information related to that task. This information includes the stat infor of the task's inputs like file paths and modification time stamps. The command line indication used to actually perform the command. And other task-specific metadata such as the version of the compiler that's being used. The build system keeps track of the signatures of tasks in both the current and the previous build. So that it knows whether to rerun a task each time a build is performed. If the signature of any given task is different than the signature that it had in the previous build, then the build system reruns that task. If they're the same, then it's skipped. That's the basic idea behind how incremental builds work. So now that we have an idea of what the build process is and how it works, how can you help the build system do its job? Let's go back to basics for a moment. A build process is a series of tasks executed in a particular order. But remember that the build is represented as a directed graph. So we don't want to think about the order in which these tasks should be executed because that's the build system's job. Instead, as developers, we need to think about dependencies between tasks and let the build system figure out how to best execute them according to the graph's structure. This lets the build system order tasks correctly and parallelize where possible in order to take full advantage of multicore hardware. So where do dependencies come from? For certain tasks, dependency information comes from knowledge built into the build system. The build system ships with rules for the compiler, the linker, the asset catalogue and story board processors and so on. And these rules define what kind of files are accepted as inputs as well as what outputs are produced. There's also target dependencies which roughly determine the order in which targets are built. And in some cases, the build system can compile sources of different targets and parallel. Previously in Xcode, when a target was built, it required the compilation of the entire dependent target to be completed before it could start. In Xcode X's new build system, targets can start building sooner. This means that your compile sources phase can start earlier providing your some parallelization for free. However, note that if you're making use of any run script phases, those script phases will need to complete before this parallelization can take effect. Somewhat related to target dependencies are implicit dependencies. For example, if you list a target in your link library with binaries build phase and implicit dependencies are enabled in the scheme editor, that's on by default, by the way, the build system will establish an implicit dependency on that target even if it's not listed in target dependencies. Next up are build phase dependencies. In the target editor, you'll notice that there's a number of build phases. Copy headers, compile sources, copy bundle resources and so on. The tasks associated with each of these phrases are usually running groups according to the order in which the phases are listed. But the build system might ignore that order if it knows better. Like if you have a link library, linked binary with library space ordered before compile sources. And note that there's cases where having the wrong build phase order can cause build issues or failures, so make sure to understand your dependencies and verify that your build phases are in the right sequence. There's also scheme order dependencies. If you have the parallelize build check box enabled in your scheme settings, you get better build performance and the order of your targets in your scheme doesn't matter. However, if you turn parallelize build off, Xcode will attempt to build their, your targets in the order you listed them in the build action of the scheme one by one. Target dependencies still have higher priority in determining which targets build first. But otherwise, Xcode will respect that ordering. Now it might be tempting to use this as it gives you a predictable build order even if you haven't set your dependencies correctly. But you're sacrificing a lot of parallelization when you do this and slowing down your build. So we recommend that you leave the parallelize builds checkbox enabled, set up your target dependencies correctly and don't rely on ordering. Lastly, dependency information comes from you, the developers. If you're creating custom shell script build phases or build rules, make sure to tell the build system what its inputs and outputs are. This lets the build system avoid rerunning the script tasks unnecessarily. And can help make sure that they are executed in the right order. You can define the inputs and outputs in the run script phase editor. And the paths of these files will be made available to your script as environment variables. Don't rely on auto-link for target dependencies in your project. The client compiler has a feature called auto-link which can be enabled using the link frameworks automatically build setting. This setting allows the compiler to automatically link to the frameworks corresponding to any modules you import without having to explicitly link them in your link library's build phase. However, it's important to note that auto-link does not establish dependency on that framework at the build system level. So it won't guarantee that the target you depend on is actually built before you try to link against it. So you should rely on this feature only for frameworks from the platform STK. Like Foundation and UIKit since we know those will already exist before the build even starts. For targets in your own projects, make sure to add explicit library dependencies. You might also need to create project references by dragging and dropping another Xcode project into your project's file navigator in order to reveal the targets of other projects you depend on. In conclusion, with accurate dependency information, the build system can better parallelize your builds, and help ensure that you get consistent results every time so that you can spend less time building and more time developing. For more information on how to speed up your builds and make the most of all those cores in your shiny new iMac Pro, we'd recommend that you check out Building Faster in Xcode Session. And with that, I'm going to hand it over to Jurgen, who's going to take you into the realm of the compiler. Thank you, Jake. And now we're going to talk about what happens behind the scenes when Xcode build system invokes the Clang compiler. Hello, everyone. My name is Jurgen, and I'm a Compiler Engineer on a Clang Frontend Team. Today I'm going to talk about two features you might not know about. The first feature is called header maps and how we use them to communicate information from the Xcode build system to the Clang compiler. The second feature is called Clang modules and how we use them to speed up your builds. Some of you might only use Swift by now. But I want to tell you, Swift uses Clang behind the scenes. So there might be something interesting for you here, too. So what is Clang? Clang is Apple's official C compiler and all the other C language family such as C, C++, and of course Objective-C which is used for the majority of all frameworks. As Jake mentioned in the beginning, the compilers invoked [inaudible] for every input file and it creates exactly one output file which is then later consumed by the linker. If you want to access APIs from the iOS, or you want to access implementations from your own code, you usually have to include something what is called a header file. A header file is a promise. You promise somewhere else this implementation exists. And they usually match. Of course if you update only the header file-- only the implementation file and forget the header file, you broke your promise. Very often this doesn't break the compile time because the compiler trusts your promise. Usually this breaks during link time. The compiler usually does include more than one header file, and this is done for all compiler invocation. So let's have a look at our example application and see how we deal with header files. This is PetWall, it's a mixed-language application. The application itself is written in Swift. And it uses a framework that's written in Objective-C. And it has a support library aesthetic archive that use-- that's written in C++. Over time, our application grew. And we start to reorganize it so we can find things easier. For example, we moved all the cat-related files into a subfolder. We didn't have to change any of our implementation files. And it still works. So it makes you wonder how does Clang find your header files? Let's look at a simple example. This is one of our implementation files we use in our code and we include our header file called cat.h. How can we figure out what Clang does? Once thing you could do is you can go into the build logs, look what the Xcode build system did to compile this particular file. And copy and paste that invocation. You drop it into a terminal and add the -v option. Dash-v is symbol for verbose. And then Clang will tell you lots of information. But let's just concentrate on the one that matter which is the search paths. I say search paths and many of you might expect to see now here search paths that point back to your source code. But it's not how this works. Instead you will see something called headermaps. Headermaps are used [inaudible] by the Xcode build system to communicate where those header files are. So let's have a look at those. Let's have a look the most important two file-- headermap files we care about. The first two entries simply append the framework name to your header. Those two headers in the beginning are public headers. I say you shouldn't rely on this feature. The reason is we keep this there to keep existing projects working but there might be issues down the road with Clang modules so we suggest that you always specify the framework name when you include a public or private header file from your own framework. Third entry is a project header. In this case, this is not required. And the whole purpose of the headermap is to point back to your source code. As you can see, we do the same thing for the public and private headers. We always point back to your source code. We do this so that Clang can produce useful error and warning messages for the files in your source directory and not a potential copy that might be somewhere else in the build directory. Since many people were not aware that we use headermaps, you run into certain issues. A very common one is that we forgot to add the header to the project. It is in the source directory, but it is not in the project itself. So always add your headers to the project. Another issue is that if you have headers with the same name, they might shadow each other. So always try to use unique names for your headers. This also applies to system headers. If you have a local header in your project that has the name as a system header, it will shadow the system header so you should try to avoid this. Talking about system headers. How do we find those? Another example from our PetWall. Here in this case we include Foundation.h header file which is in our SDK. We can do the same thing we did before when you were looking for our own header files. But now we're looking for the system header. It's just that headermaps are only for your own headers. So we can ignore them. Let's focus on the include path [inaudible] matter. So at default, we always look in two directories in the SDK. the first one is user include, the second one is system library frameworks. Let's have a look at the first one. It's a regular include directory. So the only thing we have to do is we have to append the search term. In this case, Foundation/Foundation.h. And we won't find the header because it's not there. But it's okay. Let's try the next entry. Let's look into system library frameworks. This one is a frameworks directory that means Clang has to behave a little bit different. First of all, it has to identify what is the framework and check if the framework exists. After that, it has to look in the headers directory for the header file. This case, it finds it so it's fine. But what happens if we don't find the header? For example, we have a bogus header that doesn't exist. It obviously fails to find it in the headers directory. But the next, it will also look in the private headers directory. Apple doesn't ship any private headers in its SDK. but your project, your frameworks might have public and private headers. So we always will look there too. Since it's a bogus header, it's not there either. Interesting now, we actually will abort the search now. We will not keep searching in other search directories. The reason is we already found the head-- we already found the framework. Once you find the framework, we expect to find the header in this framework directory. If you don't find it, we complete abort the search. If you're curious how your implementation file looks like, after all those headers got imported and preprocessed. You can ask Xcode to create a preprocessed file for you, for your implementation file. This will create a very large output file. So how big is that file? Let's use a very simple example. Foundation.h is a very fundamental header, fundamental header to our system. It is-- you are very likely to import this header either directly or indirectly for some other header file. That means every compiler invocation you're most likely going to have to find this header. At the end of the day, Clang has to find and process over 800 header files for the single include statement. That's over 9 megabyte of source code that has to be parsed and verified. And that happens for every compiler invocation. That's a lot of work, and it's redundant. So can we do better? One of the features you might be aware of is called precompiled header files. That's one way to improve this. But we have something better. A few years back, we introduced Clang modules. Clang modules allow us to only find and parse the headers once per framework and then store that information on disk so it's cached and can be reused. This should have improved your build times. In order to do that, Clang modules must have certain properties. One of them, one of the most important one is context-free. What do I mean by context-free? You can see here two code snippets. In both cases we import the PetKit module. But we have two different macro definitions beforehand. If you would use a traditional model of importing those headers, that means they're [inaudible] included. The preprocessor would honor this definition and apply it to the header file. But if you would do that, that means the modules would be different for each header case, and we couldn't reuse it. So if you want to use modules, you cannot do that. Instead, the module will ignore all those context-related information that allows us to be, that allows it to be reused across all implementation files. Another requirement is modules have to be self-contained. That means they have to specify all the dependencies. Which also has a nice advantage for you because it means once you import a module, it will just work. You don't have to worry about adding any additional header files to make that import work. So how do we know or how does Clang know it should build a module? Let's look at a simple example here NSString.h. First Clang has to find this particular header in the framework. And we already know how to do that. And it's the Foundation.framework directory. Next the Clang compiler will look for a modules directory and a Module Map relative to the header's directory and it's there. So what is a Module Map? A Module Map describes how a certain set of header files translate onto your module. So let's have a look. The Module Map is actually very simple. This is the whole Module Map for foundation. That's it. It obviously describes what is the name of the module which is, Foundation. And then it also specifies what headers are part of this module. You'll notice there's only one header file here, only Foundation.h. But this is a special header file. This is the umbrella header which is also marked by the special keyword umbrella. That means Clang has also to look into this particular header file to figure out if NSString.h is part of the module. And yeah, it's there, okay. So now we have found out NSString.h is part of the foundation module. Now Clang, now Clang can upgrade this textual import to a module import and we know we have to build the foundation module to do that. So how do we build the foundation module? First of all, we create a separate Clang location for that. And that Clang location contains all the header files from the foundation module. We don't transfer any of the existing context from the original compiler invocation. Hence, it's context-free. The thing we actually transfer are the command line arguments you passed to Clang. Those are passed on. While we build the foundation module, we mod-- the module itself or the framework, the framework itself will include additional frameworks. That means we have to build those modules too. And we have to keep going because those might also include additional frameworks. But we already can see there is a benefit here. Some of those imports might be the same. So we can always start reusing that module. All those modules are [inaudible] to disk into something called a module cache. As I mentioned, the command line arguments are passed on when you create that module. That means, that means those arguments can affect the content of your module. As a result, we have to hash all those arguments and store the modules we created for this particular compiler invocation in a directory matching that hash. If you change the compiler arguments for different limitation file, for example you'd say enable cat, that is a different hash and that requires Clang to rebuild all the module's inputs into that directory matching that hash. So in order to get the maximum reuse out of the module cache, you should try to keep the arguments the same, if possible. So this is how we find and build modules for system frameworks but what about your frameworks? How do we build modules for those? Let's go back to our original cat example, and this time we turn on modules. If we would use a headermap again, the headermap will point us back to the source directory. But if you look at that source directory, we have a problem now. Theirs is no modules directory. It doesn't look like a framework at all and Clang doesn't know how to do this in this case. So we introduced a new concept to solve this, and it's called Clang's Virtual File System. It basically creates a virtual abstraction of a framework that allows Clang to build the module. But the abstraction basically only points to the files back in your directory. So again, Clang will be able to produce [inaudible] and errors for your source code. And that's how we build modules for, when you have frameworks. As you remember, in the beginning I mentioned there might be issues if you don't specify the framework name. So let me give you an example where this can go wrong. This is a very simple-- very simple code example. We only have two imports. The first import imports the pet, PetKit module. The second import, you and I know this is also part of the PetKit module, but Clang might now be able to figure that out because you didn't specify the framework name. In this case, it might be possible you get duplicate definition errors. That's basically, that basically happens when you import the same header twice. Clang works very hard behind the scenes to fix the most common issues that happen like this. But it cannot fix all of them. And this is just a simple example. Let's just make a little tweak. Let's change the context. Now the module import won't be affected by this at all because as I said, we ignore the context. The cat import is still a textual inclusion of the header which will observe this change. So now you might not even have duplicate definitions. You might even have contradictory definitions. Which can mean we cannot fix, Clang cannot fix this for you. So as I recommended in the beginning, always specify the framework name when you import your public or private headers. And now I'm handing it over to Devin who's going to talk about Swift and how Swift uses Clang modules. Thanks, Jurgen. We're now going to dive into the details of how Swift and the build system work together to find declarations across your project. To recap some of what Jurgen told you, Clang compiles each Objective-C file separately. This means if you want to refer to a class to find in another file, you have to import a header that declares that class. But Swift was designed to not require you to write headers. This makes it easier for beginners to get started with the language. And avoids you having to repeat a declaration in a separate file. However this does mean that the compiler has to perform some additional bookkeeping. I'm going to tell you how that bookkeeping works. Let's return to our example, PetWall app. The app has a view in ViewCcontroller, written in Swift. An Objective-C app delegate. And Swift unit tests. In order to compile even just this top PetViewController part of the file, the compiler has to perform four different operations. First, it has to find declarations. Both within the Swift target and also coming from Objective-C. Further, it has to generate interfaces describing the contents of the file. So that its declarations can be found and used in Objective-C and in other Swift targets. In the rest of this section of the talk, I'm going to walk through this example to illustrate each of these four tasks. Let's start with finding declarations within a Swift target. When compiling PetViewController.swift, the compiler will look up the type of PetView's initializer so that it can check the call. But before it can do that, it needs to parse PetView.swift and validate it. To make sure that the declaration of the initializer is well formed. Now, the compiler's smart enough to know that it doesn't need to check the body of the initializer here. But it does still need to do some work just to process the interface parts of the file. What this means is that unlike Clang, when compiling one Swift file, the compiler will parse all the other Swift files in the target. To examine the parts of them that are relevant to the interfaces. IN Xcode 9, this resulted in some repeated work in repeated build-- in incremental debug builds. Because the compiler compiled each file separately. This enabled the files to be compiled in parallel, but it forced the compiler to repeatedly parse each file. Once as an implementation to produce a .o, and multiple times as an interface to find declarations. Xcode 10 reduces this overhead. It does so by combining the files into groups that share as much work as possible. While still allowing maximum parallelism. This reuses parsing within a group and only repeats work across groups. Since the number of groups is typically relatively low, this can significantly speed up your incremental debug builds. Now, Swift code doesn't only call other Swift code. It can also call Objective-C. Returning to our PetWall example app, we can see that, that this is crucial since the system frameworks such as UIKit are written in Objective-C. Swift takes a different approach than many other languages. And it doesn't require you to provide a foreign function interface. This is where you would have to, for example, write a Swift declaration for each Objective-C API you want to use. Instead, the compiler embeds a large part of Clang inside of it and uses it as a library. This makes it possible to import Objective-C frameworks directly. So where do Objective-C declarations come from? The compile-- the importer will look in headers depending on the type of the target. In any target, when you import an Objective-C framework, the importer finds declarations in the headers exposing Clang's module map for that framework. Within a framework that mixes Swift and Objective-C code, the importer finds declarations in the umbrella header. This is the header that defines the public interface. In this way, Swift code inside the framework can call public Objective-C code in the same framework. Finally, within your app and unit tests, you can add imports to the target's bridging header. To allow declarations from them to be called from Swift. Now when the importer brings in declarations, it often changes them to make them more idiomatic. For example, it will import Objective-C methods that use the NSError idiom as throwing methods using Swift's built-in error handling language feature. In particular, it will drop parameter type names following verbs and prepositions. For example, the drawPet atPoint method has the word pet. For a parameter of type pet following the verb draw. And similarly the word point for a parameter of type CGPoint following the preposition at. These words are omitted in Swift when the method is imported as simply draw at. So how does this work? Well, you might be surprised to know that the compiler contains a list of common English verbs and prepositions. Because it is just a hard-coded list and human language is messy, sometimes it's missing words. Furthermore, in order to match Swift's naming conventions, the importer will also rename methods to remove words based on the part of speech. For example the verb feed is not in the list, and so feedPet is not imported as feed as we might expect. When this happens, you can use the NS Swift Name annotation to have the compiler import the method exactly as you want. Now if you want to check to see how your Objective-C header will be imported into Swift, you can always go to Xcode's related items popup. This is in the upper left-hand corner of the source editor. If you select generated interfaces, it will show you how the interface will look in different versions of Swift. So that's how Swift imports Objective-C. But what about the other direction? How does Objective-C import Swift? The answer is that Swift generates a header that you can pound import. This allows you to write classes in Swift and call them from Objective-C. Let's see how this works. The compiler will generate Objective-C declarations for Swift classes extending NSObject and methods marked at obc. For apps in unit tests, the header will include both public and internal declarations. This allows you to use internal Swift from the Objective-C parts of your app. For frameworks, however, the generated header provides only public declarations. Since it's included in your build products and it's part of the framework's public interface. On the right, you can see that the compiler ties the Objective-C class to the mangled name of the Swift class which includes the name of the module, PetWall. Now I'm going to tell you about modules in a bit, and Louis will tell you about mangling. But for now, the thing to know is that this prevents a conflict in the runtime when the two modules define a class with the same name. You can tell Swift to use a different name for the class in Objective-C by passing an identifier to the obc attribute. But if you do this, you're responsible for making sure the names don't conflict. Here I've used the PWL prefix so to reduce the likelihood of a conflict. With this change, I can refer to the class as PWLPetCollar in Objective-C. The compiler takes a similar approach to generating interfaces to other Swift targets. To do this, Swift builds on Clang's concept of a module, which Jurgen told you about. And more deeply integrates it into the language. In Swift, a module is a distributable unit of declarations. And to be able to use those declarations you have to import the module. You can import Objective-C modules. XEtest, for example. And in Xcode each Swift target produces a separate module. Including your app target. This is why you have to import your app's main module in order to test it from your unit tests. When importing a module, the compiler deserializes a special Swift module file to check the types when you use them. For example, in this unit test, the compiler will load the PetViewController parts of the PetWall Swift module to make sure that you're creating the controller correctly. This is similar to how the compiler finds declarations within a target as I showed you earlier. Except that here, the compiler loads a file summarizing the module rather than parsing Swift files directly. The compiler produces this Swift module file a lot like a generated Objective-C header. But instead of text, it's a binary representation. It includes the bodies of inlineable functions much like static inline functions in Objective-C or header implementations in C++. However, one thing to be aware of is that it does include the names and types of private declarations. This allows you to refer to them in the debugger which is really handy. But it does mean that for example, you shouldn't name a private variable after your deepest, darkest secret. For incremental builds, the compiler produces partial Swift module files and then merges them into a single file that represents the contents of the entire module. This merging process also makes it possible to produce a single Objective-C header. In many ways, this is similar to what the linker does when it smooshes together your object files into a single executable. And to tell you more about that, I'll hand it off to Louis and the linker. Louis? Thank you, Devin. Hi, I'm Louis Gerbarg. I work on the linker. Which is one of the final steps in the Xcode build process. So let's get to it. So first off I want to go over what we're going to talk about. We're going to talk about what a linker is. We're going to talk about the input that it takes which are dylibs and object files and what they are. And we're also going to talk about symbols which are what those contain. So at the end of this, I'm going to tie this together with an act-- with an example because this is pretty dense stuff. So if it seems a little confusing. Hold on, and hopefully we'll make it clearer. So what is the linker? It's like I said, it's one of the final processes in the build. And what we do is we combine all of these .o files that have been built by the two compilers into an executable. All it does is move and patch code. It cannot create code, and this is important and I will show that in the example. But we take these two kinds of input files. The first one being dylibs or being libraries. There are multiple-- the first one being object files. Which are what come out of your build process. And the second one being libraries which consist of several types including dylibs, tbd's, and .a files or static archives. So what are symbols? A symbol is just a name to refer to a fragment of code or data. These fragments may refer to other symbols which you would see if you write a function that calls another function. Symbols can have attributes on them that affect how the linker behaves. And there are a lot of these. I'm just going to give you one example which is a weak symbol. So a weak symbol is an annotation on a symbol that says it might not be there when we run the syst-- when we build the, when we run the executable on the system. This is what all the availability markup that says this API is available on iOS 12. And this API's available on the iOS 11. This, that's what it all boils down to by the time it gets to the linker. So the linker can determine what symbols are definitely going to be there versus what symbols it may have to, it may have to deal with at runtime. Languages can also encode data into the symbol by mangling it, as Devin mentioned earlier. And you'll see that in both C++ and Swift. So we have symbols which are these names referring to code and data. So the compilers generate object files. And those object files are just collections of those code and data. They are not executable. While they are compiled code, they aren't finished. There are bits missing which is what the linker's going to glue together and fix up. Each fragment in one of those files is represented by a symbol. So for the printf function, there's code represented by a symbol. For any of the functions in PetKit which we're going to see in a second, the same thing. And fragments may reference undefined symbols. So if your .o file refers to a function in another .o file, that .o file's undefined. And the linker will go and find those undefined symbols and link them up. So I said that object files are the output of your compiler actions. So what are libraries? Libraries are files that define symbols that are not part of the target you are building. So we have dynamic libraries, and those are Mach-O files that expose code and data fragments for executables to use. Those are distributed as part of the system. That's what our frameworks are. And a number of you also use your own frameworks. There are also TBD files, or text-based dylib stubs. So what are those? Well, when we made the SDKs for iOS and macOS, we had all these dylibs with all these great functions like MapKit and WebKit that you may want to use. But we don't want to ship the entire copy of those with the SDK because it would be large. Ant the compiler and linker don't need. It's only needed to run the program. So instead we create what's called a stub dylib where we delete the bodies of all of the symbols and we just have the names. And then once we did that, we've made a textual representation of them that are easier for us to use. Currently, they are only used for distributing the SDKs to reduce size. So you may see them in your project, but you don't have to worry about them. And they only contain symbols. Finally, we have static archives. So static archives are just collections of .o files that have been built with the AR tool or in some cases the lib the lib tool which is a wrapper for that. And according to the AR [inaudible] page, the AR utility creates and maintains groups of files combined into an archive. Now that may sound a lot like a TAR file or a ZIP file, and that's exactly what it is. In fact, the .a format was the original archive format used by UNIX before more powerful tools came around. But the compilers of the time and the linkers of the time natively understood them, and they've just kept using them. So it really is just an archive file. One thing worth noting is they also prenate dynamic linking so back in those days, all of the code would be consid-- would be distributed as archives. Because of that, you might not want to include all of the C library if you're using one function. So the behavior is if there's a symbol in a .o file, we would pull that whole .o file out of the archive. But the other .o files would not be brought in. If you're referencing symbols between them, everything you need will be brought in. If you're using some sort of non-symbol behavior like a static initializer, or you're re-exporting them as part of your own dylib, you may need to explicitly use something like force load or all load to the linker to tell it bring in everything. Or these files, even though there's no linkage. So let's go through an example to try to tie this altogether. So up on stage we have an example of a playSound function because what fun would be looking at pets without listening to them? And you know it calls playSound. You know, there's a function on cat that calls playSound. That seems pretty simple. So let's look at the assembly that generates. So here's cat.o that comes out of it. Now if we look, we can see we have the string purr.aac which would be our AAC sound file. And that gets copied into cat.o. You'll note that the name purr file does not appear in there anywhere. The reason is, that's a static. And those of you who are familiar with C, that means it's a nonexported name. Nobody else can refer to it. Since nobody else can refer to it, we don't need it. It's not included. And then we see Cat purr becomes an actual symbol: dash, open bracket, Cat purr, close bracket. Pretty much what you'd expect. And then we see we actually have to get this variable we're going to pass into playSound. And you'll note there are two instructions referring to that. And that's because we don't know where this string is going to end up in the final executable. We don't have a concrete address for it. But we know that on RM64 which is what this assembly is, it could take at most two instructions. So the compiler leaves us with two instructions. And it leaves the symbolic offset, the symbolic values page and page off that the linker will come in and fix up later. Finally, now that we've loaded that string into x0, we can call playSound except for we're not calling playSound. We're calling underbar underbar z9playSound PKc. Now what is that? That's a mangled symbol because if you note, it says cat.mm. This is Objective-C++. And playSound is actually a C++ function. So if you're not familiar with those, you can go to terminal and you can actually use tools. So if you run Swift-demangle and pass in a symbol, it will attempt to demangle it. It didn't work. It's not a Swift symbol. But C++ filts, which is the C++ demangler shows us that this is in fact a symbol for playSound. But not only is it playSound, we know it takes one argument. And that one argument is a const char star because C++ encoded that extra data into the mangled symbol. Okay, so now we have a .o. In fact, we're building a project, we'll have a lot of them. So what are we going to do with that? Well, first off, the build system is going to pass all of the .o's in as inputs to the linker. And the linker's going to start by creating a file to put them in. In this case we're building PetKit which is an embedded framework inside of PetWall. And so we'll just start by copying. We'll create what's called a text segment. Which is where we keep all of the code for an application. And we'll take the cat.o and we'll copy it in. But we're going to split it into two sections. One for that string, and one for the executable code. But we now know the absolute addresses of these things so the linker can go and rewrite cat.o to load from a specific offset. You'll notice that the second instruction just went away. We replaced it with what, with a null instruction that does nothing. But we cannot remove the instruction because we can't create or remove code. It would mess up all of the sizing that we've already done. So instead it just is removed. It, it's just replaced with a no operation. Finally we branch. Okay, but we branch and what are we going to do? Because we have this undefined symbol? Well, we're going to go and keep looking through, and all of our .o's have been included at this point. So we'll start looking at the static archives [inaudible] and PetSupport.a is there. And inside of PetSupport.a, we have a couple of files including PetSounds.o. And if we look, we see that matching file-- that matching symbol for playSound. So we pull it in. We'll note that we do not pull in PetCare.o. Because that .o file did not have any symbols that were referred to by any of the other portions of the application. Okay, so we pulled that in, but now that needs, open which we didn't define. You'll see in the, in the, the conversion we copied in. We actually replaced it with something called open$stub. Now why did we do that? Well we kept looking, and we found a copy of open. And that copy of open is in the lib system TBD file. And so we said I know this is part of the system library. I'm not going to copy this into my application. But I need to put enough information in the information to make it so that it can call this. So we create a fake function that's just a template where we replaced a function with whatever we're pulling out of lib system which is open in this case. And if you look in that function, it actually loads from a pointer, open$ pointer. And jumps to it. So we just need a function pointer, just like any normal C function pointer. And we'll create that down in the data segment which is where global variables would be if you had any globals. But it's just set to zero. So if we dereference null and jump to it we're going to crash. So we then add what's called a link edit segment. And the link edit is metadata that the linker tool uses to leave information for the operating system and what's called the dynamic linker to fix this up at runtime. And for more information on that you should look at the Optimizing App Startup Time session from 2016. So I just want to go back over what we went through today. So Jake talked about the build system and how you can use dependencies to optimize your built process for multi-core. Jurgen walked through Clang and how it finds headers for you. And what you can do to optimize your build-through modules. Devin walked through how Swift expand modules and all of the things that we've implemented this year like [inaudible] processing to speed up your builds. And then finally the linker takes the outputs of both compilers and builds them into your application. At which point Xcode will go and, and code [inaudible] and package it up with other parts of your application for distribution. So most of this is open source, available if you're really curious. You can look at Swift or Clang or the llbuild execution engine at these URLs. And I just want to thank you all for coming. And I hope you've had a great WWDC. And we'll be in the labs. Thank you.  All right. My name is Pierre-Olivier Martel. I'm an engineering manager in the Security, Engineering, and Occupational Group here at Apple. And today, along with two of my colleagues, I am here to talk to you about some of the new security features in macOS Mojave and how you can make the most of it in your apps. I'll start by going over some of the high-level security improvements that went into the OS this year. And then I would like to dedicate the rest of this session to talk about some of the enhancements that we are putting in Gatekeeper. A few years ago we introduced a feature called Systemic Security Protection to macOS with the goal to increase the protection of the operating system by making it so that platform binaries are protected from modification both on disk and at runtime. This year we are taking it one step further by enforcing additional code-sending requirements on platform binaries. The system will now enforce the validity of code signatures throughout the lifetime of system processes and will automatically abort any process that deviates from its code signature or that attempts to execute any code that is not properly signed by Apple. In addition to that, the system will monitor it - the dynamic loading of any libraries, frameworks, and plug-ins and enforce that - those objects also be signed by Apple. It's a mechanism that has been enforced on iOS for several years now, and we are excited to make this to be for the macOS. Of course, not that this only applies to the system. This does not apply - it does not apply to your apps. However, there still remains a few system extension points throughout the operating system that have not yet made the move to the app extensions model. And those are technically considered to be platform binaries; OK, it's the plug-in loaders. So for those we are going to relax that policy, and the system will still allow these processes to load code that is either third-party signed or completely unsigned. So if you ship any of these plug-ins in your app, please make sure that you test on the developer preview and make sure it works properly. If not, please let us know. Next, I would like to talk about UI automation. The security model of a macOS, especially when it comes to user data access, relies on the user making security decisions for their self - for themselves. We deliberately captured those decisions, either via the user intent using the mechanisms like the open/save dialogs or drag-and-drop operations -- or by a user consent, with more explicit authorization dialogs or asking the user to make security configuration changes in System Preferences. As such, it's critical for the operating system to be able to differentiate between the user actually making these decisions and software driving the UI on their behalf. In the past we have made a few targeted enhancements to some of the system authorization dialogs to make sure that they can detect and block these synthetic events. But these decision points have become so pervasive across the OS that we need a better model. In macOS Mojave, the system will only allow UI events to be dispatched by processes that the user has specifically configured to control the UI on their behalf and effectively impersonate them. This configuration is made in the Security and Privacy Press pane in the System Preferences. And it's currently co-located with the existing accessing to the key list. This is the list of APIs that will be impacted moving forward. They mostly fall under two categories. First, the kidlet [phonetic] Layer: IOHIDPostEvent and the IOHIDSetMouseLocation; and then second, at the CG Layer. If you attempt to post any CG event or if you create any CG event tab, we are now providing the Listen Only option -- effectively creating what we call a modifying tab. And the user will have to approve those processes as well. And then next, we put a lot of focus of hardening Safari itself. It is, after all, one of the most important Internet facing applications on the machine. So it deserves special attention. In Mojave, Safari, as well as every satellite processes that ship as part of it, it's fully sandboxed. If you are a web developer, this may potentially have an impact on your local development workflow due to the way that Safari now acts as its local resources on this. If you want more details, please check out the What's New in Safari and WebKit session this Friday. And then of course, WebKit has been sandboxed for many years now. We put a lot of attention in auditing the existing sandboxes as well as removing any risky and unnecessary dependencies from those. For example, the web content process no longer needs access through the Windows server, to the dock, or to the network. If your app uses the system WebKit, then there is nothing for you to do in order to adopt this. You will automatically benefit from those improvements. Next, I'd like to talk about Gatekeeper. So Gatekeeper has done a great job over the years in stopping widespread malware attacks on the platform. It's here. We would like to make it better. And we are focusing on three main aspects. First, I invite Kelly onstage to tell you about a new security mechanism that puts the user in control over the way apps access their personal data. Then I'll introduce a new security feature that you as a developer can adopt in your apps to increase their security and their transparency. And then finally, I am sure you are all very impatient to hear more about the Developer ID notary service that Sebastian mentioned in his talk yesterday. So I invite Garrett onstage to tell you more about that. All right? Let's get started. Kelly? OK, thank you Pierre. Hi, my name is Kelly Yancy. I'm with the OS Security Team here at Apple. And I am really excited today to tell you about the new user data protections in macOS Mojave. Now in macOS High Sierra, these APIs -- -- prompt the user for consent before allowing apps to access their respective data. With these prompts, well-intentioned software can honor the user's preferences with regards to how they access their personal data. Well now, in macOS Mojave, access to this data requires user consent -- even for apps that access the Backing/Store directly via the file system. Now apps that currently use prescribed APIs to access this data should be well prepared. But as always, we encourage you to test your apps against the latest macOS release. Now if your app does access the underlying databases directly via the file system, be aware that that may - that access may now block the calling thread while the operating system presents an authorization prompt to the user. And in a world where computers do billions of operations per second, it turns out people are relatively high latency. So you don't want to do that I/O in your app's main thread, or it may appear hung while the prompt is displayed. So here is an example of code that accesses the user's pictures via the file system. Now by default, the Photos app stores the user's Photos library in the Pictures folder. So when this code traverses the user's Photos library, it may now trigger an authorization prompt for access to the user's photos -- where it would not have in macOS High Sierra. Now apps that traverse the user's Home folder could trigger multiple approval prompts, not just for photos -- for contacts, for calendars, and so on -- as many apps that traverse the entire file system, such as disk management or backup software. So user can pre-approve such apps by adding them to the new System Application Data category in the System Preferences Security and Privacy pane. By doing so, the user preauthorizes those apps to access all of their privacy-sensitive data without prompting. Authorization can also be preconfigured in education or enterprise environments via MDM server as long as it's a user-improved enrollment. When the user is prompted to authorize access to their personal data, it is important to communicate the purpose of that access. Imagine that you just installed an app that you had never used before. And the very first time you launch it, you saw this prompt. That's a tough decision. But we can make that decision easier by including purpose text, which makes it clear why the app is requesting access and what the consequences of declining that access would be. Now your app can specify the text displayed in the authorization prompts displayed on its behalf by including keys in your Info.plist file. So here are the Info.plist keys for each of the APIs that we have been looking at. Now you may notice that these are largely the same as on iOS. I want to call out one difference, and that's the Locations Services Info.plist key. This key is actually deprecated on iOS, but I assure you it is the correct key to use in macOS Mojave. So these keys will be required for apps linked against the 10.14 SDK. So besides informing the user, these keys are also being used to inform the operating system that -- as the developer -- you intend for your app to access the user's personal data. And should an app try to access the user personal data but not include the appropriate key and purpose string for that data, macOS Mojave assumes that that access was unintended -- and the app exits. For compatibility, these keys are optional for apps that link - that target older SDKs. So for example, if you have an app that still targets the 10.13 SDK and tries to access the user's personal data via one of these APIs, the app will not exit but will display a prompt lacking the purpose string -- like the first one that we saw a minute ago. So we still encourage you to include the Info.plist keys in your apps, even if you are targeting an older SDK, so that the user is better informed why your app is accessing their data. So in addition to these prompting categories, macOS Mojave also restricts access to this privacy-sensitive user data as well. So the operating system will not prompt for authorization to access this data via the file system. Only the respective system apps or services have access to this data plus any apps that the user has preauthorized for system application data via the Security and Privacy Preference pane that we just saw. In addition, authorization may be preconfigured via MDM enrollment. Another way that this data can be accessed is via scripting. So Mail, Messages, and Safari are all scriptable, exposing some of their functionalities automation by other apps. Let's use Mail for an example. Mail's local database contains lots of personal information. It has the e-mail addresses of my friends, family, companies I do business with. And then there is the contents of the mail messages themselves -- my private correspondence, my shopping receipts, my shipping notifications, and my temporary passwords for services I forgot the password of. So the Mail app necessarily has access to its own local database. And in macOS Mojave, other apps do not. But since Mail can be scripted by other applications, we need to be sure that Mail only shares my personal information with my consent. Because we want the user to be in control of how apps they have trusted with access to their data act on behalf of other apps, macOS Mojave will include prompts like this one, authorizing apps to control other apps. Now the developer preview does not currently require authorization to automate other apps. But you can try this new protection out in a future preview. Only if the user consents will Mail accept being automated by other apps. So this applies to Apple events. But there are exceptions. And there are exceptions for Apple events that are not particularly privacy sensitive -- for example, opening documents in a default application, opening URLs in the default handler for that URL scheme, or opening other applications. Finally, macOS Mojave brings every one of my favorite features from iOS. User authorization for camera and microphone: I think I am going to decline this one. So apps can - that's not a link in [inaudible] - apps can enumerate the camera and microphone hardware without user consent. But initiating capture requires user authorization. And this applies to all devices supported by the built-in drivers. Now your app can query the authorization status. And this is useful if you'd like your app's user experience to reflect user's previous decisions. And this API provided by AVFoundation is the same as on iOS. And what's interesting is it doesn't just return a simple Boolean yes/no. It actually returns an enumeration of four possible values. And I'd like to drill down on those because I think these are relevant. The first one, notDetermined, means that the user has not previously been consented for your app to access the camera or microphone. So should your app try to access that hardware, the operating system will display a prompt at that time. The restricted value actually means that the user cannot consent, that the hardware -- the camera or microphone -- has been disabled -- the parental controls or mobile device management. The denied value means the user has previously been prompted, but the user declined to give consent. They don't want your app to access that hardware. And finally, the authorized value means that the user has previously been prompted, and they did consent and your app can access that hardware. So we believe it's important to prompt in context. And this is the reason why the operating system currently displays the prompts kind of just in time. At the moment that your app accesses the microphone of the camera, the operating system will present a prompt when necessary. And we feel that that's the right time because the user has the most context to understand why your app is accessing the hardware. But there are times where your app might want more control over the timing of those authorization prompts. For example, if your app opens a new window, and in that window displays frames from the camera or a visualization of the audio from the microphone, you might want to avoid displaying an empty window if the user declines authorization -- in which case it's better to prompt before opening the window. So AVFoundation provides this API so that you can preflight that authorization. And I'd like to call out that this API is actually asynchronous. It takes a block with - that is called with a Boolean value of whether your app has access or not. So this is a little bit different than the enumeration that we just saw. That block may be invoked immediately when you make - when your app makes this call. For example, if the user has previously already consented or denied access, the operating system already knows that answer and can immediately invoke the block telling you what that answer is. Similarly, if the parental controls or MDM server has made that hardware unavailable, the callback is immediately invoked with a Boolean value of fault. But it's also possible that this callback will be invoked later -- possibly much later -- as a prompt is displayed while we are waiting for - and then only when the user makes their decision will this callback actually be invoked. At that time we will know the Boolean "Yes your app has access" or "No it doesn't." So your app can include purpose text for both the camera and microphone. And as with the other purpose strings, these are required for apps linked against the 10.14 SDK, and optional and certainly encouraged because they are informative for apps linked against earlier SDKs. So to recap: Here is an overview of all the topics that we just looked at. macOS Mojave does not solicit user approval for access to this user data. Only the respective system apps and apps the user has preapproved via the Security and Privacy Preference pane, or preconfigured the parental controls or MDM are permitted to access this data. And here are the categories of user data and devices the apps may access with user consent. Now the operating system -- macOS -- presents the authorization prompt to the user the first time the app accesses this data. And then it remembers the user's decision. And this is great for users because then they only see one prompt per app per piece of data. They don't get multiple prompts. But as a developer, you may actually want to get pre-prompted so that you can test how your app behaves while the prompt is displayed, if the user consents, and if the user declines authorization. So for that purpose macOS provides a tool, called tccutil, that you can use to make the operating system forget your previous answers to consent prompts. So the next time that you run your app, and your app requires authorization, the operating system will re-prompt. So this tool is provided for testing purposes only. Your app should not invoke this tool automatically, even in debug builds. So in summary, ensure your approval prompts are presented in context. This is important so that the user understands why they are being prompted. And add Info.plist keys so the user understands what your app is going to do with the data and what functionality will be unavailable if they decline. Access approval-gated resources from threads other than the main thread so that your app doesn't appear to be hung while waiting for approval. And gracefully handle failure to access approval-gated resources in the event that the user does decline. But finally, when the user does consent, be responsible with the user's personal data. So these are the new user data protections available in macOS Mojave. Thank you, and I will turn the stage back to over to Pierre to tell you about the enhanced runtime protections. Thanks Kelly. So in the few years since the maturity of system integrity of protection to the Mac, a lot of you have asked us what you could do in your apps to adopt some of the runtime protections that we afford to the rest of the system. Until now, there wasn't really a good story for this. Today, we are introducing a new set of runtime protections that you can easily adopt within your app. It is a new opt-in mechanism that is available within the 10.14 SDK. It effectively sets a new security baseline for your app by enabling the full set of runtime protections that the system provides, and requiring you - requires you to opt to back into more risky behaviors or ideas. It is easily configurable with a new set of unrestricted entitlements. Unrestricted here means that these entitlements are available to everybody without any prior approval. It can be easily configured from within the Xcode UI. And then finally, it is fully backward compatible if you need to be able to deploy your app to older releases of macOS and its version. So as we add more features to this one time in the future, apps that have already shipped will not be impacted. Now let's see what - let's talk about these new protections. First off, code signing: When you opt into this new runtime, the system will enforce that every single executable page within your address space must be backed by the original code signature that shipped with your app. It is a fine behavior for most apps out there. Of course it might be undesirable in some of these cases. So let's see how we can configure this. Let's say, for example, that your app has a scripting runtime for which you have high-performance requirements -- in which case, you'll most likely use JIT compiled code if you want to execute a runtime. With this, you can use the first entitlement in this list. It gives you access to the new MAP JIT flag in the MMAP system call, which allows you to create what we call JIT regions, which are memory regions that are readable, writable, and executable. And if you have a poliacre [phonetic] system, where you might expect to be able to execute load plug-ins that are not properly signed or not signed at all, then you have to use the second entitlement in this list. And finally, if your app needs the ability to modify its own code pages at runtime, effectively breaking its own code signature, then you will have to use the last entitlement in the system. And note that the vast majority of apps should not need that functionality. But we still want it to operate anyway. Next, library validation: When you opt into the new runtime, the system will enforce the code signature of every library, framework, and plug-in that you dynamically load at runtime, and by default will enforce that. These modules have - these objects have to be signed by Apple and shipping as part of the OS -- or that they be signed with the same Team ID as the main executable that ships with your app. If you have a plug-in ecosystem here and you need the ability to load objects that are signed by another team, you can use this entitlement to relax that policy. Note that it still requires you to load signed code; it just can be signed by other teams. If you need the ability to load fully unsigned code, you will need to use one of the entitlements from the previous slide. Next, debugging: Apps that are opting into the new runtime cannot be debugged and cannot debug other apps. If you need the ability to debug your app while it's opting into the new runtime, you can use the get-task-allow entitlement. Note that Xcode will automatically add this entitlement for you when you hit Build and Run from Xcode. So if you use Xcode as part of your local development workflow, then you don't need to do anything. Executable will also make sure that the entitlement is properly stripped from your signature when you do an export for distribution. If you don't use Xcode, make sure you don't ship your app with this entitlement unless you really need it. If your app is actually a debugger, then you will use the second entitlement in the list. And then finally, in the few rare occasions where your app needs to rely on the DYLD environment variables to modify its BFU [phonetic] at runtime, then you can use the last entitlement in this list. And finally, your resource access: This runtime requires you to be transparent about the kind of data classing that your app needs to access. So if your app attempts to access any data that is part of one of the protected categories that Kelly described in his previous section, and you don't have the appropriate entitlement signed in your app, then the system will automatically terminate it. We have entitlements that map to every single category that Kelly described. But remember that just adding this entitlement is not enough. You also need to add a purpose string if you are linking against the 10.14 SDK. And finally, it does not automatically grant you access to the data. The user is still in control of the final decision. So how do we enable this? Well, from Xcode you can go into Target setting in the Tap-ability tab. There you will find a new hard and Runtime section, which you can enable. We have checkboxes for every single of the entitlements that are previously described. And then a few more comportable [phonetic] using the using the Command Line interface, then we can use the new runtime option in the codesign command. And in order for you to validate that you have properly enrolled and opted into this new runtime, you can use the dash-dash display option. And here, the things to look for are the runtime flag as well as the runtime version. All right. Now let's look at the final piece of that puzzle and talk about notarized apps. Garrett? Thanks Pierre. Hi everyone. I'm Garrett, and I work on the Trusted Execution Team here at Apple. Now my colleague earlier, Kelly, talked about one way we help protect users from malicious software by ensuring the user is always in control of their private data. I'd like to talk about another way that we protect users from malicious software. And that's by identifying and blocking malicious software so it doesn't have the chance to run. Now the Mac App Store is a great place for users to find and download new software. And people can confidently install apps knowing that Apple has taken steps to ensure they are not harmful. Now the Mac App Store is getting a lot of attention this year, and that's great. But we also understand that some developers need the flexibility of the Developer ID program. Together, Gatekeeper and Developer ID have done a great job of preventing widespread outbreaks of malicious software. And today, we are making the Developer ID program even safer with the introduction of app notarization. Now app notarization is a process designed to help identify and block malicious software before distribution while still maintaining all the flexibility of the Developer ID program. That means that you can continue to ship your apps the way you do today with the same capabilities that they have today. And the key to this is the Develop ID notary service. Now the notary service is an automated service that performs security checks of Developer ID signed content. It's an optional extension to the Developer ID program, and it's not an app review. Starting today, developers can start uploading distribution-ready content to the notary service. And assuming that those applications, installer packages, and disk images don't contain any malicious software, the notary service will issue a notarization ticket back to your application. That ticket can be stapled and then distributed alongside your application. And when Gatekeeper launches a notarized app for the first time, it can verify the notarization and provide a new first-launch experience. Now your development workflow prior to distribution is completely unchanged. Now you may have heard, in the State of the Union, that this process will be required in the future. And while that's true, for now notarization is completely optional while we roll the service out and listen to feedback. Before we move on, I want to make one important point. This is not an app review. The notary service is simply performing a set of security checks to ensure that your content can be distributed safely. So what does this actually look like for your development workflow? Well here's a high level and simplified overview of the development process. Over on the left, a developer iterates on an application local to their system building features, debugging, and signing with their Mac Developer certificate. When the application is ready for release, it's signed with the Developer ID certificate, and a final test task can be performed. After that, the content can be distributed directly to users -- who can run it -- or Gatekeeper can verify that it wasn't tampered with; and the user gets to run your application. For this app to be notarized, only one additional step is required. You continue to develop locally, just like before. And once you sign it with your Develop ID certificate, a copy can be uploaded to the Apple notary service. Assuming that the Developer ID content doesn't contain anything malicious, a ticket will be issued back that can be attached to the content through a process we call stapling. And then the stapled content can be distributed just like before. And when it ends up on a user's system, Gatekeeper will verify notarization and provide the new first-launch experience. Now because this is a highly automated process, we are targeting providing tickets back within an hour. But we'll be working to improve the speed and quality of the service as we roll it out. So let's talk a little more about those security requirements I mentioned earlier. Well, first and foremost, no malicious software. If the security check finds anything malicious inside of the Developer ID signed content, it will immediately create a special kind of ticket called a revocation ticket and immediately notify the developer. Second, all executables need to be properly code signed. And this is important to make sure that when the application ends up on a develop - on a user's system, Gatekeeper can verify that it wasn't tampered with. And finally, all binaries must opt into the enhanced runtime that Pierre described earlier so they can benefit from the additional protections of the operating system without reducing their capabilities. So assuming that you have an app that's ready for notarization, how do you actually go about uploading it? Well, it's built into the Archive and Distribution workflow inside of Xcode. So if you already use that, you are pretty much set. Here you can see the Archive pane of the Organizer. And I have the Watch Grass Grow app already archived. So I click the Distribute App button on the right. I'll be taken to my distribution options. I'll continue to distribute it via Developer ID just like I have in the past. And when I select that, I'll have a new option -- to upload to the notary service. Clicking Next will show a progress bar while the Developer ID content is uploaded to the notary service. And as soon as the upload is complete, you'll be notified that notarization has started. From here you can export a copy of your app to perform any local testing that you might want to do in parallel. And if you click Close, you'll be taken back to the Archive pane; and you can see that the status has switched to Processing. When notarization is complete, a push notification will arrive and Xcode will notify you that notarization is done. Xcode will then automatically download the ticket, staple it to your content. So the next time you go into the Archive pane and click Export, you'll already have a stapled Developer ID notarized application. And when the user launches it for the first time, they'll see a new version of the first-launch dialog that includes your app's icon. Now we understand that not everybody uses Xcode's distribution workflow, although this Xcode works - dah. The Xcode workflow is available today in the preview. We have also built a set of command line tools to perform every step of this so that you can integrate them with any custom flows [phonetic] you have. And the first step is to upload a copy for notarization. Now the notary service accepts Zip files, installer packages, and disk images. So if you have a bare application, you'll need to zip it up prior to uploading to the notary service. The tool for working with the notary service is altool. Here you can see a command line indication sending the Watch Grass Grow image up for notarization. Note that altool does require authentication, but you can pass in your credentials via Environment Variables or the key chain. Once upload is complete, you'll get a UUID that can be used to monitor status of processing. But you can also turn back and pass the altool to check on the status of your notarization. Here we see another command line indication of altool to just check the status of notarization. And then you can see here that notarization was complete, was successful. And importantly, there is a log file that comes back from the notarization service. This is a great place to check for any warnings, or it tells you exactly what was included in your notarization ticket. So you can make sure that everything in your package was notarized properly. And the last step before you distribute content is to staple your notarization ticket to whatever content you are going to distribute. Now we have a special tool for that called Stapler. Now you can staple tickets directly to applications, disk images, and installer packages. And Stapler combines both retrieving the ticket and attaching it to the content in one easy step. Here you can see I am stapling the ticket directly to this image. After that's done, I can distribute the disk image, and the users will already get a notarized application. So as a quick reminder, in macOS Mojave, for non-notarized apps we'll continue to keep the first-launch dialog that Gatekeeper has always had for Developer ID applications. For notarized applications, they'll get a new version of this dialog that prominently displays the app's icon. And if the user does manage to download software that has malicious content in it, the content won't run, and they'll be treated to this warning. Now this is not a new capability; macOS has always had the capability to block applications like this. But notarization does allow us to identify specific malicious content earlier and provides a much better experience than if a developer needs to revoke their entire Developer ID certificate. So that's how to build notarized apps and how they are going to help keep Mac users safer. What can you do? Well, the notary service is available today. So feel free to start uploading your applications. And there is no reason to wait for your next app update. The service will gladly check your back catalog, too. So send in anything you have got. We are looking for feedback while we roll the service out. So if you have any issues with notarization, please come to the labs and we'd love to work through them with you. Remember that signing issues, for now, will be warnings but are going to become errors in the future. And when macOS Mojave ships later this year, Gatekeeper will be highlighting notarized applications to users. But then in a future macOS release, Gatekeeper will be requiring notarized apps by default. And that's all for Gatekeeper. So now I'd like to hand it back to Pierre for some closing words. Thank you Garrett. All right, let's look at the key takeaways from this session. First, the macOS Mojave, the user is now in control over the way apps can access their personal data. It means your app needs to be ready to handle their decisions -- first, because the user make take a while before they actually give you an answer. Second, because they might completely decline the request. Additionally, please be transparent to the user about what did I need to access and why you need to access it, using new entitlements and purpose strings for this effect. Also, make sure you request access to this data in context -- in the context where the user can actually understand why you need it and is more likely to grant you this request. We have a new higher runtime, which you can adopt in your app today. It effectively increases the security and the transparency of your app. It does not take away any of the capabilities that you have today. It just requires you to opt into them. And finally, please help us make our users feel more secure and stop malware as soon as possible by getting your apps notarized. The Developer ID notary service is open for submission right now. If you need more information about this session, please check out our developer website. We have a lab immediately after this section from 3:00 to 6:00 and then another lab tomorrow from 9:00 to 12:00. And finally, there will be another lab on Thursday dedicated to signing and distribution. Again, thank you very much for coming. Hope you have a great rest of the week.  Welcome to CarPlay Audio and Navigation Apps. I'm Jonathan, an engineer on the iOS Car Experience Team. We are so excited to share some updates with you today to help you build a great experience in CarPlay. We'll start by reviewing CarPlay Audio apps. Then, we'll introduce the brand new CarPlay framework for your navigation apps. And we'll drive a quick detour to show you how easy it is to build a great navigation experience in CarPlay. Let's start with a quick review of CarPlay. CarPlay is a smarter safer way to use iPhone in the car. You can seamlessly perform common scenarios while driving using your car's built-in display. You can make calls, send and receive messages, listen to audio, podcasts, and radio, and now, get directions with your navigation apps, all while staying focused on the road. You can think of the car screen like a second external display for your phone. Your app can present a version of itself on the car screen with UI and features optimized for use in the car. There are some special design considerations for your app when presented in CarPlay. CarPlay vehicles have a wide variety of input styles. Some cars have touchscreens. Others have a rotary knob that lets you move focus around and select elements. And some have a touchpad that allows for swipe gestures, and you can even do character entry by tracing with your finger. In a right-hand drive vehicle, the CarPlay status bar and app content will automatically switch sides to be closest to the driver. Some cars, also, inform iOS when to transition to a dark interface mode. Usually, when it's nighttime or when the vehicle's headlights are turned on. Your app can use this as a signal to, for example, update your map tile display style. CarPlay vehicles, also, support a wide variety of screen sizes and aspect ratios. iOS handles all of these input methods for you, so you can write your app just once for CarPlay and rest assured it'll be supported on all of these configurations. So, what kind of apps can you build in CarPlay? These are the five categories of CarPlay apps. The car is a special place and if you have an app in one of these five categories you'll need to apply for an entitlement for your app to work in CarPlay. If you're unsure whether your app fits into these categories and you're here at the conference, please come talk to us in our lab, later today. More details on that, soon. You can, also, go to this URL to request an entitlement for your app. Let's quickly review these categories of CarPlay apps. Auto maker apps are a special category of CarPlay app. These apps are built by the auto maker and work only in vehicles from that manufacturer. They can display fully customized UIs for scenarios like heating and cooling, seat controls, and other direct integrations with the vehicle. SiriKit integration into these apps allows you to change all of these settings with just your voice. Last year, we introduced messaging and VoIP apps to CarPlay. Your messaging and VoIP app can tap into SiriKit. So, your users can perform many common messaging tasks while connected to CarPlay, again, while staying focused on the road. Check out SiriKit for more details and see our session from last year, Enabling Your App for CarPlay. In a prior release of iOS, we introduced third party audio apps to CarPlay. Audio apps use a template interface optimized for the car, where your app provides data and metadata for the car's screen. We have some exciting new performance improvements and optimizations to share with you today for audio apps. And last but not least, today we're introducing the CarPlay framework, a brand new iOS framework for your navigation apps in CarPlay. Just like with audio apps, iOS displays your app on the car's screen and handles the interface with the car. Your app doesn't need to worry about input hardware or many vehicle specific considerations. You only need to update your map tile experience to be appropriate for the car. The CarPlay framework provides a fixed set of UI templates that iOS renders on the car screen. Your app decides how to create and configure these templates and what actions to take in response to the user interacting with your app's templates. We'll hear much more about the CarPlay framework, soon. Back to our CarPlay app categories, today we'll be focusing on two of these categories; audio and navigation. Let's start with audio apps. Albert. Thanks, Jonathan. I'm super stoked to talk to you all today about CarPlay audio apps. We're going to go through a brief overview of how to get your audio app set up for CarPlay. Some performance improvements and optimizations we've made for iOS 12, and some best practices to make. And some common scenarios to account for when developing your audio app for CarPlay. Now, driving and audio go hand in hand. It's very clear that when you're driving you want to listen to your favorite hits, your classics, or even stream some podcast episodes, or listen to the latest news on radio. We want to make sure that if you're developing an audio app that you want to provide the best experience to your drivers. Now, to illustrate this, I want to let you guys in on a little side project that I've been working on. It's a little app that I think will be a great hit. My app is called Srirocka. It combines two of my most favorite things; hot sauce and hot tracks. And with these synergies I think it'll be one of the top audio apps on the App Store when it launches. Now, Srirocka is already a fully functioning audio app and we'll need to add CarPlay support to it. So, let me illustrate how this'll be done. Like Jonathan mentioned, CarPlay uses templates that will abstract away many of the different complexities CarPlay has, such as input methods and screen sizes, and so on. So, your audio app just needs to be able to display information onto the CarPlay display and provide the best content. This is, usually, done by using a table view or tabs, depending on how you want to present your data. You'll need to focus on delivering the appropriate content to a CarPlay user. And if you're already developing an audio app, it uses existing APIs you may be already familiar with. So, let's take a look at this in detail. So, these are the three APIs that you'll need to know to launch your app in CarPlay. If you want to learn more about this in detail, we had a session last year that goes through the details of each single API. But I'll go through a brief summary of every single one of these. To browse your content on the CarPlay display, you'll need to use MPPlayableContent. MPPlayableContent has a data source and a delegate, so that you or your audio app can populate your app's information onto the CarPlay display. As well as a delegate to receive callbacks whenever the user selects something on the CarPlay display. Now, if you already set Now Playing metadata to Control Center or Lock Screen, or you're already working on this on your existing audio app, you're already familiar with these two APIs. MPNowPlayingInfo Center allows you to populate your now playing metadata and things such as title and album artwork and etcetera onto the CarPlay's now playing screen. As well as Control Center and Lock Screen if you've already been doing so. MPRemoteCommandCenter allows your app to respond to remote command events, such as the Play command, Next Track, and so on. We want to allow your CarPlay app for a seamless Now Playing experience. Now, let's take a look at how this is done in code. So, when Srirocka is launched, I'm going to use Srirocka as an example, here. This is what is needed at a minimum to support CarPlay audio apps. Srirocka will provide a data source and a delegate to MPPlayableContentManager, so that Srirocka can provide information to the CarPlay display and respond when absolutely necessary. Next, I've set NowPlayingInfoCenter to let MPNowPlayingInfoCenter know that my app will be providing NowPlaying metadata when it becomes a Now Playing app. And finally, I've responded to MPRemoteCommandCenter events. Specifically, just a Play command in this case, so that when Srirocka becomes the Now Playing app it can respond to these events. So, we can see here that the one API that was needed for CarPlay is MPPlayableContent. And for iOS 12, we took a good look at how to better optimize this and we can safely say, for iOS 12, we've remastered MPPlayableContent. We took a deep dive into MPPlayableContent and we pushed for performance improvements in how the data source and delegate calls are made. Without changing your current audio app's implementation in CarPlay, we sped up the startup sequence and provided much smoother animations whenever content is changed on the CarPlay display. We've also provided much better communication to your app to anticipate whatever the user may want to play or to browse on the CarPlay display. Now, there's lots of room to improve on your audio app. And let's see how we can do this. The first of which, is that we looked into the implementation of reloadData. Which, is a call in MPPlayableContent, and figured out how to better optimize this for your audio app. What we found is that you should really only call reloadData when absolutely needed. What it does is that it deconstructs the entire apps' hierarchy on the CarPlay display and asks your audio app to reconstruct everything again. And this can be a very expensive operation, leaving your app not very responsive. Instead, if you just have content that just needs to be updated, you should wrap them together inside of a beginUpdates and an endUpdates call, so then that content can be updated, appropriately. Now, these calls that MPPlayableContent has are asynchronous operations when we ask you app for your data. So, keep an internal representation or a cache of your information on somewhere in your app. So, then that when we ask for your content information you're able to provide us information quickly and makes your app responsive. Next, let's discuss some ways to further optimize your audio app's performance in CarPlay. So, Srirocka, already, has a bit of an implementation and is partially implemented. And the user has decided to make a selection on the Heating Habaneros playlist. It is one of the top playlists that's super-hot. But it seems to be stuck loading. We're not entirely sure what's going on, here. And CarPlay, actually, will timeout if the app doesn't provide content in time. Because it doesn't call completion handlers or just simply isn't returning its information. But what's going on, on the phone, here? So, CarPlay users are, usually, driving in areas where there's not a speedy connection or their screen is locked. A vast majority of CarPlay users are driving with the screen locked and with a passcode. After all, they're driving. If your app has data protection policies that are dependent on the phone being unlocked, you won't be able to access your app's information and, ultimately, CarPlay will timeout. So, if your data needs to be accessed while the phone is unlocked, you'll need to audit your app's data intergradation policies. The other issue is that you may be driving in areas, or CarPlay users are driving in areas with little to no cellular network-tivity [phonetic]. Drivers are driving everywhere, from the country roads and rural areas, to big cities. And these are very varied areas with different CarPlay, different data service. And you need to test for situations that are not just the constant WIFI network connection. Now, Srirocka does account for all of this, as well. But it uses some very advanced machine learning to see what are the hottest tracks. And after all, all of this happens on device, since privacy is a very important issue. And this just takes some time to process. Well, what happens in this case? We have an API called beginLoadingChildItems at indexPath to initiate fetching content. This API will be called whenever any of your index paths are visible on the CarPlay display. So, when the user is scrolling through table cells or selecting different tabs, beginLoadingChildItems will be called for every single index path that shows on the display. This gives your app a chance to start loading before the user, actually, selects the content. And here's a code example inside of Srirocka, where when the user sees that the Heating Habaneros playlist is visible on the CarPlay display, we start processing that playlist. And when the user makes a selection, will have either been midway through the network request or ready to go and able to provide content. So, let's take a look at some app scenarios that may occur when developing your app for CarPlay. Now, Srirocka provides a very rich user experience when logged out. It's very clear here, with beautiful typography and great graphic design, that the user is logged out and will need to log in. But how does this look on CarPlay? Well, Srirocka, actually, doesn't provide data when logged out. And this leads to a very poor user experience. The user doesn't really know what's going on here and isn't able to interact with your app. You should make sure that you should provide some type of experience, so that the user can at least interact with your app, even if your app is logged out. So, this leads to a very good user experience for your audio app. So, wrapping all this up, we can say that CarPlay audio apps have its greatest hits. MPPlayableContent allows your app to provide templates to the CarPlay display so that your app can provide users a good user experience to the CarPlay display. You should account for real world scenarios, such as when your app is logged out or your screen is locked, so then that your app can still function beautifully in CarPlay. And with iOS 12, we made some great optimizations and performance improvements to make your app even better in CarPlay. So, you should run your app again and see if there's any performance improvements you can make to your app, to make it even better. Now, to make us sure that we're steering in the right direction, I'd like to invite my colleague, Mike, to talk about navigating with the CarPlay framework. Thank you. All right. Thank you, Albert. I am thrilled to share with you, today, how to add CarPlay support to your navigation apps. Navigation is such a huge part of the CarPlay experience and your users will be so happy to see their favorite apps on the car screen. As Jonathan and Albert have previously mentioned, we've employed a template based approach for previous app categories. Your apps have been able to provide data and metadata that iOS will display on the car screen and manage these interactions on your app's behalf. And this has worked extremely well. But, we realize that navigation apps are a little bit different. Your apps have beautiful maps. And such incredible and immersive navigation experiences. And your users will expect to see that on the car screen. So, new in iOS 12, we're introducing the CarPlay framework. The CarPlay framework is your toolkit for building great interactive experiences on the car screen. It uses a fixed set of template objects that your apps can build and use, and that iOS will translate into a UI presented on top of your app. With this and a little bit of effort on your part, your apps will support all CarPlay systems. So, let's take a look at an app in CarPlay. We've been building our own navigation app that we call Country Roads. It takes us to our favorite destinations using the scenic route. And let's see what it looks like on CarPlay. We're looking at the CarPlay home screen and you can see our Country Roads app icon right there with the rest of the apps on iOS. As we launch it, the first thing you'll be greeted with is our beautiful map tiles. When connected to CarPlay, a navigation app will be given a window to draw content on the car screen. This window is your canvas for displaying all that beautiful noninteractive mapping and navigation related content. Then, as the app used templates iOS will take those templates and translate it into a UI presented on top of your app. So, let's take a look at the code on what happens when your app is connected to CarPlay. The first thing you'll need to note is that your application delegate will need to conform to the CarPlay application delegate protocol, CP Application Delegate. One function in that protocol is application didConnectCar InterfaceController to window. This function will provide two very important objects to your apps. The first is an instance of CPInterfaceController. CPInterfaceController is your connection to the template world and what your apps will use to manage what is displaying on the car screen. Additionally, you're given the window I just mentioned, which is where you can draw all your mapping content. You want to be sure to keep a reference to both of these objects throughout the duration of the CarPlay session. Next, you can create a new view controller to populate that window with content and assign it to windows review controller. And then, create a root template. And using that interface controller object, you can setRootTemplate on the car screen. So, let's talk about some of these templates that are available in the CarPlay framework. And we'll start with one you've already seen and where your users will spend a majority of their time. And that is on the map template. The map template is unique from all the other templates in that it is transparent to your app window. So, the content that you draw on that window will be displayed underneath the content provided by the map template. The map template can be configured in a few different places. You can place buttons in the navigation bar. You can place up to four; two on the leading side and two on the trailing side. These are instances of CPBarButton and are created either with text or images and are automatically sized and styled by the system. There's no notion of predefined button actions in the CarPlay framework. But when you create a button, you provide a custom action handler. And when the user interacts with your button on the car screen your custom action handler will be called in your app. You may also place up to four buttons that appear above your map window. And these are map buttons. You can create them with an image, and unlike the bar buttons, these are not styled by the system. The map template is also home to a lot of other mapping and navigational related functionality. Such as panning the map, providing important contextual information to your users using navigation alerts. And of course, turn by turn guidance. We're going to talk about guidance in a little bit. So, let's get started by going over panning and navigation alerts. If your app supports panning the map, we require that you present a button on the map template, either in the navigation bar or as a map button that enters pan mode. When a map template enters pan mode it will display four directional buttons that your users can interact with and are translated into an API, where your app only needs to know which direction to pan the map. This is an example of how the CarPlay framework has abstracted away a lot of the complexities of all the various CarPlay systems. These buttons can be interacted with using touch, both high latency and low latency touchscreens. Using a rotary knob or touchpad to select one of these buttons. And additionally, for CarPlay systems that have a rotary knob that also supports joystick of directional movement, those same movements will be translated into this API. Your app does not need to worry about where it came from. While your users are driving, you may want to present information to them, such as a road closure ahead or maybe, a better route is available. For this, you can use navigation alerts. Navigation alerts present on the map template and can be configured with a title, subtitle, image, primary and secondary action, and if it makes sense, you can use a dismiss interval after which the alert will self-dismiss. Let's take a look at some code on how we built our root template. And let's add a new button for a missing feature on our Country Roads app. Since this was our function that created out root template, the first thing we're going to do is create an instance of CPMapTemplate. We will then create a new button for a missing feature. One of the great things about Country Roads is that our users can search for their favorite categories. And we'll add a new button for that. So, let's create a CPBarButton of type Image. And in its custom action handler we will call a function of our own that is display our favorite categories. We'll then retrieve an image and assign it to the button. And we want this button to appear in the trailing space alongside our traffic button. So, we'll assign those two buttons to the trailing navigation bar buttons on our map template. And then, return our map template. Great. Now, that we have a button on our map template, we want a way to display those favorite categories to our users. The CarPlay framework has a great template for this. And that is the grid template. The grid template will display an array of up to eight buttons in a grid format. These buttons appear here and can be configured with an image and a title. The grid template also has a navigation bar, where you can place the same leading and trailing buttons. But you can also create a grid template with a title that will appear in the navigation bar. Let's jump over the code and let's create a grid template. We'll start by retrieving an image for our grid button. We're going to add a Parks button. We'll then create an instance of CPGridButton with a title of Parks and the image we just retrieved. That, with the other buttons, we'll assign to an array. And then, create an instance of CPGridTemplate with a title of Favorites, since these are our favorite categories, and the array of grid buttons we've created in code. And this is where we jump back to that interface controller object I mentioned earlier. That is how we manage what is presenting on the car screen. So, we would use that interface controller to push this new grid template onto the car screen. Now, that we have a way to search for our favorite categories and our favorite items, we need a way to display the search results to our users. For that, we can use a list template. The list template will display a list of CPList items. They can be configured with text, detailed text, an image, and a disclosure indicator. You can provide multiple sections. And you also, have a navigation bar where you can provide a title and those same leading and trailing buttons. One of the nice things about the CarPlay framework is that, when appropriate, it will also display a nice scroll bar on the side that helps your users page through their search results while driving. Let's build a list template. The first thing we're going to need to do is take that array of our own search results and translate them into an array of CPListItems. So, we'll do that by using some of the properties on our search result class. We will then initialize a new CPListTemplate with one section of those items, assign a title of Parks, and also, assign a delegate to manage interaction from the user. We, again, use the interface controller to push our new template onto the car screen. When the user interacts with an item on the screen listTemplate didSelect item completion handler will be called on your list template delegate. There are a few other items available in the CarPlay framework that I'd like to go over, briefly. The first is a great example of how complex CarPlay can be and how the CarPlay framework helps you not have to worry about that. What we're looking at, right now, is the search template available in the CarPlay framework presenting a touchscreen keyboard. But one thing we've learned is that not all CarPlay systems have a touchscreen. Many only have a rotary knob or touchpad for user interaction. On those systems, when appropriate, the CarPlay framework will display a linear style keyboard. And of course, for those touchpads that do support character recognition, through the same search template API your app will receive those characters as entered for free. You may, also, want to present information to your users in a way that demands a little bit more of their attention. For that, you can present a CPAlert in ActionSheetStyle or for information that truly commands their attention, you can use a full screen template. And lastly, since voice is such a huge part of your apps, we'll be providing a template that will help you manage voice control. And with that, I'd like to invite Jonathan back up to give you a demo of our Country Roads app. Jonathan. Thanks, Mike. Let's take a quick look at Country Roads. This is the CarPlay simulator. It's included with Xcode. Let's start on the CarPlay home screen, where we can see our custom navigation app enabled for CarPlay. Let's tap the icon to launch our app. When our app launches, we start on the map template. Here, the app overlay that Mike mentioned is fully transparent and it allows our app's beautiful map tiles to show through. This map is really out of this world. I have some basic controls on this map, like zooming in and out with ECP Map buttons. I can, also, switch in and out of pan mode using this pan button. All of these buttons are simple configurable template objects. CarPlay tells my app when the user has interacted with a button and my app can zoom in or out or take any other appropriate action in response. That new Favorites button that Mike just added shows up in the navigation bar on the trailing side. When we tap it our app has created, configured, and pushed a grid template that lets us navigate to certain very important destinations. If we select Parks our app creates, configures, and pushes a list template where we can visualize search results for interesting nearby parks. We'll revisit Country Roads, shortly. But for now, back to Mike. All right. Thanks, Jonathan. So, let's talk about one of the most important functions that your apps will provide to your users while driving. And that, of course, is turn-by-turn guidance. Let's start by walking through a typical flow your users may see while beginning a navigation session. The first thing they need is a destination. This could be something that is surfaced by your app through a navigation alert. Or perhaps, they've done a search and found a destination they want to travel to. They may want to, then, preview the route and see important information about it, such as how long it might take to get there and what their estimated arrival time is. If there are multiple route choices for the trip they're about to take they'll probably want to preview each of those routes and make a choice that suits them best. They'll then select a route and begin navigation. This is when your app will begin providing turn by turn updates to the user. Eventually, they'll arrive at their destination or cancel navigation. And let's take a look at how you can accomplish this same flow using the CarPlay framework. Let's start by talking about Route Preview. Your app can provide data that will appear in a way that the user can see and visualize all the important details about their upcoming trip. You can provide destination names, information that is pertinent to the route, such as advisory notices. You can, also, provide estimates to let them know when they'll arrive. And if the trip has multiple routes the CarPlay framework will automatically display a more routes button. This will allow your users to toggle between individual routes and select the one that they want to take. Let's look at the classes and methods you'll want to be familiar with to build an experience in the CarPlay framework route preview. The first is CPTrip. This is a representation of the upcoming trip for your users. It includes items such as the origin and destination, as well as multiple CPRouteChoices for each route that they may take. CPTravelEstimates describes how long it will take for an entire trip, as well as for individual maneuvers. On your map template, when you are ready to show a route preview to your users, you call ShowTripPreviews on your map template with the CPTrip object you've created. As the user toggles through individual routes you have an opportunity to update the content that is being displayed in your window. MapTemplate selectedPreviewFor trip using routeChoice will be called and you can change what is being displayed. This is a great opportunity to mention how the map template is drawing content that covers up your window. And you'll want to make sure that the content you're drawing is visible to your users. For this, we've leveraged the existing safeArea API you may already be familiar with. As the content changes from the map template it will update the safeArea insets on your window to let you know where it is safe to draw. You'll just want to implement safeAreaInsetsDidChange and pay close attention to those insets and make sure your content is drawn within them. Now, let's talk about turn by turn guidance. Similar to our preview, your app can provide data to the CarPlay framework that will cause it to draw a guidance card populated with information, such as an image representing the maneuver. How much farther it is until you arrive there. And of course, instructions on what to do when you get there, such as turn left or continue straight. Let's take a look at what you'll need to learn to use turn by turn guidance in the CarPlay framework. The CPMapTemplateDelegate and the mapTemplate are two places you'll need to start. First, when the user hits that go button from Route Preview mapTemplate startedTrip using routeChoice will be called on your template delegate. This is your indication to begin navigation to that destination using the route choice the user has selected. This is when you would begin navigation in your app, and also, begin providing turn by turn updates to the CarPlay framework. You'll want to call startNavigationSession for trip on your map template. And this will provide you with an instance of CPNavigationSession. And this is where you manage those upcoming maneuvers. Using CPNavigationSession and CPManeuver you will set upcoming maneuvers and the system will automatically display a primary maneuver. And if you provide a second maneuver it can display a secondary maneuver on that guidance card. You will then use update estimates for maneuver to continually update what is being shown in that guidance card. And let your users know when their maneuver is approaching. And most likely, your app will be playing back audio prompts to let them know, as well. For this, you'll want to make sure that your audio session is configured appropriately for CarPlay. We have to remember that when in the car your users may be listening to an audio source from iOS or they may be listening to an audio source from the car, such as FM radio. You want to configure your audio session with AVAudioSessionModeVoicePrompt as this will let the CarPlay system know when to appropriately duck the audio source, regardless of where it's coming from. You can, also, configure your session with the category options duckOthers and interruptSpokenAudio AndMixWithOthers. And this will help ensure that your audio plays well with other iOS audio sources. So, what happens next? Well, your users will continue to drive. You'll want to continue to provide updated maneuvers and estimates and help them make those turns when they need to. What happens if they veer off route? Well, you might want to calculate a new route. You can set a pause reason which will display a rerouting status to the user. Perhaps, a new route is available or something has occurred along the route. And you can use a navigation alert to present that information to them, right there on the map template. And ultimately, they'll arrive at their destination, thanks to your app. So, we've talked a lot about what your apps can look like while frontmost on the car screen. But we know that our users in CarPlay often switch between their mapping app and an audio app, like our favorite audio app Srirocka. What happens when your app is backgrounded and you need to present important information to your users? You will have an opportunity to provide banner notifications that will display when the user is away from your app. And this will work for maneuver updates, as well as navigation alerts. On your map template delegate there are three methods you'll want to implement. The first is mapTemplate, ShouldShowNotificationFor maneuver. When you set a new maneuver and your app is backgrounded this method will be called. And if you return true, the system will take that maneuver and translate it into a banner notification. If you're already presenting a banner notification for a maneuver, and simply are updating the travel estimates, mapTemplate shouldUpdateNotificationFor maneuver with travelEstimates will be called. Allowing you to update the content that appears in that banner. So, instead of rolling a new notification, you can simply update the contents that are already there. And for navigation alerts, you will have an opportunity to display those, as well, using mapTemplate shouldShowNotificationFor navigationAlert. With that, I'd like to invite Jonathan up one more time to give you a demo of how we've integrated our Country Roads app with the CarPlay framework to do route preview and turn-by-turn guidance. Jonathan. Thanks, Mike. Let's take a quick look at some code for how your navigation apps can provide guidance in CarPlay using our sample Country Roads app. When the user selects a destination, we're going to grab that placemark and hand the current location and destination to our navigation engine to calculate some route choices. For our purposes here, we have only a single route choice, but your navigation engine can provide multiple alternative routing options when available. Plus, we can include some extra details to help the user make an informed decision about a route. We'll create a CPTrip and call showTripPreviews to inform CarPlay that we're starting navigation guidance. Lastly, we'll update the estimates to provide our user an ETA for this trip. Let's take a quick look at how we can provide the maneuvers that CarPlay needs for navigation. When our user has selected a route and started navigation, first we can hide the route previews that our user was browsing. We'll tell our map template to start a navigation session. And we can move the session to the loading state while we're calculating maneuvers. We'll build a list of CP maneuver objects from our navigation engine. And last but not least, we'll provide the first instruction to our navigation session. And now, we're ready to drive. Let's see it in action. All right. Let's browse to our Favorites list and choose Parks. When we tap one of these locations we'll see our app pops back to the map template and presents a route guidance card. If we choose to start guidance by tapping on the Go button here, we'll see the app enter navigation mode. Your app can also update the Nav bar if needed to show custom buttons for navigation mode, like a Cancel button or a toggle for voice prompts. Looks like we're making pretty good time, here. When we arrive at our destination our app transitions back out of navigation mode. That's it for maps with the CarPlay framework. Let's review the roads that we've driven together today. We've shown you some big CarPlay audio performance improvements and optimizations for a better experience for your users. We introduced the brand new CarPlay framework for navigation and guidance apps in CarPlay. Your app can work seamlessly on different strings and input devices. You're the navigation experts and you only have to do what you do best; draw beautiful maps in your beautiful apps. And we'll take care of the rest. You can draw your map template, use CPMapTemplate and CPNavigationSession to manage navigation. And show other templates as needed to present information and handle user interactions. If you have any questions for us, please come meet our team at the CarPlay lab, later today, in Lab 11 at 2 p.m. You can, also, check out developer.apple.com/carplay for more details. Otherwise, you have everything you need to build a great CarPlay app. And we are so excited to see what you create. Thank you so much.  Welcome. Last year, we introduced Metal 2, which includes new ways for the GPU to drive the rendering pipeline. This year, we're introducing even more new and exciting features to solve common game development challenges. My name is Brian Ross, and together with my colleague, Michael Imbrogno, we'll explore new ways to make your applications better, faster, and more efficient. But first, I want to talk about some of the challenges that I'm trying to help you solve. Your games are using an ever-increasing number of objects, materials, and lights. Games like Inside, for example, use a great deal of special effects to capture and support the mood of the game. Making games like this that truly draw you in is challenging because it can require efficient GPU utilization. At the same time, games are requiring more and more CPU cycles for exciting gameplay. For example, games like Tomb Raider that features breathtaking vistas and highly-detailed terrain, but, at the same time, they're also managing complex physics simulations in AI. This is challenging because it leaves less CPU time for rendering. And finally, developers are taking AAA titles like Fortnite from Epic Games, importing them to iOS so you can run a console-level game in the palm of your hand. This is a truly amazing feat, but this also leaves us with even more challenges, like how to balance battery life with a great frame rate. So now, let's look at how Metal can help you solve these challenges. Today, I'm going to show you how to harness parallelism on both the CPU and the GPU to draw more complex scenes. We'll also talk about ways to maximize performance using more explicit control with heaps, fences, and events. And then, I'm going to show you how to build GPU-driven pipelines using our latest features, argument buffers and indirect command buffers. Now, while all these API improvements are key, it's equally important to understand the underlying hardware they run on. So the next section, my colleague Michael is going to show you how to optimize for the A11 to improve performance extend playtime. And finally, I'm really excited that we're going to be joined by Nick Penwarden from Epic Games. He is going to show us how they've used Metal to bring console-level games to our devices. So let's get started. Harnessing both CPU and GPU parallelism is probably the most important and easiest optimization you can make. Building a command stream on a single thread is not sufficient anymore. The latest iPhone has 6 cores, and the iMac Pro can have up to 18. So scalable, multithreaded architecture is key to great performance on all of our devices. Metal is designed for multithreading. I'm going to show you 2 ways how to parallelize on the CPU, and then I'm going to close this section by showing you how Metal could automatically parallelize for you on the GPU. So let's set up an example of a typical game frame. With a classic, single-threaded rendering, you'd [inaudible] build GPU commands and GPU execution order into a single command buffer. Typically, you're then having to fit this into some small fraction of your frame time. And, of course, you're going to have maximum latency because the entire command buffer must be encoded before the GPU can consume it. Obviously, there's a better way to do this, so what we're going to do is we're going to start by building in parallelism with the CPU. Render and compute passes are the basic granularity of multithread in Metal. All you need to do is create multiple command buffers and start encoding each into separate passes on a separate thread. You can encode them in any order you wish. The final order of execution is determined by the order they're added to the command queue. So now, let's take a look at how easy this is to do in your code. So you can see this is not a lot of code. The first thing that you're going to do is create any number of command buffers from the queue. Next, we're going to define the GPU execution order upfront by using the enqueue interface. This is great because you can do all this without waiting for the command buffer to be encoded first. And finally, we're going to create separate threads and caller encoding functions for each. And that's it. That's all you have to do. It's really fast, it's really efficient, and it's really simple. So now, let's go back to the previous diagram and look at another example. So as you can see, we did a pretty good job parallelizing these on the CPU, but what if you have 1 really long rendering pass? So in cases like this, Metal has a dedicated parallel encoder that allows you to encode on multiple threads without explicitly dividing up the render pass or the command buffer. So now, let's look at how simple this is in your code. It looks a lot like the previous example. The first thing you're going to do is create a parallel encoder. And from that, you create any number of subordinate encoders. And it's important to realize that this is actually where you define the GPU execution order. Next, we're going to create separate threads and encode each of our G-buffer functions separately. And finally, we set up a notification so that when the threads are complete, we call end encoding on the parallel encoder. And that is it. That's all you have to do to parallelize a render pass. It's really fast, and it's really easy. So now that I've shown you 2 ways to parallelize on the CPU, now let's see how Metal can parallelize for you automatically on the GPU. So let's look at the frame example from the beginning and see how the GPU executes the frame. Based on the capabilities of your platform, Metal can extract parallelism automatically by analyzing your data dependencies. Let's look at just 2 of these dependencies. So in this example, the particle simulation writes data, which is later used by the effects pass to render the particles. Similarly, the G-buffer pass generates geometry, which is later used by the deferred shading pass to compute material lighting. All this information allows Metal to automatically and cheaply identify entire passes that can run in parallel, such as using async compute. So you can achieve parallelism and async compute for free on the GPU. It's free because Metal doesn't require you to do anything special on your part. So I think we all love getting free optimizations on the GPU, but sometimes you as a developer, you may need to dive a little bit deeper. For the most critical parts of your code, Metal allows you to incrementally dive deeper with more control. For example, you could disable automatic reference counting and do it yourself to save on CPU time. You could also use Metal heaps to tightly control allocations really cheaply. And Metal heaps are complemented by fences and events, which allow you to explicitly control the GPU parallelism. Many of your games are using a lot of resources, which can be costly. Allocations require a round trip to the OS, which has to map and initialize memory on each request. If your game uses temporary render targets, these allocations can happen in the middle of your frame, causing stutters. Resource heaps are a great solution to this problem. Heaps also let you allocate large slabs of memory from the system upfront. And from those, you can later add or remove textures and buffers from those slabs without any costly round trip. So starting from a case where you allocate 3 normal textures, Metal typically places these in 3 separate allocations, but putting these all instead into a single heap lets you perform all memory allocation upfront at heap creation time. So then, the act of creating textures becomes extremely cheap. Also, heaps can sometimes let us use the space more efficiently by packing allocations closer together. So with a traditional model, you would deallocate textures, releasing pages back to the system, and then reallocate, which will allocate a new set of textures all over again. With heaps, you deallocate and reallocate without any costly round trip to the OS. Finally, heaps also let you alias different memory resources with each other. This is really helpful if your game frame has a lot of temporary render targets. There's no reason for these to occupy a different memory all the time, so you could alias and save hundreds of megabytes. Now, the faster allocations in aliasing are great, but it's not entirely free when it comes to dependency tracking. Let's return to our frame example for a better explanation. With heaps, Metal no longer sees individual resources, so therefore, it can't automatically identify the read and write dependencies between passes, such as the G-buffer and deferred shading pass in our example. So you have to use fences to explicitly signal which pass produces data and which pass consumes the data. So in this example, the G-buffer updates the fence, and the deferred shading waits for it. So now, let's take a look at how we could apply these basic concepts in your code. So the first thing that we're going to do is we're going to apply this to our G-buffer and deferred shading example. First, we're going to allocate our temporary render target from the heap. This looks just like what you're probably already doing today when you allocate a texture. Next, we're going to render into that temporary render target. And finally, update the fence after the fragment stage completes. This will ensure that all the data is produced before the next pass consumes it. So now, let's switch gears over to the deferred shading pass. Now, we're going to use this temporary render target to compute material lighting. Then, we're going to wait for the fence to make sure that it's been produced before we consume it. And finally, market is aliasable so that we can reuse this for other operations, saving hundreds of megabytes. So now that we've talked about how to parallelize and optimize performance with explicit control, this is great, but what if you want to put the GPU more into the driving seat? So let's talk about GPU-driven pipelines. Your games are moving more and more of the decision logic onto the GPU, especially when it comes to processing extremely large data sets or scene graphs with thousands of objects. With Metal 2, we've made another really important step forward in our focus on GPU-driven pipelines. Last year, we introduced indirect argument buffers, allowing you to further decrease CPU usage and move a large portion of the workload to the GPU. This year, we're also introducing indirect command buffers, and this will allow you to move entire rendering loops onto the GPU. So first, let's briefly recap the argument buffer feature. An argument buffer is simply a structure represented like this. Previously, these would have only constants, but with argument buffers, we can have textures and samplers. Before, these would have to have separate shader bind points. So since this structure, you have all the features of the Metal shading language at your disposal, so it's really flexible and really easy. You could do things like add substructures, or arrays, or even pointers to other argument buffers. You could modify textures and samplers, creating new materials on a GPU without any CPU involvement. Or you can make giant arrays of materials and use a single-instance draw call to render many objects with unique properties. So argument buffers allow you to offload the material management onto the GPU and save valuable CPU resources. But this year, we're putting it a little bit, extending it a little bit more. We started by adding 2 new argument types. This includes pipeline states and command buffers. Now, these are used to support our brand-new indirect command buffer feature. With indirect command buffers, you could encode entire scenes on the GPU. On the CPU, you only have a few threads available for rendering, but on the GPU, you have hundreds or even thousands of threads all running at the same time. With indirect command buffers, you can fully utilize this massively parallel nature. Also, indirect command buffers are completely reusable, so you could spend the encoding cost once and reuse it again and again. And since an ICB is a directly accessible buffer, you can modify its contents at any time, like change the shader type, or the camera matrix, or anything else that you might need to change. And of course, by moving your rendering to the GPU, you remove expensive CPU and GPU synchronization points that are normally required to hand over the data. So let's take a look at an example. Here is a typical game frame. The usual rendering loop has a few common stages. First, you walk your scene graph to determine which objects you need to render. You probably use frustum culling to determine what objects are within the view frustum. Some of you might use a more complex solution that accounts for occlusion. Also, level of detail selection naturally occurs at this stage. Only once you encode and submit your command buffer will the GPU start to consume it. More and more games are moving the process of determining visible objects onto the GPU. GPUs are just better at handling the growing scene complexity of the latest games. Unfortunately, this creates a sync point in your frame. And the, it makes it so that the CPU cannot encode draw calls until the GPU produces the data. It's extremely difficult to get this right without wasting valuable CPU and GPU time on synchronization. With ICBs, the benefits are immense. Not only can you move the final bits of processing to the GPU, you naturally remove any sync points required to hand over the data and you improve your CPU and GPU utilization. At the same time, you reduce your CPU overhead to a constant. So let's look at the encoding in a little bit more detail. I'm going to start by expanding on our previous example and look at the massively parallel nature that only the GPU can provide. We could begin with the list of visible objects and LODs coming from our culling dispatch. Also, keep in mind that we're utilizing the power of argument buffers here. So in this case, each element has a pointer to the actual properties, so we don't need to store everything in the same buffer. This solution saves us a lot of memory and performance, and it's because we only build a very small list of information. The actual argument buffer contains several levels of detail for geometry. This includes position, vertex buffer, index buffer, and a material argument buffer. For rendering, we only select 1 of these LODs per object. The actual encoding happens in a compute kernel, and we encode into an indirect command buffer. Each thread of the compute kernel encodes a single draw call. So we read the object with all of its properties, and we encode these into the ICB. There's a couple of details worth noting. You can think of an ICB as an array of render commands. A render command consists of a pipeline object with shaders, any number of buffers, and a draw call. Next, an ICB is built for parallelism, so you could encode concurrently and out of order. And lastly, we kept the API very simple, so it's just like what you might be doing today on the CPU. Another thing -- each command could have different properties and even draw types. So this is a really, really significant step forward from all the flavors of indirect rendering that many of you may have seen elsewhere. Now, let's take a look at how we can do this in your code. So this is how easy it is to encode a draw call. The first thing you're going to do is select the render command by index using your thread ID. Then, we're going to set the properties. So in this example, we're setting a shader with a pipeline state and then a separate buffer for the geometry and material. And finally, this is how you encode a draw call. Thanks to the Metal shading language, encoding on the GPU is really, really simple. Even though this is in a compute shader, this looks just like what you're already doing on the CPU today. Now, let's look at 1 more sample. Here are some of the basic things you need to do to create, encode, and execute an ICB. To create it, you first fill out a descriptor. The descriptor contains things like draw types, and inheritance properties, and per-stage bind counts. This describes the way that the indirect buffer will behave. When it's time to encode the ICB, you simply create compute encoder and call dispatch just like what you've been doing already. Once the ICB is encoded, you can optionally decide if you want to optimize it. When you optimize it, you remove all the redundant state, and the end result is a lean and highly-efficient set of GPU commands. Now, once the ICB is encoded and optimized, it's time to schedule it for execution. You notice here that you could actually specify the exact range of commands that you execute. Also in this example, we use an indirect buffer, which itself can be encoded with a GPU. So once the ICB is encoded, it could be reused again and again, and the overhead is completely negligible. So I'm really excited, but we actually went ahead and we put together a sample so you could take a look. So here you could see a number of school buses in the middle of a city. Each bus is composed of 500,000 polygons and 2000 individual parts. Each part requires a separate draw call, its own material argument buffer, index buffer, and vertex buffer. As you could imagine, this would be a lot of API calls on the CPU, but we are using indirect command buffers here, so everything is being encoded on the GPU. We're also selecting the appropriate level of detail, and therefore, we're able to render multiple objects without increasing the CPU or GPU cost. So on the left, you could see a view of the regular camera. And on the right, we've zoomed in to a single bus, so you could see the level of detail actually changing. ICBs enabled us to introduce another really incredible optimization. We're able to split the geometry into chunks of a few hundred triangles and analyze those chunks in a separate compute kernel. You could see the chunks in different colors on the screen. Each thread of the kernel determines whether triangles are facing away from the camera or if they're obscured by other objects or geometry in the scene. This is all really, really fast because we've performed the calculation for a chunk only and not on each individual triangle. We then tell the GPU to only render the chunks that are actually visible. And again, let's see the side-by-side view. The left side is your camera view, and the right side is another view of the bus. You could see the red and pinkish tint there. That is what our compute shaders determined is invisible. We never actually send this work to the GPU, so it saves us 50% or more of the geometry rendering cost. Here's 1 last view showing which, how much this technique could save you. So notice on the right, many of the buses and ambulances are actually invisible. This is really amazing. I love this. So please take a chance to explore the code, and I hope I'll see this technology in some of your games in the future. I think if utilized, ICBs can really push your games to the next level. So now, I'm pleased to introduce Michael, who will show you how to optimize for the A11, improve performance, and extend playtime. Thank you very much. Thanks, Brian. So everything Brian's just showed you is available for iOS, tvOS, and macOS. Next, I'm going to dive into some of the new Metal 2 features for Apple's latest GPU, the A11 Bionic, designed to help you maximize your game's performance and extend your playtime by reducing system memory bandwidth and reducing power consumption. So Apple-designed GPUs have a tile-based deferred rendering architecture designed for both high performance and low power. This architecture takes advantage of a high bandwidth, low-latency tile memory that eliminates overdraw and unnecessary memory traffic. Now, Metal is designed to take advantage of the TBDR architecture automatically within each render pass, load and store actions, make explicit how render pass attachments move in and out of tile memory. But the A11 GPU takes the TBDR architecture even further. We added new capabilities to our tile memory and added an entirely new programmable stage. This opens up new optimization opportunities critical to advanced rendering techniques, such as deferred shading, order-independent transparency, tiled forward shading, and particle rendering. So let's start by taking a look at the architecture of the A11 GPU. All right. So on the left, we have a block representation of the A11 GPU. And on the right, we have system memory. Now, the A11 GPU first processes all the geometry of a render pass in the vertex stage. It transforms and bends your geometry into screen-aligned, tiled vertex buffers. These tiled vertex buffers are then stored in the system memory. Now, each tiled vertex buffer is then processed entirely on ship as part of the fragment stage. This tiled architecture enables 2 major optimizations that your games get for free. First, the GPU rasterizes all primitives in a tile before shading any pixels using fast, on-ship memory. This eliminates overdraw, which improves performance and reduces power. Second, a larger, more flexible tile memory is used to store the shaded fragments. Blending operations are fast because all the data is stored on ship next to the shading cores. Now, tile memory is written to system memory only once for each tile after all fragments have been shaded. This reduces bandwidth, which also improves your performance and reduces your power. Now, these optimizations happen underneath the hood. You get them just by using Metal on iOS. But Metal also lets you optimize rendering techniques by taking explicit control of the A11's tile memory. Now, during the development of the A11 GPU, the hardware and software teams at Apple analyzed a number of important modern rendering techniques. We accelerated, we noticed many common themes, and we found that explicit control of our tile memory accelerated all of them. We then developed the hardware and software features together around this idea of explicit control. So let's talk about these features. So programmable blending lets you write custom blend operations in your shaders. It's also a powerful tool you can use to merge render passes, and it's actually made available across all iOS GPUs. Imageblocks are new for A11. They let you maximize your use of tile memory by controlling pixel layouts directly in the shading language. And tile shading is our brand-new programmable stage designed for techniques that require mixing graphics and compute processing. Persistent threadgroup memory is an important tool for combining render and compute that allows you to communicate across both draws and dispatches. And multi-sample color coverage control lets you perform resolve operations directly in tile memory using tile shaders. So I'm going to talk to you about all these features, so let's start with programmable blending. With programmable blending, your fragment shader has read and write access to pixels and tile memory. This lets you write custom blending operations. But programmable blending also lets you eliminate system memory bandwidth by combining multiple render passes that read and write the same attachments. Now, deferred shading is a particularly good fit for programmable blending, so let's take a closer look at that. So deferred shading is a many-light technique traditionally implemented using 2 passes. In the first pass, multiple attachments are filled with geometry attributes visible at each pixel, such as normal, albedo, and roughness. And in the second pass, fragments are shaded by sampling those G-buffer attachments. Now, the G-buffers are stored in the system memory before being read again in the lighting pass, and this round trip from tile memory to system memory and back again can really bottleneck your game because the G-buffer track consumes a large amount of bandwidth. Now, programmable blending instead lets you skip that round trip to memory by reading the current pixel's data directly from tile memory. This also means that we no longer need 2 passes. Our G-buffer fill and lighting steps are now encoded and executed in a single render pass. It also means that we no longer need a copy of the G-buffer attachments in system memory. And with memory, Metal's memoryless render target feature, saving that memory is really, really simple. You just create a texture with a memoryless flag set, and Metal's only going to let you use it as an attachment without load or store actions. So now, let's take a look at how easy it is to adopt programmable blending in your shaders. Okay, so here's what the fragment shader of your lighting pass would look like with programmable blending. Programmable blending is enabled when you both read and write your attachments. And in this example, we see that the G-buffer attachments are both inputs and outputs to our functions. We first calculate our lighting using our G-buffer properties. As you can see here, we're reading our attachments and we're not sampling them as textures. We then accumulate our lighting result back into the G-buffer, and, in this step, we're both reading and writing our accumulation attachments. So that's it. Programmable blending is really that easy, and you should it where, whenever you have multiple render passes that read and write the same attachments. So now, let's talk about imageblocks, which allow you to merge render passes in even more circumstances. Imageblocks give you full control of your data in tile memory. Instead of describing pixels as arrays of render pass attachments in the Metal API, imageblocks let you declare your pixel layouts directly in the shading language as structs. It adds new pack data types to the shading language that match the texture formats you already use, and these types are transparently packed and unpacked when accessing the shader. In fact, you can also use these new pack data types in your vertex buffers and constant buffers to more tightly pack all of your data. Imageblocks also let you describe more complex per-pixel data structures. You can use arrays, nested structs, or combinations thereof. It all just works. Now, direct control of your pixel layout means that you can now change the layout within a pass. This lets you combine render passes to eliminate system memory bandwidth in ways that just weren't possible with programmable blending alone. Let's take a look at an example. So in our previous example, we used programmable blending to implement single-pass deferred shading. You can also implement single-pass deferred shading using imageblocks. Imageblocks only exist in tile memory, so there's no render pass attachments to deal with. Not only is this a more natural way to express the algorithm, but now you're free to reuse the tile memory once you're finished reading the G-buffer after your lighting. So let's go ahead and do that. Let's reuse the tile memory to add an order-independent transparency technique called multi-layer alpha blending. So multi-layer alpha blending, or MLAB, maintains a per-pixel, fixed-size array of translucent fragments. Each incoming fragment is sorted by depth into the array. If a fragment's depth lies beyond the last element of the array, then those elements are merged, so it's really an approximation, approximate technique. Now, sorting the MLAB array is really fast because it lives in tile memory. Doing the same off chip would be really expensive because of the extra bandwidth and synchronization overhead. Now, the A11 actually doubles the maximum supported pixel size over your previous generation, but that's still not going to be enough to contain both the G-buffer and MLAB data structures simultaneously. Fortunately, you don't need both at the same time. Imageblocks let you change your pixel layouts inside the render pass to match your current needs. So changing pixel layouts actually requires tile shading, so let's talk about that next. So tile shading is the new programmable stage that provides compute capabilities directly in the render pass. This stage is going to execute a configurable threadgroup for each tile. For example, you can launch a single thread per tile, or you can launch a thread per pixel. Now, tile shading lets you interleave draw calls and threadgroup dispatches that operate on the same data. Tile shaders have access to all of tile memory, so they can read and write any pixel of the imageblock. So let's look at how tile shading can optimize techniques such as tiled forward shading. So like deferred shading, tiled forward shading is a many-layered technique. It's often used when MSA is important or when a variety of materials are needed and works equally well for both opaque and translucent geometry. Now, tiled forward shading traditionally consists of 3 passes. First, a render pass generates a scene depth buffer. Second, a compute pass generates, calculates per-tile depth bounds and per-tile light lists using that scene depth buffer. And finally, another render pass is going to shade the pixels in each tile using the corresponding light list. Now, this pattern of mixing render with compute occurs frequently. And prior to A11, communicating across these passes required system memory. But with tile shading, we can inline the compute so that the render passes can be merged. Here the depth bounds and light culling steps are now implemented as tile shaders and inlined into a single render pass. Depth is now only stored in the imageblock and, but is accessible across the entire pass. So, now, tile shading is going to help you eliminate a lot of bandwidth, but these tile shader outputs are still being stored to system memory. Tile shader dispatches are synchronized with draws, so that's completely safe to do, but I think we could still do better using our next feature, persistent threadgroup memory. Okay, so threadgroup memory is a well-known feature of Metal compute. It lets threads within a threadgroup share data using fast, on-ship memory. Now, thanks to tile shading, threadgroup memory is now also available in the render pass. But threadgroup memory in the render pass has 2 new capabilities not traditionally available to compute. First, a fragment shader now also has access to the same threadgroup memory. And second, the contents of threadgroup memory persist across the entire life of a tile. Taken together, this makes a powerful tool for sharing data across both draws and dispatches. In fact, we believe it's so useful that we've actually doubled the maximum size of threadgroup memory over our previous generation so that you can store more of your intermediate data on ship. Okay, so now, let's use threadgroup persistence to further optimize our tiled forward shading example. So with persistence, tile, the tile shading stage can now write both the depth bounds and the culled light lists into threadgroup memory for later draws to use. This means that now all our intermediate data stays on ship and never leaves the GPU. Only the final image is stored at system memory. Minimizing bandwidth to system memory is, again, very important for your game's performance and playtime. Now, let's take a look at how easy it is to make use of persistence in the shading language. Okay, so the top function here is our tile shader, and it's going to perform our light culling. It intersects each light with a per-tile frustum to compute an active light mask. The bottom function is our fragment shader that performs our forward shading. It shades only the lights intersecting the tile using that active light mask. Now, sharing threadgroup memory across these functions is achieved by using the same type and bind point across both shaders. That's how easy it is to take advantage of threadgroup persistence. Okay, so now that you've seen tile shading and threadgroup persistence, let's revisit our order-independent transparency example. Okay, so remember how I said that changing imageblock layouts requires tile shading? That's because tile shading provides the synchronization we need to safely change layouts. This means we actually have to insert a tile shade between the lighting and the MLAB steps. So tile shading is going to wait for the lighting stage to complete before transitioning from G-buffer layout to MLAB layout, and it's also going to carry forward the accumulated lighting value from the lighting step into the MLAB step for final blending. Okay, so now that we've covered imageblocks, tile shading, and threadgroup persistence, it's time to move on to our final topic, multi-sample anti-aliasing and sample coverage control. So multi-sample anti-aliasing improves image quality by supersampling depth, stencil, and blending, but shades only once per pixel. Multiple samples are later resolved into a final image using simple averaging. Now, multi-sampling is efficient on all A series GPUs because samples are stored in tile memory, where blending and resolve operations have fast access to the samples. The A11 GPU optimizes multi-sampling even further by tracking the unique colors within each pixel. So blending operations that previously operated on each sample now only operate on each color. This could be a significant savings because the interior of every triangle only contains 1 unique color. Now, this mapping of unique color to samples is called color coverage control, and it's managed by the GPU. But tile shaders can also read and modify this color coverage. And we can use this to perform custom resolves in place and in fast tile memory. Now, to see why this is useful, let's take a look at a multi-sampled scene that also renders particles. Now, particles are transparent, so we blend them after rendering our opaque scene geometry. But particle rendering doesn't benefit from multi-sampling because it doesn't really have any visible edges. So to avoid the extra cost of blending per sample for no good reason, a game would render using 2 passes. In the first pass, your opaque scene geometry is rendered using multi-sampling to reduce aliasing. And then, you're going to resolve your color and depth to system memory, and we're resolving depth because particles can later be included. Then in the second pass, the resolve color and depth are used in rendering the particles without multi-sampling. Now, as you probably guessed by now, our goal is to eliminate the intermediate system memory traffic using tile shading to combine these 2 passes. But tile shading alone isn't enough. We need color coverage control to change the multi-sampling rate in place. Using color coverage control is really powerful and really easy. Let's take a look at the shader. Okay, so remember that our goal here is to average the samples of each pixel and then store that result back into the image block as the overall pixel value. Now, instead of looping through each color, through each sample, we're going to take advantage of the color rate capabilities of the A11 and only loop through unique colors. To properly average across all samples, we need to weigh each color by the number of samples associated with it, and we do this by counting the bit set in the color coverage mask. We then complete our averaging by dividing by the total number of samples and, finally, write the result back into the imageblock. The output sample mask tells Metal to apply the results to all samples of the pixel. And since all samples now share the same value, the later particle draws are going to blend per pixel rather than per sample. So that's it for sample coverage control. Now, optimizing for Apple GPUs is really important for maximizing your game's performance and extending its playtime, but there's a lot more work that goes into shipping a tile in iOS, especially one that's originally designed for desktops and consoles. To talk about that now and to put into practice what we just discussed, I'd like to bring on Nick Penwarden from Epic Games. Nick? Thank you, Michael. So, yeah. I'd like to talk a little bit about how we took a game that was originally made for desktop and console platforms and brought it to iOS using Metal. So some of the technical challenges we faced. The Battle Royale map is 1 map. It's larger than 6 kilometers squared. That means that it will not all fit into memory. We also have dynamic time of day, destruction. Players can destroy just about any object in the scene. Players can also build their own structures. So the map is very dynamic, meaning we can't do a lot of precomputation. We have 100 players in the map, and the map has over 50,000 replicating actors that are simulated on the server and replicated down to the client. Finally, we wanted to support crossplay between console and desktop players along with mobile. And that's actually a really important point because it limited the amount that we could scale back the game in order to fit into the performance constraints of the device. Basically, if something affected gameplay, we couldn't change it. So if there's an object and it's really small, it's really far away, maybe normally you would cull it, but in this case, we can't because if a player can hide behind it, we need to render it. So want to talk a little bit about Metal. Metal is really important in terms of allowing us to ship the game as fast as we did and at the quality that we were able to achieve. Draw call performance was key to this because, again, we have a really complicated scene and we need the performance to render it, and Metal gave us that performance. Metal also gave us access to a number of hardware features, such as programmable blending, that we used to get important GPU performance back. It also has a feature set that allowed us to use all of the rendering techniques we need to bring Fortnite to iOS. In terms of rendering features, we use a movable directional light for the sun with cascaded shadow maps. We have a movable skylight because the sky changes throughout the day. We use physically-based materials. We render in HDR and have a tone-mapping pass at the end. We allow particle simulation on the GPU. And we also support all of our artist-authored materials. It's actually a pretty important point because some of our materials are actually very complicated. For instance, the imposters that we use to render trees in the distance efficiently were entirely created by a technical artist at Epic using a combination of blueprints and the material shader graph. So in terms of where we ended up, here is an image of Fortnite running on a Mac at high scalability settings. Here it is running on a Mac at medium scalability settings. And here it is on an iPhone 8 Plus. So we were able to faithfully represent the game on an iPhone about at the quality that we achieve on a mid-range Mac. So let's talk a little bit about scalability. We deal with scalability both across platforms as well as within the iOS ecosystem. So across platform, this is stuff that we need to fit on the platform at all, like removing LODs from meshes that will never display so we can fit in memory or changing the number of characters that we animate at a particular quality level in order to reduce CPU costs. Within iOS, we also defined 3 buckets for scalability -- low, mid, and high -- and these were generally correlated with the different generations of iPhones, so iPhone 6s on the low end, iPhone 7 was our mid-range target, and the iPhone 8 and iPhone X on the high end. Resolution was obviously the simplest and best scalability option that we had. We ended up tuning this per device. We preferred to use backbuffer resolution where possible -- this is what the UI renders at -- because if we do this, then we don't have to pay a separate upsampling cost. However, we do support rendering 3D resolution at a lower resolution, and we do so in some cases where we needed a crisp UI but had to reduce 3D render resolution lower than that in order to meet our performance goals -- the iPhone 6s, for example. Shadows were another axis of scalability and actually really important because they impact both the CPU and the GPU. On low-end devices, we don't render any shadows at all. On our mid-range target, we have 1 cascade, 1024 by 1024. We set the distance to be about the size of a building, so if you're inside of a structure, you're not going to see light leaking on the other side. High-end phones add a second cascade, which gives crisper character shadows as well as lets us push out the shadowing distance a little further. Foliage was another axis of scalability. On low-end devices, we simply don't render foliage. On the mid range, we render about 30% of the density we support on console. And on high-end devices, we actually render 100% of the density that we support on console. Memory is interesting in terms of scalability because it doesn't always correlate with performance. For instance, an iPhone 8 is faster than an iPhone 7 Plus, but it has less physical memory. This means when you're taking into account scalability, you need to treat memory differently. We ended up treating it as an orthogonal axis of scalability and just had 2 buckets, low memory and high memory. For low-memory devices, we disabled foliage and shadows. We also reduced some of the memory pool. So for instance, we limited GPU particles to a total of 16,000 and reduced the pool use for cosmetics and texture memory. We still need to do quite a bit of memory optimization in order to get the game to fit on the device. The most important was level streaming -- basically, just making sure that nothing is in memory that is not around the player. We also used ASTC texture compression and tend to prefer compressing for size rather than quality in most cases. And we also gave our artists a lot of tools for being able to cook out different LODs that aren't needed or reduce audio variations on a per-platform basis. Want to talk a little bit about frame rate targets. So on iOS, we wanted to target 30 fps at the highest visual fidelity possible. However, you can't just max out the device. If we were maxing out the CPU and the GPU the entire time, the operating system would end up downclocking us, then we'd no longer hit our frame rates. We also want to conserve battery life. If players are playing several games in a row during their commute, we want to support that rather than their device dying before they even make it to work. So for this, what we decided to do was to target 60 frames per second for the environment, but vsync at 30, which means most of the time when you're exploring the map in Fortnite, your phone is idle about 50% of the time. Using that time to conserve battery life and keep cool. To make sure that we hit those targets, we track performance every day. So every day, we have an automation pass that goes through. We look at key locations in the map, and we capture performance. So for instance, Tilted Towers, and Shifty Shafts, and all of the usual POIs that you're familiar with in Battle Royale. When one goes over budget, we know we need to dive in, figure out where performance is going, and optimize. We also have daily 100-player playtests where we capture the dynamic performance that you'll only see during a game. We track key performance over time for the match, and then we can take a look at this afterwards and see how it performed, look for hitches, stuff like that. And if something looks off, we can pull an instrumented profile off of the device, take a look at where time was going, and figure out where we need to optimize. We also support replays. This is a feature in Unreal that allow us to go and replay that match from a client perspective. So we can play it over and over, analyze it, profile it, and even see how optimizations would have affected the client in that play session. Going to talk a little bit about metal specifically. So we, on most devices, we have 2 cores, right, and so the way we utilize that is we have a traditional game thread/rendering thread split. On the game thread, we're doing networking, simulation, animation, physics, and so on. The rendering thread does all of scene traversal, culling, and issues all of the Metal API calls. We also have an async thread. Mostly, it's handling streaming tasks -- texture streaming as well as level streaming. On newer devices where we have 2 fast and 4 efficient cores, we add 3 more task threads and enable some of the parallel algorithms available in Unreal. So we take animation, put it, simulate it over on multiple frames, CPU particles, physics, and so on, scene culling, a couple other tasks. I mentioned draw calls earlier. Draw calls were our main performance bottleneck, and this is actually where Metal really helped us out. We found Metal to be somewhere on the order of 3 to 4 times faster than OpenGL for what we were doing, and that allowed us to ship without doing a lot of aggressive work trying to reduce draw calls. We did stuff to reduce draw calls, mostly pulling in cull distance on decorative objects as well as leveraging the hierarchical level of detail system. So here's an example. This is one of those POIs that we tracked over time. If you're familiar with the game, this is looking down on Tilted Towers from a cliff and was kind of our draw call hot spot in the map. As you can see, it takes about 1300 draw calls to render this. This is just for the main pass. It doesn't include shadows, UI, anything else that consumed draw call time. But Metal's really fast here. On an iPhone 8 Plus, we were able to chew through that in under 5 milliseconds. I mentioned hierarchical LOD. This is a feature we have in Unreal where we can take multiple draw calls and generate a simplified version, a simplified mesh, as well as a material so that we can basically render a representation of that area in a single draw call. We use this for taking POIs and generating the simplified versions for rendering very, very far away. For instance, during the skydive, you can see the entire map. In fact, when you're on the map, you can get on a cliff or just build a very high tower of your own and see points of interest from up to 2 kilometers away. Digging into some of the other details on the Metal side, I want to talk a little bit about pipeline state objects. This was something that took us a little bit of time to get into a shippable state for Fortnite. You really want to minimize how many PSOs you're creating while you're simulating the game during the frame. If you create too many, it's very easy to hitch and create a poor player experience. So first of all, follow best practices, right. Compile your functions offline, build your library offline, and pull all of your functions into a single library. But you really want to make sure you create all of your PSOs at load time. But what do you do if you can't do that? So for us, the permutation matrix is just crazy. There's way too many for us to realistically create at load time. We have multiple artist-authored shaders -- thousands of them -- multiple lighting scenarios based on number of shadow cascades and so on, different render target formats, MSAA. The list goes on. We tried to minimize permutations where we could, and this does help. Sometimes a dynamic branch is good enough and better than creating a static permutation, but sometimes not. What we had to do is we decided to identify the most common subset that we're likely to need, and we create those at load. We don't try to create everything. The way we achieved this is we created an automation pass where we basically flew a camera through the environment and recorded all of the PSOs that we actually needed to render the environment. Then, during our daily playtests, we harvested any PSOs that were created that were not caught by that automation pass. The automation pass also catches, like, cosmetics, and effects from firing different weapons, and so on. We take all of that information from automation and from the playtest, combine it into a list. That's what we create at load time, and that's what we ship with the game. It's not perfect, but we find that the number of PSOs we create at runtime is in the single digits for any given play session, on average. And so players don't experience any hitching from PSO creation. Resource allocation. So basically, creating and deleting resources is expensive or can be expensive. It's kind of like, think of [inaudible]. You really want to minimize the number of [inaudible] you're making per frame. You really don't want to be creating and destroying a lot of resources on the fly, but when you're streaming in content dynamically, when you have a lot of movable objects, some of this just isn't possible to avoid. So what we did for buffers is we just used buffer suballocation -- basically, a bend allocation strategy. Upfront, we allocate a big buffer, and then we suballocate small chunks back to the engine to avoid asking Metal for new buffers all the time. And this ended up helping a lot. We also leveraged programmable blending to reduce the number of resolves and restores and the amount of memory bandwidth we use. Specifically, the main use case we have for this is anywhere we need access to scene depth, so things like soft particle blending or projected decals. What we do is during the forward pass, we write our linear depth to the alpha channel. And then, during our decal and translucent passes, all we need to do is use programmable blending to read that alpha channel back, and we can use depth without having ever had to resolve the depth buffer to main memory. We also use it to improve the quality of MSAA. As I mentioned, we do HDR rendering, and a-- just an MSAA resolve of HDR can still lead to very aliased edges. Think of cases where you have a very, very bright sky and a very, very dark foreground. Just doing a box filter over that is, basically, if 1 of those subsamples is incredibly bright and the others are incredibly dark, the result is going to be an incredibly bright pixel. And when tone mapped, it'll be something close to white. You end up with edges that don't look anti-aliased at all. So our solution to this was to do a pre tone map over all of the MSAA samples, then perform the normal MSAA resolve, and then the first postprocessing pass just reverses that pre tone map. We use programmable blending for the pre tone map pass. Otherwise, we'd have to resolve the entire MSAA color buffer to memory and read it back in, which would be unaffordable. Looking forward to some of the work we'd like to do in the future with Metal, parallel rendering. So on macOS, we do support creating command buffers in parallel. On iOS, we'd really need to support parallel command encoders for this to be practical. A lot of our drawing ends up happening in the main forward pass, and so it's important to parallelize that. I think it would be very interesting to sort of see the effects of parallel rendering on a monolithic, fast core versus what we had for parallel command encoders on the efficient cores on higher-end devices. Could be some interesting results in terms of battery usage. Metal heaps. So we'd like to replace our buffer suballocation with Metal heaps -- first, because it'll just simply our code, but second, because we can also use it for textures. We still see an occasional hitch due to texture streaming because we're obviously creating and destroying textures on the fly as we bring textures in and out of memory. Being able to use heaps for this will get rid of those hitches. For us, we just, it's, the work we have in front of us to make that possible is setting up precise fencing between the different passes, right. So we need to know explicitly if a resource is being read or written by a vertex or pixel shader across different passes, and it requires some reworking of some of our renderer to make that happen. And of course, continue to push the high end of graphics on iOS. Last year at WWDC, we showed what was possible by bringing our desktop-class forward renderer to high-end iOS devices, and we continue, we want to continue pushing that bar on iOS, continuing to bring desktop-class features to iOS and looking for opportunities to unify our desktop renderer with the iOS renderer. And with that, I'll hand it back to Michael. So Metal is low overhead out of the box, but rendering many objects efficiently can require multithreading. Metal is built to take advantage of all the GPU, all the CPUs in our systems. Metal is also really accessible, but advanced rendering sometimes requires explicit control. Metal provides this control when you need it for memory management and GPU parallelism. We also introduced indirect command buffers, our brand-new feature that lets you move command generation entirely to the GPU, freeing the CPU for other tasks. Together with argument buffers, these features provide a complete solution to GPU-driven pipelines. And finally, Metal lets you leverage the advanced architecture of the A11 GPU to optimize many rendering techniques for both maximum performance and extended playtime. For more information, please visit our website, and be sure to visit us in tomorrow's lab. Thank you.  Good morning everyone. Happy Friday. My name is Skylar Peterson. I am an engineer on the iOS Accessibility team at Apple, and with me today I have Bhavya, an engineer on the Mac accessibility team at Apple. And together we're going to delve into what it means to create an exceptional experience in your app when it comes to accessibility. As a quick refresher, we tend to think of accessibility as meaning making technology usable by everyone. At Apple, we categorize the way in which we make that technology accessible into four areas of focus, cognitive, motor, vision, and hearing. Now we have a bunch of great features that we work on to address all of these areas, but at the end of the day, it's the work that you do in tandem with our assistive technologies that make our platforms stellar places for people with disabilities. Making sure that all the great content you work on is accessible to everyone creates a better, more inclusive community for all of us and fundamentally improves the lives of millions of people around the world. For today's talk, we're going to skip past a lot of the basics of making apps accessible. However, if this is your first exposure to accessibility or you've never checked out any of our accessibility APIs before, then I highly recommend that you check out some of the sessions from last year's WWDC. There's still a lot that you're going to be able to gain from our session. It may just feel like it's going a little fast for you because today we're going to focus on going beyond the basics, and I want to start by focusing in on this one word, usable. Usable is great, making sure that all of the content of your app is even visible to an assistive technology is a critical first step. But when it comes down to it, we don't want our apps to just be usable, right. No one's going to be out there bragging that their app is usable. They want it to be exceptional. We want people who use them to be delighted when they do. It may not be clear to you exactly what that means when it comes to accessibility, and that's what we're here to talk about today and hopefully provide you with some helpful guidance on how to do that. I'd like to focus on two main areas and the different considerations that you'll want to make for each when evaluating the accessibility of your own app. First, visual design, and second, the way that an assistive technology user experiences your app. So let's start with visual design. I'd like to begin by addressing transparency and blurring. We use blur and transparency across many parts of iOS. It creates a vibrant, unique look and feel for our operating system. But for some people, particularly those with low vision conditions, blur and transparency can have negative impacts on legibility and even cause a degree of eye strain. So we provide an accessibility setting to reduce blur and transparency, and it can have dramatic effects like this. Instead of blurring the backdrop, we fade it so that you still have the context of where you are in the system, but the visual noise is reduced, and the controls which before had color bleed-in from behind are more legible on their solid backdrop. Same with folders and finally spotlight. Now this is a particularly good example, because we've taken a sample of the user's wallpaper so that we can color the backdrop, and they could still feel personalized and contextual to the device, but we still get that increased contrast and legibility of having a solid background. As a developer, you can cater your own use of blurring and transparency by checking whether or not the reduced transparency setting is enabled and adapting your UI accordingly. On iOS, you would use is reduce transparency enabled on UI accessibility, and on macOS, accessibility display should reduce transparency on NS Workspace. Next, I'd like to talk about contrast. Contrast between content and it's background is hugely important when it comes to perceivability. As colors get closer to one another, they become more difficult to distinguish. As well, certain colors that are legible at bigger sizes don't work when content is scaled down. For example, with text, the letters bleed together more easily at smaller sizes. Now you can compute the contrast ratio between a particular color combination to see whether or not it's going to be legible, and we followed the recommendations laid out in the web content accessibility guidelines that states that the minimum contrast ratio you should aim for is 4.5 to 1. Obviously, the highest contrast that you're going to be able to get is between black and white, which comes out to be 21 to 1 and obviously works great at all text sizes. Now let's take a look at a shade of gray. This particular gray will work great for larger text, but it isn't so great for smaller text because your eyes can't distinguish the shapes of the letter forms. It's contrast ratio 4.5 to 1 is right on the line of passable. Let's like take a look at one last gray. It may not be apparent on this really large display, but this text is really hard to see on a small device, even at a large font size. It's contrast ratio of 2.9 to 1 is just too low. And you can use the built-in tool in our accessibility inspector in Xcode to show you what the contrast ratio between a particular color combination is and what text sizes that color combo passes for. And we follow the same guidelines that I mentioned before in this tool. However, even with a ratio that exceeds the recommended contrast ratio of 4.5 to 1, for people with low vision conditions, colors that meet the bar can still be problematic when it comes to legibility. So we provide a setting for increasing the contrast across the system. In previous versions of iOS, this setting was referred to as darken colors. So in your code you can use is darker system colors enabled to check. Though if you're using a standard UIKit control that you set a tint color on, you will actually get this work for free. On a macOS, you can use accessibility display should increase contrast. Next we have sizing. Changing the content size of your device can have dramatic effects in the way that content is displayed and perceived. And this is hugely beneficial to low vision users. So let's take for example a calendar. On the left, we have this view at the default text size, and on the right I've blown it up several sizes to one of the larger accessibility sizes. If we simulate what this looks like to someone with a low vision condition, then the benefits become immediately apparent. That text on the right is actually still legible. On iOS, you can check what content size a user has set their device to, and there are seven standard sizes with large being the default, but you can also enable even larger accessibility sizes, which increase that by five more. Now there's a lot more that I could say about dynamic type, but I'm going to leave it at that and instead encourage you to check out some of the fantastic resources that we have to figure out the best way to get dynamic type working in your own apps. Now for some, changing the size of text is overkill, but the default weight of fonts and glyphs can still make it difficult to read. So on iOS there's a setting for enabling bold text, which will also increase the stroke weight of glyphs. If you're using stand built-in UI controls and a system font, then it's likely that you don't need to do anything to get bold text working in your app, but if you're using your own text solution or a custom font, or you simply want to do something like making the dividing lines in your app thicker when bold text is on, then you can check whether or not the setting is enabled and adapt as necessary. Next, we have motion. Animation is fun, and it often makes content feel more alive. It can provide a direct correlation between user interaction and what effects their actions are having. However, certain conditions, particularly inner ear conditions that affect the balance center of the brain can make motion and animation problematic, which often results in things like dizziness or imbalance or even nausea. Now I'm going to go through some examples of animation types that can be problematic, so if any of you in the audience have a problem with motion, then you may want to look away momentarily as I play the default animations, which I am going to preface. The first is scaling and zooming. On the left, we have the default animation for opening an app, specifically the clock app, which I'm going to show now. We zoom into the location of the app's icon while it's UI scales into view. And on the right, we have the same interaction but with reduce motion enabled, which will replace that with a simple cross fade. Next, spinning and vortex effects. If we take a look at the full screen echo effect in messages, which I'm going to show now, we can see that content is spinning along the Z axis while also changing in scale, and these types of motions paired together can cause issues. In the case of message effects, if reduced motion is enabled, then we provide the user with a specific prompt to play the message effect instead of having it autoplay. So they can choose whether or not they want to see it. Next, we have plane-shifting animations. What I want to show here is the Safari tabs animation. So I'm going to tap on the show tabs button, and you see that the plane of the web cards is shifted to simulate a 3D space. And on the right, we have the reduced animation, which again is a simple cross fade, but we also blink the card so that you have the context of which one you're coming from that you got in the original animation. Multidirectional or multispeed motion is another trigger. I'm going to show the movement of the messages bubbles in a conversation. It's a little hard to see in this video, but it's a lot more obvious on device. As I scroll the messages, they have a springiness to them where they contract and expand away from one another. If we have reduce motion enabled, then we disable this effect and have them just scroll normally. Finally, we have peripheral movement. The weather app on iOS has subtle animations that play in the background to indicate the current weather condition. You cans see on the left that the clouds are sort of drifting through the air and the sun is sort of shimmering and shining. But if you're scanning the forecast below, then this animation is in your peripheral view, and this can be problematic in the same way that reading while riding in a car can make you feel sick. The horizontal motion above your area of focus triggers a reaction in your brain. With reduced motion on, we disable the background animation. Now typically we don't want to just remove all animation, so of course we have a setting that you can check to see if motion should be reduced and adapt accordingly. On iOS, you check is reduced motion enabled, and on macOS, check accessibility display should reduce motion. It's important to note that simply removing an animation is often not an adequate experience. You don't want to lessen the experience. You just want to add something else equally fun or engaging but different that works for the user. Finally, I'd like to finish talking about design by addressing UI complexity. Apps play key roles in our lives now, and it's critically important for all of us that our technology is simple and easy to use, that it enhances our lives instead of adding extra unnecessary burden. In the United States alone, one in six children have a developmental disability ranging from speech and language impairments to more serious developmental disabilities like autism. For people with cognitive disabilities or even a chronic illness, using an app can expend more energy than for someone who is neurotypical. So how do we make sure that our apps are simple and the least burdensome that they can be? Well they should be easy to navigate by having them similarly structured and through clear, logical cause and effect. We should be able to start using our apps and complete the most common tasks without encountering any barriers. And finally, our apps should behave in a consistent manner so that when I learn something one place, it applies to another. Using this standard UIKit views is great because people get familiar with way they work, but UIKit also functions as a design language for iOS. So if you're doing something custom, consistency with a parallel control in UIKit will help people intuit how to use your app. This really all boils down to the way in which people experience your apps, which is just as important for people who use assistive technologies like VoiceOver or Switch Control. So I'd like now to spend some time speaking to how you can improve the experience for those accessibility users. The experience for assistive tech like VoiceOver and Switch Control can be quite different from your typical user experience. VoiceOver uses many gestures like swipes and multifinger taps, while Switch Control scans groups and offers logical quick actions. For both, there are built-in equivalents for any standard gesture or interaction, but what makes experiencing an app with these technologies exceptional? We want to go from the bare minimum of it works to it works well. While despite the fact that assistive tech users experience your app in ways that are different, the same design principles that drive a good experience for a nonaccessibility user applies here. You want easy navigation, predictable behavior, prioritizing our action, and consistency. It's also important to note that if something is communicated contextually by separate elements, that same context is conveyed to our accessibility users. I'd like now to bring Bhavya up on stage to run through a quick audit of the accessibility experience of an app. Bhavya. Good morning, everyone. My name is Bhavya, and I'm a software engineer on the Accessibility team at Apple. Today, I'd like to talk to you about enhancing the accessibility experience of your app by sharing with you an app that Skyler and I have been working on. The name of this app is Exceptional Dogs, and the goal is to help make the dog adoption process easier by allowing users to browse a collection of dogs who are in need of a loving home. Let's take a look at this app. This is Exceptional Dogs. At the top, I have a carousel like UI, which is essentially a collection view of all the different dogs that I can browse. In the bottom left corner, I have a favorite button so that I can favorite particular dogs I like, and in the bottom right corner, I have a gallery button, which brings up a modal view with additional photos of the dog. However, not all dogs have a gallery button. Notice how this button fades away as swipe to Layla, who has no additional pictures. Beneath the carousel I have a stats view with information about the dog like its name and breed. Notice how this information updates as I swipe to the next dog in the carousel. Finally, at the bottom I have the name of the shelter with two buttons which can open the location of the shelter in maps or call the location. So there's quite a bit going on here in terms of visual complexity, but Skyler and I done our due diligence, and we've sprinkled in a little bit of accessibility, just enough to make sure that VoiceOver can reach every element on screen. So let's turn on VoiceOver now and see how a nonsighted or low vision user would digest all of this complexity. VoiceOver on. Exceptional Dogs. Pinky. So VoiceOver lands on the image and correctly announces the name of the dog. Great. I'm going to swipe from left to right now to scroll through the carousel. Layla, Bro, Pudding. Favorite off button, Pudding, Bro. So VoiceOver was able to correctly scroll through the carousel, but we weren't able to reach the favorite button until we hit the very end of the carousel. I could, of course, still get to the favorite button by placing my finger on it like this. Favorite off button. But it's important to remember that this is not how a typical VoiceOver user navigates. Rather, VoiceOver users swipe to reach to the next element, and currently, using swiping we aren't able to reach the favorite or gallery buttons until we hit the very end of the carousel. This isn't a great or exceptional experience because it really limits my ability to favorite or view the galleries for virtually all of the dogs in the carousel. I'll place my finger over the modal view now to activate the gallery button. Show gallery. Show gallery. Picture one of Bro. Image. Let's scroll through our gallery. Picture two of Bro. Picture three of Bro. Pinky. Layla. So, we were successfully able to scroll through the gallery with VoiceOver but focus went behind the modal view to the content in the background. This experience might seem workable because it gets the user to the content that they need to, but it's not great or exceptional because allowing focus to go behind the modal view is confusing and distracting for the user. I elected the modal view now by double tapping on the close button. Close, Layla. Let's swipe through our stats view. Name, breed, Layla, Labrador. When I hear a name, I expect to hear the actual name of the dog right after, but instead I hear breed. Visually, the layout of all this information makes sense, but to VoiceOver, its structure causes it to announce information out of order. This creates a lack of context for the user and places unnecessary cognitive load on them. I'll place my finger over the name of the shelter now and swipe to the buttons next to it. Skyler's animal shelter. Open address in maps. Button. Call. Button. Once again, these buttons are perfectly accessible like the labels from the stat view, yet they lack context. When I hear call, it isn't clear to me what this call action is referring to even though visually this is pretty evident since the call button is right next to the name of the shelter. VoiceOver off. So we found a couple of issues here. These issues don't completely render our app unusable, but they do place unnecessary cognitive load on the user. Remember, our goal here is to create an exceptional accessibility experience. I'd like to invite Skyler back to the stage now to talk about how we can elevate our experience for Exceptional Dogs to be exceptional. So we found some issues with our app. So let's take a look at some of our accessibility API that we can leverage to resolve them. From Bhavya's audit, we found some issues with our carousel. We could get to the favorite and gallery buttons but only after reaching the end of the list instead of for each individual dog. As well, we couldn't get to that display data below for the dog until reaching the end of the list. We need the actions and data for each dog to be quickly reachable. So what we can do is use UI accessibility element to create a new element in place of the collection view. UI accessibility element is a class used to create custom elements oftentimes for representing parts of your UI that are inaccessible, like something you've drawn in Core Graphics for instance. But we can also leverage it to create a custom experience, which is what we're going to do in this case. We'll need to return any custom elements from the accessibility elements property for the view that would contain them. This tells our assistive technology what the accessible sub elements of a given view are and the order that they should be navigable. So if we get our custom element in place, we can swipe to the content for each dog easily. Great. But then how do we change dogs? Well, we can take advantage of accessibility increment and accessibility decrement. By adding the adjustable trait to our custom element, we'll let the assistive technique know that our element responds to these callbacks so that in the case of VoiceOver when a user swipes up or down with their finger, then the collection view will scroll to the next or previous item. Now, next let's take a look at these information labels. When Bhavya was swiping through them, they were out of order, and the value labels lack the context of what information they were providing. Context is hugely important. If I just touch on the value label for whether or not the dog is fostered, all I'm going to hear from VoiceOver is yes. Well, yes what? What does that mean? What does that correspond to? As well, we have an issue with navigation. We're looking at having to swipe through 14 different labels just to find what we're looking for. But combining them would cut that in half and makes navigation much, much faster. It makes more sense to group these labels together as a single element so that I understand the relationship between label and value. We again can use UI accessibility element to accomplish this by creating elements for each pair of labels and returning them from the containing views accessibility elements property. We also saw that when Bhavya was swiping through elements in the app that the shelter info view was split up into three distinct elements, the name of the shelter, the location button, and a call button. The buttons have the same issue of lacking contexts as labels. If you reach them first without reaching the name of the shelter, then it's unclear what they correspond to. Having these three elements exist separately increases the amount of elements that a user is going to have to navigate through to find the content they're looking for. So it makes more sense to combine them into one and give us quicker navigation. As well, we want to think about what the priority is here. The most relevant piece of information is what shelter the dog is housed in. So we'll provide that first and add supplementary actions for locating and calling the shelter. To accomplish this, we can use our accessibility custom actions API. This allows us to add actions to an element that the user can cycle through until they find the one they're looking for and activate it. You can return an array of custom actions for a given view, and then for each action, you have a name, which is conveyed to the user as what's going to happen when they activate that action, and a target selector relationship, which will execute the action in your code. So we'll make that whole shelter view accessible and add custom actions to it for getting the location and calling. Finally, we have the gallery view. When Bhavya brought this up, we could swipe through all of the elements in the background, and though this view apps modal, that isn't the case, because instead of housing this view in a view controller and presenting it modally, we instead opted to just add it on top of a view hierarchy. And so since the content in the background is still visible, VoiceOver doesn't know that it should exclude it. Although everything in the gallery is accessible to VoiceOver, it gets confusing to the user whose lost the context of where they are and what effect their actions are having. To fix this, we want to override accessibility view is modal on the presented view, which will tell our assistive technology that only the elements of that view should be accessible right now. We'll also want to post a notification to let the assistive technology know that our on-screen elements have changed and that it should attempt to refocus. We can use the screen change notification to do so. I'm going to turn things back over to Bhavya now to show you how to use all the API we just covered to implement our solutions. Bhavya? Thanks Skyler. Let's elevate accessibility experience of our app from great to exceptional. I'll jump into Xcode. Let's start with enhancing the experience of the carousel at the top. Here, I have a class called Dog Carousel Container view which houses our collection view of the dogs, our favorite button, and our gallery button. As Skyler mentioned, we'd like to implement the increment and decrement functionality so that we can swipe up and down to scroll through our carousel. To do this, we can create a property on this class called carousel accessibility element, which will be a custom UI accessibility element that supports those gestures. Let's implement carousel accessibility element. This will be a subclass of UI accessibility element. It also needs to know about the current dog object it's representing, so let's create a property for that. We'll give it an initializer also, and let's give it some basic accessibility like the label and the value. Our label will be dog picker, and the value will be a combination of the name of the dog and it's favorited status. Okay. Let's get into the increment and decrement functionality. First, we'd like to override accessibility traits to return the adjustable trait. Let's first implement accessibility increment. We want our increment function to scroll the collection view forward. So let's create a function that does exactly that. Here, I have a function called accessibility scroll collection view. It takes in a bullion, which determines if the collection view will scroll forwards or backwards. Let's dive into this function. I get a reference to my container view, my current dog, and my list of all the dogs. If I'm moving forwards, I get the index of my current dog, and I validate that it doesn't exceed out of bounds, and if doesn't, I'll ask my collection view to scroll forward by one index. I'll do something similar if I'm going backwards except I'll make sure that my index won't become negative, and then I'll ask my collection view to scroll backwards by one index. Let's go ahead and make use of this function in our increment function. I'll pass in true to indicate that I want to scroll forwards. And similarly, I can implement accessibility decrement, the only difference being that I'll pass in false since I want to scroll backwards. And that's it for the carousel accessibility element. Let's scroll back up to our container view. Here, we need to expose the carousel accessibility element, and to do that, we can override accessibility elements. Within this function, I'll get a reference to my current dog and make sure it's valid before I go ahead and create my elements. Let's start by instantiating the carousel accessibility element. I'll first get a reference to the carousel accessibility element if one already exists, and if it doesn't, I'll instantiate one here. I'll set it's accessibility frame to be the frame of the dog collection view, and then I'll store a reference to it here. And at the end, I'll return this array of elements. Before I do that, I want to add some elements to my array. I'll check to see if my current dog has more than one image, and if so, I want my array to comprise of the carousel accessibility element, the favorite button, and the gallery button, but if the dog doesn't have more than one image, we should probably omit the gallery button. One thing I need to remember to do is to update the current dog property on the carousel accessibility element if that corresponding property gets updated on my container view. We're almost done. Recall that when I swipe to the carousel, the information, the content of the stats view is changing. However, when I'm swiping with VoiceOver, I need to let the app, I need to let VoiceOver know to update it's understanding of the accessibility elements on screen. To do that, I can post a layout change notification whenever a scroll occurs. I'll head over in my view controller file, and inside my scroll view did scroll view at the very end I'll post that layout change notification like this. And that's it for the carousel container view. A core part of this solution was using and creating accessibility elements, and I can use this same concept to help me solve a different problem in the stats view. Recall that in the stats view all the information was separated, and it lacked context. I can use this same concept to help me group the information for the stats view together and to add context. I'll head over to the doc stats view where all of these stats are laid out. I'll start by overriding the accessibility elements to create my elements. I want to cache the elements I create from this function so that I don't have to recompute them every single time I make them. To do that, I can create a private property called underscore accessibility elements, which will act as that cache and store these elements. Let's get into my function. First, I'll see if my cache is empty, and if so, I'll go ahead and populate my elements. I'll iterate over all my title labels, and I'll get a reference to the title label, which would be something like breed, and it's corresponding value, which would be something like Labrador. With the references to both the title and it's corresponding value, I can now create an accessibility element, which encompasses the information for both of these together. I'll start by creating my accessibility element here, and I'll set it's label and it's accessibility frame to be a combination of the corresponding values of both the title and the value. I'll add this element to my array of elements, and once I'm done looping over all the title and value pairs, at the end I'll set my cache equal to this array, and I'll return these elements. One thing we need to remember to do is to reset our cache every time the dog object on this gets updated so that we can compute the new elements for the new dog. To do that, I can simply set it to no over here. And that's it for the dog stats view. Next, let's work on adding some context to the call and open location buttons. I'll head over to the shelter info view where I have a reference to the name of my shelter, the map button, and the call button. We'd like the actions of both of these buttons to be custom actions on their containing view, and to achieve this, we can start by exposing that containing view as an accessibility element. Let's go ahead and give this a descriptive label, which would be the name of the shelter. Finally, let's override accessibility custom actions. Within this function, we'll create two custom actions, the map action and the call action. VoiceOver will describe these as open address in maps and call, but be sure to correctly localize these strings. Each of these actions will call a corresponding function, which activates that button. So, for example, map action will call activate map button, which activates the map button. I've conveniently defined this function right here. Be sure to return to [inaudible] to indicate that your method was successful. And that's it for the shelter view. Last but not least, let's work on improving the experience of the modal view. I've defined my modal view in dog modal view, and as Skyler suggested, to help VoiceOver recognize this as a modal view, I need to override accessibility view as modal. We're almost done. One thing to remember is that when I press the gallery button, we need to let the app know that the layout of the content on the screen has changed so VoiceOver can focus to the right content. To do that, I can post a screen change notification whenever the gallery button gets pressed. I'll head over to my view controller file, and within my gallery button press function, once I animate in the modal view, I'll post a screen change notification like this. And that's it. I already have the app prebuilt with all this new accessibility work, so let's take a look at what the new and improved experience is like. I'm going to turn on VoiceOver. VoiceOver on. Exceptional Dogs. Dog picker. Pinky. Adjustable. Swipe up or down with one finger to adjust the value. VoiceOver tells me I can swipe up or down on this. Let's go ahead and do that to scroll through our carousel. Layla. And I'll scroll down, sorry, swipe down. Pinky. I can also swipe from left to right to get to the favorite and gallery buttons like this. Favorite, off, button, show gallery, button. Now, not only is every dog in the carousel accessible to me but so are the favorite and gallery buttons from every dog in the carousel. I can swipe once more from left to right to skip the carousel and get to the stats view like this. Name, Pinky. So I don't have to navigate to the whole carousel. This feels like a much better experience now. Let's go ahead and keep navigating through our stats view to see what the experience is like. I'm going to swipe from left to right to navigate through it. Breed, mixed. Age, 7.0 years. Since this information is grouped together now, it provides me with way more context and navigation is faster since I don't have to navigate through each and every individual label. I'll place my finger over the name of the shelter now. Skyler's animal shelter. Actions available. VoiceOver informs me that there's actions available on this, so let's swipe up to hear them. Open address in maps. Call. Swiping up makes it clear to me that these actions are in reference to the name of the shelter and will be performed on them. It gives me way more context. Finally, I'll activate the modal view by double tapping on the gallery button. Show gallery, button. Picture one of Pinky. Image. I'll swipe from left-to-right to navigate through it. Picture two of, picture three, close, close, close, button. Focus no longer goes behind the modal view. This is a much better experience now. VoiceOver off. Well you just elevated the accessibility experience of our app from simply working to exceptional. We've achieved this by shaping the experience around the user and modelling the interaction around this experience, and in doing so, we've lifted a huge cognitive burden off the user. Your users will love this extra bit of effort you put towards the accessibility of the app so they can love it as much as anyone else. I'd like to hand it back to Skyler now. Thanks, Bhavya. Hopefully Bhavya's demo demonstrated for you how small tweaks can transform the accessibility experience for the better. It's important to note that creating custom interfaces for the accessibility of your app is not always the right answer. Sometimes the simple fix is the best fix, but what you really should be doing when making your apps accessible is thinking through beyond the surface level what it will really mean for someone who's using an assistive technology to use your app. To wrap this up, I'd like to return to where we started today. Usable is a great first step for making apps accessible, but we can do better. We can and we should provide our users with exceptional, delightful experiences that cater to their unique needs. There are a lot of ways to take an accessible app from being functional to being exceptional. Designing for accessibility up front can get you a lot of wins. Doing things like making sure your colors meet minimum recommended contrast ratios or that your layout can adequately adapt to changing size have benefits for all users. But you should also extend you default design to accommodate the settings that users have enabled. Finally, when crafting the accessibility experience for assistive technologies, do so purposefully. Prioritize your content. Make it easy and intuitive to navigate and provide context when necessary. For more information as well as access to a simplified finished version of the sample app you saw there, you can visit our session link on the developer portal. As well, right after this session we're going to be going over to tech lab 9 for an accessibility lab where you can bring any questions you have about making your apps accessible or about elevating the experience of your own app in terms on accessibility. So please come on by if you have any questions. Thank you and have a fantastic rest of your WWDC.  Hello everyone, I'm Julian, later we'll be joined by Hugo who designs sounds across our products. Before we begin I wanted to explain a little bit about why we're going to be talking about a button for a while here. I design and prototype user interfaces for future hardware. Because of the nature of my work I can't always use standard UIKit. I have built and rebuilt a lot of basic user interface elements like scrolling, zooming, paging, and of course buttons and I've learned a lot about the details involved in designing and building these controls. So I'm here to share a little bit about what I've learned with you. I want to show you how something so simple, a button, has a lot of consideration and complexity behind it. And I hope that through looking at this button you can learn a few things. I want you to use standard controls more thoughtfully. To consider when to build your own custom controls. And when you do build custom controls I want you to consider your interactions all the way through. So let's get started. Hugo and I are expanding our artisanal toast app empire. We are building a new simpler app that is connected to a toaster. If you imagine for a moment you're getting ready in the morning, you want to make sure you have breakfast ready by the time you're heading out the door. All you have to do is launch our app, press a button and expect a fresh piece of toast waiting for you in your kitchen. Now this works with our connected toaster. Our connected toaster goes through a few different states, it's ready and waiting for a new piece of toast, it's making toast, and finally the toast is done, the important part. But the way that our connected toaster ends up transitioning between these states is people using our app. When they use our app they can request a piece of toast to start the toaster going. When the toast is done they will of course eat it. Maybe for some reason they'd want to cancel it, I'm not really sure about this one. And of course we want to make sure our toaster is always ready to serve. I'm not going to disclose exactly how this works but trust me it does and it's great. We also have big plans for the future, we could have AR toast, queues of toast, multiple toasters per household, and eventually we'll connect this back up to our toast social network. My point is there's a lot going on in here and for today's session we're going to dive deep into the details of a single screen, in fact a single button on a single screen of our app. If we get to the details of our most important interaction right the rest will flow from it. So if we look at our system again I think the most important interaction here is actually when we request toast. Obviously, all the rest of this has to work, but requesting toast is the first impression and if people aren't able to start the toaster the rest of it doesn't really matter. So to do this today we're going to start by talking a little bit about what a button is, then I'm going to share with you a way of thinking about interactive controls. And finally, Hugo will join us at add some sound to our Button. So what is a button anyway? The buttons that we are talking about today or onscreen user interface controls. Buttons are indirect controllers of action. What I mean by this is when you tap a button the action happens somewhere else. This is as opposed to a direct interaction where here I'm grabbing the lever and controlling the lever directly. Direct interactions can reference the real-world experience and feel more fun. But the real power of indirect interactions like buttons is that they can be clearer and more powerful because of their separation. The interesting thing here being that even the physical buttons that these onscreen buttons are imitating are indirect. Even though the action, the result is happening somewhere else the button controls that action. And just like the action a result can be designed a button can be designed, we can consider both of these things separately and connected. To do this today we're going to be relying heavily on a single lens of design feedback. Feedback is how you let people know what your app is doing. I'm sure a lot of you have thought of a variety of different ways that you can provide feedback within your apps and are already doing so. But I think there are two major categories of feedback. The first is that you can tell people, you can literally put text on screen explaining what's going on or design icons that explain things. The second is that you can show people, you can allow people to experience the results of their action more like the real world through visual, audio or even haptic change over time. Now we're going to apply feedback to three phases of an interaction, not just its completion. We can think of these phases in terms of a physical button. Before I have pressed the button it's just hanging out there. During my interaction with that button it's being pressed in. And finally after I've pressed that button my finger is lifting off of it and it's returning to its original state. So what does it even mean to be providing feedback before I have begun interacting with a button, why does this even matter, the button's just there right. Well if we think about our iPhones for a moment, the iPhone screen is just a piece of glass. This piece of glass happens to have glowing pixels underneath it and what matters is what people think those pixels mean. Glass doesn't tell you that it's tappable, but something on screen might. You could see that you could page through something, drag something or of course for the purposes of this presentation tap on something. The academic term for what I'm talking about is perceived affordance. Affordance refers to the connection between an object and the person interacting with it. A person might know that they can grab a handle or press in on a physical button. But the perceived part of this is talking about what people know about those glowing pixels on screen. People only know what they can interact with on that iPhone screen because of their prior experience having used iOS before and their current context, the fact that they're on this phone in an app that they may or may not understand. So let's apply this to our toast app of course. How do I know what will happen when I tap this button, I just put a button up and call it okay right, no. What we do is provide a label, a label is feedback in which you are telling people literally what is going to happen. In this particular case I tried the label toast, but unfortunately toast refers to both the verb to toast and it also refers to the delicious piece of bread you're going to be eating for breakfast. So what should we call this label, toast toast? No. We could try to design an icon for this button but I think this isn't particularly clear, especially for something that people will be encountering for the first time in an unfamiliar context. So we thought about it a little bit and we came up with the phrase make toast. I know that in our diagram we refer to this as request toast but we spent some time thinking about what people using our app might understand and what feels most comfortable for our app. The next thing to think about is how do I even know that this is a button, just a rectangle floating on a black background might not be enough. But if you add some context like an iPhone screen you might start to understand it or you could add some prior experience. It could be the shape of the button, the manner in which is rendered or if it's used consistently enough just the color of text. In this case, I don't think people are going to be very familiar with our app they're going to be launching it once a day, more if we're lucky so we're going to focus on more standard iOS button shapes because we don't want to throw in something new. So we try these out on screen. Another thing that we can think about with respect to feedback before we begin using this button is how do I even know what this button relates to. It could be something about where in the app a button is located, these buttons have different meaning and feel. Or it could be something about how it's grouped with other controls or how close it is to other objects on screen. I'm starting to like this last one here so we're going to try it out on a real screen. I drew this user interface in Keynote and I put it on a phone and I'm holding this phone in my bedroom where I might expect to use it in the morning before I order toast. And I'm just trying to understand do I know what this button will do. The next thing we should think about is what happens during my interaction with this button. For a physical button I'd be pressing it in. Now the term feedback refers to telling people something about the result of what has happened and in this case, we haven't actually done anything yet, so we might use the term feedforward for this. Because feedforward refers to helping people understand what is happening while they're interacting with this button. Feedforward is a component of making interactions feel fluid, you can convey responsiveness for things like 3-D touch, pinch or slide movements. Thinking about how feedforward applies to our button we should be thinking about what is happening now, what is happening while my finger is making contact with the screen, with that button. In the case of our overall toast system we are in the middle of touching a button to request toast. So the part we're going to focus on is the toaster being ready and eventually transitioning to the toaster making toast. We should be thinking about what is happening and how I can hint toward what will happen. Let's say I've begun touching this button, how do I know that I've begun tapping this button, why does it even matter? Well, for one, the hit area of this button might not be what you expect, if you look closely at the circle maybe I'm not quite touching it. But perhaps the hit area is actually a little bit larger than the visual size, this is particularly important for very small controls if we made that lever interactive. So how do I know I've begun tapping this button? Well you might be tempted to animate the button, to have it fade in or grow very slowly, but this can make things feel unresponsive. This kind of animation might be appropriate for 3-D touch if it's responding continuously as I'm pressing in. But for a quick contact with the screen we might want to do something faster, if we are going to animate it do it very quickly. So what we do is provide a confirmation sound, a haptic or visual change. In this case, we're just going to highlight it instantaneously, make it feel a little bit faster. This also helps us know that a button is enabled and that it can work. Maybe we try connecting the feedforward on the button to the feedforward of the toaster itself, as I said we can start to hint toward what will happen. So here what I'm trying is as I touch this button maybe we show the piece of toast starting to appear. I feel like that might be a little bit too much for this button though, so I'm not going to go forward with that but I tried it. Another thing that we should think about with feedforward is what happens if I change my mind, let's say I'm kind of groggy in the morning, I pick up my phone I accidentally touch this button but I don't want to start my toaster yet. Well for most buttons I can actually drag my finger outside of the button to begin canceling it. This is something that's actually very important for all sorts of fluid interactions because it's what makes a fluid interaction undecided. If you're paging through different screens you don't know which screen it's going to end up on until you lift your finger and the same thing applies to a very simple button. I can bring my finger back inside the button and have the same feedforward show me that I am now activating this button again. Now that we've decided a little bit about the feedforward that we're going to have we should try this out on a device as well. So again, I know this seems really simple for a button, but this is very important for scroll, pinch, paging, a lot of touch interactions. Finally, we should think about how to apply feedback after I have lifted my finger up off of this button. For a physical button it's going to return to its original state. For our onscreen button we're going to have lifted our finger off the screen. This is of course the realm of classic feedback that most of you are familiar with. But the thing I want to point out here is that we can be thinking about feedback both for the button itself and the action, the result that is happening, and we can think about how to connect the two so people understand what has happened. Again, within our system we're only focusing here on getting people into making toast. So what happens when I lift my finger from this button? Does the unhighlight happen immediately or is there a slight delay so that even fast taps can be seen? Is there a timer, does this button support double tap, if so I may have to wait a little bit before showing the confirmation? If we think back to the two types of feedback we can just literally tell people what is happening. I can put some text on screen, I think it's very clear but it also requires reading and more text. So maybe we can show people, maybe the button itself provides some feedback here it's flashing or maybe it's something that represents the action that provides feedback. For example, the icon of the toaster itself could show you that it's starting to toast. Or what we can do is we can just do both, I can put text on screen and I can show the toaster. This is great because I might have missed that animation, but I can still see what state the toaster is in. But similar to interacting with the button with feedforward I should think about what happens if I change my mind. I've started toasting but maybe I want to cancel, maybe I want to stop the toaster. We could replace that button with a stop toasting button, but I think that's not particularly clear because this button looks an awful lot like the make toast button that was up there before. So maybe there's a separate stop button. I feel like that red is a little bit too much, so maybe we try that same button but separate from our make toast button. This is starting to feel a bit better, so what I should do is try it on device again. We should see how the feedback on the button itself connects to the feedback from the toaster communicating what has happened. To recap a little bit of what I went over. Feedback is how you let people know what it is that your app is doing. You can literally tell people, you can put text on screen it's fine or you can show people, you can allow them to experience the change over time. We can apply feedback to all three phases of an interaction, even a simple button. And something I was implicitly doing here is trying out options to understand what's too little and too much. The only way I can really know if an animation is appropriate or not is to try it. So what we've done here is we've designed a very important part of a much larger system, but I think it'll be a lot easier to figure out the flow of the rest of this once we get this one piece right. Now that our button is designed and pretty great I think we can start to think about sound, so I'd like to invite Hugo up to the stage. Thank you Julian. Hello everyone. Now we can make this quick and add a simple click sound to this button. Sounds pretty good or could it be better? Could Julian have done a better job and invited me a little earlier on in the process? I'd like to tell you a few things about how as I a sound designer could approach a project like this. I'll talk about where to look for inspiration, about designing the sound, and some key building blocks that we use while doing this. But before we dive in why do we talk about sound? Now some of you may think our users mute the sound on their phone quite often, is it important to add sound to my app? Well of course it's up to you but George Lucas has said that 50% of the experience of a movie is determined by what we hear, by its music and sound design. And sound is all around us, we use it to navigate through the world. And sound can help enrich the experience of your products. And it helps us remember the experiences of your app, as well as your brand, so it can have a big impact. So where do we look for inspiration when we want to design sound for a button? Well we can start by taking a little detour and looking at some real-world buttons. Some of them are made of cheap materials and sound like this, while others are made of high quality materials and sound like this. Well this was actually a recording of the volume button on an iPhone X, of course we amplified it quite a bit. These sounds though are essentially byproducts, they're not necessarily designed they're defined by the materials these buttons are made of. But because we're designed for software we have a lot of freedom, so we can choose to not play any sounds at all and often that is what we do. But if we do decide it's useful to add sounds to UI elements then we can take cues from real-world analogs to add meaning to these UI sounds and to help our users understand them. Now do you want to add sound to your app? Well it depends, think about category your app falls in, who will use the app and what will their expectations be, and what is the context, where will the app be used. For our app we know it will be used by people who like to eat toast and they're used to making toast in the kitchen, but from now on they can make it from anywhere in the house. They can still though use the audible cues they're used to getting from their toaster in the kitchen, so for that reason we think that this app is a perfect candidate for the use of sound. So let's look at the process of making toast and listen to sounds we encounter. Let's define if these sounds are useful and if they are if we want to use them in our design. So we'll use Julian's timeline for this and listen to the sounds that this toaster makes. First, we pull down the lever and then we press it in place. Now what we hear then is the toaster heating up with a subtle buzzing sound of the coil. [ Buzzing Sound ] Until our toast is done and the toaster ejects the toast. Now let's go through these steps one by one. First, when we pull down the lever we feel the resistance of the spring while we push it down, it's almost like if we compare it to what happens in our app it's like designing sound haptics and animation and synchronizing them all. But in our app, we decided to replace this with a simple button so we don't need to design a sound for it. And when we request the toast that is when we press our button, the next step, and that is what Julian asked me to design a sound for so let's get to work on that. We can record sounds, we can use electronic instruments, synthesizers to generate sounds and then we can use software to scope these sounds into the right shape. Now I'm not going into great detail about this today, but please watch last year's talk Designing Sound if you want to learn more about this process. In the end, we came up with three different options. This is option A, this is option B, and this is option C. Okay now option A is quite minimalist, it's so simple, it works pretty well with the simple UI of the app but it feels like there's something missing. And we found out that while we were designing these that it's much more satisfying if these sounds have two clicks, one when you press down and one when you lift off your finger. Now option B has this character, but and now it is a sound that's much more realistic, it doesn't really sound like a toaster but it does have this physical quality to it. For the same reason though it does sound a little harsh and a little metallic. And then there's option C, it's a bit of an odd one but it does have character and it's a sound that I would recognize if I hear it again if I use my app again. Another thing that's good about this sound is that it has this tonal confirmation built in, it's like throwing a checkmark telling you your action succeeded. I'll play it one more time see if you can hear this. And for all these reasons I think that option C is the one I would recommend going for, it's a simple sound, it has a friendly character, and it's something unique it's not something I'd recognize from any other app. Okay so it looks like we're done, at least we made our button click but we're not done making toast yet. So let's keep going, maybe there are some other opportunities for the use of sounds in this app. Now as good sound designers we want to tell the whole story. So after pressing our button the toaster heats up and we hear this subtle buzzing sound. Now we can -- is this helpful at all this sound? Maybe, it tells us something about the state of the device. So we can choose to play a sound that resembles this on our app as well, maybe a continuous sound, something like this. [ Notification Sound ] But making toast may take several minutes and having this sound play over and over again might not be the experience we're looking for right. So how about something a little more subtle like a timer sound that fades in and out every half a minute or so. That could work. But does this really need a sound at all because what we're really waiting for is what comes next, that's when our toast is done and this is what we want to notify our users about, that's the important moment. So we want to play a clean and simple notification sound that they won't miss, something like this. [ Notification Sound ] Okay so now let's listen to these sounds together. First, we press our button and then we decided for the next step when the process is active we don't really need a sound until we get our notification and the toast is done. And it's important to listen to these sounds together to make sure they family well together because this is how our app communicates to its users, this is the voice of our app. So now our app is in good shape. I'd like to tell you about some key building blocks of sound that we use when designing any kind of sound. The first one is timbre or tone color. And most of you will instantly recognize this one. It's a piano right, but how do we know it's a piano? How do we know it's not a vibraphone? It's determined by the material an instrument is made of, by its shape, and how it's being played using a mallet, a hammer, a bow or your hands. So these instruments can play exactly the same tone but sound very different. Now for our app we decided to go for a character, for a timbre that is friendly and it's not too harsh or metallic. The next one is frequency or the pitch of the sound. Some instruments play at very high frequencies and others play at very low frequencies. But also nonmusical sounds can be played at very high or low frequencies. And when we do this it gives an indication of the size of an object. So if we take our toaster and we record the toast eject sound, we play it at a very high frequency it sounds like a tiny little toaster. But if we take the same sound and play it at a low frequency it sounds like this big giant toaster. So next up is duration or the length of a sound. Sounds can be very short or ring out much longer. And if we design UI sound and we know our button is going to be pressed a lot of times in a row it's important for this sound to be very short and subtle. But if it's only pressed once in a while, which is actually the case for our toast app, it's okay if it's a little longer, if it's a little more elaborate. In fact, this sound only plays once every time we use our app. And then there's loudness or amplitude or volume. Now when we design ringtones or alarm sounds we don't want our users to miss them, we don't want them to miss their phone calls or sleep through their alarms. [ Notification Sound ] Our audio engineer Mitch is calling me from the back of the room and I hear it loud and clear. But if a UI sound would be loud like this it would become unpleasant. It only needs add this subtle [inaudible] layer to the interaction. Now let's quickly reply to Mitch. [ Keyboard Sound ] Now if I would miss this keyboard sound because I'm in a noisy environment it's not a real big problem, but I definitely don't want to miss my ringtones. So to recap the four building blocks of sound, our timbre, frequency, duration and loudness. And when you start thinking about designing sounds and adding sounds to your app it's really good to keep these in mind. And that concludes what I want to tell you about sound today, but before we end the session these are the most important things that Julian and I want to leave you with. Details are designed even if they seem obvious, even if they're as simple as a button, even if they're as simple as a click sound. Now you can take inspiration from real-world elements, but there's no need to copy them one-to-one. Also use the freedom and the flexibility of designing for software to create something that is completely unique and fresh. And then what we see, the animation, and what we feel, the haptics, and what we hear, the sound, they all combine into this one single experience. And lastly, but most importantly, learn by trying things out. You know we tried out a lot of things today and some of them didn't work that well while others worked great. But the only way for us to find out was to try them out first. Now I hope this all inspired you to spend some more time on the details you might normally take for granted. If you get these details right it can make your apps even more amazing. Thank you for listening, have a great day.  Hello. My name is Chris Fleizach and let's talk about making iOS talk using AVSpeechSynthesis. So our agenda for today. What and why is AVSpeechSynthesis? Let's talk about some of the basics. We'll talk about choosing the right voice, some properties that are available like rate, pitch, and volume, and finally attributed strings. AVSpeechSynthesis is an API for generating computer synthesized speech on your iOS devices. It has many uses. For example, you might want to post announcements within your app. You might be creating an interface that's not meant to be looked at. Or you might be creating an education app where having synthesized speech provides a good reinforcement for learning materials. One example, providing audio updates during a workout can be communicated effectively using synthesized speech. A note about synthesis and accessibility. So speech synthesis is a powerful tool for helping many users with many disabilities. For example, cognitive users can get reinforcement about the output that they are experiencing. Users who have trouble vocalizing speech can use synthesis to generate speech for them. Non-sighted users use speech synthesis to consume their interfaces. However, it's important to note that it is not a replacement for VoiceOver or other screen reader technologies. For example, speech can overlap with what VoiceOver is speaking at the same time. And most of the speech won't be available to any devices that are connected such as a Braille device. Instead, you should make your app accessible using the UIAccessibility API. Okay. So let's get to the basics. First step, create an AVSpeechSynthesizer. You can use that with this code snippet. One thing to note is that you want to make sure that it's retained until speech is done. If the synthesizer goes out of scope, any synthesis in-flight will be canceled. Now that we've created a synthesis, the next job is to create an utterance. We can then dispatch that to the synthesizer. So, in this example, we create an utterance with the string hello and then we dispatch it using the speak method. A note about audio sessions. So when AVSpeechSynthesis is activated using speak, the audio session will automatically set to active. If you want to mix with other audio, you can use the setCategory with options on your shared AVAudioSession mixed with others. If you wanted to duck other audio so that your speech is primary but then other audio becomes lower in volume, you can use the duckOthers option. Another note is that the audio session will not be set to inactive after speech is done. That's because it's a shared session only so that if there is any other audio playing at the same time, we would not want to stop that. So if you want to set the audio session to inactive, you would do that yourself. There are some callback methods available in AVSpeechSynthesis that will help inform about the life cycle of an utterance. These are optional methods in the synthesizer delegate protocol. For example, you can know when speech starts, speech finishes, even the character ranges that will be spoken. You can also know when speech is paused or continued. So an example of this in code snippet form, you can set your synthesizer delegate to an object and then implement these methods, for example the didStart in which case you'll get the utterance back, the didFinish which will do the appropriate thing, and then the willSpeakRangeOfSpeechString which will give you back an NSRange which then you can convert into a string range to use that in your string. Now a little demo. Hello, my friends. I'm inside the iPhone. Great. Those are the basics of AVSpeechSynthesis and, as you can see, it's very simple to get off the ground. The next topic is how do you choose a right voice? There are many built-in voices, one for each supported language. Something to note is Siri voices are not available through this API. Users can, however, download higher-quality voices. And when those are downloaded, they will appear in the list of voices. So you can select a voice using either an identifier or a language. If you select a voice by language, it will select the user's default voice. So how do we do that? First you make your utterance and then you set the voice property using AVSpeechSynthesisVoice. As an example of using the identifier initializer, you could get all the speech voices which is an array of speech voice objects and then choose the first one, for example, and then pass in the identifier property. Here are some of the languages that are supported on iOS by default. Now let's talk about some of the properties that you can use on AVSpeechSynthesizer. Speech rate can be controlled using the rate property, and it goes from 0 to 1. Now this will scale speech to go slower or faster. Speech will be scaled from about 0 to 1 with values from 0 to .5. So that means if you pass in .5, you will get the default normal speaking rate. If you want to go faster, you can pass in values from .5 to 1 and that will scale speech from normal speaking rates at 1x all the way up to 4x. To do that, you can create a speech utterance and then set the rate to a float value or you can use some of these constants which are default rate maximum or minimum. Let's talk about pitch and volume. These are also properties you set on the speech utterance. Pitch controls how high the voice is or how low. Volume controls the volume of the voice itself. So, for example, we can set pitch to a very high pitch voice using a one value. We can set volume to a lower volume using .25. Note that lowering the speech volume does not affect the system volume. Finally, let's talk about attributed strings. Attributed strings allow us to customize how speech sounds using different attributes. One available attribute that I'd like to discuss is IPA notation. IPA stands for International Phonetic Alphabet, and it allows us to customize how specialized names, nouns, and other things are pronounced. It's available in these languages. And here is an example of what it might look like. To pronounce the iPhone in the way that we are used to, you would use this IPA notation. Now the obvious question is how do you generate this IPA notation? It is not easy to type with a keyboard. One way to do that is to use the pronunciation editor in accessibility settings. So when you go to the settings app, go into accessibility and speech, you will find pronunciations and find a screen like this. You can enter in the phrase that you want to find the correct pronunciation for, tap the microphone button, and then speak it. After you're done, you'll be presented with options on possible variations. As you tap those, it will say those out loud. You can choose the one that sounds correct and then copy this value that you get into your code. So let's see how that's done. First, we make an attributed string then we can add the attribute for the speech IPA notation using the value that we got from the settings. Finally, we can create a speech utterance with the attributed string. So in summary, AVSpeechSynthesis is a great way to augment your app experience if you add speech at the right time. Multiple languages and voices are available and can be dynamic. Finally, you can customize pronunciation for the way words sound using the IPA notation attribute. For more information, visit the session URL and thanks for watching. Maps are everywhere. People have been recording information about the world around them as far as we can tell. And we use maps for a wide variety of purposes. Like finding a new hiking trail or keeping tabs on the progress of our dinner or dessert order. We can even use maps to orient ourselves in imaginary places, tracking our movement through levels of a game, or a character's position on a fictional battlefield. Mapping technologies continue to evolve, but the essential characteristics of maps remain the same. They are abstract representations of real or fictional places, that show features relative to other features in space, and they offer information about the attributes of those features. Maps are tremendously useful and they're really fun to make. One option to consider is MapKit, our native framework. You can see it here in the activity app. MapKit does a lot of work for you. It integrates cleanly with the rest of our UI. It contains base map data and satellite imagery for the entire world. It allows users to pan and zoom, and it looks great. But depending on your data and design requirements, you sometimes need something a bit more custom. So, we'll talk about the process of map creation and review three key tips for making a map shine, which will be relevant regardless of how you're creating your maps. There's a lot to know about making exceptional maps, but here's an excellent place to start. First, know your audience and what they need. Second, carefully choose your map elements and leave out the things that aren't necessary. And third, focus on hierarchy so that it's immediately clear what is most important. Why know your audience? Well, you need to know what your users are trying to do. Because this knowledge will shape every part of your map, from the features you include to your design decisions. And the success of your map really depends on its responsiveness to those user goals. Keep in mind that you sometimes have more than one group of users with different needs. For instance, if you're making a navigation map, perhaps you'll have some users traveling by bike, others on transit, and the rest by boat. So, these different modes of transportation will mean that your users are concerned about different things. Grade matters a lot to cyclists, so they might need contour lines or other indications about topography. Train passengers are especially likely to want transit stations represented very clearly. And those arriving by boat care much more about dock amenities and bathymetry than others. Keeping these types of needs in mind helps you make a more useful map. Let's look at an example. We're going to build a map for a fictional game about my cat. It's is called Pantera's Big Adventure and in it, players travel Pac-Man style around the neighborhood picking up as many catnip mice as they can before it's time to go home for dinner. We'll build out the game map and use this exercise to illustrate those three tips. This map is a bit unusual for a few reasons. It will also function as a gameboard, it's showing an imaginary place, and perhaps most importantly, it's only going to exist at one scale. If we were making a map where users could zoom in and out, we'd need to think carefully about how our data and display might change at different zooms. But this is a design studio short, not a design studio long. So, we're going to stick with just one zoom. However, the principles are going to be the same whether your mapping a cat's escapades through a fictional neighborhood or making a dynamic map of the whole entire world. So, let's get started by thinking about the purpose of this map. We want to make a game that would be perfect to play for a few minutes on the bus, or while relaxing in a park. And we're going to want to make sure that our map really reflects this. It should be fun, kind of cartoony, cute and lighthearted, and really simple to understand. So, let's think about what players will be doing within this game. It's pretty intuitive, they're going to be moving around the map finding the mice and avoiding the hazards. So, they'll need to see sidewalks, mice, and hazards. Okay, enough talking about maps. Let's get started building it. We're not going to take the time to style it right away and we're going to be iterating on this design throughout. This is going to be a work in progress but we're going to keep the map purpose and the user goals in mind throughout. So, we'll start with sidewalks because the street is too hot for delicate kitty toes and sometimes the grass is damp. And then, we'll add the mice, and now those hazards. Dogs, Pantera is not a fan, as well as fountains, which sometimes unpredictably spray water. So, these are the essential elements from that list. But we should also include Pantera's house, where she begins each round. Of course, Pantera herself. And the pet store where she can regain her health. Okay so, this is starting to come together. Remember we're going to restyle later on, but this map looks kind of bare in places, and doesn't give us all the information that we might really want. So, we can all think of elements that we've seen in other maps. Let's add some of them and then decide whether or not they really belong in this particular map. We could, for example, include the rest of the building footprints. Maybe some polygons for those parks. Perhaps a title. Possibly even a legend. Lots of maps have a compass rose and a scale bar. Oh, right, and since this map is doubling as a gameboard, we'll need a few other elements too. Like a health indicator, a mouse counter. How can we know if we're successful otherwise. And a timer. This game doesn't last forever. All right. But wait. It's easy for maps to get overloaded. And this is a great example of that exact problem. Way too much is happening here. Our map is chaotic and it's not necessarily reflective of our intent, which was to create something that's cute, fun to look at, and easy to understand. And, thinking back to the purpose of this map, we're losing sight of the features that are most important to users, mice, sidewalks, and hazards. So, let's fix this. It's time to think seriously about leaving things out. Because when it comes to maps, knowing what to omit is just as important as knowing what to include. We're going to remove some of the elements that aren't really necessary for this particular map. So, as we know the map exists within our game. So, don't really need a title here though of course we'd want a title on many other maps. Legends are really important for maps with more complicated symbology. But I don't think we need one here. It's pretty straightforward. The compass rose is demanding a lot of attention. But this isn't a navigation map and cardinal directions are irrelevant in this game. And let's lose the scale bar too. We'd want it if we were using the map to plan a trip but were not. Okay. So, take a look over there on the right. We've removed a lot of noise from the map. And we've ended up with a leaner set of elements. Notice too that we aren't including things like road or park labels. They aren't needed for this map, and they would add a lot of clutter. You can probably think of other map elements that we're leaving out too. So, this is moving us in the right direction. But our map definitely still need some work. Let's think about how to modify our display and make sure that we're highlighting what really matters and minimizing what doesn't. So, to do this, we're going to look back on our list of key elements. Sidewalks, mice, and hazards. And remind ourselves of the overall vibe that we're aiming for. Fun, cute, and simple. We'll start with some big changes and then we'll move on to tuning the map. So, first up, our buildings. I like this color of blue, personally. But it's really not working here for two reasons. First many map users associate blue with water, making this neighborhood look like some strange collection of pools. And second, I think the color is distracting here. Remember that the building polygons are really only here to give us some context and fill out the city blocks a bit. This gray is more subdued, and it lets other elements stand out better. But remember, there are those two important buildings. Let's restyle Pantera's house and the pet store too. And for the pet store we'll add a heart that matches the health indicator as a visual cue. Okay. Now to restyle the parks. They're really prominent right now, but they're not as important as the sidewalks, mice, or hazards. It makes sense to leave the park polygons green, since this color is often associated with parks. But I'm not satisfied with the shade. It's too dark and that makes it distracting. Okay, I think this lighter green is a nice change. White still seems like a good color for the sidewalks. Notice how it's standing out against the dark background. But I think we should bump up the line weight. Yeah, better. Okay. So, the so the base map is more muted now, and that means that we can turn our attention to the symbols and game elements. We need to talk about the photos. They're cute, but I don't really think that they're the best choice. The colors are kind of distracting and there's a lot of detail. Remember, maps are an abstracted representation of a place. We don't have to be literal. We often use simple symbols in maps. So, let's try a drawing of a mouse instead. I think this is better in line with our fun and simple aesthetic. But now we need to make sure that the other symbols match too. So, we'll switch up our dogs, and now the fountains. Notice that we lost some detail here. When we were showing the fountains as photos, they were different sizes. But, this is a pretty simple game. So, our fountains don't deal different amounts of damage. We're simplifying the map here by having the symbols match. Also, if we had sized their placement symbols the way we did the photos, the fountain in the larger park would be the biggest symbol on the map, which would suggest that it's way more important than it actually is. Okay, now to change Pantera from a photo to a symbol. Still pretty cute. So, this map is coming right along. We've moved away from the realism of the photos to some symbols that are easier to perceive at just a glance. But now, let's tackle those game elements. They're just floating around at the top of the screen where we placed them initially, and I think that's kind of confusing. We don't need to do a lot to restyle them because I think they're looking pretty good already, but let's collect them and make it really clear that they're part of the game UI and not part of the map. So, first we'll add a shaded bar. And now we're going to reconfigure our health indicator to fit inside of it. Nine lives is hard to accommodate, but could a cat game really have a different number? I don't think so, we've got to make it work. Now, we'll adjust the color, position, and size of our timer and our mouse counter. And now, I think we're getting closer. This map is shaping up to be pretty cute and fun. And it's easy for users to glance at it and see the key features. But you know I think we can make it even better. I want to make sure that the symbols are really standing out against the colors of the base map. Remember, the goal of the game is to collect the mice. So, we can give users a helpful cue by making them a really eye-catching color. Let's try out an orangey-yellow. So, now they pop a little bit more. And let's see what else we can do with color here. I notice that Pantera and the dogs have similar markings on their faces. So, let's color them so that they're more distinct. Pantera can be purple, like her house. And let's make the dogs brown. Now the fountains look a little bit out of place. We should color them too. We'll go for blue, they're water after all. Okay. So, I think we're really getting close. But this isn't quite it yet. I have some concerns about the dogs. We don't want them to actually looks scary, of course. Remember, our game is supposed to be fun and lighthearted. But they almost look too friendly. And we want to make it clear that players need to avoid them. So, let's try adding some eyebrows. Okay. With the addition of these dog eyebrows, I think we've got something pretty good. Happens all the time. So, let's take a look back at our progress. On purpose this time. We started here with all the elements we knew we'd need. And some that we weren't sure about. And then we removed some of those extraneous elements to get here. This is getting clearer, but it's not really done yet. Then we restyled the elements that we kept and we ended up here. Notice that the data is very similar. We've changed the way that we've depicted it, but it's a lot of the same information just presented in a different way. So, thinking back to those initial goals, is this map successful? Is it immediately apparent what matters most? Does it look kind of fun and cute? Do you think players will be able to understand what is where and what it means with just a glance? These are the kinds of questions to ask yourself throughout your design process. Make sure that you're taking every opportunity to show your map to other people and get their feedback too. Maybe your friends, family, and colleagues will agree that your map is nailing all those goals. Maybe they'll feel like it's still work in progress. If that's the case, keep iterating until you feel satisfied. Are you ready to see this map in action? All right. So, I hope this exercise has helped inspire you for the next map that you get to make. Remember to think carefully about knowing your audience, leaving out the things that they don't really need, and making it clear with visual hierarchy, what matters most.  Hi, everyone. My name's Paul Salzman, and my friend Josh Ford and I are very excited to talk to you today about servicing your Siri shortcuts on the Siri watch face. Last year, we released the Siri watch face, which has glanceable information and tappable actions, sorted by their relevance to the user, at every wrist raise. That means our users have access to dynamically updated content, relevant to them throughout the day without any configuration required. And now, in watchOS 5, we are very excited to add your applications as data sources to the Siri watch face. The shortcuts you provide will show up on the watch face, on these items that we call platters. When the user taps on one of these platters, the underlying shortcut will be executed. That can do something like launching into your application into a specific context, or if your shortcut supports background execution, we can run that inline on the watch face. That means users can take advantage of your application's functionality without leaving their watch face. So, let's take a look at what we're going to talk about today. We're going to start off by going over how content appears on the Siri watch face. After that, we'll talk about the Relevant Shortcut API, which is the API you use to provide content to the Siri watch face. We'll also go over how you can use this API within your iOS application to provide content to the Siri watch face. And then, I'm going to hand things over to Josh, who'll talk about our prediction engine, as well as give you insight as to how best to use these API's for your application. So, let's talk about how content appears on the Siri watch face. Everything on the Siri watch face is sorted by its relevance to the user. The more relevant a piece of content is, the higher up on the watch face it's going to appear. And, we calculate relevance by incorporating a number of inputs across the system, like the current time of day, the user's location, their routine, and their engagement with a given data source. You'll provide this content to us with a relevantShortcut, which associates a shortcut with UI customization, and the ability to give us hints as to when to deploy your content. Now, we will derive implicit relevance for the shortcuts you provide based off of your user's past interaction with the shortcuts. But, often you have much more insightful suggestions, especially when showing glanceable information, or if you want to suggest a shortcut that hasn't yet been executed by a user. So, you can provide us things called relevance providers. And, just like a first-party data source, users can disable or re-enable your data source in the Siri face customization page in the iOS Watch app. Now, before we get too far into adopting the Relevant Shortcut API, we want to make sure we're not contending with these relevance calculations when we see how our relevant shortcuts behave and look on the watch face. So, while we're developing, we're going to want to go into the iOS Settings app, into the Developer's page, and find the Shortcuts Testing section. In there, we can ensure that our most recently provided relevant shortcuts show up at the top of the watch face, by enabling the Show Recent Shortcuts option. Additionally, as we get further into adopting this API in our iOS application, we can cause the periodic syncing of relevant shortcuts from the iOS device to the watch to occur immediately by tapping the Force Sync Shortcuts to Watch button. So, now let's talk about relevant shortcuts, and at the core of a relevant shortcut, is a shortcut. Shortcuts encompass key functionality, within your application, that you want to make more accessible to your users. And, they can access these shortcuts by saying key phrases into Siri, or tapping on various system UI. And, in the case of watchOS, that's a platter on the Siri watch face. There's a lot of in-depth discussion about how to make great shortcuts this year. And all will give a high-level overview of how they work with a watchOS persecutive. I highly recommend seeing the "Introduction to Siri Shortcuts" talk, and the "Building for Voice with Siri Shortcuts" that happened earlier in the conference. So, shortcuts can be made out of one of two things. An NSUserActivity, which represents a state within your application you want to accelerate users back into, or an intent, which can execute a task on your user's behalf. Now, intents are really powerful, because they can support background execution, which means that users can take advantage of your functionality without having to launch your app into the foreground. In fact, users can request background-capable intents that are available on their iPhone from their Apple watch or HomePod. And, our frameworks provide a lot of built-in intents you can take advantage of right now, like sending a message, starting a workout, or requesting a ride. But, new in watchOS 5, and iOS 12, you can make your own custom intents that have the functionality that your app does best. There's this awesome in-depth intents definition file and editor built into Xcode. And, I'll give you a couple of pointers related to relevant shortcuts. But, I highly recommend seeing those other talks for full details. Now, let's go over a couple of examples of how shortcuts will execute when requested from a watch app. User generates their shortcut request by tapping on the Siri watch face, or by saying a key voice phrase to Siri, and the watch receives that request. It'll examine it and determine, is there an application that's installed that can handle the shortcut? And, in this example, yes, there is one that's installed. So, we're going to dispatch that shortcut to the appropriate application. If your shortcut is implemented by an intent that can handle background execution, the application's intent execution will run that shortcut. But, if instead your shortcut is built off of a NSUserActivity, or an intent that can't run in the background, the application itself will be launched to handle the shortcut. When the shortcut execution is complete, a result will be generated, and then handed back to the user. Now, let's take a look at another example. In this case our user taps on the Siri watch face, or says a key phrase to Siri to generate the shortcut request. And, the watch will examine it. And, in this case, it'll determine that there isn't an application that's installed that can handle the shortcut. So, we'll check with the phone and see if it has an app that can handle the shortcut. And, in this case, yes there is. So, we're going to forward that request over to the phone. And, the proper application, or intents extension will handle execution. When execution is complete, a result will be generated on the phone, and forward back to the watch, and conveyed to the user. So, now that we understand the key concepts that make up a shortcut, and how it'll execute on the watch, let's talk about relevant shortcuts, which take your shortcuts and show them on the watch face when they're most relevant. We can automatically populate the fields on the platters on the Siri watch face, based on your shortcut content. But, you can also customize the platter's display, which is really useful for displaying glanceable information. And, of course when your platter is tapped, the underlying shortcut will be executed. So, let's take a look at how this will run on this watch face. If your shortcut is backed by a user activity, when the user taps on it, your app will be launched into the appropriate context. If instead, your app is-- or, your shortcut is based off of intents, when the user taps on the platter, we'll see this intent confirmation view. And, if they tap to confirm it, if your intent runs at the background, we'll execute it inline. If instead your intent can't run in the background, we'll launch your application, and hand you the intent to continue execution. So, let's look at the API for Relevant Shortcuts. At the core of a relevant shortcut, of course, is a shortcut. If you want to give us hints as to when this content is relevant, you can provide us relevanceProviders, which we'll go over soon. And, if you want to customize the UI beyond what your shortcut will provide, you can give us a defaultCardTemplate on the watchTemplate property. Now, once you're done creating all of your wonderful relevant shortcuts, you need to let us know about them. So, you want to inform the default relevantShortcutStore. And, the way you do that is providing an array. And, every time you give us an array, it'll erase the previous contents we had in our shortcut store, which is really useful for invalidating stale relevant shortcuts. But, you just need to keep in mind to provide us all of the relevant shortcuts we should be considering. So, let's look at how your content will display on the platter. You can see on the top left, your application's icon will be displayed, followed by your app's name. Below that is a required title string, and below that is an optional subtitle string, that you can use for more context. We'll display that in italics. To the left of both of these strings is an optional custom image. This image supports transparency, and automatically applies rounded corners. You can see more about the dimensions of these assets by looking at the human interface guidelines resources for watchOS. Now, as I mentioned, we can automatically populate all these fields based off of the shortcuts you supply. In the case of a custom intent, every parameter combination you supply has an associated title and subtitle we can use. And, as you create your intents in code to generate a shortcut, you can set an image for any of the parameters. We will choose an image for the custom image, based off of what is available on the most specific parameter. And, the specificity of a parameter is defined by the order of parameters you have listed in your intents definition file. In the case of an NSUserActivity, when you're creating your userActivity to make a shortcut, you'll supply it-- userActivity type that you've listed in your app's info.plist. And, for us to be able to display it without a default card template, you'll need to give us a title on the title property. In iOS you can also supply a subtitle and a custom image by creating a CSSearchableItemAttributeSet. And, on that attribute set, we'll extract a subtitle from the content description property, and the custom image from the thumbnailData property. When you're done configuring this attribute set, set on your NSUserActivity's contentAttributeSet property. If you don't want to give us the content that's baked into your shortcut, you can supply a default card template, which has properties for each of the fields on this platter. And, depending on what we have available to us, we will display a different layout. You can see in the right two configurations, that if no image is provided, we'll lay out the text further to the left, giving you more space for your words. In the bottom two cases, if there's no subtitle, we'll allow the title string to wrap from the top line to the bottom line. If your shortcut is based on an intent, when a user taps on it, they'll see this intent confirmation view. In the top left is your application's icon, which is a bit bigger this time, followed by your app's name. Below that we'll display the title and subtitle directly from your intents definition file. And, at this point, the user has three options. If they want to run your intent, they can click Confirm in that middle pink button. If they don't want to run it, they can tap dismiss or the digital crown. And then, the third option is a bit more subtle. Sometimes users see an intent, realize they want to tweak some of the parameters. So, they can tap that top module, and we'll launch into your application, and hand you the intent you've customized. So, you can present some UI to allow your users to tweak some of the parameters before continuing execution. Now, let's talk about that middle pink Confirm button a little bit more. The string we show there, where it says Action Verb, is derived based off of the category of intent you've defined in your intents definition file. Additionally, that color is chosen from your application's global tint color in your watch app storyboard. Now, it's very important to keep in mind that the intents that you support in your watch app must be a subset of the intents you support in your iOS application. That means you're going to be sharing the same title strings, and subtitle strings on iOS and watchOS. And, watchOS is a very constrained canvas. Every word counts. So, we highly recommend using string dictionaries, with the NSStringVariableWidthRuleType key. That allows you to give us a list of varying sizes of strings that we can choose from, depending on the context we're displaying them in. When providing content to watchOS, we recommend supplying a string with a width rule of 20 for a 38 millimeter watch. And, a width rule of 24 for a 42 millimeter watch. To get more information on adopting this API, please see the "Localizing with Xcode 9" talk from last year's conference. So now, let's talk about relevance providers. Relevance providers are your way to give us a hint as to when we should show your content. They really let us know how we should incorporate inputs like the time of day, the user's location, or their routine, when calculating relevance. And, in fact, you can give more than one relevance provider on a given relevance shortcut. And, we're going to take the inner section of their output. So, if you give us a relevance provider that says a certain time of day's very important. And, another one that says a specific location's important, you'll get high relevance output when both it is that time of day and the user is at that location. If instead you want the union of these relevance providers, you're going to want to provide two separate relevance shortcuts, each one with a different relevance provider. So, let's take a look at the options we have available. The first one is INDateRelevanceProvider, which has a required startDate parameter, and an optional endDate parameter. When you create an instance of this relevance provider, the closer time of day gets to the start that you've provided, the higher the relevance output of the provider. After that time of day passes, the relevance will fall off on a curve, allowing more content to appear on the top of the watch face. And, if you give us this optional endDate, we'll adjust that curve to accommodate. If your content instead is more relevant to a given location, you can use the INLocationRelevanceProvider. It takes a CLRegion as its main parameter when you create an instance of this. As the user gets closer and closer to that region, the relevance output of this relevanceProvider gets higher and higher. Now, you don't always have a specific time of day, or location in mind, for where your content should be relevant. User's schedules, their favorite locations vary all over the place, and you don't want to have to get in the business of tracking all of this. So, you can take advantage of the Siri face's smarts, and use an INDailyRoutineRelevanceProvider, where we have situations for both time of days that might be favorite for your users, and location. For instance, if you have some content you want to show as a daily forecast when your user wakes up, you don't want to have to know if they wake up at 5 A.M. or 10 A.M. You should just give us the morning situation. And, if you've got a workout you want to suggest that requires some gym equipment, you can pass us the gym situation. So, now that we have all of this API in our toolbox, let's build a couple of relevant shortcuts. In my examples we've got a hypothetical meal logging application that allows users to log their meals, and they can opt to do challenges. This week's challenge, our users opted into a veggie challenge. So, we're going to create a user activity for logging meals. It's an activity type is com.myapp.LogMeal. And, because we're going to be displaying this for the dinner meal, we're going to set the value for the meal key in our user info dictionary to Dinner. Once we have our userActivity set up, we can create a shortcut, and from that shortcut, we can create a relevantShortcut. Now, we want to really let our users know that this is showing up on the watch face, because they've opted into the veggie challenge. So, we're going to create a default card template to customize their UI with Log Dinner as our title. And, we're going to convey the veggie challenge in our subtitle and our custom image. Now, our users haven't always been necessarily logging their dinners, but they've opted into this challenge. So, we want to give the system a hint that this should be displayed in the evening by passing a dailyRoutineRelevanceProvider with the situation .evening. And now that our relevantShortcut is configured, we can pass it along to the default relevantShortcutStore. For our next example, in our application, users also have a bunch of favorites they can configure for snacks they eat often, or perhaps a breakfast they had every day. So, we've created a logFavoriteMealIntent. We want a couple of these to surface on the watch face to make it even easier for our users to log their favorite snacks and meals. So, we'll create an instance of our intent, and take one of our favorites, and set it on the favorites parameter. In this case, our user likes to eat cookies. We'll also set an image for that parameter, so that when it shows up on the watch face, they get a little bit more context about what they're about to log. From here, we can take our intent and create a shortcut, and from our shortcut, create a relevantShortcut. At this point, we're ready to pass along our relevantShortcut to the relevant shortcut store. We don't need a default card template, because the title strings and image, the image that we pass along to our intent, should be sufficient. We also don't really want to provide a relevanceProvider here. Because this is something habitual for our users. They log these often throughout the day, and we can take advantage of the Siri face's prediction engine to show it when it most matters to our users. And, once we're done creating our relevantShortcuts, we're not done here actually. We need to be able to handle them as well. And, new in watchOS 5 is a method on your WKExtensionDelegate called handle user shortcut. And, our first example, if the user taps on that Siri platter, our application will be launched, and we'll be handed a user activity to this method, whose activity type matches com.myapp.LogMeal. At this point, we want to make sure we go into the right part of our application. So, we'll jump up to the rootInterfaceController, and push on our logMealInterfaceController. We've got to be sure to pass along that context we put into our user info dictionary, so we know which meal we're about to be logging. For our second example, the common case will be that that background-capable intent will execute successfully in our intents extension. But, there are a couple instances where our application will get a callback directly. The first one being if there's an error during execution, and we say we need to handle this within our app. The other one, though, might happen if the user wants to tweak a couple of the parameters. For instance, they see that they're about to log a cookie, when they really know they just ate 5. So, we'll tap on the top module, we'll launch into your application, and you'll get a callback here. The userActivity you'll receive will have an activity type equal to the name of the class of intent you gave us. At this point, you can extract an interaction off of userActivity, and an intent off of that interaction, which will have all the parameters you set when you created your relevantShortcut. So, now we're ready to create relevant shortcuts whenever our application has runtime. But, it's very important to note, that just because your content is showing up on the Siri watch face, doesn't mean your application is actively running. So, to help you get more runtime to provide relative shortcuts, we've created the WKRelevantShortcutRefresh BackgroundTask. We'll be handing this out to applications that are providing awesome relevant shortcuts that our users are spending time looking at, tapping through, but not scrolling past. When you get one of these refresh background tasks, it's your opportunity to update the data that supplies your relevant shortcuts, and give us a fresh set of relevant shortcuts. And, on a related note, if your background-capable intent executes, it's inside of your intents extension. That means that if you update your data store, it's possible that your UI and your application has become stale. So, in watchOS 5, we've also created the WKIntentDidRunRefresh BackgroundTask. And, we'll hand that to you with some runtime when your intents extension completes a execution of a shortcut. That is your opportunity to make sure that your UI's up to date, perhaps request an updated snapshot, or reload, or extend your complication timeline. So, let's talk about how we can take advantage of these API's on iOS. And, the great news is, with the exception of those WKRefreshBackgroundTasks, we can use all of the same exact API's in our iOS application. The relevant shortcuts we provide in our iOS app will be periodically synced from the phone to the watch. And, merged in with local, relevant shortcuts for consideration to be displayed on the watch face. You have all of the same UI customization options available. The only difference is if your relevant shortcut will execute back on the phone, we'll show your iOS application's icon instead of your watchOS applications icon. And now that we can be showing relevant shortcuts that will execute on the phone or the watch, let's talk about where they will execute when the user taps on them. If a relevant shortcut can be handled by an application that's installed locally on your watch, regardless of where it came from, we're going to execute it in the watchOS application. That gives the user the best experience and the lowest latency. You can ensure that your watchOS application supports execution of a shortcut by making sure the user activity's activityType is in your info.plist, NSUserActivityTyped array. Or, if it's an intent, that your intent is listed in your intents extension. If instead there is no application installed that can handle your shortcut, except on iOS, we'll execute it back on the phone, even if your phone isn't near the user. Now, because we can be executing content back on the phone, we want to make sure that users don't tap on a platter, and then are immediately told to go find their phone. So, we have a couple of rules about the type of shortcuts that will execute back on the phone that we're willing to surface. Requirements are that they're intent-based. They handle background execution. And, they don't require access to protected data. And, that's the data that you keep encrypted when the phone is locked. The way you can ensure that your content meets these requirements is by taking a look at the parameter combination in your custom intent, and ensuring that background execution is supported. And, looking at your overall custom intent and seeing if you have any authentication restrictions you've applied. There are three options here, and we support the first two. The first one is that there are no restrictions. The second one says that you are restricted while the device is locked, which means that at least the watch has to be unlocked for us to execute. And the final one is Restricted While Locked, or Protected Data is Unavailable. Now that you have all this information you're ready to build some awesome shortcuts from both the watch and the phone. So, I'd like to hand things over to Josh to talk a little bit more about our prediction engine. Thank you. Thank you, Paul. Good morning. My name is Josh, and I am an engineer on the watchOS team. And, we're really excited to see the kinds of relevant shortcuts that you can surface on the Siri watch face. So, Paul just walked you through some of the API's and different ways that you can provide your relevant shortcuts to the system. Now, I want to talk about how the system predicts your relevant shortcuts, and the things that you can do to ensure the system surfaces your content when it's most appropriate to the user. It's like Paul talked about earlier, content on the Siri watch face is ordered by relevance to the user. At a wrist raise, we want to be surfacing the content the user cares about most. Whether that be based on the current time, location, or other factors from across the system. And, to figure out what content the user cares about, we're looking at how they interact with the different platters on the Siri watch face. What things are they tapping on? What things are they spending a lot of time looking at? And, what things are they scrolling past to find other content? And, we use all this information to understand what are the different platters that the user wants to see right on a wrist raise. And so, because we're trying to surface, again, the content the user cares about most, you want to make sure that you're providing relevant, engaging content to be surfaced on the Siri watch face. So, let's talk a little bit more about how we actually put your relevant shortcuts on the Siri watch face. So, like Paul mentioned earlier, the first step is you need to provide your relevant shortcuts to the system, into the defaultRelevantShortcutsStore. Once you've provided your relevant shortcuts, we can run them through our machine learning model to figure out what is the best way to be surfacing your relevant shortcuts to the user. So, let's look at what we take into account inside of this model. So, the first thing that we look at is your relevance providers. This is your way to provide additional context to the system that we wouldn't otherwise have. This might be a concrete time, location, or other context. We also look at past behavior. So, how has the user interacted with this relevant shortcut before? Is it something they're spending a lot of time tapping on and looking at on the watch face? Or, something that they're scrolling past to find something more interesting? We also look at a number of different factors from across the system, such as, you know, the current time, current day of the week, current location, user's routine, among a number of other factors. And, take all this into account to try to better understand what is the context under which the user wants to interact with the particular shortcut. And, it's also worth noting that this model is secure and personalized to each user. All of our learning happens on-device, and we're building a model for each and every single user of the Siri watch face. The way that you interact with the watch face may be different from the way that I interact with the watch. And, once we have this model trained we can then take your relevant shortcuts, and again, surface them on the Siri watch face based on their relevance to the user. If you attended some of the previous shortcut talks, you may have heard about donations. This is your way to indicate to the system what the user's doing within your app. And, although they don't actually surface within the UI of the Siri watch face, we take them into account when trying to understand the user's past behavior. So, again, donations are your way to indicate important tasks, or other information about what users are doing inside of your applications, and providing that to the system. And, by giving this information to the system, we can understand and learn patterns in the user's behavior. So, if the user, for example, is looking or performing the same action at the same time every single day, or around similar locations, the system can pick up on these patterns, and again, surface the relevant shortcuts when they're appropriate to your users. So, if you're using NSUserActivities to represent your donations, there's a few things you need to do to be able to provide those donations to the system. The first is you need to set both these properties to "true" in your NSUserActivity, the eligibleForPrediction and eligibleForSearch. Next, you need to make sure that your user activity is supported by an application, by indicating that within your info.plist. And, finally you want to make sure that you're donating these user activities whenever this piece of content is visible to the user, so the system can start to pick up on the patterns. And so, to provide that donation, there's a method on NSUserActivity, becomeCurrent, that allows you to donate to the system. New in watchOS 5, though, there is this method updateUserActivity on WKInterfaceController, where you can attach your user activity to an interfaceController, and whenever this interfaceController's visible to the user, the system will automatically be donating the user activities on your behalf. This is similar to the API's that we have over on iOS, on UIViewController, and UIResponder, where you can, kind of, attach an NSUserActivity to a piece of your UI. And so now, this is the recommended way to be providing NSUserActivity donations on watchOS. If your shortcuts are backed by an intent, you can provide those intent donations by using the INInteraction API. So, if you attended any of the previous shortcut talks, this probably looks familiar to you. But, to provide this donation, first you create your intent, and adjust any parameters as they're necessary for what the user just did within your app. Next, you create your INInteraction with intent, and call the donate method when users perform this particular interaction. The other thing to look out for, is this primary shortcut type for when we're making watchOS predictions. And so, let's take a look at this. Inside of the Xcode intents editor, you'll notice that there's a field to select the primary shortcut type. So, this allows you to indicate to the system, like, kind of, the most critical use cases for your app. And so, let's talk more about what that means. So, by indicating the primary shortcut type, you're, kind of, telling the system what are the use cases that you think your users care about most. And, this helps us expedite our learning process. You can indicate one of these per intent that you've defined. And, for the best performance, the parameter combination, the selection, have a subset of the parameters that you're providing in your relevant shortcuts. And, we'll walk through a couple of examples to try to better understand what that means. But, by giving this information to the system, we can much more quickly pick up on patterns in the user behavior, and much more quickly understand what are the relevant shortcuts that the user cares about most. So, first up, we have this Messaging app. And so, this is something that Paul and I have been using for a while now, so every morning we're sending messages back and forth. Sometimes it's in preparation for this talk, and other times it's about what cookies we want to get at lunch. And so, this app has gone ahead and adopted Siri shortcuts. And, they defined a couple different parameters that their app supports. So, the first one is the recipient. So, who am I sending a message to? And, the second one is the message. So, what is the content of the message I'm trying to send? So, like I mentioned before, right, Paul and I are using this app every single morning, but the actual content of the messages that we're sending varies from day to day. So, because of this, this may not make a good candidate for the primary shortcut type. It's going to take a lot longer before the system can, kind of, understand what are the shortcuts I'm trying to perform. Whereas, Paul and I are having very consistent conversations, so the system can really quickly pick up on this pattern, that every morning, I'm sending messages to Paul. Another little more interesting example, is this app that I use to order my morning coffee. And again, this app has gone and adopted Siri shortcuts. So, they support a couple different parameters. The first one is the type of coffee I want to buy. The next one is the condiments, so do I want cream or sugar? And, finally, the location that I want to pick my coffee up from. So, every morning I use this app and go place an order for my morning coffee. But, depending on where I have a meeting, the location that I want to pick my coffee up from will change from day to day. Maybe I'm going to Infinite Loop, or Apple Park, or up to San Francisco, and I want to pick up my coffee at, you know, the closest store. So, again, because there's a lot of variability in the location, this may not make a good candidate for the primary shortcut type. It's going to take a lot longer for the system to, kind of, understand the patterns in my behavior. Whereas, the actual order that I place is the same thing every single time. I always order a latte with the same amount of cream and same amount of sugar. So, this does make a really great candidate for the primary shortcut type. And, coffee-- it's on its own, might be too generic, right? The coffee plus the condiment might make a better primary shortcut type. And so, that is how you can provide information from within your apps about what users are doing. So, now let's talk about how we predict your relevant shortcuts. And, to talk about this, we have a couple different apps that we're going to be looking at. The first one is this recipes app. And so, this is an app that I use every single day that provides me suggestions for recipes that I might want to try. Next, we have this fitness trainer app that provides me a nice reminder to go out and do my evening run. And finally, there's this travel guide app that tries to surface interesting locations of interest while I'm out and about so I can make sure to go check them out. And so, each of these three different apps falls into one of these, kind of, three different categories that we have. The first one is what we call "Downtime." And so, this is something that doesn't have a concrete time or location associated with it. When I want to interact with this recipes app may vary from when you want to interact with this particular app. Whereas, this fitness trainer app has a very concrete time associated with it, right? I've gone and configured that 8 P.M. is the perfect time to be surfacing this reminder to me. And, by giving this information to the system, we can much more accurately rank this particular relevant shortcut against everything else that we have to consider. Similarly, this travel guide app knows where they want to be surfacing their particular relevant shortcuts, right? They know all the locations of interest, in this case, Golden Gate Park. And again, by passing that information to the system, we can much more accurately predict the relevance of this particular shortcut. And, to provide this additional context to the system, whether that be a concrete time, location, or other information, you can provide that through the relevanceProvider API. And so, this allows you to indicate hints to the system at when your content is most important to your users. It also allows you to surface new content that users may not have seen before. Since you're providing this additional hints to the system, we don't need to have seen as much of the user's past behavior and consistency before the system becomes confident that this is something the user cares about. But, it's also important to keep in mind that user engagement is taken into account at every step of the process. If you provide relevance providers or not, we want to make sure that the content that we're surfacing is stuff that the user cares about. So, let's take another look at this recipes app. So, like what we talked about earlier, there's not really a concrete time or location associated with it, right? The time that I want to interact with this app may be for cooking dinner, whereas for you it might be for cooking lunch. And so, in this case, we may actually indicate an empty set of relevanceProviders. And, in this case, the system will determine the relevance of this particular shortcut based on how the user's interacted with it before. So, for me, I always use this around 7 P.M. because that's when I go cook dinner, so that's when the system will start to surface this particular relevant shortcut. If we take a look, however, at this fitness trainer app, which does have a concrete time associated with it, you can provide that by creating a dateRelevanceProvider, and in this case providing the 8 P.M. start time. And, by giving this information to the system, we will surface this content around a specific time of day. And, as you move closer and closer and closer to 8 P.M., this content becomes more and more relevant to the user. So, let's look at what-- that. So, here's a couple cards that I might have on my Siri watch face. The first is a reminder from Calendar for a prep meeting for this talk. Next, is an app that I use that gives me interesting tidbits of space news. And finally, we have this fitness trainer app, so let's focus in on that. So, I just woke up, I'm getting ready for my day. You know, it's, kind of, 8 A.M. And so, you can see that this fitness trainer app is actually already starting to surface on the Siri watch face. We want to make sure that users are, kind of, aware of all the things they have going on during the course of the day, but it's kind of placed much lower on the face. It's not the most pressing thing to the user right now. But, as we start moving throughout the day, right, that calendar event that I had is no longer relevant to me. And so, this fitness trainer suggestion will start to bubble up the face, right? We're getting closer and closer to 8 P.M. This content is becoming more important to me. And, once we finally reach 8 P.M., right, this is the most important thing to me right now. Want to make sure I don't forget to go for my run. So, it'd be surfaced up at the top of the Siri watch face. Now, let's take a look at the travel guide app, which again is trying to surface content around interesting locations of interest the user. And, as you may have guessed, you can provide this information to the system using a locationRelevanceProvider. In this case, near Golden Gate Park. And, by providing this information to the system, we will automatically be surfacing this content when the user starts getting close to Golden Gate Park. And, again, as the user gets closer and closer and closer to Golden Gate Park, this content becomes more relevant to them. So, if I just arrive up in San Francisco, you can see that we're already starting to surface this suggestion to go check out Golden Gate Park. It's displayed lower on the face, because it's not the most pressing thing to me right now. But, we want to make sure that the user is aware about this. And, as I move closer and closer to Golden Gate Park, this content becomes more relevant to me, until I've finally arrived, and I can really easily tap on this to dive into more details, and check out what are the things I can do while I'm at Golden Gate Park. When you're creating your locationRelevanceProviders, you provide to the system a CLRegion. And so, this allows you to indicate two important properties. The first is the actual coordinates, the lat, long that you're interested in surfacing your content, along with the radius. So, how close does the user need to be before this content is relevant to them? There's also a couple properties in CLRegion that you can use to, kind of, adjust how the system interprets your region. The first one is notifyOnEntry. So, as the name might suggest, the system waits until the user has entered into this region before this piece of content becomes relevant to the user. Similarly, there's a notifyOnExit property that you can set, where the system will wait until the user has left this particular region before this content becomes relevant. We actually use both these two properties with the existing location based reminders on the Siri watch face, to get, kind of, a geofencing-like behavior. By default on CLRegion, both these properties are set to "true." And, in that case, we'll smoothly interpolate the relevance based on how close the user is to a particular location. And so, creating your CLRegion is really easy. In this example, we're creating one around Apple Park, so I've brought in the coordinates for Apple Park, along with the radius. So, how close do I need to be? In this case, 2 kilometers. Next, I will go adjust the notifyOnEntry, notifyOnExit properties as they make sense for my use case. And, once I have my region fully configured, I can really easily create a locationRelevanceProvider. A couple of quick notes about the locationRelevanceProvider. In order to use it, your app needs location authorization, otherwise we're going to ignore this particular locationRelevanceProvider. And, to preserve user battery life, the number of location updates we get during the course of an hour is limited, so keep that in mind. So, that's some of the ways that you can provide concrete times and locations to the system. Now, let's talk about what you can do to try to provide personalization. So, we have this dailyRoutineRelevanceProvider. And so, like Paul mentioned earlier, this allows you to surface content at meaningful times or locations to the user. And, these times and locations are personalized to each and every single user of the Siri watch face. So, the first situation that we'll talk about is the morning situation. This allows you to surface content right when the user wakes up in the morning. We actually already used this for the existing first-party weather data source on the Siri watch face, so we can provide the user a nice summary of the forecast at the beginning of the day, and it quickly dismisses, making room for other content they might be interested in. We also have an evening situation. And so, this allows us to surface content before the user goes to bed. So, we use this one for the new heart rate card in watchOS 5, so that we can provide the user a summary of their heart rate activity throughout the day. And, creating a dailyRoutineRelevanceProvider is really easy. You've just got to specify which situation you're interested in. So, in this case, we can really easily create a dailyRoutineRelevanceProvider to surface content right when the user wakes up. And, the behavior for both these two situations is very similar to a dateLocation-- or a dateRelevanceProvider. Except that the system is automatically figuring out what are the dates that we should be surfacing this content. We also have a few situations available to you that allow you to surface content in meaningful locations to the user, whether that be home, work, school, or when the user arrives to the gym. And again, creating one of these dailyRoutineRelevanceProviders is really easy. You just need to specify which situation you're interested in. And, for all of these situations, the behavior of this relevanceProvider is similar again to a locationRelevanceProvider, except that the system is automatically figuring out the locations on your behalf. And so, as the user gets closer to these locations, your content will become more relevant to your users. So, that is some of the insight into how we predict your relevant shortcuts on the Siri watch face. And, some of the things that you can do to provide additional context to the system so that we can surface your content when it's most appropriate. Now, let's talk about how you can build a great experience on the Siri watch face. So, we've been working on the Siri watch face for a while now. And, we've learned quite a few things along the way. First one of those things is there's kind of, two high-level categories of content that we think works great on the Siri watch face. The first one of those is glanceable information. So, being able to really easily, on a wrist raise, get a snippet of information that I care about, and being able to tap on it when it's appropriate to dive into more details. The second is tappable actions. So, being able to really easily, from my watch face, tap on a platter, execute a complex action, and get right back to my day. So, let's take a look at the glanceable information. So, you can see we have this recipes app again. And, throughout the day, when I'm glancing at my wrist, I can get a nice snippet of information, right? I can see the recipe that I might be cooking later today. And, I can really easily decide is this something I want to make, or do I want to go out tonight? And, when I'm getting ready to go home in the evening, I can really easily tap on it to dive into this app and see, do I need to stop at the grocery store on my way home? So, if you're trying to provide glanceable information to the user, there's a few things that you want to keep in mind. Make sure that you're surfacing the most critical information in your relevant shortcuts. This is the content that users are going to see at a wrist raise throughout the day, and this is the content that they really want to see right on their watch face. When the user taps on your relevant shortcut, it should be opening up into a location within your app, and providing them additional details. Accidental taps on the Siri watch face do happen, and so, you want to make sure you're not kicking off any long-running background task that the user isn't aware of. And, like Paul mentioned earlier, the system will automatically be giving you an additional background refresh task to go provide new content. Take advantage of this, so you make sure you have relevant and interesting content available on the Siri watch face. And, whenever you get new data, whether that be through this background refresh task, because you've gone to the network and downloaded new data, or because the user has started using your app, make sure that you provide new relevant shortcuts the system can always be surfacing the most interesting content to the user right on a wrist raise. And finally, if the information that you're trying to surface is timely, indicate that to us through the relevance providers, so the system can surface your content, again, when it's most appropriate. And so, that is glanceable information. Now, let's talk a little bit about tappable actions on the Siri watch face. So, you can see we have this fitness trainer app again. And, I can really easily, right on my wrist, get this reminder that I need to go out for my run. And tapping on it, I get this nice confirmation to make sure that this is the actual action that I want to execute. And, in two taps, I can start this workout. I don't need to spend time digging through apps to find this particular fitness trainer app, and start-- kick off my workout. I can really easily in two taps, right from my watch face, kick off this workout. So, if you're providing tappable actions, a few things to keep in mind. Your intents are running within your Siri extension. And, this allows the system to automatically run your intents in the background, so the user never has to leave the Siri watch face to go run your actions. We always display the standard system confirmation UI, so that users know the action they're going to run, and we can make sure this is what they want to perform. You also want to make sure that the relevant shortcuts that you're creating are fully specified, so that your intents extension can handle them without any additional user confirmation. So that when the user taps on one of these relevant shortcuts on the Siri watch face, we don't need to open up your app to continue and get, you know, additional information from the user. We can kick off your SiriKit extension, run it in the background, and the user can get on with their day. And, you want to be providing commonly used tasks within your app as relevant shortcuts. These are the kind of tasks that users are using your apps a lot for, and that they want to have really quick and easy access to from the watch face. And, you also want to make sure that you're providing relevant shortcuts often. You may not know the last time that you provided relevant shortcuts to the system is, and you want to make sure that your users can always access your actions right from the Siri watch face. If you're going to be providing these when your app starts up, do it on a background thread so you're not slowing down your app launch. So, we've talked a lot about relevant shortcuts, and the ways that you can surface your information and actions right on the Siri watch face. You want to make sure that you're also providing relevant and engaging content. This is the kind of content that users want to see right on a wrist raise, and the kind of content that we are surfacing on the Siri watch face, and trying to promote to the top. The richest experience happens when you have a watchOS app. This is when the system can give you additional background refresh tasks, and there's no latency between the user tapping on one of the relevant shortcuts, and us beginning execution. If you have any questions, we have a lab later today. We would love to talk to you guys about relevant shortcuts and the Siri watch face. And, we're really excited to see the kinds of experiences that you guys can create right on the watch face. Thank you.  Good morning, everyone, and welcome to New Ways to Work with Workouts. My name is Niharika [phonetic]. I'm an engineer on the Fitness team here at Apple, and today I'm going to be joined by my colleague Karim, an engineer on the Health team. And we are so excited to share with all of you some of the brand-new features and capabilities that have been added to HealthKit in iOS 12 and watchOS 5. Apple Watch was released 3 years ago now, and since the beginning, our users have loved engaging with its health and fitness features. They've loved closing their rings and earning achievements, sharing with their friends, but they've especially loved to do workouts. And the watch has really become just the beginning of what is a vibrant, evolving ecosystem, and that is thanks to contributions from developers like all of you. In just the last quarter of 2017, there were over 200 million downloads of apps in the Health and Fitness category on the App Store, and that number itself is incredible. Two hundred million is so, so cool. But more than that, it's a testament to 2 things. Number 1, it's a testament to the dedication all of you have shown in getting these great user experiences out there. But number 2, it's a sign that people really care. People are engaged in the space, and they are so excited about what all of you are putting out there. And at the heart of this ecosystem are these 2 apps, Activity and Health. Activity is a home for you to visualize just that -- your activity data. You can see your workouts. You can see calories, exercise minutes. And Health is all of that and more. And of course, at the heart of all of that, which is the reason we're here today, is HealthKit. And so we have a ton of exciting things to talk about today, but before we get started with any of that, we have to talk about something extremely important, and that is privacy and authorization, because data is sensitive, especially health data, and it is so important to make sure that you have a comprehensive privacy story when working with your apps. Next, Karim is going to share our brand-new workout features, which have made it easier than ever before to develop a robust workout app. And lastly, I'm so excited to share our new Quantity series API, which is a brand-new way to both store and relate high-frequency data. So let's get started. Like I mentioned, privacy and authorization has to come at the beginning of any development story. And at Apple, we think about privacy in 1 simple way: Privacy is about people. And HealthKit is designed with this in mind. HealthKit is designed to put users in control of their data. Users have the ability to grant access as well as revoke whenever they want. And as developers, all of you are the last link of that puzzle, and we want to make sure we are giving you the tools to make sure that your users' privacy is respected. And we think about this in a few simple rules. Number 1 is this idea of proportional collection, and this is the idea that you should only be collecting data relative to what your app needs. And this isn't a fixed set of data. At the beginning, this amount might be small. As your use cases, as your features grow, the amount of data you need to collect might be more, but it is so important that you only collect what you need at a specific time. And second is the idea that HealthKit authorization can change. And this one's a little nuanced. For example, if a user resets their location and privacy data, their location and privacy setting, it is so important for you to make sure you're honoring that, and it is your responsibility to make sure you do that. And the best way to do this is to treat HealthKit as the source of truth because HealthKit will update to reflect that authorization status, and it is so important to make sure you don't cache any of that and make sure you're always turning to HealthKit to ask what data you have access to. And these 2 ideas can be distilled into 3 simple rules. And when we're thinking about privacy and authorization, you should ask for only what you need, only when you need it, and every single time you need it. And in code, this is just as simple. Let's say I'm creating a workout app, and I'm just getting started, and, of course, I need to start with privacy and authorization. First, since I'm creating a workout app, I want to share the workout type, so I explicitly declare that I'm going to be sharing the workout type. Next, since I'm just getting started, I only have a few things that I'm trying to track. I want to be able to track my user's heart rate, calories, as well as distance walking and running. And this is all I need right now, and so that's all I'm going to ask for. And lastly, on my healthStore, I request authorization for just that. I request my types to share and my types to read. And with these 3 simple steps, we can really make sure that we are taking the steps necessary to respect our user's privacy. And privacy is so important because great apps have great privacy, and it's so important that you give attention to this at the beginning of any development process. And now that we have access to our users' data, I'm so excited to introduce Karim [phonetic] to tell you all about the brand-new workout features. Hello, everyone. Thank you so much for joining us today. I am really excited to tell you all about the new workout API. If you're new to HealthKit, you'll see just how easy it is to create a full-featured workout app from scratch. If, on the other hand, you have a workout app on the App Store, we'll show you all of the great new features of the API that your app can benefit from. So let's dive in. First, I would like to take a look at the general workout app life cycle. So let's say we're building a workout app to track our users' activity, and the first step is to set up the app. So at this state here, we know that the user wants to perform a workout, the type of activity, and we need to make sure that our UI is ready for that. Once the setup is done, we can go ahead and start the workout. So after this point, the user will be actively working out. And then, of course, after some time, the user will want to end the workout. And from there, we can save this workout and all of its associated data in HealthKit. So this is the general life cycle of a workout app. Now, let's see what happens when the user is active. With your user interface at the center, one of the tasks that you want to accomplish is to collect data that is relevant to the workout and displayed in your user interface. And you also want to make sure that the user can control the state of the session so that the user can pause or resume the workout. And if you're familiar with HealthKit, you might know about HKWorkoutSession on watchOS. HKWorkoutSession allows you to handle a few of these steps in the life cycle. It will prepare the device's sensors for data collection set so that you can accurately collect data that's relevant to the workout like calories and heart rate. It will also allow your application to run in the background when the workout is active. You'll be, also be able to use HKWorkoutSession to control the state of this, the workout. And finally, you can also generate events like swimming laps in segments. But of course, this is not enough, and you also need to collect data that is relevant to the workout that's generated by the device and be able to save it all in HealthKit. And to do just that, I'm really happy to introduce a new class, HKWorkoutBuilder. The workout builder is your one-stop shop to collecting data and saving it in HealthKit. So the builder will create and save an HKWorkout object, which is a representation of a workout in HealthKit. You can then add samples, events, and also custom metadata to the builder while building the workout. And if you're on watchOS, you can use its subclass, HKLiveWorkoutBuilder. So it's watchOS only, and because it's a subclass of the workout builder, it has all of the great benefits of the builder, but because it's on watchOS and it works closely with HKWorkoutSession, it has all of these really cool features like automatic sample and event collection, which we'll talk about later. So let's go back to the workout app life cycle, and we'll see how we can now set up and start the workout using the new workout builder. So first, we need to create a workout builder. And to do that, we use the initializer, and we have to pass a healthStore object, a configuration which contains information like the workout type, whether it's indoor or outdoor, and other information. And finally, an optional device if the data comes from an external device, for example. Once my builder is created, all I need to do is call beginCollection and pass a start date. It's that simple. If you are on watchOS, you can use the live workout builder. And so first, you need to create a workout session by passing the healthStore and workoutConfiguration, and then you do not create the builder yourself, but you retrieve it directly from the session by using this call. Once we have the session and builder, we can go ahead and start them up. And so it's as easy as calling startActivity on the session and beginCollection on the builder by passing a start date. So this is how you set up and start a workout using the builder. Now, let's see how we can collect data displayed directly in our user interface and be able to control the state of the workout when the user requests it. So to collect data, if we want to add samples that are relevant to the workout, like calories, or distance, or heart rate, we can do that using a very simple call, builder.add, and give it an array of HKSamples. If we have events that we want to add to the workout, it's very similar. We just call builder .addWorkoutEvents. For metadata, you guessed it. With the dictionary of metadata, we just call builder.addMetadata and pass the dictionary. But on Apple Watch, because the device is on your wrist and has all this wide range of sensors, you can actually generate data like calories and distance. And wouldn't it be cool if we could have all of this data automatically collected by the builder? Starting with watchOS 5, we now have automatic data collection, and it all starts with HKLiveWorkoutDataSource. So the data source will collect samples that are relevant to the workout currently running. You do, however, have the option which types you want to collect, so you can add or remove types as you wish. And this is how it works. First, we create a data source. And here, I pass the healthStore as well as the workoutConfiguration. And since I'm passing the configuration that contains information about the activity type, the data source will be able to infer which types should be collected for this workout. Next, I simply need to set the data source on the live workout builder. And optionally, you can add or remove types that are being collected. So let's say I want to add the type. I can specify which type I want to add here as well as an optional predicate if I want to further limit which types are being collected. And finally, we just call dataSource. collectStatistics for and pass the type as well as the optionalPredicate. So now that we have data being collected, let's see how we can display it directly in our user interface. So every time new data comes in the builder's delegate method, workout builder did collect data of collected types will be called. And so here, if, for example, I'm interested in the heart rate type, I will make sure that it's one of the collected types. And then, I simply make use of the builder's statistics for quantity type method, which gives me an HKStatistics object that will contain information, in this case, like minimum, maximum, average, as well as most recent value for the heart rate that's recorded throughout this workout. From there, I can simply update my user interface. One thing that's also very common in workout apps is to have an elapsed timer. And so, of course, we start the timer at the beginning of the workout, but that's not enough because pause or resume events will affect duration of the workout. So every time new events are added to the builder, the builder's delegate method workoutBuilderDid CollectEvents will be called. And from here, you can simply use the builder's elapsedTime property, which will give you the current elapsed time for the workout. It's that easy. Next, let's see how we can control the state of the workout. So first, I'd like to talk about all of the different states that the workout session can be in. And the first one is Not Started. So here, the workout has not started yet, and we're not yet, the user is not performing the workout. So after that, we'll be moving to the Prepared state. And so the Prepared state will put the system in Session Mode, which means that your application will have background running, the sensors will also be ready to track the activity, but the workout has not started yet. So if you, for example, have a countdown in your app before a workout, you can move the session to the Prepared state before that to make sure that the device is ready to start tracking the activity. After that, we can move to the Running state. And here, the user is actively working out. And of course, the user can move to the Paused, back to Running, and back and forth between these 2 until the workout is Stopped. So here, the device is still in Session Mode, which means that your app is still running in the background, so that gives you some extra time that, if you needed to save the workout. After that, we can simply end the workout. And from there, the system will exit Session Mode. And to be able to move between all these states, you simply need to use these calls directly on the workout session. So that's how we can collect data displayed in our user interface as well as control the state of the session. Let's see now how we can end the workout and save it in HealthKit. So to end the workout, you first call session.end, and then you also call builder. endCollection and pass the workout's end date. At this point, no data is going to be collected for this workout. And then, if you want to save the workout in HealthKit, you just call builder. finishWorkout, and you will get back a workout object in the completion handler that's already saved in HealthKit with all of its associated data. So that's how you now build a workout app using the new API. And now, I'd like to show you how this all works in a demo where we will be building a workout app for Apple Watch. So I'm actually currently building a workout app that will allow my users to track running workouts. And it's a very simple app. I have a big Run button here. When I tap on it, I'm presented with this user interface that will show an elapsed time of their workout as well as some metrics, like calorie burned, the most recent heart rate, as well as the distance that's run in the workout. Of course, my user will have the option to control the state of the workout, so my user can pause, resume, and end the workout. So I already went ahead and implemented the user interface. So all that's left for me now is to use the new workout API and make my app functional. And the first thing that we need to do here is make sure that the project is properly set up. And so to do that, I'm going to head over to my Project Settings, go to the Capabilities tab, and here just make sure that the HealthKit Capability is turned on. And you should also do the same for your WatchKit app extension. So again, make sure that the HealthKit Capability is turned on. Once I did that, we should also add the 2 purpose strings in the info.plist file that will tell my users why I need access to their health data. And so here, the first string is "health share user description" that tells my users why I would like to save data in HealthKit as well as help update usage description. That will tell them why I need to read data in HealthKit. Once that's done, I can now start using the new workout API. And of course, the first step before being able to use the API is to make sure that I request authorization for the data that I need. And so, of course, every app is different. And in my case, if we go back to my application, I will be saving a workout here. So that's one of the types that I will be needing access to. And I also need to be able to read calories, heart rate, as well as distance. So let's do that. And of course, we need to make sure that we always ask for authorization when we need it. In my case here, I want to do that every time the user is presented with this UI so that I can make sure that I have the authorization before starting a workout. So this view here is backed by my workoutStartView Watchkit interface controller, and I'm going to go ahead and implement the authorization in the didAppear method. So first, my typesToShare here is workout because I want to be able to save a workout at the end. Next, I would like to read heartRate, activeEnergyBurned, as well as distanceWalkingRunning. Once I have my typesToShare and typesToRead, I just need to call requestAuthorization on the healthStore and pass the types. So let's go ahead and run this code now and see what happens. And because the watch screen is small, the user will be presented with an authorization sheet on the phone. So you need to make sure to also handle authorization on your iPhone app. Going to go ahead and dismiss this view here. And so let's open my iPhone app. And here, I'm presented with an authorization sheet where I can decide if I want to grant this application access or not. For the purpose of this demo, I'm going to turn all the categories on and tap Allow. And that's how we have now granted the authorization to my application. Next, we can now finally get started with the workout API. So when I tap on this button here, this view will be passed, all the workoutConfiguration object that contains information about the workout. In this instance here, it's a running workout. And from there, I can go ahead and set up my workout. So this view here is backed by my workout session, WatchKit interface controller, and I'm going to go do the setup in the awake method. So first, I'm going to create the HKWorkoutSession as well as the HKLiveWorkoutBuilder. So we create the session using its initializer, and we pass the workout configuration to it. From there, we simply retrieve the builder directly from the session. Creating a session might fail if the workout configuration is invalid, so that's why I wrap my code here in a do-catch block and dismiss if I have any failures. Next, let's set up the session and builder. So in this case, I want my interface controller to be the delegate of both the session and the builder. And because I also want to collect samples generated by the device, I'm going to use here a live data source and pass it a workout configuration so that the types are automatically inferred for me. Finally, I simply start the session as well as the builder. So now that my workout is started, the first thing that I need to do is to start my elapsedTime timer. And so I'm going to do that here in the callback of the beingCollectionCall. I have a method here that will do that for me. And of course, because pause and resume events can also affect the duration, I'm going to have this call in the builder's didCollectEvent call as well. Now, let's implement this method. So in my UI, I'm using a WatchKit interface timer. If you're not familiar with this object, this object needs a date in the past from which it will start counting from. So I'm going to go ahead and create my date object using the builder's elapsedTime property. And because I want the date to be in the past, I'm going to make sure that I have a minus sign here. Next, I simply dispatch on the main queue because I'm doing UI work, and then I just set the date on the timer. I also need to make sure that the timer is only running when the session is running as well. So let's do that. First, I grab the session state, and then, again, I dispatch on the main queue. And if the session is running, I'm going to start the timer. And if it's not, I'm going to call Stop on the timer. And that's all you need to do to get the timer tracking the elapsed time. Next, let's make sure that our metrics displayed in the UI are also accurate. So every time new samples are collected by the builder, this method here will be called. And so here, I'm just going to go and iterate over the collectedTypes. And, in my case, all of the data that I'm interested in is quantity samples. So I'm going to make sure that I only handle those. And from there, we can make use of the new builder's method, builder.statistics, for quantity types, which will give me an HKStatistics object that contains information like minimum, maximum, average, and most recent values for each type. So I grab the statistics object here, and from there, I have a method that I already implemented that will give me the user interface label for this particular quantity type. And from there, it just update my label using the statistics object. Next, let's make sure that we can control the state when the user requests it. So every time my user will tap the Pause button, this method here will be called. And from there, I simply need to call session.pause. For resume, it's very similar. I'm going to do session.resume. And finally, to end the workout, we're going to call session.end and then builder.endCollection and pass the workout's end date. And from there, we're just going to call finishWorkout to actually save the workout and all of its associated data in HealthKit. And then, we just dismissed it here. So let's go ahead and run this code and see what happens now. So I'm going to go ahead and start my workout here. And as we see now, I have my timer updating. I have data directly collected by the builder and displayed right in my user interface. And of course, I can also react to the state changes, so going to go ahead and pause the workout. My timer has paused. Data collection as well. I can resume. And just like that, with a few lines of code, I now have a fully functional workout app. So I'm going to stop the workout now, and let's see what we have just saved in the health app on the iPhone. So this is my workout here that I have just saved, and we have now a very rich representation of this workout saved directly in HealthKit. I have the device details as well as all of the samples that were associated, like heart rate, distance, and active energy, and, also, the workout events that were generated when I paused and resumed the session. And that's how easy it is now to build a full-featured workout app on Apple Watch. But of course, sometimes things don't go as planned. Let's say I'm running my first marathon and I'm really excited about it and using my favorite workout app to track this marathon, but I only realize at the end that the app actually crashed sometime in the middle, making me lose all of this data. Well, with watchOS 5, we are introducing workout recovery that will solve that problem for you. If your application happens to crash during an active workout, we will automatically relaunch it and give it a chance to recover the workout. The workout session as well as the builder will be recovered in their previous state, so you should not call startActivity or beginCollection on the builder. If you are using a data source to automatically collect data, you do need to set it up again, and this is how it works. If your app is relaunched after a crash, we will be calling the WatchKit extension delegate method, handleActive WorkoutRecovery. From there, you simply need to create a healthStore object and then call recoverActive Workout Session, and you will get back a session in the completion handler. It's that simple. So this is our new workout API. It makes it easier than ever to build great workout apps. We also have a new Quantity series API. And for that, I'd like to hand it back to my colleague Niharika to tell you more about it. Isn't that incredible? In less than 10 minutes, Karim was able to build a fully functional workout app. And this is so important because, like I mentioned earlier, our users love doing workouts. And with these workouts come a lot of data. Let's say I'm interested in making a soccer app. I really want to understand exactly how my users are moving, and I want to track the distance they're moving as they go side to side, across the field during the duration of a workout. And what happens when we start the workout is that samples start coming in. My user first goes 2.25. They then go 1.6. They go a little more. And then, they finish it off. And in the past, each one of these distances would be stored as 1 individual HKQuantity sample. Each one would be independent, and they would be stored separately. And so we thought that, wouldn't it be great if you could actually just have 1 sample that tracks that cumulative total but is still backed by those individual quantities behind it? And that's why we've introduced HKCumulative QuantitySeriesSample, which is a brand new sample type that allows you to more efficiently store high-frequency data, and it's great for 2 reasons. Number 1, you only have to store 1 sample that is backed by multiple quantities, and so you're more efficiently storing the high-frequency data that comes from things like workouts. And number 2, you now have a sense of connection between the quantities that make up a sample. So our new quantity series sample is a subclass of HKQuantitySample, which some of you may already be familiar with. And if you've interacted with HKQuantitySample before, our new sample type will be very much the same. So why would you want to use this? Let's say you're interested in data visualization. Your app requires complex charting or graphs, and you're trying to really visualize the data that your users have in a beautiful way. We actually suggest that you continue using our HKStatisticsQuery or our HKStatistics CollectionQuery. And that's because those are preexisting queries that do just this. You have access to really fine-grained, rich data, and these queries will already get the fine-grained data on our new series samples, so you won't have to do any extra work to get those details. If, for example, you're interested in data analysis, you want to go 1 step deeper. You want to understand the actual quantities that make up a sample. We suggest you use our new series sample query. And lastly, if you're like me and have a soccer app [laughs] or any app that has really high-frequency data collection, we suggest that you use our new Quantity series sample builder, which is a brand-new way for you to actually be able to create those new cumulative Quantity series samples and store that data in a more efficient way. So let's start with our sample query. Like I mentioned, this is a way for you to be able to really look deeper and understand the individual HK quantities that make up a Quantity series sample. And in code, we, of course, first want to start with figuring out where we're going to store our new quantities. In my case, I'm going to store it in an array of quantities. We next want to initialize our query. We initialize it with the sample that we're interested in. And in our completion handler is where the bulk of the work happens. In my case, I have a method called analyzeQuantity which is going to do the work that I'm interested in with the quantity. In my case, that's adding it to the array of quantities that I declared earlier. And lastly, we execute our query. It's really that simple. For those of you familiar with some of our other queries, this behaves in a very similar way. Next, we have our Quantity series sample builder, which, like I mentioned, is really great for apps that do have high-frequency data ingestion because it's a more efficient way to store that data but also allows you to kind of create connections between that. So first, we, of course, want to start with creating our sample builder. We initialize with the healthStore and we initialize it with the quantityType that we're interested in. Next, for whatever cadence you're interested in inserting samples, you do that. Let's use my soccer app as an example. I am super interested in the actual distances that my users are moving, so I'm going to declare the meter unit as the quantity that I'm interested in, and I insert that into my seriesBuilder. And at the end, we call finish on my series, and we're actually going to get the new Quantity series sample and be able to have stored that in a more efficient way. And so we're really, really excited for you to start implemented our new Quantity series APIs in your apps because it's a brand-new way to store your data more efficiently, and its ability to be able to relate data together really allows for some interesting analysis. So we have talked about a lot of interesting things, and we think that the best way to think about them is actually as parts of a whole because together they really make up a way for you to respect your users' user experience from start to finish. Of course, we have to start with respecting user privacy. Our users are giving access to one of their most sensitive data, and it is so important for us to make sure that we're treating it with the respect that it deserves. Next, with the brand-new workout features Karim mentioned, we're able to create seamless user experiences. With things like workout crash recovery, we're able to really deliver an experience to our users that they expect. And lastly, by implementing our new Quantity series APIs, you're respecting the device's capabilities and really making sure that, from start to finish, you are giving your users the best experience possible. And as the health and fitness space and as workout apps just keep growing and users keep using them more, we're so excited to see how these steps come together to create really incredible experiences. So for more information, please check out our developer documentation, but we'd actually love to see you in person today. We're going to be at a lab today from 1 to 3 as well as our Health, Fitness, and Research Get-Together later this evening, and we'd love to answer any of your questions, and hear your stories, and meet you in person. We also want to mention that we have a brand-new features in HealthKit, and we'd love for you all to check out their talk, Accessing Health Records with HealthKit. So thank you so much. We're so excited to see what you all do with this. Thank you.  Hi. I'm Eric Dudiak, an engineer on the Xcode team. And today we're going to talk about source control workflows in Xcode. Now, developing apps requires making lots of changes in your source code and projects. To help manage that change Xcode provides a number of source control tools. So, today we're going to take a look at a few of them. To start, we're going to take a look at how you create your first project using source control. Next, we'll look at making and viewing changes to your project using source control. We're also going to look at how you host and share your repositories with your team, including synchronizing changes. We'll take a brief look at resolving and avoiding conflicts. And finally, take a quick look at pull requests and forking. So first, let's take a look at creating a new project using Git, the industry standard source control system, supported and included in Xcode. The first step to using Git is to set up your author name and email. This identifies you in Git commits and makes it easy to identify who made what changes when working on a team. Xcode supports quickly setting this up in the preferences under the Git options of the Source Control tab. When creating a new project in Xcode, you can also choose to create a Git repository. During the Save operation for a new project you can simply check the box shown here, and Xcode will create one automatically. Now let's look at what that does and what a Git repository is. So you may be familiar with the typical Xcode project, represented here by a folder on disk. This includes your source files and things like that. Now, when you check the box to also create a Git repository, you'll get a .Git folder. This represents the repository for your project and will be hidden from you normally. Now it gets put into your project folder, making it a working copy. The first thing that happens with a new repository created in Xcode is that the whole copy of your project is made and snapshotted at a point in time. This snapshot of your project is known as a commit. Each commit in Xcode gets a unique identifier. As you make changes to our project source code you can create commits, which will take further snapshots of your project throughout time, while you see those changes at the point they were created. These snapshots make up your project's history. As you make more, they form something of a timeline. And this information is what makes up your repository. So now that we've discussed conceptually how this works, let's take a look at making changes and easily tracking them in Xcode. One of the easiest ways to do this is with the Source Control Change bar. It highlights the lines of code you change as you change them. The change bar makes it easy to see where in a file you've made changes when you come back later. As changes are made, the bar highlights the lines of files with modifications. In large files, it also makes it quick to jump between your changes. In fact, you can quickly jump between them from the Navigate menu in Xcode. In addition to the change bar, you'll see status flags in the project navigator telling you which files have changed since you last committed. So now that we've made some changes, let's talk about committing. These are snapshots of your project at a point in time that you can reference later. Let's take a brief look at creating these commits in Xcode. So we've seen you can easily see changes as you make them in your project. Now, when you're ready to record these changes to your repository, use the Source Control menu to create a commit. The commit sheet will show all of the changes currently in your project. They show the side-by-side comparison for you to review. You can select which files and even which specific changes to include in the commit. With the appropriate changes selected, enter a message to record the reason for your changes. This message makes it easy to understand the reason for the changes later when going back to review them. Now that we've made a couple commits, let's talk about viewing them. You may want to refer back to them to better understand how your code and project have changed over time. This can be useful for discovering the timeline of when code was introduced in your project, and why specific changes were made. Xcode offers several ways to view this history. First is the comparison mode, which we just saw a little bit in the commit sheet. Xcode lets you view every file in your project as it has changed through time in this way. To access it, use the Version Editor mode from the toolbar, seen here. Clicking and holding the selector will let you jump to a specific mode of the version editor. The comparison mode provides a side-by-side view of source code changes, allowing you to compare the file between two points in time. While in the comparison mode, you can change what version of the file you are viewing using the jump bar at the bottom. Next is the Author View of the version editor. It is accessed in the same way in the toolbar. The Author View groups code by the author who made the most recent change to a particular line in the file. It shows the author, message, and date of the commit. Additional information about the commit can be seen by clicking on one of the slices. Last, it is possible to see a log of each change made to a file. Just like authors and comparison, it is accessed from the toolbar. The source control log looks at a file's entire history so you can see who made changes and why over the entire history of the file. Now, sometimes you want a way to see changes made using source control across not just a single file, but your entire project. To see this, you can select the Source Control Navigator as the second navigator in Xcode. The Source Control Navigator provides a view of your whole repository by listing branches and tags. Branches are the individual streams of history that make up your project, such as the current in development work. When you start a new project, you'll have just one branch, typically named Master. Selecting it will show the history of that branch, which can be seen here. In this history view, you can see the history is marked with tags shown in purple. Tags are bookmarks of particularly important points in your project such as shipped versions. For more information on branches, viewing history, and tags, please see the 2017 WWDC Session, GitHub, and the new Source control Workflows in Xcode 9. Now that we've looked at the benefits of having a project under source control, let's look at taking things to the next level. So far we've seen what happens when a project is just locally managed by Git on your machine. But in many cases, you'll want to store a copy on a server. This provides an off-site backup as well as a means to synchronize your changes from one machine to another. This is also the basis of collaboration in Xcode projects. Xcode works with any server that supports hosting Git repositories. In these cases, the features are limited to standard Git operations such as push and pull. And there is no account to sign into. You just authenticate on demand as needed. Xcode also supports many common hosting solutions such as GitHub, BitBucket, and GitLab.com. With these services, additional features are supported, such as searching for repositories to clone and creating new repositories on the server directly from Xcode. In addition to the cloud versions of these services, self-hosted and enterprise versions used on premise by larger teams are also supported in Xcode. So let's take a look at adding one of these hosted accounts. You can add this account type from Xcode's preferences under the Account Preference pane. You just sign into the account using your username and password, just like you would on the website. If you're signing into a self-hosted version, you'll also get a chance to provide the URL for the specific server that you want to connect to. Now that Xcode is signed into a hosted source control account, we can start sending our changes up to the server and getting other changes from it. But before we start pushing and pulling code in Xcode, let's take a brief moment to discuss how Git and these hosting solutions handle security. Git supports two main methods for securing changes when connecting to a server. The first, HTTPS, is the same way most websites are secured. It trusts the server based on a certificate and uses a username and password to authenticate you as a user. The other method, SSH, works a bit differently, particularly with these hosted solutions. In general, SSH connections for Git are secured using a public and private key pair that is generated on each machine. The public portion of the pair is then uploaded to the hosting site. This means that it is easy to have one set of keys per machine. And often even one per service so they can be managed and revoked much more easily than a password. Xcode can help you easily do these setup steps for SSH. When you sign into an account but have not created an SSH key pair locally, Xcode will offer to create a pair. The private portion of the SSH key should be protected by a pass phrase. This adds an additional layer of security and prevents it from being used even in the event someone else were to get a copy of your private key. Once the key pair has been created, Xcode can also upload the public portion of the key directly to the hosting site. With the public key upload complete, Xcode can transmit Git data securely to the server over the SSH protocol in addition to HTTPS. That also means we can now create new repositories on the server and clone existing ones. So let's take a quick look at that. If we go back to our local only repository, we can now create a new remote for it. This is a full copy of that repository up on the hosted site. We can do this from the Source Control Navigator in the Context menu by selecting Create New Remote. We then have a few options depending on the hosting site. And we can choose to make it either public or private, based on our exact needs for this project. It can then be shared with other developers or synched across machines. And when you want to download a project that has already been hosted on a server, such as on a new machine or when joining a team, you can browse and search for a repository in the clone window, accessed from the source control menu in Xcode. If you already had the URL to use for the repository, you can directly enter it in the search field. Additionally, all of the hosting solutions supported in Xcode offer the option to clone projects directly in Xcode via a button on their websites. For additional information on using hosted services including creating remote repositories and the varies clone workflows, please refer to the 2017 WWDC session GitHub and the New Source control Workflows in Xcode 9. Now that we have a local and remote copy of our project, we need to make sure that they stay in sync. In Git that is done by operations known as pulling and pushing. After you commit one or more changes locally, you will want to push to upload them to the server. This can be done either directly while committing, right in the commit sheet, or from the Xcode source control menu. The push sheet allows you to select exactly what branch you want to push to and optionally allows you to include tags you've created locally during the push. Now, when working with others, it'll often be necessary to get their changes locally. For this you'll need to do what is referred to in Git as a pull. This, similarly, can be done from the source control menu. Now, Xcode offers two options for pulling, either the Git default of using a merge, or Xcode can pull using a rebase operation. These work slightly differently in Git. So let's take a look at them. Here we will look at a conceptual timeline of commits. If you've made local changes and others have also made changes while you were working, you will have to pull before you push. In this situation, your work, shown in green, has diverged from your coworker's work, shown here in blue. To rectify this, you'll have two options in Git: merging and rebasing. Let's take a look at both. So again, in this scenario you have two changes to push and three to pull. When merging, this situation is resolved by creating a new commit, after both yours and your coworker's, that indicates how the divergence should be handled. With the commits now unified back into a single branch, this can be pushed up to the server and synchronized across all your machines. A rebase pull works a little differently. Instead of creating a new merge commit, your local changes are set aside and then replayed after the changes you just pulled. This can make looking back at history much more simple, as there is no merge commit or divergence in history. Sometimes when pulling, you will have made local commits that change something in the same place as someone else is making changes. This can cause what is known in Git as a conflict, where it is unclear how to have both changes coexist. Xcode allows you to resolve conflicts when pulling or merging a branch. Xcode will present a sheet similar to the commit sheet with options to take your change, or the other change. It is also possible to manually edit the file, or take both changes if there are better ways to combine the work. In this case, we see that two users have both made changes on the same line. It seems that the local changes are a bit more up to date, so the easiest way to resolve this is to take the left changes. Now, with all the conflicts resolved, the pull can continue. If this were a merge pull, conflicts are all resolved at once and the resolution is stored in the merge commit. If we were doing a pull rebase, it is possible to have to resolve a set of conflicts multiple times as each individual commit is replayed on top of your coworker's changes. The resolution information in this case is stored on the original commit as if it never occurred. Now, because conflicts can be a bit annoying to resolve, it can be very useful to anticipate conflicts and avoid having them happen in the first place. In Xcode Source Control Preferences, the Change bar can optionally be shown to show coworker's changes as they push them. This makes it easy to tell what part of a file is out of date. The changes are fetched from the server at a 10-minute interval. Here we see the Change bar that we saw before, when upstream changes are showing, turns to red to indicate where our coworkers have been making changes. This indicates a conflict will have to be resolved to reintegrate the local changes. In many cases it can be easier to pull before making changes to a file that already has changes upstream. Some details about the conflicting commit can be seen by clicking on the Change bar. All of the hosting solutions supported in Xcode also support two other common workflows, pull requests and forks. These are based on Git concepts and features, but are distinct from Git's feature set. Pull requests are a method for doing code reviews. It is usually best practice to do all disruptive work to a project on a branch. This is an isolated line of commit history that is separate from the other history of a project. Just like when pulling, the work done on a branch will diverge from the main branch of code. So a merge will be necessary to resolve any conflicts and reintegrate the work. A pull request, shown here in yellow, is a way to see what will be merged and allow the commenting on that work by other individuals. Often this will mean making additional changes on the branch before merging and integrating it. Since pull requests are built on Git's branching model, you can always locally check out that branch from the source control navigator in Xcode when reviewing it. This lets you build and test the work on your local machine before approving it and merging it back into the main branch. Forks take advantage of the distributed nature of Git. Just like how the server copy of the repository and the local copy on your machine are two copies of the same repository, you can create multiple copies of a repository on the same server. This is often useful when there is a canonical copy that is tightly managed, such as a large open-source project. A fork can be useful for making experimental changes without disrupting the main copy. This can be more useful than a branch when there are many contributors, because each fork can have its own named branches, minimizing the amount of noise in the main repository. Just like the local copy, the fork can be synced with the main copy. This is often done in the form of PR's and is where pull requests get their name. Xcode and Git allow setting multiple remotes on your local repository. So you can sync with both the name and the fork from the same local copy on your machine. So that covers the basics of using source control and Git in Xcode. We've seen how to use Git locally to manage your project. We've taken a look at making and viewing changes in your project. We also saw how repositories can be hosted and changes synchronized when working with teams. And when syncing those changes, how you resolve and even avoid conflicts. And last, we took a brief look at some additional hosting features like pull requests and forks. More information for this session is available at Developer.apple.com. And thank you. Good morning and welcome to Getting Ready for Business Chat, I'm Dan and very shortly my esteemed colleague Scottie will be joining me on stage. Last year at WWDC2017 Grant and Scottie announced Business Chat Developer Preview as a great new feature of our Messages app, our most popular app for Apple users to securely and richly be able to message with brands, consumer brands, all around the world for customer service and commerce. We also launched our Apple Business Register at registered.apple. com/business-chat as the fundamental way for businesses and customer service platform providers, which I'm going to call CSPs from now on to easily submit their application of interest for access to a Business Chat account. Since then we've had a ton of interest with thousands of businesses and developers submitting applications for Business Chat. Thank you very much for the overwhelming fantastic response and thank you for your patience. After a few months of working closely with some major brands and their CSP providers we announced the public release of Business Chat Beta in March. Now when we announced this we also announced that in order to use Business Chat users had to upgrade to these versions of iOS, macOS and watchOS or later in order to message with brands using Business Chat. And experienced some really rich message conversation with great showcase brands in the retail sector, in finance, in hospitality, and mobile services supported by their CSP providers like LivePerson, Salesforce, Nuance, and InTheChat. Hopefully you've had a chance to actually experiment and play with and, in fact, use some of these great experiences with these brands. But in case you haven't just as a quick reminder let me show you an example where I can easily start a conversation with T-Mobile by swiping on the home screen for iOS search, typing in T-Mobile and then looking at the search results, finding this specific business preview, tapping on the message button, and immediately going into a conversation with T-Mobile. And in this example the T-Mobile automated response clarifies whether I'm a current customer or not with a list picker, so I tap on that, I get a choice, I select Heck yeah. And then now the automated response wants to get me to actually log in so we can get into a lot of personal details about my account. I tap on the rich bubble, it opens an iMessage app for authentication, I enter my credentials and I'm in. All easily done within the message conversation. And so now I can continue to have a very detailed conversation about my account and by the way, if I get really busy and I have to leave to go do something else when I get back home or back to the office I can pick right back up where I left off on my Mac with the Messages app. Or if I'm busy and I'm on the go I can receive and send Business Chat messages from my Apple watch. Okay so that brings us to today and I'm really excited to announce that we're going to open the doors significantly wider to many more consumer brands around the world and more customer service platform providers. Very excited yes, thank you. So let's look at some key steps that I think are important that we've learned that we want to relay back to you about how to get ready whether you're a CSP, whether you're a consumer brand or whether you're a brand developer. Starting with CSP providers. There are two qualifying criteria that we look at closely when thinking about the potential for a new CSP provider for Business Chat and I'm going to talk about those in a sec. We also then once approved spend and invest time with CSP providers to help them get ready to provide a great business chat user experience and to get off to a very fast start and we'll talk about those quickly. Starting with servicing medium to large consumer brands. For now, we're going to be very focused on medium to large consumer brands. So as a qualified CSP you have this base of customers and you're providing them with a platform solution that provides contact management for these brands, especially for messaging. And as a qualified CSP you have these consumer brand customers around the world in various regions, whether it's North or South America, whether it's Europe, whether it's Japan, Southeast Asia, Australia, New Zealand or globally, but you do have a base of these consumer brands, medium to large consumer brands around the world in one or more regions. And as a qualified CSP you provide a fully featured platform, so what do we mean by that? Well let's look at some major features that we look for in perspective CSPs that are going to provide a great experience for their customers with Business Chat. So starting with let's look at this first example of an agent console from LivePerson. The LivePerson live engage console has the opportunity because of its built-in capabilities for what we call dynamic widgets. And so a dynamic widget provides the agent the capability to populate with the right content both image and text based on how Business Chat operates based on our API and built-in functionality so that the agent can simply tap on that to send the payload down to me on my iPhone when I'm communicating with the Lowe's agent so that I can make a simple selection because I'm looking for smart locks that support HomeKit. Very easy to do, but the console functionality is very important to have this built in. So then I'm ready to make a purchase and I see oh, not my HomeKit smart locks I see red heeled shoes, thanks Scottie. So now I guess I'm going to be buying red heeled shoes. So but in this case the same thing an intelligent dynamic widget that populates for the agent the Apple Pay payload that they can easily tap on again to send the payload to me so that all I have to do now is tap to authenticate using Touch ID or Face ID. A very efficient, very powerful way to implement Business Chat. So let's talk about backend systems, so backend systems integration is also very important. Your platform must have APIs that support your customers' backend systems such as CRM, scheduling, product inventory, inventory ordering, order entry, payment systems. And in this example, it's a Salesforce lightning service console where because of a previous conversation they had my stable identifier, but they've now -- I've identified myself and they've now connected me to a profile. So now when I come back, the agent CRM profile comes up and they know about me so I can start a very natural conversation right off the bat. And the agent has the ability to pull from their engagement case history all the previous engagements I've had to again be able to pull back some history and go into a very fluid, easy conversation with me-- super important to have this backend systems integration capability. The next key point, intelligent message routing. So as you probably may recall from last year, we announced intent and group IDs as part of the infrastructure support for message payloads that are being sent from me through the business chat API to the customer service platform provider. In this case, I may be searching in the Maps app for a specific location for Home Depot in Santa Clara and I start the conversation here by tapping on the Message button. And it sends and appends in the intent and group ID the actual address that I've started at. And so this is important because the intelligent routing of your platform may need to be able to based on your business rules route this to the right agent group perhaps on the West Coast because of hours of operation. But it also supports the agent's ability to then know where I started my journey and be able to help me right away knowing I'm probably interested about that specific store location. Okay lastly, quality of service monitoring, QOS, very important and so in this example with the Nuance dashboard. Your platform needs to be able to monitor, analyze and provide statistics, KPIs, important KPIs for your business customers. And so being able to monitor Business Chat conversations by agent and provide important statistics and KPIs, like the average response time for Business Chat for agents, as well as perhaps for commerce type transactions in Business Chat, the percentage conversion rates, all about trying to figure out how to increase their productivity and increase my level of customer satisfaction. So your platform needs to support this type of analytics. So once again four key areas that we look for when we think about what qualifies as a great customer service platform provider for Business Chat. Okay now that you've been approved there's a couple of key things that we're going to ask you to do, one is to invest time in training your sales and systems engineers. So we've created three modules that we walk you through, all of your sales and system engineers wherever they are around the world about general business with working with Apple, about implementation and lots of details that we go into here technically. And then lastly, about how to design great user experiences and how to properly set yourself up for a great marketing launch. And then lastly, you need to get ready and help your customers get ready when it comes to integrating great consumer brand customers and all of their interactions. And so there's a couple of key things that we want to highlight here that we've learned. Number one, believe it or not once you become an approved CSP platform you need to make sure you let all your customers know that you are and more importantly, you need to get them to come and register at registered.apple.com and make sure that they're selecting you in the drop-down menu and notifying you when they have done that so that the two of you can actually start working together right from the start. Second of all, in parallel you should be already starting to integrate to the Business Chat API, starting an Apple Business Register and then continuing to test all of the payloads across the networks to get ready for helping your customers get ready to launch. And then lastly, you need to start to develop and implement some great use cases for your customers and help them get to the point where they're really ready to actually launch Business Chat. Okay and then sorry one more point, you do need to notify us when you and your customer or customers are ready for a launch so that we can do a public launch review, a final QA before we turn you on and publicly launch. So again, these are a couple of key qualifiers and some important steps of investment that you need to make as a potential CSP around the world for Business Chat. So let's talk about the consumer brand, some points about how to get ready, steps to get ready for launching Business Chat as a consumer brand. Number one, the one thing we learned is that Business Chat is an asynchronous, more asynchronous like communication method is a little different from synchronous and so just to make sure that everyone understands what that means I'll illustrate with a quick example between Hilton and Marriott, a couple of our partners. I may start a conversation with the hotel brand looking for a common use case like loyalty points. And I may start that and have a very synchronous communication back and forth on a Wednesday, but then I get distracted, I'm doing other things so a couple days go by before I come back. This becomes more asynchronous now in nature, but because of the long-lived sessions of Business Chat and because of the stable identifier about me and because I previously identified who I am a new agent that picks back up in this conversation should pick back up right where we left off, it's not about starting over. So it's a different etiquette and so we've come up with some great guidelines and some help in documents to help get your agents properly trained to be able to manage these types of conversations differently say from what they may be used to in live webchat synchronously. You also need to adjust your KPIs for measuring synchronous versus asynchronous communications. Okay let's talk about integrating Business Chat entry points. So as you probably remember with Business Chat only users can initiate a conversation and therefore it's even more important to make sure it's very easy to discover ways in which to start a Business Chat conversation. So there's two key groups that I'm going to talk about, there's Apple native entry points and then there are your entry points as a consumer brand. Starting with Apple's native entry points. So when we announced the public release of Business Chat Beta in March we also announced the native release of being able to search for brands in Siri, in Search, in Safari and in Maps. And we made this very simple so that you can go into Business Chat, your Business Chat account in Apple Business Register and set up and configure these entry points for your business. And in case you haven't experienced this again here's a quick example where I would start a search again on iOS on the home screen with a simple swipe down, I would type in Discover the brand, I get the search result, the business search result for Discover, I tap on the Business Chat button and boom I'm right into my conversation. Very easy, three steps. Or coming back to the same search result I may tap on the left side of the search result and that opens up what we call the full preview of the business card, which has a lot of important information that again as a consumer brand you set up in your Business Chat account in Apple Business Register, such as what to expect from your agents or your automation with response time to messages or your live agents' hours of operations dependent on where they are. So these are all things that you can set up in your account and configure and update as required in your Business Chat account. So again, really important to be taking advantage and setting up all of these and getting ready to launch in your Business Chat account in Apple Business Register. But these are not as important as your entry points and so by your entry points we mean these are the places in which your customers are most commonly engaging with you, it's your content, it's your various properties, whether it's websites or apps. This is where your touch points are most valuable. And so we've created the Business Chat button which is an API driven way to embed and initiate an entry point for Business Chat and it has a unique URL embedded in it for your business ID. So let's look at some examples. This is an example of how TD Ameritrade has implemented the Business Chat button in their mobile website. And so contextually they look for opportunities where there's the highest level of engagement, where their customers typically or prospects start to figure out okay I want to do something but I'm not sure exactly what to do. And so they place the Business Chat button there and it's getting tremendous results because then I can simply tap on it in this contextual call to action to talk to a TD Ameritrade advisor and immediately start a conversation with them. It's fantastic. All right, so now let's look at an example of an in-app placement of a Business Chat button. 1-800-Flowers in their app have the Business Chat button placed in a number of really important areas again, but I may have started a conversation with 1-800-Flowers a while ago and because of my tray and because I communicate with so many different people that conversation may have been pushed way down in my message tray. So in fact you know I'm getting old, I may have forgotten that I actually had this conversation. So I may have started a conversation, looking for an opportunity to buy flowers in the 1-800-Flowers app and I spot the Business Chat button and then it comes back to me. Oh my goodness all I have to do is tap on that button and once again it will bring back all the previous conversation because of its long-lived capabilities and I'm already identified, the agent welcomes me back and I can right away get into a productive conversation to order a new set of flowers for my wife which are probably way overdue at this point. All right, so let's talk about phone calls. There's a huge opportunity that we're discovering with many brands to be able to start to deflect inbound phone calls to your 1-800 contact centers. Massive potential here for cost savings and higher levels of productivity. And so the idea is at the top of your IV or your call tree with whatever options you're offering in terms of getting into a queue for a live agent you offer an option to be able to have an SMS text message sent to you if you want to start a conversation in Apple's Messages app. And that SMS text message would have an embedded link that all they have to do again is tap on it and it opens up the Business Chat conversation right in their Messages app. Again, huge potential opportunity to be deflecting a significant amount of inbound costly phone calls to a high productive channel like Business Chat. Okay brand email. So again very targeted because you obviously have to target this to specific Apple device users that most brands track and know who they are. But a great opportunity where 1-800-Flowers is knocking it out of the park being able to target these emails, offer promotions, remind them of an easy checkout benefit with Apple Pay and again select that button, tap on it and away you go, you're into a conversation. Last key example I'm going to touch on is QR codes. And so there's lots of opportunities that we're starting to see pop up where brands are using QR codes in storefront windows to capture attention, they're using them at the checkout lanes, and they're also using them at check-in at hotels. But this is a great example where T-Mobile is in fact exploring the opportunity to create a post-sale relationship by offering at the end of a conversation in-store on the ID badge of their in-store advisors the opportunity to start a conversation right there by simply holding up your iPhone with the built-in capabilities of the camera to read the QR code and start a conversation even before you leave the store. Great way to again personalize that journey with your customers. All right, so super important that is part of getting ready for a launch that you've thought through and found the most valuable engagement touch points with your customers and embedded the Business Chat button as you launch. So next key step, two more things. One, preparing your logos because they are well displayed throughout the Business Chat experience and so you have your square logo and you have your wide logo. And so as a reminder in case you haven't seen this, the square logo shows up in a search result, the square logo shows up on your actual full business card, it shows up in the message tray so it easily, you know you can scan easily and look for the brand that you were communicating with. And then lastly, the wide logo always the optimal choice to be showing it at the top of every business conversation to reinforce your brand. And it's easy to set up in your Business Chat account once again by simply going into your account, tapping on the Brand button, uploading your square and wide logos and selecting your background color. Last key point, begin your app planning early and by that we mean as you develop your Business Chat team make sure you're pulling in the app team at the same time. The combined collaboration of your two groups will deliver a much more powerful result with Business Chat user experiences, largely because of the opportunity to take advantage of iMessage apps as part of that. And so to that point, let's start talking about the brand developer and Scottie is going to join me back up on stage to walk us through that and other great app development tools to take advantage of for the best Business Chat user experiences. Scottie. Thanks Dan. So today I'm going to talk to you about working with Business Chat from the perspective of a brand developer. Now what do we mean by brand developer. It's not a developer with a bunch of labels all over their clothes. Really what it is is we see brand developers being in one of three categories. The first category is maybe you're working with a CSP, a customer service platform, and you're working to deliver custom iMessage extensions in-app experiences for all of the businesses that they work with. The second group is maybe you're a developer working with a specific brand, a specific company that's signed up to work with Business Chat and you have some great ideas of how you can make iMessage apps tie into the transcript to make a great experience for the customers. The third group we see as maybe you're not working with anyone at all, but you have some really great ideas of how Business Chat can come alive with some custom iMessage apps. In the past week we've heard from a number of you and there's really some exciting opportunities here for some great experiences that can potentially be sold to customer service platforms or brands. We've also heard from a lot of you from all the three categories over the last year and I want to thank you for contacting us because we see some real great potential and some great user experiences that will come into Business Chat soon, so thank you. Let's start talking about if you even need an app at all. So building custom iMessage apps for Business Chat is great, but we have some built-in functionality already in every iOS. So these are the List Picker, the Time Picker, and Apple Pay. I went through these last year in detail, but this year with iOS 12 we heard your feedback and we're adding one of the most requested features which is Authentication new in iOS 12. Great. So let me go through these now. First, we have List Picker. List Picker allows your business to send down a list of products grouped in sections, each section can have a number of different products and new in iOS 11.3 and above we allow your business to specify if a section should be a multiple selection or a single selection. Now this is great for some experiences, such as like trying to order a pizza where maybe the crust is only, you can only have one crust on your pizza, one crust type, but the toppings you'd want to offer a range of toppings the user can select from. So we allow that section to be have a multiple selection, so that's new in iOS 11.3. Next, we have the Time Picker. The Time Picker allows your business to send a number of preselected dates and times to allow your user to select. These also -- the Time Picker also ties into your local calendar store and will feature certain conflicts that you may have conflicting with the time selected. Once you've selected a time and you've sent this back to the business you can easily then add the appointment to your own calendar and even get directions to the event if the company specifies a location. Next, we have Apple Pay. There's those high heels again. Apple Pay allows your business to quickly and easily send the payment request to your customer. The customer all they need to do is tap that bubble, up comes an Apple Pay sheet, they can quickly Face ID, Touch ID and right within Messages quickly pay your business. It really wouldn't be Business Chat without Apple Pay. Now new in iOS 12 we have the Authentication plug-in. This allows your business quickly to allow you to log in and then get great customer information from your login. So let me go through that a little bit more right now. It starts off in a conversation when you get to that point where maybe you need to get a little bit more details from the customer that would be provided by their account. Now as you see here, the customer service agent will send down an authentication request. Their user can tap that to sign in, up comes a sheet. This sheet contains a link, basically it's displaying the business' OAuth website directly in the Authentication plug-in. The user can then tap in to one of the fields and they can simply enter in their username and password if they have it remembered or they can tap the Password Lock icon which will bring up a sheet of all the saved passwords currently already in your keychain. The domain matched to the one that is provided will be bubbled to the top, you can then select that. The fields that we filled out. And once you log in you can basically get a response to the business verifying that they're logged in. So that's great, but how does this all work? So let me go through that now. As I mentioned, it starts off with the CSP agent sending an authentication request. This includes two pieces of really important information, the first is a public key. This public key is going to be later used to encrypt the token that you received from the authentication request. It also includes the URL of where the OAuth webpage lives and we'll go through that a little bit more in a minute. So this is where it starts, the authentication request comes down and it passes through, stop on the way at Apple Business Chat Server. Now why do we do this? This we added and we did a lot of thinking about this about adding security. We want to make sure that your business has authenticated user to the proper OAuth website. And for that we make sure it's the proper information that you've already as a business registered through Apple Register previously. We verify also that the request contains all the required fields. And then finally we deliver it in your Messages app for the customer. From there as I showed you can either type in the password manually or access the system keychain to pull out any stored credentials that you may have. From here the next step is we go directly to your business server to hit the OAuth website. Now this can be existing or potentially a little bit change for Business Chat but it allows you to carry the user through your standard login flow, including any extra additional information, extra verification fields, anything along those lines. At the end of OAuth your OAuth returns a token, this is your login token and there it's sent back to the Messages app. What we do next is we encrypt that token so no one else can see it with a public key that I mentioned earlier that's included in the request. Now your token is encrypted and it passes back up through the Apple Chat Server which Apple can't read and it lands back to the agent. Now at this time your agent may have the private key to decrypt that or your business may have that private key depending on the relationship and the security that you've worked out. We think a lot of people will actually want to send this encrypted token back to the business to verify and decrypt to make sure that this is in fact the person who has just recently logged in and once you've decrypted it you can use that to get all the important customer information sent back to the agent to help service your account, such as your shoe size or other order information. So that's Authentication in iOS 12. We think a lot of companies, especially in healthcare and financial services will really like this. And that's what we have for built-in features, so you don't have to write a single line of code you can take advantage of these by integrating with your CSPs and your businesses today. So next, you may want to take this further and as I mentioned we heard a lot of great examples of how this can be done and some really great innovative ways. So today I'm going to talk for your features how you can leverage native iOS functionality, some tips about making your iMessage apps a shared app architecture, and talk a little bit about how to make that experience great. So remember with iMessage apps you can really take advantage of full native iOS frameworks and code throughout the system, such as a front facing camera. Wouldn't it be great if you could try out, have your users send pictures of them using your products, testing your products or even seeing what kind of situation they are working with to help give better customer support? This is a whole new level of interaction that you can have with your customers by actually seeing what they're experiencing. Remember we have MapKit allowing your iMessage app to display a map and then the business can send down things such as places that your business may be operating in in a certain region. Or even providing information such as drawing custom overlays on the map to designate maybe coverage areas or delivery zones. Taking advantage of ARKit provides a really a whole new level of interaction where customers right from Business Chat could try out seeing what products may look like in their natural environment right from within Business Chat. We think these are some really powerful examples of using your own custom iMessage app. So by now you've probably determined you want to do a custom app, here's some things you can do to get ready for Business Chat. First, we'll start by talking about what it means to have and leverage a shared app architecture. So your business, your brand may already have a full-fledged iOS app and when you're working on your iMessage app as an extension to that there's certain things you can do to really speed up that development. Your main app may have several business logic classes, a whole bunch of utility classes, even custom views and view controllers that you could easily reuse in your iMessage app. All you have to do is add an iMessage app target and then check off those classes so they're provided, also available in your iMessage app. Now sometimes code is not the cleanest and by checking one class you may have to pull a whole bunch of other classes that that represents so that's not always the best way to go about it and we think this is a great opportunity where if your classes aren't fully encapsulated it's a great time for you to potentially refactor to get your specific business logic into its own kind of encapsulated place for much better code in the future. So that's shared core resources, really you know adding your iMessage app target to your project, checking off a few classes, you can get really far really quickly. Next, we have a shared filesystem between your app and your iMessage app. And with that if your main app has already downloaded of images or files or anything important that your app needs to do you can reference them from your iMessage app in Business Chat without having to redownload those and provides a much better customer experience. You can also go the other way and save things from your iMessage app back to the app so then when you launch back to the app that information is readily available. Now that's great for files that are not really sensitive or need to be secured, but if you have those needs the system keychain is also shared. So if you were writing down potentially even login credentials or tokens from a previous login in the app you can quickly make use of those in your iMessage app and have those speed up your login process when using Business Chat. Even provide you know customer data right when launching the iMessage app. Keep in mind, you can always go from the iMessage app to your main app. So let's say you're in a Business Chat conversation and the user is navigating through your iMessage app and there's something that maybe your full app would be a better experience for you can provide a link, as well as a deep link. So you can go directly from the iMessage app directly to the full app right into the place or maybe the product or the information that they were looking for with just one tap. And keep in mind that once you make these iMessage apps not all your customers will have that preinstalled on their device right then or all their devices. So if you run across this scenario as we showed last year as well a user just has to tap the bubble, up comes an App Store sheet where they can quickly and easily download the app and continue right there in the iMessage conversation having now downloaded that app and iMessage app to your device. So that's covering some great ways that you can take advantage of the shared app architecture, a lot of the code and scenarios that you've already built with your main app. Next, let's talk about the Chat with Messages button API. As Dan showed, this is a great new API that allows your apps to leverage this standard Apple button now in iOS 11.3 and above. There's two ways to use this, we have the BC Chat button in the Business Chat framework which you can use in your iOS apps and there's a JavaScript version for websites that'll be made available to you as you sign up through Apple Business Register. Today I'll show you some code examples of how to use the BC chat button in your iOS apps. So here we have an example showing the first step I just have to import the Business Chat framework which is on the SDK. Then I can go ahead and instantiate a button of the BC chat button and we allow you to pass in some style information depending on the look and feel of your app, it may look better with a light style or a dark style. Then this is a standard button so I'm going to add a target. And what you'll note here is this is available on iOS and macOS. So if you're making Mac apps or iOS apps you can take advantage of this with even a shared code to have this available across your platforms. Once I've configured the what's going to happen when I tap the button I can go ahead and add this button to my views and configure constraints and all the things to make the button look great. Now that's putting the button in the app. So what happens when the user taps the button? Well from here as Dan mentioned in his example he is showing an address getting passed through. In my example I'm showing here that you can programmatically define an intent. Maybe the user is browsing through your app and has landed on a certain shoe product. You could tack on the product information so right when the Business Chat conversation comes alive the customer service agent already knows the product that you're referring to. The group allows you to specify potentially where to route on the backend this conversation, things like support or maybe sales would be appropriate or other as Dan showed, other regional based kind of specifications. And then you can also even specify some default text that when the user lands on the message conversation will be automatically filled in on the text field so all the user has to do is hit Send. Great, so now I've configured these parameters and the next step I do is I call the BCChatAction to open the transcript and provide the business identifier which is provided to you upon acceptance into Business Chat through the register. I also send in the parameters so they are taken up and sent. At the end of the day once the user taps that button they're switched directly to Messages into the conversation directly with that business. And the information with intent and group are provided automatically. So that's Chat with Message button API. Next, I'm going to talk about Sandbox testing. Last year at WWDC we introduced the Business Chat Sandbox. Here's the URL and last year I even gave a demo of how this works and how you can make use of this for your testing. Here's a screenshot of the design and what you see here is we have tabs across the top letting you try out all the experiences of Business Chat right from kind of pretending to be a fake CSP or business. Every one of you can log in and do this today, you don't need any approvals. You basically can log in and get started sending sample text messages and instantiating all of the different built-in functionality. As we showed last year you can even test out your own custom iMessage apps by using the My App tab. And new this year for iOS 12 we've added the Auth tab where you can try out and play with the new with the new Auth functionality, including a sample OAuth provided webpage through the Business Chat Sandbox which is great. Or if you have your own OAuth website already you could override that and enter your own so you can try it out and see if it works with your business setup. So the Sandbox really allows you to kind of see and feel this and we really encourage you to use this because you'll kind of, you'll see the use case and you'll feel how the use cases interact. We also think it's a great way to demo this to maybe decision-makers in your business so they can make a great, kind to fine-tune those experiences before you go live. So I can't stress enough how valuable this Business Chat Sandbox can be. And finally let's cover some best practices. So as I covered, start with the Sandbox, really see how these experiences will look and feel, try it out with some people. You can have your you know laptop set up with Sandbox on one side and have someone on the other side with the device trying this interaction out. Really valuable to kind of see and feel how to build these first-class customer experiences. Then think about enhancing your app and adding a messages app to that to really take that experience even further. Now I have a separate bullet point about customizing that user experience specifically because we want to make sure that you understand that iMessage apps are not the full apps. They are specifically designed for quick interactions that'll add value and not be too daunting for someone who is in a conversation who just wants to get stuff done with Business Chat. For that we recommend really taking a look at all the things you've done in the app and what's appropriate in iMessage apps, but also to custom tailor it so they kind of fit and flow into the user experience you're looking for. This is also a great way to really stand out in Business Chat and in your customer service experiences. Then as I mentioned, integrate with your message API button, BC chat button and add it to your apps, into your websites and your emails. So that's some best practices, that's what we would like you to know about getting started as a brand developer. And now I'm going to hand it back to Dan to finish this up. Well done buddy. Thanks Scottie, that was great. And it's super exciting about finally being able to launch the OAuth authentication built-in feature. That is going to be huge because we have a ton of demand coming at us for that type of a built-in functionality, so that's very exciting. All right, some key things. First of all, we've covered a lot of highlights today but there's a lot of great documentation and information available to you at this link. And so, please make note of it, if you haven't it now you'll be able to afterwards in the video. But it will take you to lots of different documentation that will cover many of the things we've talked about would but be able to drill down much deeper than that into our getting started guides and best practices, again being able to get right into understanding how to use Business Register, the Sandbox which is critical of course, and even last year's video which we'd recommend you revisit to just get again a good primer. And so I'm going to leave you with this one important thought that once again today for the first time we're going to open the door significantly wider to medium to large consumer brands around the world and new qualified CSPs that can support those medium to large consumer brands. And so just as a reminder, if you feel you're a qualified CSP, please come right away to Apple Business Register and qualify by registering and providing us your information and we'll move you through a very rapid process now. So we're speeding up the approval process in Apple Business Register. If you're a consumer brand, please again make sure you come register, make sure you notify and pick your CSP and then notify your CSP that you've selected them so you can start working together rapidly and start integrating. And then lastly, if you're a brand developer once again start the planning process early, use the Sandbox to start prototyping the new OAuth authentication built-in feature and all the built-in features. It will immediately help you accelerate the ability to get to a Business Chat launch faster. And of course take advantage of the opportunity to start developing your iMessage app as part of the Business Chat experience. On behalf of Scottie and I thank you very much for your time today and we'll leave you this one last message that please go start getting ready now to launch Business Chat. Thank you very much, have a great afternoon.  Hi, I'm Alex. I work for a group at Apple called Tools Foundation. Normally we get to do fun stuff like operating systems and compilers. But this year we got to do something a little different. We built a game called SwiftShot. Some of you may have seen it earlier today and you might have played it downstairs. But the important part is that SwiftShot is a showcase of some of the new functionality in ARKit. ARKit 2 is now available on hundreds of millions of devices, providing a platform for engaging AR experiences. And you are able to reach the widest possible audience with that. There is no special setup, just point the device's camera at a surface and start playing. It's integrated into iOS. First-party and third-party engines like SceneKit and SpriteKit as well as third-party ones like Unreal and Unity have a full integration at this point. A little agenda for you. First we're going to talk some game design principles for augmented reality, a few of the things we learned along the way. We are going to go deep into the internals of the game and in particular, we are going to cover WorldMap sharing which is a new feature in ARKit 2. And we will also talk about networking and physics and how we made that work. First, let's, you know, take a deep look at the game. So, let's talk a little bit about designing games for augmented reality. Above all else, gameplay comes first. You should ask yourself if you are designing a game, would this game be fun and enjoyable if it were just 1970s graphics or plain, flat-shaded grey cubes. It is worth prototyping with those kinds of artwork and get that gameplay down. Because if it's fun with those boring grey boxes, it's going to be fun when you add all the graphics and sound later. You should spend time refining that and don't convince yourself that if I just add another 5% better graphics, or that one feature, that the game is suddenly going to be fun. Because, you know, there's a wasteland of games out there that were never fun from the get-go. So, try not to fool yourself. Let's start with the gameplay. Keep games short. You are looking for a typical mobile experience still - easy in, easy out. You want to keep a variety of content so that it is fresh, avoid mental fatigue on the part of the player of repeating the same thing over and over again. One of the things we learned is that spectating the game turned out to be just as fun as playing it. Sitting there on the sidelines and watching like it is a sporting match going side to side, that is just a really enjoyable experience. It's something to think about. Games are a key form of social and personal interaction. Augmented reality can offer a kind of personal touch that you might have had before playing like a traditional card game around the table with older relatives. But now you have technology to help along the way. It isn't enough to just take a 3D game and put it on a table in front of you. With augmented reality, you know how the device is positioned. You also know a little bit about the user's environment and you should try to take advantage of that in the game and make experiences that are really for augmented reality first. Your device can be used as a camera to look inward at an object of focus. In this case, this is a 3D puzzle game where we're looking to repair a broken vase. We can look all around it, figure out what piece goes where, and do our best on the repairs. In SwiftShot, we took a similar concept. The focus is the table you're playing on and you can walk around it. But the table isn't just a tracking surface for augmented reality. It's an integral part of the gameplay. The height of the table is actually significant and as a result, you'll see in the game that there are slingshots at different heights on tops of stacks of blocks in order to give you better shots or take advantage of the player dodging and weaving a little bit. Another possible principle is your device is a camera you use to look around you. In this case, we're looking for unicorns hiding out in the wilderness and we're taking pictures of them. It's just around you, not inward. The device can also be a portal into an alternate universe. You don't need to see what the camera sees directly. The environment can be entirely replaced. Laws of physics can be bent or completely changed. Whatever you need to do to make it fun. In this case, we're able to see the stars, even though it's bright daylight. Also, your device can be the controller itself. You're able to fuse yourself with the virtual world using the device as the controller. In this example, we're sort of magnetically levitating blocks and placing them in the sorting cube. That's the focus of the interaction in SwiftShot. You want to encourage slow movement of the device. That gives the best images to the camera without motion blur and it can do the best job at tracking. And despite how thin and light these devices are, waving them all around at arm's length turns out to be a little bit tiring. So, you're looking for slow and deliberate movements. You want to encourage the player to move around the play field In this case, our shot of the enemy slingshot is blocked by those blocks. So, we have to move over to another slingshot to clear the obstruction. Control feedback is important for immersion. In SwiftShot, we give feedback using both audio and haptics. There's a variety of dynamic behavior in the stretching band sound and haptics on the phones to give you that feel that you're doing it. We'll talk a lot more later about the dynamic audio. So, next I'd like to bring up David Paschich, who will go deep into the details of SwiftShot. Thank you. David? Thank you Alex, and hello, everybody. I just want to echo what Alex said. The response that we've seen from people here at the show to SwiftShot has been really amazing and it's been gratifying to see some people already downloading it, building it and altering it from the sample code. So, I thank you for that. We're really excited about that. I want to talk by talking first about the technologies we used in building SwiftShot. The first and foremost is ARKit, which lets us render the content into the physical world around the players, immersing them in the experience. We use SceneKit to manage and draw that content, using advanced 3D rendering and realistic physics for fun gameplay. Metal lets us harness the power of the GPU devices. It came into play both within SceneKit for the shading and rendering and also for the flag simulation, which I'll talk about a little later on. GameplayKit provides an entity component architecture for our game object. It let us easily share behaviors between objects in the game. Multi-peer connectivity provides the networking layer, including discovery of nearby devices and synchronization, and encryption as well. AV Foundation controls both the music for atmosphere and the sound effects for the devices, really giving you that immersive experience. And lastly, we built the entire application in Swift. Swift's type safety, performance and advanced features like protocol extensions let us focus more on the gameplay and worry less about crashes and mismatched interfaces between code layers. Those are the iOS technologies we use. I'll talk about how we use those as we implemented several of the features of the game. Establishing the shared coordinate space. Networking. Physics. Asset important and management. Flag simulation. And the dynamic audio. We'll start by talking about setting up a shared coordinate space. The key in the experience is having the player see the same object in the same places on both devices. To do that, we have to have a shared coordinate space, allowing them to talk about locations in the world in the same way. ARKit provides a number of features you can use to set this up. In iOS 11.3, we introduced image recognition, allowing your apps to recognize images in the world around you. Now in iOS 12, we're adding two additional technologies - object detection and world map sharing. Both image detection and object detection let you add content to things the user sees in the real world but they require you to have pre-recorded those objects for later recognition. You saw that in the keynote during the Lego demo, recognizing built models and adding content. For this game, we wanted to enable users to play anywhere with a table such as a caf, their kitchen and so forth. WorldMap sharing is how we did that. You can also apply this technique to applications besides games, like a fixed installation in a retail environment or a museum. In the game room downstairs, we use iBeacons so devices know which table they're next to and can load the correct WorldMap for that area. That really makes the experience magical. One of the features of SwiftShot you may have used if you built your app yourself is the ability to, ability for players to place the game board in the virtual world. At the tables downstairs, we're using preloaded maps. But here's an example of building your own board and placing it in the virtual world. This is how that works. As you saw in the video, you start by scanning the surface, letting ARKit build up a map of the area. You can then serialize that map out as data and transfer it to another device. The target device then loads the map into ARKit and uses it to recognize the same surface. At that point, we now have a shared reference point in the real world, and both devices can render the game board into the same place in that world. The first step in the implementation is getting the World Map from the ARSession on the first device. That's the call to a new API in iOS 12 in ARSession, getCurrentWorldMap. It builds an ARWorldMap object from the session's current understanding of the world around you and then returns it in an asynchronous callback. We then use NSKeyedArchiver to serialize that out to a data object. You can then save the data or send it over the network. Once you have that data object, you next have to decide how to get it from one device to another. For ad hoc gaming like you saw in the videoing, SwiftShot uses a peer-to-peer network connection which we'll get into more detail on shortly. When the second device joins the network session, the first device serialized the WorldMap and sent it over the network. This is great for casual gaming situations, allowing users to set up anywhere they can find a surface to play on. For the gaming tables downstairs, we used a different approach. We spent some time during setup for the conference recording WorldMaps for each of the tables, ensuring that we could localize that shared coordinate space from multiple angles. Each table has its own unique characteristics as well as slightly different lighting and positioning. We then saved the files to local sstorage on each device. Since the devices in use are managed by our conference team, we're able to use mobile device management to make sure that the same files are present on every device in the game. To make the solution even more seamless, you can use iBeacons on each table. By correlating the identifier of the iBeacon with particular WorldMaps, each instance of the SwiftShot application can load the correct WorldMap automatically. Now, if you're building a consumer application, you can also use things like iOS's on-demand resources or your own cloud-sharing solution to share WorldMaps between devices. This would allow you to for instance select the correct WorldMap for a particular retail location somewhere out in the world. There's really a lot possibilities here to tailor users' experience and really build something great. So, those are a couple of the ways to get that WorldMap data from one device to another. Let's talk about how you then load it on the second device. In this case, we use NSKeyUnarchiveder to blow up that WorldMap again from the data that we received. We then build an ARWorldTracking configuration and add the WorldMap to that configuration object, setting up the way we want. And then lastly, we ask the ARSession to run that configuration, resetting any existing anchors and tracking. ARKit on the target device then starts scanning the world around you, correlating those feature points from the original map with those that it sees there. Once it's able to do that, you've got that shared coordinate space. Both devices have 000 in the same place in the real world. So, a quick word about privacy with WorldMaps. In the process of recording the WorldMap, we take into account features of the world around you, physical arrangements of objects and so forth. While it does include geographic information like latitude and longitude and thus your application doesn't need to ask for location permission to use ARKit, it may include personally identifiable information about the user's environment. So, we recommend that you treat a serialized WorldMap the same way that you would any other user-created private data. This means that you want to make sure that you're encrypting it both at rest and when moving across the network. You may also want to let your users know if you're planning to save that WorldMap information for an extended period of time, past a single session of your application. In SwiftShot, we're able to take advantage of iOS's built-in encryption for encrypting the data while at rest. I'll talk next about how we did the networking for encryption, on the networking. Now, in addition to setting up shared coordinate space for SwiftShot, we needed to tell the other device where the user has chosen to locate the board. We use an ARAnchor to do this. When you create an ARAnchor, you provide a name as well as position and rotation information as a 4 x 4 transform. ARKit can then include the Anchor in the ARWorldMap we generate and serialize out, and then, so we can transfer that board information to the other device. Now, the system ARAnchor class just has the name and the orientation we created. We can look up the anchor that we're interested in by name on the other side. For our application though, we need to include some additional information for the other device, and that's the size that the user chose for that board, deciding whether they're playing on a, you know, a small table top and surface, or they want to blow the board up to be the size of a basketball court. We thought about, you know, adding that to our network protocol alongside the WorldMap, but then we came up with a better solution. We created a custom subclass of ARAnchor that we called board anchor and added that information to that class, the size of the board. We then made sure that we implemented the NSCoding required classes or override them to include that information when the object is serialized out. Now, the information is included directly within the WorldMap when we transfer it over to the other device. It makes it very easy and straightforward. One thing to keep in mind, and this bit us for a little bit. When you use Swift to make a subclass like this, when you serialize it out, the name of the module or the name of your application is included in the class name. This is something to be aware of if you're planning to move WorldMaps between different applications. NSKeyedArchiver can help you accommodate that. So, that's WorldMap sharing. It's a new feature in iOS 12. We're really looking forward to seeing what everyone can build with that. Next, let's talk about the networking we built into the game. We used iOS's multi-peer connectivity API which has been in the system since iOS 7 in order to do this. Multi-peer connectivity. Allows us to set up a peer-to-peer session on a local network, allowing devices in the session to communicate without going through a dedicated server. Now, in our application, we designate one of the devices as the server but that's something that we did for our application. It's not inherent in the protocol. Encryption and authentication are built into multi-peer connectivity. In our case, we didn't use authentication because we wanted a very quick in-and-out experience but we did use encryption. We found that turning on encryption really provided no performance penalty, so there's either in network data size or computation. So there's really no reason not to use it. Multi-peer connectivity also provides APIs for advertisements and discovery. We use this to broadcast available games and allow players to select a game to join. So, here's how we get that session set up. First, on one device, the user decides to set themselves up as hosts for the application. They scan the world, place the gameboard within that world, and then the device starts a new session, a multi-peer connectivity session, and starts advertising it to other devices on the local network. A user on the other device sees a list of available games. When he selects one, his device sends a request to join the existing session. Once the first device accepts the request, multi-peer connectivity sets up a true peer-to-peer network. Any device in the network can send a message to any other device in the network. In SwiftShot, we designate the device that started the session as the source of truth for the game state. But again, that's the decision we layered on top of the networking protocol; it's not inherent in multi-peer connectivity. Once the session is set up, multi-peer connectivity lets us send data between peers in three ways. As data packets. As resources, file URLs on the local storage. And as streams. Data objects can be sent, broadcast to all peers in the network whereas resources and streams are device to device. In SwiftShot, we use the data packets primarily as a way to share game events and also the physics state. We'll talk about that later on. And then we used the resources to transfer the WorldMap. It ended up we didn't need streams for our application. Under the covers, multi-peer connectivity relies on UDP for the transfer between devices. This gives a low latency for, great for applications like games. Now, UDP inherently doesn't guarantee delivery, so multi-peer connectivity lets you make that decision and specify whether a particular data packet is to be sent reliably or unreliably. If you choose reliably, multi-peer connectivity takes care of the retries for you, so you don't have to worry about that in your code. Even when you're broadcasting to all members of the session. Now that we have a networking layer, we need to build our application protocol on top of it. SwiftEnums with associated types make this very easy. Each case has a data structure around it, ensuring type safety as information moves around the system. Some of those can be further enums. So, for instance, in this example, gameAction includes things like a player grabbed a catapult. A projectile launched, and so forth. The PhysicsSyncData is a strut and we'll talk more about how we encoded that later on. Again, Swift makes this very easy. For struts, if all the members of the struct are codable, then all you need to do is mark that struct as codable and the Swift compiler takes care of the rest, building all the infrastructure needed for the serialization. Swift doesn't do that for enums and so we ended up implementing that ourselves, implementing the init and then coding method from the codable protocol to make that work. Serialization then is very easy. Just build a property listing coder and have it encode the object out for you. We can then send a data packet within the multi-peer connectivity session. Now, a reasonable question here might be how's this going to do in size and performance? Property-- binary property lists are pretty compact and the encoder's pretty fast. But sometimes, you know, the soft implementation in many ways is optimized for developer time, which is sometimes your most precious resource on a project. Now, we ran up against some of those limitations as we started to build the next feature, and we'll talk about how we overcame this. So, let's talk next about the physics simulation in the game. For a game like SwiftShot, physics is really key to create a fun interaction that comes from the realistic interaction between objects and the game. It's a really great experience to take that shot and bounce it off an object in a game and take out the opponent's slingshot. And that really comes from the physics simulation. We use SceneKit's built-in physics engine. It's integrated with the rendering engine, updating positions of the object and scene automatically, and informing us of collisions using delegation. In our implementation, we decided that the best approach was for one device in the session to act as a source of truth or server. It sends periodic updates about the physics state to the other devices in the network using that multi-peer connectivity broadcast method. Now, the other devices also have the physics simulation on. That's because we don't send information about every object in the game, only those objects that are relevant to the gameplay such as the box, projectile and catapult. Things like simulating the swinging of the rope and the sling, particles and so forth, those are just done locally on each device since it's not critical to the game that they be in the same place on every device. Now, one of the things that we discovered was when we were doing this was that the physics engine responded very differently depending on the scale of the objects. And so the physics simulation thinks the objects are about 10 times the size as you would see them in the real world. We found that gave the best gameplay experience and the best performance. We had to tweak some of the laws of physics to make that look right but, you know, when you're building a game, if it looks right and feels right and it's fun, then it is right. Now, to share that physics state and make sure everything looked right, we need to share four pieces of information. The position. The velocity. The angular velocity. And the orientation. That's a lot of information about every object in the game, so it was vital that we minimize the number of bits actually used. I'll walk you through that using position as an example. SceneKit represents position as a vector of three floating point values. This is the native format and gives the best performance for calculations at run time. However, there are really more bits than necessary to specify the object's location. A 30-bit float has 8-bits of exponent and 23 bits of mantissa. For a range of plus or minus 10 to the 38th meters. It's way more than we need for this game. So, because the physics simulation thinks our table is 28 meters long, we said you know, 80 meters is going to give us plenty of buffer space around that on either side. When we're coding that then, we're able to eliminate the sign bit by normalizing that between 0 and 80 meters, even though our origin is at the center of the table. Now all values are positive. We then scale that value to be in a range of 0 to 1. That way we don't need the exponent information that's inherent in the protocol. And then lastly, we take that and we scale it to the number of bits available so that all 1s is a floating point 1 and all 0s is the floating point 0. This gave us millimeter scale precision which, as we discovered, was really enough to achieve that smooth synchronous appearance in the game. Now, we did a similar technique for all the other values that you saw. The velocity, angular velocity and orientation. Tailing the ranges and the number of bits for each to really make sure that we transmit the information using the minimal amount of data. Overall, we reduce the number of bits for each object by more than half. Now, even though we've compressed the numbers, property lists still have a fair amount of overhead for the metadata around it, sending each field by name. We said there's no reason for that. We all know what these objects are. That's not information we need. So, to do this, we implemented a new serialization strategy which we call a BitStream. BitStreams are designed to pack the information into as few bytes as possible by providing fast serialization and deserialization. Now, our implementation is purpose-built for communicating binary data with low latency in an application like this. Strategies like this wouldn't work well for data that needs to persist or data that, where you need to keep track of the schema and watch it changing over time. But for an ephemeral application like this, it was just the thing. To help implement this, we created two protocols, BitStream Encodable and BitStream Decodable. Combine those and you get BitStream Codable. Then we took that and marked all the objects that we needed to serialize, using that protocol, helping us to get the implementation. That includes both our own data objects and the object we use from the system such as the simD floating point vector type. So, here's the implementation of compressing floats. The compressors, configured with the minimum and maximum range, and the number of bits we wanted to use. It clamps the value to the range and then converts it to an integer value for encoding using the specified number of bits. Each component for each object in the scene is compressed in this way. We also use an additional bit at the front to tell if an object has moved since the last update. If it hasn't moved, we don't resend that information. So, let's go back to our action enum, with the three different actions to talk about how we apply BitStream to do this. For regular codable, if you're doing your own serialization, you specify encoding keys for enums for the different cases in the enum. For BitStream, we used integer values for this rather than string values. And then in our encoding method, we're able to then append that value first followed by the data structure associated with that case of the enum. Now, if you look at this code though, there's kind of a pit fall here. We know that this one has, this case has three different cases. And so we only need two bits to encode it. But what happens when we add another case, 4 bits with 4 cases, we'll still find. We add that fifth case and now we need to go through and change that so that every time we do this, we're using three bits instead of two. Now, that's kind of tedious. This code's a little bit repetitive and, you know, there's stuff that could go wrong there. We really, if we don't remember this, we're just going to end up in a bad place. So, we took a look at this and figured out that there was a way that Swift can help us do this. So, we used a new feature in Swift 4.2, which is case iterable. We added that protocol compliance to our enum type. When you do that, Swift adds a new static member of the type called all cases, containing each of the cases in the enum. That lets us automatically get a count of the number of cases. We then added another extension, this time on the raw representable type which all enums with number types like that conform to. Where it's case iterable and where that number is affixed with integer. And to this, we get to automatically take those number of cases and figure out how many bits it takes to represent all those cases on the wire. Lastly, we added a generic method on the writable BitStream type allowing us to encode that enum. It appends things of that type and it uses that new static property to figure out the number of bits that are needed to use. Now, our encode method is much simpler. We just used append enum on the proper coding key for each and Swift takes care of the rest. When we add more cases to the enum, the BitField expands automatically. If we remove cases, it contracts automatically. We don't have to worry about it. So, how much faster and more compact is BitStreamCodable? We ran some tests using XE test support for performance testing using a representative message in which we send information about object movement. The results were pretty impressive - 1/10 the size, twice as fast to code, 10 times as fast to decode. Now when we talk about going from 75 microseconds down to 6 microseconds, that seems like small potatoes. But there's around 200 objects in the game and we want to do this very frequently to make sure the game remains smooth for all participants. By using this encoding format, we were able to do those physics updates at 60 fps, ensuring that you get a smooth experience for everyone in the game. Now, I've talked about this. We did some things with codable and some things with BitStream Codable that, you could have a problem there because we're encoding things two different ways. And that means now we need to have two different code paths through our application. Swift helps us out again and lets us figure out how to combine them. We then added constrained extensions so that anything that is codable in BitStream Codable, we provide default implementation of the BitStream encoding. And then we just go ahead and use a binary [inaudible] encoder to encode the data and stuff it into BitStream. And then anything, any struct that is codable, we just add that by marking it BitStream Codable. Now, this implementation then is not as fast and compact as if we went forward and made everything BitStream Codable directly. But we discovered we didn't need to do that for every object in the game, only the most frequent messages. This let us really move quickly and keep better rna on the game. So, that's how we did the physics. Next I want to talk about how we dealt with the assets on the game levels and this is the question that a lot of people asked us downstairs. You know, the assets include the 3D modules, the textures, the animations and so forth. So, we have some text angle artists here in Apple and they used some commercial tools to build the visuals for the games. The blocks, the catapults and so forth. They then exported those assets in the common DAE file format. We're looking forward to the commercial tools supporting USDZ but for this game they weren't quite there yet. We then built a command line tool in Swift that converts the object from DAE into SceneKit files using the SceneKit API. Because SceneKit provides the same APIs on both iOS and macOS, we're able to run this tool as part of our build process on macOS and include the SceneKit files directly in our iOS build in the application. We structured the data so that each individual type of block is its own file and then for each levels, we combine those blocks together. This let us iterate on the appearance and physics behavior of each individual block and then pull them all together for those levels and iterate on gameplay design. Try out some of the different levels that you'll see if you look in the source code to the application. To optimize, further optimize for different distances, SceneKit supports varying the assets used based on the level of detail required. Nearby objects use more polygons and more detailed textures while far away objects use fewer polygons and less detailed textures. This really optimizes the rendering of the scene. However, we still want the gameplay to stay consistent. And so we specified the physics body separately. SceneKit provides a number of built-in physics body types such as cube, sphere, cylinder. And if you use those, you really get the best performance. If you don't specify one, SceneKit will build a convex hull automatically for you and that works. But it is a lower, can be a lower performance implementation by adding these objects where they were available and where they made sense, we really sped up the performance of the game. So, here's some examples of the physics finished product. First one is one of the blocks from the game. In this case, a cylinder with textures for a great wood grain look. Next is the slingshot with the sling head idle. We add the [inaudible] colors at RunTime using shaders and built some custom animation for the sling's motion during gameplay. Lastly, we included some extra assets that didn't get included in the gameplay. Even though we had to sacrifice them, we want you to have them and use them in your own sample code. So, one of the other fun things we included is this flag animation. It really improves the immersion in the game environment. We wanted a realistic wind effect on this. Now, we could've used a cloth simulation out of the physics engine. But instead, we decided to use the GPU and do it with Metal. We started with a SceneKit asset built by our technical artist. To get the Apple logo on the flag, we applied a texture at RunTime. Then we built a Swift class around the Metal device. Swift code builds a metal command queue and inserts information from the state of the game, such as the direction the wind is blowing. That command queue is running a custom Metal compute shader. That comes from a legacy code built in C. But because Metal is based on modern C++, it was a very easy conversion to make. We then also run another compute shader to compute normal for the surface, so we can get a great, smooth flag look without a huge number of polygons in the scene. And it really makes the flag look amazing. Each frame, the shader updates the geometry of the match to its new position. By taking advantage of the GPU in this way, we get a great effect without it impacting the main CPU. So, lastly I'd like to talk about the audio implementation in SwiftShot. Audio can make any game even more immersive and engaging. We knew we wanted to provide realistic sound effects positioned properly in the world for that really immersive experience. And giving the user great feedback on how they're interacting with that world. We also wanted to make sure it was fast and pay attention to how much adding the audio would add to the size of our app. So, we came up with what we think is a great solution. We created a few representative sound samples using some toys we borrowed from children of people on the team. We then recorded those and used those to combine them into an AU preset file and use those to build a custom Midi instrument in AV Foundation using AV Audio Unit Midi Instrument. That made it easy to quickly play the right sound at the right time in response to user inputs and collisions in the game. We didn't just play the sounds as is. To give good feedback to the user, we pull back on the slingshot. We vary the sound in a couple of ways. We change the pitch based on how far back they've pulled the slingshot. And we vary the volume based on the speed as you pull back. And we do that at RunTime by selecting the right Midi note and then using some additional Midi commands to alter that sound before we play it. So, let's take a listen and this is, we'll play it. [ Sound effects ] Now, we also wanted to make sure that when you're using the slingshot, we also give users some audio feedback as to whether or not they're within range of the slingshot and whether or not they've grabbed that. And those are the little beeps you heard at the start. Because those are UI feedback for the users, those sounds only come out of the device that the user is using to interact with the slingshot. However, we also want everybody else in the game to know what's going on with the slingshot, whether someone else is pulling something or something like that. But we want one of those to be quieter. So, we use positional audio so that if my opponent across the table is pulling their slingshot, I still hear that sound from my device but it's quieter and positioned correctly in the world. For colliding blocks, we took a similar approach but slightly different. We really wanted a cacophonous effect. And the blocks are generally not near any one player so again, using the positional support from SceneKit really made this sound great. Each device makes sounds separately without worrying about synchronizing across devices because we want it to be cacophonous, blocks smashing about. Again, we use a custom Midi instrument to take a small number of sounds and vary them. In this case, varying the attack rate based on the strength of the collision impulse coming from the SceneKit physics engine. These sounds again are localized in 3D coordinates based on the device's position in the scene. So, collisions in the far end of the table are quieter than those at your end. Let's take a listen to this. [ Sound effects ] One more shot. There we go. Right. So we wanted to share one more little trick that we discovered as we were working on this. In the process of setting up the sounds, we discovered that we needed to have a script run at RunTime to do some file name path conversions on the property list for the DAU preset. We found that we're able to build that tool using Swift but set it up as a command line tool. Do you notice at the top of this, the traditional Unix shebang-style statement at the top of the script. That tells your shell to fire up Swift to run this. By doing this, we can then treat Swift as a scripting language. You can develop one of these by using a Swift playground to work with your code interactively and make sure that you've gotten it right. Once it's ready, just save it out to a file, add the shebang line to the top and make the file executable in the file system. Now you've got a command line tool that you can use either, you know, outside the application or in Xcode using a RunScript phase. It's very easy and it really gives you access to all the system frameworks. In this case, we're able to edit the P list directly. It's a really great technique and we hope that you'll be able to take advantage of it. So, today I hope you've seen how AR provides really new opportunities for engaging games and other experiences. We encourage you to design with AR in mind from the start. And remember that for games, the play is the thing. You can't sprinkle fun on top at the end. We really hope that you'll download the SwiftShot available as sample code and use it to guide you as you build your own apps and we're planning to update that with each subsequent seed of iOS 12 as we go to the release. And finally, if you haven't had a chance yet, we hope you'll play SwiftShot with us downstairs in the game room. For more information, there's an ARKit lab immediately after this session and the get together this evening. I'm also happy to announce that for those of you here at the conference, we're going to have a SwiftShot tournament this Friday from noon to 2, so we hope you'll join us for that. Thank you very much.  Oh yeah, good morning, thank you for coming. I'm going to be joined by some very bright people in a minute. We have some very cool stuff prepared for you today. So let me start first by saying if you never write code that crashes this session is not for you. This is a talk for the rest of us, for those of us who make mistakes. Today we're going to talk about the tools and techniques that you can use to stay on top of your crashes if they're affecting your users. In particular I'm going to cover the fundamentals of crashes, why do they happen, what do they look like. Then I'm going to show you some of the tools that we have to access crash logs when they're occurring in the wild. Then Greg is going to come up and give you some more detail on how to read the content of a crash log. And then he's going to go in depth in how to read a tricky memory issue with a crash log. And then Kuba will show you how to catch threading races early, these threading races lead to crashes, and these crashes are very hard to reproduce. So first we should define it, what is a crash? A crash is a sudden termination of your app when it attempts to do something that is not allowed. So what's not allowed? Well, sometimes it's impossible for the CPU to execute code, the CPU won't divide by zero. Or sometimes it's the operating system that's enforcing some policy. The operating system will preserve the user experience by killing your app if it's taking too long to launch or it's using too much memory. Sometimes the programming language you're using is trying to prevent a failure and will trigger a crash. A Swift array and NSArray will halt your process if you attempt to go outside of your array bounds. Or sometimes it's you, the developer that's trying to prevent a failure. You may have an API where you assert is a parameter is non nil and that's perfectly all right. So stop me if you've seen this before, this is what it looks like in Xcode in the debugger when the debugger is attached to your app and has paused the process just before your app is terminated. And let's take a closer look at this backtrace on the left. You can see here how the app was started by the operating system and while we're paused we can see how the main function was called and functions are calling other functions within your app and eventually we get to this point where we hit a point where in your code where the only option is to crash, something has gone wrong. And ultimately the debugger receives a signal that this app is about to crash and pauses the app. Now sometimes you're not always conveniently attached with the debugger like we are here. When you're not attached with the debugger the operating system will capture this backtrace in plaintext and save it out to disk in a human readable crash log. Now the truth is one a release build of your app crashes the log doesn't actually look this pretty. What's actually written out is a list of binary names and addresses. This is a snippet from a unsymbolicated crash log. Now thankfully Xcode takes care of symbolicating crash logs so that what you'll see are those pretty function names, file names and line numbers that you're familiar with. So there are a number of ways to access these crash logs. I'd like first to talk about how you can access those crash logs from your beta testers on TestFlight and your customers on the App Store. You can download these crash logs using a feature in Xcode called the Crashes Organizer, this is what it looks like a beautiful dark mode. And let's take a tour through this UI. On the left, you can see all of your apps distributed over TestFlight and the App Store and we support all of our platforms including watchOS and app extensions. On the right, for a given crash point you can see the number of unique devices affected and we group crash logs by similar issue, by similar crash point and we rank them in the source list by the number of unique devices affected. And you can page through a sampling of the individual logs down below. And when you click on this button you can open the crash log in your project, in the debug navigator and see it alongside your source code. This is very cool if you haven't seen it before, we'll see that in a bit. And in the detailed view of course we show you a fully symbolicated backtrace with the crash point highlighted. So now that we have the lay of the land let's play with it. So I have Xcode open here and I'm going to open up the Organizer window and you can see that I've selected the Crashes tab, that's the second tab here and I've selected this chocolate chip app that Kuba and I are working on. And I've uploaded this build to TestFlight and you can see I'm looking at build 5 right now and a number of testers are reporting crashes, so that's not too good. But I've worked on a couple of these crashes you can see, but this first one I haven't worked on yet so let's try to resolve that. So this is affecting 242 devices and I can see the backtrace that was captured from the time that the app crashed and the crash point highlighted. Now I'm not quite sure what's going on yet but I bet if I open this crash log in our source code I can reason about what's going on. So I'm going to click the Open in Project button, select the project that matches build 5 of my app and what I do, what you can see is that this crash log has opened up in the debug navigator as if this app has just crashed. And you can see here that we're stopped on this fatal error. So a good question to ask myself is, is this a valid use of fatal error, I only want to be crashing if it's absolutely necessary. So I can see that this is an initializer of an enum from an int and that enum can only be zero or one, if it's not I'll hit this fatal error. So I think that this makes sense, this should only be crashing if there's some kind of misuse by the programmer. So I can see the caller of this initializer if I navigate up the call stack here and I can see it's this tableView delegate method. This method is asking for a title for a header in a given section number. So it must be that that section number is not zero or one. So I think I have a better understanding of what's going on, but let's try to reproduce this issue in the app and see if we can learn more. So I've hit play. And Chocolate Chip is a recipe app I store all of my favorite recipes and I've been testing with this whipped cream recipe and you can see everything's fine, I can see my list of ingredients, my list of steps. These are the two recipe sections, that's ingredients that's section zero and steps that's section one. And if I click on another recipe we've hit a crash and what I can see is that we're stopped on the same fatal error and the backtrace looks very similar to the crash log that we've been looking at. So that's a very good sign that we're looking at the same issue. So I'm going to clear this crash log by hitting delete and let's look at this debug session. So in this fatal error I can see that the message is nicely printing out that the section number is eight. So that's the reason we're crashing it's not zero or one. And what's occurring to me now is that this is my fault, when I implemented this class I implemented another delegate method called numberOfSections. And numberOfSections is saying how many headers it should look for and what I'm returning here is the number of ingredients and the ingredients count is eight. But believe it or not I have a clever way to solve this. I know what I want to be returning is the number of cases in this recipe section enum and I know that in Swift 4.2 some new functionality was added by the Swift Open Source Community, thank you very much, a protocol called CaseIterable. If I have my recipe section conformed to CaseIterable I can reimplement this number of sections to return the count of all the cases in that recipe section enum. And that way what I'll be returning is two, I'll be returning the number of accurate sections. This is going to work out just great. So now if I check out this chocolate chip cookies recipe no crash, I see all of my ingredients and my steps. I did a very good job, I'm very pleased with myself. And I can go back to the Organizer and mark this issue as resolved, step away from the computer and get back to my baking. Okay so you've just see how you can use the Crashes Organizer to download crash logs from TestFlight, open the log in your source code, and resolve an issue. So what do you need to do to get started? Very simple, your customers if they opt into sharing with third-party developers this just works, their crash logs are uploaded automatically. All you need to do is sign into Xcode with your Apple ID. When you upload your app you should include symbols so that you get server-side symbolication of your crash logs. And open up the Organizer window to the Crashes tab to start viewing those crashes. Okay so we have covered viewing crashes in the Organizer. But if you're not distributing over TestFlight or the App Store there are a couple of other options for you. There is the devices window. When you have a device connected you can click this View Logs button and will show you all the logs that are saved on that device and these logs are symbolicated using the local symbol information on your Mac. When you run your [inaudible] tests with Xcode, Xcode Server or Xcode Build the test results bundle will include any of the crash logs that come from your app that are written out during the execution of that test run and that's very convenient and these crash logs are also symbolicated. You can use the Mac Console app to view any crash logs from your Mac or from the Simulator. And on the device under Settings, Privacy, Analytics, Analytics Data you can see all of the logs that are saved to disk and your users can share a log directly from this screen. Okay so to ensure that symbolication just works I have three important best practices for you. Number one, if you use the Crashes Organizer upload your symbols with your app. This is the default, this will ensure that server-side symbolication works, it's very easy. Number two, be sure to save your app archives. Your archive contains a copy of your debug symbols, your . Xcode uses Spotlight to find these dSYMs and to perform local symbolication when it's necessary automatically. And if you upload an app that contains bitcode you should use the Archives Organizer Download Debug Symbols button to download any dSYMs that come from a store-side bitcode compilation. Okay we've covered all the tools that we offer for accessing crash logs when they occur in the field. Now to give you an in-depth guide into reading the content of a crash log please give a warm welcome to the helping, friendly Greg Parker. Thank you, Chris. So we've just seen how you can use Xcode to find crashes and to examine them in the Xcode tools in the debugger. But the crash log file contains more information, it contains much more information than just the stack trace. It is frequently useful to look at that extra information in order to debug your problem. So how can you get the full text of a crash log? Here's our Xcode Organizer, if we bring up the contextual menu there's a Show In Finder button. The Show In Finder button will bring up a text file which we can open up in the Console app or your favorite text editor, it looks something like this. So what's in this file, let's take a look. The top of the file starts with some summary information, this contains your app name, the version number, the operating system version it was running on, and the date and time of the crash. Below that we have the reason for the crash. This was the specific error, the specific signal that the operating system sent to kill the process. We can also see some logging information, the application-specific information section. This section will include console logs in some cases, if you have an unhandled exception it may include the exception backtrace. This section is not always available, on iOS devices it is often hidden for personal privacy reasons. But on Simulator to macOS this section can include information that is useful. Below that we have the thread stacks, these were the backtraces of all the threads that were running at the time of the crash. One of them is marked as the crash thread and we also have whatever other threads were running at the time that the process died. Below that we have some low-level information, we have the register state of the thread that crashed and we have the binary images that were loaded into the process. This is the application executable and all the other libraries. And Xcode uses this for symbolication in order to look up the symbols, the files and line number information for the stack traces. So that's the content of a crash log file. So how do we debug this, how do we read this, what do we look at? We start with the crash reason, the exception type. In this case, the exception type is an EXC bad instruction exception, the SIGILL signal was the illegal instruction signal. That means the CPU was trying to execute an instruction that does not exist or is invalid for some reason and that's why the process died. We can also look at the thread that crashed, what code was running at the time of the crash. Here we see the fatal error message function in the Swift runtime. No guesses as to what the fatal error message function does. The error message in this case is included in the application-specific information, so we can see what the Swift runtime printed as the process exited. So let's take a closer look at that stack trace. We saw the fatal error message function and that was called by a function in our code. We have a recipe class, an image function being called and that function in turn called fatal error message as a result of some error. Because this is a symbolicated stack trace with full debug information we have a file and line number in our code of what the crash was. So we can take a look at that line, we can open up our project, this is RecipeImage.swift, line 26 was the one that was marked in the crash. And those of you who are experienced Swift programmers will have a pretty good guess as to why this line might crash. We have a force unwrap operator. We have a function, The UIImage Constructor which returns an optional value. If the optional value is nil the force unwrap operator will halt the process, generate a crash log and exit. If we remember the application-specific information it included the error message that the Swift runtime prints as this error check fails and it says, unexpectedly found nil while unwrapping an optional value. So that's good, that's consistent with the code. We have a force unwrap operator on line 26, we have an error message in the crash log that says we were unwrapping an optional value. This all makes sense as a consistent story for what caused this crash. So a force unwrap failure is an example of a precondition or an assertion in the code. Preconditions and assertions are error checks that deliberately stop the process when an error occurs. Some examples of this are of course the force unwrap of an optional that we just saw. The Swift runtime will assert that the optional is not nil and crash if it is. We have out-of-bounds Swift.Array access is another example. If you access an array and your index is out-of-bounds the Swift runtime will fail, interim check it will fail a precondition and halt the process. Swift arithmetic overflow also contains assertions, if you're adding two numbers together and the result is too large for an integer variable there is a precondition for that, the process will halt. Uncaught exceptions are frequently caused by preconditions in the code. There are many error checks where if the precondition fails it will throw an exception and if the exception is not caught the uncaught exception will cause a crash log. And of course you can write assertions and preconditions in your own code if you have errors that you want to crash the process and generate a crash log in response. Another example of a crash log is a case where the operating system kills your process from the outside. An example of this are watchdog events, such as timeouts. If your application takes too long to do something the operating system may detect that, kill the process of generate a particular crash log as it does so. Environmental conditions can also cause the operating system to halt the process. If the device is overheating the operating system will kill processes that are using too much CPU. If the device is running out of memory the operating system will kill processes that are using lots of memory. Another case is an invalid code signature. The operating system enforces that code be signed and if a signature is invalid or the code is unsigned the operating system will kill the process and generate a particular type of crash log. These terminations by the operating system can be found in the Devices window in Xcode, they can be found on the macOS console. They do not always appear in the Xcode Organizer, so be careful of that. In Apple's developer documentation we have a technote that describes many different signatures and structures of crash logs like these particular examples, what they look like, how you can recognize them, and it goes into much more detail than we have time for here. But let's look at one example, here's another crash log file and again in order to understand the crash log we start with the crash reason. In this case the crash reason is in EXC crash exception with the SIGKILL signal. The SIGKILL signal is commonly used when the operating system wants to stop your process. It sends the SIGKILL signal, the SIGKILL signal cannot be handled, it cannot be caught by your process, it will die in response to this signal. We can also see the reason the operating system sent the signal in the crash log. In this case we have a termination reason with a code ate bad food. If you look in the developer technote I mentioned earlier, it will describe what ate bad food means. And we have a textual description that says exhausted real clock time allowance of 19.95 seconds. So if we combine this information with the information in the technote it will tell us our application took too long to launch. We had 20 seconds to launch, we weren't done in that much time, the operating system killed the process. Below we can see the crash logs at the time the process died. It's possible that those crash logs were the code that took too long, maybe were stuck in an infinite loop, maybe were stuck waiting for network I/O and that's why we took too long to launch. Or on the other hand, maybe this code is innocent and there's something earlier in the launch process that was too slow and that's why the process died. So launch timeouts, how can you avoid them. We want you to avoid them, launch timeouts are a common reason for app rejection during Apple's app review. So how do you avoid this? Well test your app of course. But there's a catch, the launch timeout watchdog is disabled in the Simulator and it's disabled in the debugger so if you're doing all your testing in the Simulator and the debugger you'll never see watchdog timeouts. So when you test your app be sure to test without the debugger. If you're a macOS app launch your app in the Finder. If you're an iOS app run in TestFlight or launch your app using the iOS App Launcher. All of these will run your app outside the debugger and the launch timeouts will be enabled and enforced. When you test, test on a real device of course, test outside of Simulators. And test your devices with older hardware, whatever's the oldest hardware you want your app to support. If you test only on newer hardware you might find that your app launches fast enough on a faster device, but slower devices might take too much time. So let's talk about another class of errors, let's talk about memory errors and what they look like in crash logs. When I say memory error I mean cases like reference counting of an object being over-released or using an object after it has been freed or a buffer overflow where you have a byte array or another C array and you access that array out-of-bounds. So let's look at another crash log and spoiler alert this one is a memory error. We start again with the exception type. This is an EXC bad access exception, the SEG violation signal. This is typically caused by a memory error. The bad access exception means one of two things, either we were writing to memory that is read-only or we were reading from memory that does not exist at all. Either of those will cause a bad access exception and the process will stop. We see here the address that we were accessing at the time of the crash. We can look at the stack trace, this is the function that performed the bad access of course. This is the objc release function, it's part of the implementation of reference counting in Objective-C and some Swift objects. So that again sounds like it's likely to have been a memory error that caused the bug. So what code caused objc release? We can look at the rest of the stack trace. We have the object dispose function, this is a function in the Objective-C runtime that is used to deallocate objects. The object dispose function called a function called ivar destroyer on one of our classes, our LoginViewController class. The ivar destroyer function is part of Swift code, this is a function that cleans up the properties, that cleans up the ivar storage of an object as it is deallocated. So that gives us part of the story as to what caused this crash. We were deallocating an object of our login view controller class. That class in its d init code was trying to clean up its properties and its ivars and while releasing one of those properties we crashed. So that gives us a little bit of detail as to what went wrong. Can we do better, is there more information in the crash log that will tell us more about what happened? We can look at the invalid address itself, sometimes the actual bad address value will contain useful information in it. And this particular bad address I can tell you looks like a use after free. Well how do I know that? Partly it's just long experience, when you read enough crash logs you start to learn patterns of what the bad values are. This particular bad value looks very much like the address range for the malloc memory allocator which we happen to have available in this crash log. So we have the address range that was used by the memory allocator and our invalid address looks like it's inside the malloc range, but it's been shifted by 4 bits, it's been rotated by 4 bits. So it looks like it was a valid malloc address rotated. That is a clue from the memory allocator itself. Let me show you why that is. Here's what our object looked like when it was still valid. An object starts with an isa field, the isa field points to the object's class. This is how Objective-C objects are laid out, this is how some swift objects are laid out. So what does the objc release function do? It reads the isa field and then dreferences the isa field so it can get to the class object and perform method lookups. Ordinarily of course this works, this is how it's supposed to work. What happens if our object has already been freed. When the free function deletes an object it inserts it into a free list of other dead objects. And it writes a free list pointer to the next object in the list where the isa field used to be. With one slight twist, it does not write a pointer into that field it writes a rotated pointer into that field. It wants to make sure that the value written there is not a valid memory address precisely so that bad use of the object will crash. So when objc release goes to read the isa field it instead gets a rotated free list pointer. When it dreferences the rotated free list pointer it crashes. The memory allocator did that for us, it deliberately rotated that pointer to make sure we would crash if we tried to use it again. So that is the signature we see in this crash log. We had the invalid address field looks like a pointer in the malloc region but rotated the same way that malloc rotates its free list pointers. So that's a strong sign that whatever object we are trying to release at this point in the code has already been deallocated, that's the memory error that occurred. So that's additional detail of what our story is. We have our object being deallocated, we're cleaning up its ivars, one of those ivars was already a freed object and that's what caused our crash. Can we do better? Can we figure out which object was being released by objc release? Ordinarily, the function calling objc release would give us a clue as to what that was. But the problem with the ivar destroyer function is it is a compiler generated function. We didn't write a function called ivar destroyer, which means there's no filename or line number associated with this point in the crash, we don't know which of our properties was being deallocated at that point. Here's our class, we have three properties in this class, we have a username, a database, and an array of views. And at this point in our story we don't know which of these objects was the one being released, it could've been any of them. Can we do better, can we work out which of those objects was the one being released from the information in the crash log because of course if we can't reproduce it in the debugger the crash log is all we have to go on? In this case we can do better. We can see a +42 where the file and line number would've been. And the +42 is our clue because the +42 is an offset in the assembly code of the function. We can disassemble the ivar destroyer function, look at the code and work out which property was being accessed at offset 42. So how does that work? We go into the debugger, into the debugger console. We can run lldb at a terminal. We can run lldb in the Xcode debug terminal. The debugger has commands to import a crash log a little bit as if it crashed inside the debugger. So we run this command to load the crash log interpretation commands and then we run another command to import our crash into the debugger. So we want three things to make this work. We want a copy of our crash log on the Mac, we also want a copy of our app, and a copy of the dSYM file that went with the app, all of them matching this crash log, all the same version of our app. So this is why we want you to keep your app archives. If we have all these files on the Mac, we run the crash log command, lldb uses Spotlight to find the matching executable, find the matching symbols, and load it into the debugger. So we see the stack trace of our crash thread, we see the file and line number information where available, and now we can go to work. Now we can find the address of the ivar destroyer function and disassemble it. This shows us the assembly code of our function. Now I don't have time to teach you how to read assembly code, but luckily for crash logs you don't actually have to be completely fluent in reading assembly code. It often is sufficient to be able to skim the assembly code and get a general idea of what is going on, you don't have to understand every single instruction to be able to get useful information out of a crash log. If we read through this function and we know about the call instruction and the jump instruction which are how you call functions, we can separate this code into three blocks. We have this top section which is making a function call into a reference count releasing function and this one is releasing our username property. The next region is releasing the database property. And the next region is releasing the views property. So we don't understand what all these instructions mean, but we have a general sense of what each region in the code is doing. It's a little bit like having a line number associated with the code. So now we go back to the information in our crash log. We had the ivar destroyer function +42 calling objc release. So there is instruction at +42, but there is one more catch. The catch is in a stack trace the assembly level offset of most stack frames is the return address, it is the instruction after the function call. So the instruction of the called objc release was the previous instruction, it's this instruction. If we read this it's the call of objc release which is good, that's consistent with what we saw in the stack trace in the crash log which was a call to objc release at this offset. And this release function is releasing the database property. So now we have more detail about what our crash is doing. We released the username property and that was successful. We have not yet gotten to the views property, it might be valid, it might be invalid we don't know. What we do know is we tried to release the database property and that object looked like it was already freed object based on the malloc free list pointer signature. So that gives us a pretty good story of what caused this crash. We were freeing a login view controller object and the database property was invalid. So we haven't actually found a bug yet, none of this code is incorrect, the ivar destroyer function is not wrong, something else was buggy in our code. But from the crash log we have been able to narrow down where our testing should go, where our attempts to reproduce the bug should go. We should be exercising this class, we should be exercising the database field, we should be reading the code that uses that database object and try and find the bug. So what have we just done? We read a crash log from scratch. We started with the crash reason, we read the exception type, we understand what the exception type means. We examined the stack trace of the thread that crashed, understand what it was doing, and what the actual error that failed was. And we looked for clues elsewhere in the crash log, in this case we used the bad address of the memory error and we used the disassembly of the crashing function. So memory errors are a wide variety of crashes, there are many different signatures in crash logs that can be caused by memory errors. Here are some examples for you. Crashes in the Objective-C message send function or in the reference counting machinery or deallocation machinery in Swift and Objective-C are very commonly caused by memory errors. Another common memory error symptom is an unrecognized selector exception. These are often caused when you have an object of some type, a code that is using that object, and then the object gets deallocated and used again. But instead of getting the malloc free list signature that we saw in that previous crash log, instead a new object is allocated at the same address where the old object used to be. So when the code tries to use the old object, call a function on the old object, we have a different object of a different type at that same address and it doesn't recognize that function at all and we get an unrecognized selector exception. One other common symptom of memory errors is an abort inside the memory allocator itself, inside the malloc and free functions. This is an example of a precondition like we saw earlier, this is a precondition inside the memory allocator. It may be identifying cases where the heap data structure of the malloc memory itself has been corrupted by a memory error and it halts the process and response. Or it may be detecting incorrect use of the malloc APIs. For example, if you free an object twice in a row the malloc allocator can sometimes recognize that as a double free and immediately halt the process. So let me give you some final tips for analyzing crash logs in general and analyzing memory errors in particular. In the crash we just looked at we spent most of our time looking at the code that crashed, the specific lines of code that crashed, and the thread that crashed. It is important to look at the other code in your process related to the code that crashed. For example, in this crash the ivar destroyer function is not wrong, that's not where the bug is. The bug is somewhere else, some other code is incorrect. You should also look at the stack traces in the crash log other than the crashed thread. The crash log contains all the stack traces in the process and that can contain useful information and clues for helping you puzzle out what the process was doing. Perhaps the other threads show more details about where in the application it was running, maybe it was performing network code and that's visible on one of the other stack traces. Or perhaps there's a multithreading error and the other threads may provide clues as to what the thread race was. You should also look at more than one crash log for a particular crash cause. The Xcode Organizer helpfully groups your crashes based on where in the code they crashed. And sometimes there will be multiple crashes at a same crash point, but some of the logs will contain more information than others. For example, the malloc free list signature that we just saw that might appear in some crash logs but not in others. So it is useful to browse through multiple crashes in the same crash set to see if some of them have information that is more useful than others. In addition, the Organizer as a group crashes together will sometimes put crashes with different causes together in the same group. There may be other threads or the backtrace of a crashed thread that identify to you, to a human eye you will recognize that there are multiple causes of this group of crashes even though the Xcode Organizer put them all together. If you only looked at one crash log you might not even know that that second crash is occurring until you fix the first one and ship it and start getting crash logs from your users. Once you've done some analysis of your crash, once you've narrowed down where in your process it might be occurring or which objects it might've been using you can use tools like the Address Sanitizer and the Zombies instrument to try and reproduce the crash. Because even though we did a good job with the malloc free list crash log of narrowing down what happened it's a lot easier to debug crashes when they occur in a debugger, in a test with Sanitization error messages telling you what went on. So a moment ago I mentioned looking up multiple stack traces, multiple thread stacks in order to diagnose multithreading errors. And to talk more about debugging multithreading errors let me introduce Kuba. Thank you. Thank you. As Greg mentioned, some memory corruptions can be caused by multithreading issues. Multithreading bugs are often one of the hardest types of bugs to diagnose and reproduce. They get especially hard to reproduce because they only cause crashes once in a while, so your code might seem to work fine in 99% of cases. And these bugs can go unnoticed for a very long time. Often monitoring bugs cause memory corruptions and what you'll see in crash logs looks just like memory corruptions as well and we've seen examples of those in the previous section. So when you're dealing with crashes inside the malloc or free or retained counting operations those are typical symptoms of memory corruptions. There's specific symptoms of multithreading bugs as well. The crashing thread often contains, sorry the crash log often contains multiple threads that are executing related parts of your code. So if a particular class or a method shows up in a crash log in multiple threads that's an indication of a potential multithreading bug. The memory corruptions that are caused by multithreading issues are often very random. So you might see crashes happening at slightly different lines of code or slightly different addresses. And as Greg mentioned, you can see those show up as different crash points in Xcode even though it's the same bug. And also the crashing thread might not actually be the culprit of the bug, so it's important to look at the stack traces of the other threads in a crash log. So now let's take a closer look at an example of a multithreading bug and I will show you how we can diagnose such a bug using a tool called Thread Sanitizer which is part of Xcode. So let's take another look at our cookies recipe app that Chris and I made and let's look at some more crash logs that we received from our users. Let's focus on this second app crasher of our app, this one. This crash log indicates that there's something wrong going on when we're doing with a class called LazyImageView, which is a class that I wrote and we'll look at it in just a second. But let's try to understand more from the crash logs first. Let's look at the entire stack of this thread which I can do by clicking this button right here that also shows all other threads. And if we focus at the top most frames here we'll see that what's really going on is that the free function is calling abort which indicates a heap corruption, a type of memory error. If we look at the stack traces of some other threads, like thread 5 here, we'll see that it's executing also some code inside LazyImageView. Let's look at another crash in this group of crashes and we'll see that this a common theme in all these crash logs. One thread is reporting a heap corruption when the free function is calling abort and a secondary thread is processing in a very related part of the code, actually in the same class, inside LazyImageView again. That's most likely not a coincidence, I highly suspect that this is a multithreading issue. So let's actually take a look at our LazyImageView class. So I'll click this button right here to jump straight to this line of code and open it in our project. And you can see the source of LazyImageView right here. It's a subclass of UIImageView, but it has an extra feature because it loads images lazily and asynchronously. And we can the logic for this right here in the initializer. What we do is we dispatch a job to a background queue where we will create the image on a background thread and once that is done we will dispatch back to the main queue to actually display the image on the screen. The crash log points us to this line of code here where we are accessing an image cache which we are using to make sure that we don't unnecessarily create the same image multiple times. So maybe there is some bug in the way my cache is implemented. Let's try to avoid guessing, instead I'll run the app in the Simulator and we will try to reproduce this crash. And let me close the crash log session first. All right, so this is our cookies recipe again and you'll notice that if I try to add a new recipe by clicking this + button right here we get asked to select an image for our new recipe. So this controller which is on the screen right now displays all these images using LazyImageView. So just showing it up on the screen and scrolling through the content already exercises all of the code inside LazyImageView, but I don't see any crashes. Unfortunately, it's a very common problem with multithreading bugs, they are notoriously hard to reproduce. So even when you repeatedly test the code that has such a bug you might not actually see a crush. Let's try actually doing that, let's try closing and opening this controller a couple times and let's see if maybe eventually we will be lucky and we will be able to trigger this crash. All right here we go, the debugger has stopped because it has crashed, but even when you actually do catch this crash in the debugger it doesn't really help us. All the debugger provides is that this is some sort of XC bad excess but it does not what caused the corruption or why did it actually happen. Luckily Xcode has a tool that's perfect just for this situation and it's called Thread Sanitizer and that's what I'm going to use here. So let's open up the scheme editor of our project which I can do by clicking the name of our app here and choosing Edit Scheme. And if I then switch over to the tab called Diagnostics you'll see that we have several runtime diagnostic tools here, like Address Sanitizer which is great at finding buffer overflows. Let me choose Thread Sanitizer and I'll also select Pause on Issues which means that the debugger will break every time that Sanitizer detects a bug. Let's run the app in the Simulator with Thread Sanitizer enabled and let's see what happens now if I try to add a new recipe. If I click the + button now you'll see that he app gets immediately stopped because Thread Sanitizer found the bug. And notice that I didn't have to try multiple times, Thread Sanitizer reproduces multithreading issues extremely reliably. So let's look at some details of this bug. We see that it's a Swift access race. And if we look at the left side into the debug navigator we get even some more details about this bug. We see that there's two accesses performed by two different threads, here is thread 2 and thread 4, that are both trying to access the same memory location at the same time, which is not allowed. So if we take a look at these two lines of code that are forming the race we see that they are both accessing the image cache again. So since this is a data structure that is shared between multiple threads as we see here it needs to be a thread data structure. So let's look at how it's implemented, let's jump to storage here which is what we're using and let's see if it actually is thread safe. So here's the source of image cache, right here at the top of the file and we can immediately spot what's wrong. This is just a plain Swift dictionary so that's not good. Swift dictionaries are not thread safe by default. So if we want to share a mutable Swift dictionary between multiple threads we have to protect it with synchronization, which means we have to make sure that only one thread accesses it at a time. So now let's actually work on fixing this problem and making the class thread safe and I'll do that in two steps. First, I'll refactor this code a little bit so that we have more control over storage and then in a second step I will use a dispatch queue to actually make this class thread safe. So first of all what I don't like here is that storage is declared as a public variable. That means that potentially any code inside my app can be accessing it. And it will be really hard to make sure that all the code in my app does it correctly, so let's change it to private instead. And let me introduce a different way of accessing image cache and I'll do that by introducing a subscript. That means that users of image cache can use brackets to load and store data from the cache. So subscripting needs a getter, like this one and a setter. And for now let's just implement these by directly accessing the underlying storage. And to make the rest of this file actually build I need to update the users, so instead of accessing the storage property we should be using brackets and index into image cache directly like this. So if I hit Build Now you'll see that the code will compile fine now, but I didn't really fix any bugs yet. But I did achieve something, I now have direct control over all the code that can access storage. It's either going to be code in this getter or the setter, no other code in my app has access to it. So I'm in a very good position to actually fix this Swift access race. And let me do that by using a dispatch queue. So let's create a new private variable called queue and let's assign a new dispatch queue into it. Dispatch queues are serial by default, so this one is also serial, which means that it will only allow one piece of code to execute inside that queue at a time. So that's perfect, that's exactly what we need here. How do we actually execute code inside a dispatch queue? We can use queue.sync and whatever code I move into queue.sync will be executed inside that serial queue and only one at a time. And I can return a value here because I need to return something from the getter. And I can fix the same thing in the setter as well. And if I move this line of code into this queue.sync it will be executed as part of that dispatch queue. So this way the code is now thread safe because every single line of code that accesses storage is always executed inside a serial dispatch queue, which means it will only be executed one at a time and is [inaudible] thread safe. Now it might be tempting to only use the synchronization in the setter where we are modifying the storage and avoiding it in the getter like this. But that's not correct, that can still cause memory corruptions and still cause crashes. Let me actually prove that to you by running this version of the code in the Simulator and let's see if the Sanitizer finds this little more subtle bug now. As I expected it does. We really have to protect both the getter and the setter with the synchronization. So let me run my app one last time in the Simulator and you will see it if I try to a new recipe this time. The controller loads up just fine and we don't get any more warnings because the class is now correctly thread safe. So I can actually go back to our Organizer window and mark this crash as resolved because we have found, identified, and fixed this bug. All right so what we've just seen is that I've started with a set of crash logs that all have symptoms of a multithreading bug. And then I've used this tool called Thread Sanitizer to identify and eventually fix this bug. Thread Sanitizer not only detects multithreading issues, but it also makes them reproduce much more reliably. Notice that in the demo I didn't have to invoke the controller multiple times. This tool works on macOS and in the Simulator, but just like all other runtime diagnostic tools it only finds bugs in codes that you exercise by actually running it. So you should keep that in mind and make sure that your automated or manual testing uses Thread Sanitizer, especially on any code that is using threading or GCD. And if you'd like to know more I recommend that you watch a video from my WWDC 2016 session called Thread Sanitizer and Static Analysis where we have introduced this tool and we also talked about how it works under the hood. As a reminder, Thread Sanitizer can be found in the Scheme Editor. So if you go to Product, Scheme and Edit Scheme that will bring up the Scheme Editor. And then we can switch over to the Diagnostics tab where you can find Thread Sanitizer among the other runtime diagnostic tools. I have one more debugging tip that I would like to share with you that's useful when dealing with multithreading. When you are creating dispatch queues you can provide a custom label in the initializer. You can assign a custom name to an operation queue and if you're working with threads you can use custom names with threads as well. These names and labels are displayed in the debugger but they also appear in some types of crash logs and that can help you often narrow down possible causes of multithreading bugs. Okay so to stay on top of crashes I have three takeaway points for you. Number one, always test your apps on real devices before submitting them to the App Store, that will help you avoid getting rejected in App Review. Number two, when you do get crashes from your users you should always try to reproduce them. Look at the crash logs and look at the stack traces and try to figure out which parts of your app do you need to exercise to trigger a crash or to try to reproduce the crash. And finally, for crashes that are hard to reproduce I recommend using bug finding tools, such as Address Sanitizer or Thread Sanitizer which work for editing memory corruption bugs and multithreading problems. So now let's recap what we've learned today. Chris has showed us how we can and should use the Organizer window in Xcode to get statistics and also details about crash logs. Greg showed us how to read and analyze the text of crash logs. In many cases, they can be reproduced like if you are dealing with app lunch timeouts. Then we've crash log crashes that are hard to reproduce because they happen randomly like memory corruptions. And we have mentioned what signs they leave in crash logs. And finally, I've shown how bug finding tools like the Sanitizers can help you reproduce issues that are memory corruptions and threading issues and I recommend that you use these tools as well. For more information please visit our session's webpage where we will also include links to the technotes that we mentioned and also the other documents that provides debugging tips that are helpful when dealing with crashes. And I'd like to remind you that there is a crash logs lab starting right after this session at 12 p.m. in Technology Lab 8, so please stop by if you have any questions about crashes and crash logs. And enjoy the rest of WWDC, thank you very much.  Good morning. My name is Michael LeHew, and I worked on the collections for the foundations team at Apple. And today, I want to talk about specific things that you should know about to ensure that your use of collections in Swift is as effective as possible. I'm going to cover a lot of territory today about details and all aspects of collections available for use in Swift. We're going to explore some common pitfalls and how to avoid them including performance issues, and I'm also going to offer very specific advice about when to choose to use specific collections. So let's begin. I want you to imagine a world, a world without collections. And in this world, we may not have arrays, but it still has bears, but every time we need another bear, we're going to need to define another variable, and we want to do things with these bears. We're going to need to repeat ourselves. Pretty scary, right? It gets worse though. This world also doesn't have dictionaries, but thankfully being clever developers, we can work around that with the function, where we painstakingly define each case that we care about, and using this function is very similar to using a dictionary, except none of those dynamic capabilities that we need. But who likes a mutable state, right? Fortunately for us though, we don't live in this world. Thankfully our world has bears and collections along with a rich first-class syntax for defining them. And APIs that help us not repeat ourselves when we iterate or retrieve the elements that they may store. Collections are so pervasive and share so many common features and algorithms that in Swift they all conform to a common protocol. Not surprisingly, it's called collection. And in Swift, collections are sequences whose elements can be traversed multiple times, nondestructively, and whose elements can be accessed via a subscript. Let's see, look at how they do that but considering an abstract representation of a collection. This could be an array defined contiguously in memory, a hash table, a red black tree, a link list or anything else that you can imagine. What matters is that it supports the concept of an initial index called start index. It can be used to access the initial element of the collection. It has an end index, which identifies the end of the collection, and collections support the ability to iterate from their start index to their end index. They can do this multiple times, and they also support a subscript to retrieve elements from the collection itself. Let's see what this looks like in code. Indeed, if we look at the definition of collection, it's declared as a sequence of elements. It also has an additional associated type called index, which must be comparable. It offers a subscript to retrieve elements from, or using that index, and we define a start and an end index identifying the bounds of our collection. And finally, we have a function called index after, which lets us get from one index to another. And this last function is so important because it allows the standard library to define many useful and powerful default behaviors with the incredible power of protocol extensions. Let's look at some of these. When you conform to collection, you gain access to an incredible range of functionality with properties that let you get the first and the last element or identify the count or check to see if a collection is empty. You also gain API that lets you iterate through a Collection using 4N syntax. And super useful functions like map, filter, and reduce. Now let's go ahead and make Collection even more powerful by adding our own extension. Collection already provides a way to iterate through every element, but I want a function that will let me iterate through every other element, skipping some of the values along the way and will do this by adding an extension to Collection. Let's start with the methods signature. We're going to call our function every other, and it's going to take a closure that will call on each element that we care about. We'll get the bounds of our iteration, and to find another variable, that starts at the start, which will mutate along as we go. We'll call the closure on the current element and advance our current index to the next one. And we may have invalidated our index at this point, we may have reached the end of the collection, so we need to make sure that we did that, and if we did, we can then advance our index one more time and thus skip every other element. And if we call this we see, if we call this on the close range from one to ten, we see that we skip the even elements. And so we see that Collections let us describe some really powerful behavior, but it turns out collections aren't alone. In fact, Collection is not the only protocol that we have. In Swift, we have access to a rich hierarchy of collection protocols, each greatly improving on the kinds of assumptions that we can make about our types. Let's go ahead and talk about a couple of these. We've already established that Collection lets you go forward from a given index, but there are also bidirectional collections, which let you go in the other direction. Now, of course, bidirectional collections are collections themselves, and so we can still iterate forward as well. The next most flexible form of collection is what's known as a random access collection, and with these, these add the requirement that it would be constant time to compute, or compute another index from another or to compute the distance between two indexes. The compiler cannot enforce this, and so when you conform to random acts as a collection, you're making a promise. But if you satisfy this promise, if you can deliver on this promise, the protocol gives you the power to access any index in the collection in constant time. And of course, random access collections remain collections, and so you can still iterate forward and backward. Now as Swift developers, we have access to many useful collections that conform to these protocols, collections such as array, set, and dictionary. But thanks to the general purpose utility of these protocols, many other types conform to these collection protocols as well, such as data, range, and string. Or the index collections, and they all gain access to all of this rich functionality by simply conforming to Collection in their own fashion. Indeed, once you know how any one of these types works, you can apply that knowledge to any of the others, and there are quite a few. So I'm going to talk about the details about how a type conforms to Collection, and it all begins with describing how it is indexed. Each collection has its own kind of index. And that index must be comparable. In some cases, the indices can look like integers, like arrays, but just because an index happens to look like an integer doesn't mean that you should use it like one. Now I want to ask a few questions that might have surprising answers. The first is how do we get the first element of an array, and instantly you probably all thought, well I'll just call array subzero. An array's index happens to be it. So you can sometimes say this and get exactly what you intend. But it isn't the best way to do this. I'm going to go ahead and ask the same question, but this time about a different collection. What's the first element of a set? Now this might seem like a really weird question, right? Sets are unordered. However, they are collections, and being collections, you can iterate through them, and when you iterate through a set, you're going to iterate through one element first, and so that's really the question we're asking here. So can I say set subzero? Happily, the compiler will say no, and that's because set's index type is an int, the type system wants us to use the correct index type. Lucky for us we already know how to do that. We know the collection protocol tells us that all collections have a start index, so let's go ahead and use that, and if we do this, this will actually work with all collections. Start index is always going to be the element of the first item that you would see when you iterate. But there's a nuance to watch out for when using indices to directly subscript into any collection. And that is, it can crash. I haven't actually asserted that these collections have contents in them, and so when you're, I'm just using start index, and these collections could be empty. It turns out though that accessing the first element in a collection is such a common task that there's an even better way. Simply call first. And if you call first, this is a lot safer, because the return time of this is optional, reflecting the fact that not all collections have a first element. So I have another question. It's the second element of a collection, and I want to emphasize collection here. It could be any collection, an array, a set, or something that doesn't even exist yet. Let's go ahead and add a new property to collection via protocol extension, and we'll call it second, and just like first, it's going to return optional, because not all collections have two elements. So, let's try writing this by subscripting 1. But here our zero-based indexing instincts are going to lead us astray, and then we'll be caught by the compiler again. We want this code to work with every collection, and not all collections use integers to index. So let's try a different approach. What I really want is one greater than the start index. But lucky here, the compiler will catch this as well. You can't add 1 to an arbitrary index type. Index types are supposed to be opaque or can be opaque. And what we really need to be doing here is we need to be using the API provided by the collection protocol to do this. So let's go ahead and do this. I commented out a sketch of the things that we're going to need to do to find the second element. The very first thing that we need to do is we need to check to see if the collection is empty. Collections are empty when their start index is exactly equivalent to their end index. So let's check for that and return nil, because such a collection doesn't have a second element. Oh, and so now we can assume that our collection has one element in it. We can use index after to get the second element or the second index, but we need to make sure that that index is valid, and let's see this visually. We advance after, but if the collection only had one element in it, we have now actually produced an invalid index, and if we tried to subscript with that index, we'd get that fatal error that we saw just a moment ago. So we check for it to be valid, and this is very similar to the empty [inaudible]. We just make sure that our index is not equal to the end index returning nil. Again, because two-element collections don't have a, or one-element collections don't have a second element. At this point, we have all the assumptions we need to know that our collection has at least two elements in it, and so we're safe to use the subscript operator with that index, retrieving the value that we desire. Now, it turns out, or that looks like a lot of code, but it's worth pointing out that this general purpose index math will work with any collection, which is pretty awesome, and it turns out Swift has a better way to do this though. There's something called slices, but before I show how to do it with slices, I want to talk about what slices are and how they work. Slices are a type that describe only part of a collection. And every slice has their own start and end index, and slices exist separately from the collections, from their originating collection. And what makes slices so efficient is they occupy no extra storage. They simply refer back to the original collection, and when slices are subscripted, they read out of the original buffer. And they can do this because they share the same indices as their underlying collection. And let's take a look at how that works. We can prove this to ourselves. We'll start with an array, and we'll ask that array to drop its first element, producing a slice that's one element shorter. And because we care about proving about the indices, we'll actually get the second index of the array by asking to advance one after the start index, and then we'll compare those. And indeed, they are the same. Now this dropfirst function looks exactly like we need to succinctly get the second element of our collection. So let's go back to our previous solution, and see how much more expressive we can be with slices. Remember all that fancy index [inaudible] code we had to write earlier? Well, by using dropfirst, we're going to let slices take care of all that fancy index bounds checking for us. And since first returns an optional, this will work as expected with empty and single element collections. Let's visualize what's happening here. We start with an array, and we form a slice by dropping the first element. We then use the first property to subscript into the slice, retrieving the element from the original collection. Now I don't know about you, but I'd much rather maintain this code. Now every type is free to describe its own slice type, and many do. For instance, arrays define array slices that are especially attuned to the most common use cases that arrays work with. Similarly, string defines a substring slice type, and substrings, again, are tuned to the special cases that are most common with strings. Some types, like set, will make use of the generalized slice type defined in the standard library. That's because sets are unordered. There's not very much else that they can do. They just basically need to maintain a start and an end index [inaudible] to the original collection. Data and range on the other hand are their own slice types, and so there's a lot of options that you have here. And there's one more thing about slices that I want to talk about before we move on. Let's suppose that we had a really large collection, like thousands and thousands and thousands of elements. And we add a couple of small slices to parts of that collection. It's important to remember that the slice keeps the entirety of the originating collection alive as long as the slice is around. And this can lead to surprising problems. Let see how this works in code. Let's suppose I have an extension on an array that allows me to return the first half, and I'm using the droplast function here to do so. And we have an array of eight numbers, and we call our extension, producing the slice, and then to try to get rid of that original storage of eight numbers, I go ahead and assign that array to an empty array. Our first clue that something interesting is happening occurs when we ask our slice for its first element. We're somehow able to return one, even though we threw away the storage for the original array. Either there was a copy or something magical is going on. And so if we wanted to eliminate that buffer though, and oh the magic that's actually going on is that we're holding onto the buffer. And so if we wanted to eliminate that, what we could do is we could form an actual copy of the array from the slice, and then if we set that slice to an empty array itself, that copy would still be valid. Let's visualize what just happened. We started with an array. We then formed a slice on the first half of that array. We then created a copy of that, setting the array to be empty and setting that slice to be empty. And only after we did that did the underlying storage go away. So in this matter, slices sort of work like lazy copies. You get to choose when you make a copy of the elements yourself, and it turns out that this concept of being lazy and doing something later is really useful in other contexts too. One such context is function calls. Now function calls in Swift are eager by default. That is, they consume their input and return their output as demanded. Consider this example. We start with a range from one to 4000, and ranges are a really succinct way of representing a lot of numbers. It's just a start and an end, and it knows how to produce them. We then map this though multiplying each value by two, and so we've now actually allocated an array of 4000 elements and performed our mapping function on each of them. We then filter that down to four elements. And so at this point, we've actually gone ahead and allocated 4004, you know, space for 4004 elements, but we only really needed the final four. And that's an awful lot of intermediate computation that maybe we don't always desire. It would be great if there was a way just to not do any of it, unless it was absolutely needed. And Swift's answer for that is called being lazy, just like in real life. We'll start as we did before with the range, and then we'll tell that range to be lazy. And when we do this, what happens is we wrap the original collection with a lazy collection, and when we perform operations on this lazy collection, what's going to happen is we're going to wrap it again. And so when we wrap the, when we call map on it, we actually aren't mapping. We're not doing anything with that closure other than storing it for later should we ever need to use it. Further, if I filter that lazy map collection, the filter simply wraps the map collection, noting that it's going to filter later on demand, but not right now. Now let's go ahead and ask our lazy filter collection for it's first element. When we do this, we'll start by asking the lazy filter collection for it's first element, but the lazy filter collection doesn't know. It wraps something that might know. And so it'll defer to the map collection. And the map collection also doesn't know it's first element, but it wraps a collection that might, and indeed, the range knows it's first element. The first element of the range is the value one, which it returns to the lazy map collection where now the lazy map collection can actually perform it's closure, computing the value 2, which it returns to the lazy filter collection as a candidatefirst element. Now lucky for us in this case, 2 happens to be less than 10, and so the lazy filter collection finds it first element on the first try, which it returns back to its caller. Now that's a lot of different computation. And I mentioned that lazy aims to only do calculation as needed on demand, but another thing that it avoids is creating intermediate storage. So I want to show an example of that. Let's suppose we had an array of different kind of bears. However, I want to point out that some of these bears are redundant. We already know that they're bears. They don't need to tell us again. So let's write some code to find those silly, redundant bears, and we'll do this using a lazy filter, as before. And in this case, producing a lazy filter is going to be a lazy filter collection that wraps an array of strings. In our closer here, were going to print out which bear we're currently iterating on before we do our predicate check. And we're doing this because I want to understand how filter works a little better, and then we'll call first, and when we do this, we'll defer to the lazy filter collection, and then the lazy filter collection will in turn defer to the original storage where we'll print out Grizzly, check the predicate, which in this case is false, Grizzly does not contain the word bear, and advance on to panda. When we get to panda, when we get to panda, we'll again print out panda, check to see if it contains the word bear, and advance on to spectacle. Spectacle gets printed, also doesn't contain the word bear, and we advance finally to gummy bears, which mercifully has the word bear in it, and which the lazy filter collection can now return to its caller. Now what would happen if we called first again? Well, it's the same story. We ask the lazy filter collection, which defers to the underlying collection, which repeats that calculation, returning it to its caller. Now this might not typically be what you want, and so if you find yourself needing to repeatedly ask the lazy collection to calculate its result, there's a way to make sure that that happens just once. We can ensure that the lazy collection is iterated exactly once by creating a new nonlazy collection from the lazy, and when you do this, it will still defer to the lazy collection, but now the iteration will proceed through the entirety of your underlying collection, producing essentially, you know, the nonlazy version of that lazy calculation. And so in this case, we get an array containing the string gummy bears. And if we print the first element of that ray, we don't need to consult the closure at all or the lazy collection at all. We basically stamped out the laziness and now have an eager array. So when should we be lazy? Well, lazy collections are a really great way to eliminate the overhead of chained maps and filters. They excel when you find yourself only needing part of the result of a collection calculation, or we should avoid using lazy if your closures have side effects, and your closures should rarely have side effects. And be sure to reify back, or I should say, be sure to consider reifying back into a regular collection when you cross API boundaries. Lazy should often be an implementation detail. Now up until now, we've been able to do a lot of cool things with just mutable collections, but of course Swift lets us mutate our collections as well. Let's talk about the two kinds of collections that we haven't talked about yet. The first of these is mutable collection. This adds a setter to the subscript so that you can change the contents of a collection but not its length. And you have to be able to do this in constant time. The next is called a Range Replaceable Collection, and this is the kind of collection that you get when you can remove elements from a collection or insert them. And now I want to talk about one of the questions I get asked all the time. Why does my perfectly reasonable collection code crash? And like all good question answers, I almost always follow up with a few questions of my own. Sometimes I start with the classic, well what are you trying to do? And I usually quickly follow up with, well how are you using your collections? Are you mutating them? Are you sure you aren't accessing your collections for multiple threads? And I ask these questions because their answers often lead to the root cause of the problem. Well, let's begin with the assumption that threads aren't involved. I'm not ready to think about threads yet. It's not even 9:30. Suppose we had an array, and we get the index of an element that we know is there, in this case the value e. And then we mutate the collection, say by removing its first element, and we go ahead and print the element associated with that index. Well when we do this, unfortunately this will produce a fatal error. The index is no longer valid. In fact, the index became invalid the moment we mutated our collection. A far safer approach would be to mutate our collection first and then calculate the index. It's worth pointing out that mutation always invalidates indices. This doesn't just apply to arrays. Let's take a look at how this problem could manifest with dictionaries. Let's suppose that we have a dictionary showing a few of a bear's favorite things. We'll grab the index of our bear's favorite food and print it out, confirming that, indeed, it's salmon. Next, we'll add a couple more favorite things that this bear has, and then we'll make sure that our favorite food is still salmon. And we'll see that, wait a minute, our favorite good isn't hibernation, it's salmon. And just like arrays, we invalidated our index the moment we mutated the dictionary. It's also worth pointing out that this code can crash. So how do we go about fixing this? Well, it turns out it's the same exact fix that we used with the array. We just simply recompute the index after we mutate. Well, one thing to keep in mind that when you're recomputing indices this is something that can sometimes be expensive. Some index search methods take linear time. And so you want to take care to only find the indices that you need. So here's my advice if you want to avoid finding yourself in these kinds of situations. Remember that mutation almost always invalidates your indices. You might get away with it sometimes, but it's really best to just treat this as a hard rule. You'll be much happier for it. Also remember that slices hold on to the underlying original state of the collection even after it was mutated, and because of that, really think twice about holding onto indices or slices when the underlying collection is mutable. And keep in mind that index computation can sometimes take a nontrivial amount of time. So take care to only calculate indices as needed. So a little bit later, let's bring threads into the discussion. I mentioned that one of the questions that I ask is are your threads accessible for multiple threads? And the reason why I ask this is because our collections assume that you will access them from a single thread. And this is a really good thing for reasons of performance. It makes it so that all single-threaded use cases don't have to pay the tax of locks or any of those other primitives that you could use to ensure mutual exclusion. And when threads are involved, only developers using the collections will have all the information needed to restrict access with the appropriate lock or a serial queue at a much higher level abstraction than us lowly framework developers could ever offer. So let's see what these kinds of problems could look like. Let's suppose we have an array that we aim to fill up with sleeping bears, and to simulate each bear being their own bear and in charge of themselves, we're going to get access to a concurrent dispatch queue that we'll use to tell each bear to go to sleep. And because this is a concurrent dispatch queue, it's some time helpful to like imagine the code running at the same time, which I'll simulate by putting them on the same line. And later in our application, let's go ahead and check in on those sleeping bears, and sometimes we'll see grandpa and cubs snoozing happily. Other times, cub will go to sleep first, and then it'll be grandpa. Sometimes, quite mysteriously, only grandpa is sleeping in. And other times, it'll be the cub, and sometimes our program just up and crashes, and nobody bear's getting any sleep. All of these possibilities suggest that there's a possible race condition, and indeed, it seems likely given all the potential threads involved in this example. And we can prove this to ourselves using the thread sanitizer or TSAN that's included within Xcode. And if we were to do so, we'd get output that kind of looks like this, and indeed, TSAN would catch the race. It would tell us there's a Swift access race. It would tell us which threads are involved and give us a summary at the end telling us which line to actually go start looking for our problem. And all that evidence is actually going to be really helpful to find the bug. So we've proven that there's a bug here. TSAN has never lied in my experience with them. So we can fix this by eliminating the bears' ability to go to sleep at the same time, and we'll do that with a serial dispatch queue. And now only one bear can go to sleep at a time. And so if we peek in on our sleeping bears again now, taking care to do so on the appropriate queue, we see that sure enough grandpa and cub are snoozing away peacefully like we expected. So my advice for working with collections for multiple threads is try to isolate your data so that it can only be seen from a single thread, and when you can't do that, make sure that you have an appropriate form in mutual exclusion, such as a serial dispatch queue or locks. And always use the thread sanitizer to double check your work. It's far better to catch bugs before they ship in your app than after. And I have a little bit more advice for working with mutable collections. The first of which is if you can avoid it, don't use mutable state at all. So far all the difficulties that I've described have come about because we've been working with mutable state. You can avoid all the potential for this complexity by avoiding mutable collections in the first place. Many times you can actually emulate the mutations that you want to perform by using a slice or using a lazy wrapper, and it's almost always easier to understand data that cannot change. And thanks to mutability being built into Swift, the compiler will help you if you're leaving a state mutable when you're not actually mutating it. Now I have one more piece of advice that actually concerns how best to use mutable state when you have to. And that's when you're forming new collections. You can help performance if you're lucky enough to know an exact count or a really good approximation of how many elements you're actually going to need. Most collection APIs have a way of being able to provide this hint, and when you do this, you get exactly the size you need with no overhead. If you don't, our collections are general purpose tools. They're meant to work on a wide variety of cases, and as you incrementally add elements, you may actually end up over allocating the amount of storage that you need, but taking care that you don't over estimate when providing such hints, because otherwise you'll find yourself in the same exactly situation where you're using up more storage than you actually need. And now, I want to move on to my final topic for today. And that's the wide range of collections that become available for you when you import Foundation and when you should consider using them. In addition to the standard library collections, when you import Foundation, you gain access to the great reference-type collections that objective C developers have been using for decades. And many of these also gain conformance in Swift and thus behave just the collections that we've been talking about. That said, there are a couple important things to keep in mind. First thing to keep in mind is that these NS [inaudible] collections are reference types. And this is best examined by considering an example. We're going to define value types and reference types and do the same things with them on two sides. So with our value type, we'll call it x. It will be an array of strings. We get an empty array called x. With a reference type, we get an empty array, but x is pointing to it. We then mutate that array with the value type. That array is mutated in line. With the reference type, that array is, the reference, the array that is being referenced is mutated in line. We add another variable. With the value type, something really special happens. We actually don't copy the storage at this point. Why is an array that knows that its storage is actually owned by x? And why is that actually going to perform that copy until either of those collections is mutated. The reference type is a little bit different. Y is just another pointer to the same underlying array. So let's go ahead and mutate y. We'll put another bear in that array. With the value type what happens is first we invoke that copy on write machinery. We're writing to a y, so we need to copy it, and then we can insert the next bear. With the reference, it's a little bit simpler. There's only one array. We simply put the panda in the array. There's a second thing that you need to keep in mind when working with the foundation collections in Swift. And that is, all objective-C APIs in Swift appear as Swift native value types. And this is actually really wonderful because it let's code in each language speak naturally with the types that they work with best. But how can this work? The two languages have completely different implementations for these collections. And the reason why it works is something known as bridging. Bridging is how we convert between the two different runtime representations, and this is something that's necessary because Swift and objective-C, I'm sure you've noticed, are very different languages with very different compile and runtime features. And while we've optimized bridging to be as fast as it can be, it's not free. There will always be a cost when bridging between the two languages. So what happens when we bridge? Well, when we bridge between the language, we have dispersed set up new storage, equivalent storage, so if you're taking n things in one language, you'll take up n space in the next one. Then we need to go element by element and convert potentially between them, and this per-element bridging can sometimes be recursive. For instance, if I have an array of strings, first we'll bridge the array, and then we'll bridge each individual string. And when this happens at the boundary between the two languages, we call it eager bridging. And collections will always be bridged eagerly when the elements of the collection need bridging as well. And this arises most often with dictionaries keyed by strings. When collection bridging is not eager, we call it lazy. And this happens when the element types of the collection aren't bridged themselves, such as NSViews. In this case, bridging will be deferred until the collection is actually first used. Let's make this concrete with some examples. We'll first consider an objective-C API describing an NSArray of NSDatas. Now NSArray is bridged to array, and NSData is bridged to value-type data. And so such a collection would be bridged eagerly. I mentioned a moment ago that NSViews are not bridged in Swift. They remain reference types in Swift. And so an NSArray of NSViews will be lazily bridged. The bridging won't happen until you first access or try to use that array. And finally, an NSDictionary with keys that are NS strings will be bridged eagerly because the strings need to come across in Swift as value-type strings. So now that we know what bridging is, how it works, and when it happens, we can move on to the most important question of all, which is when should you care about it. And the answer is really straightforward. When you measure it negatively impacting your app. Specifically, when you take a time profile or trace an instrument, pay special attention to where your code crosses between the languages, especially when this happens inside a loop. Some bridging is going to happen, and that's totally okay. What you're looking for though is a disproportionate amount of time or a surprising amount of time spent in code that you didn't write that has the word bridge in it. Let's look at a concrete example. Suppose I have a manuscript for a great children's story that I'm working on, but it's really long, so I'm only going to show a little bit here, but to really make it pop, I want to make every word or every instance of the word brown actually be the color brown, and in the interest of space, I'm only going to show highlighting the first word. To do this, I'm going to use an NS mutable attributed string. I'll pass my story in there. And then using the attributed strings string property, I'll ask for the range of the Swift string brown, which will produce a range of strings native index type. And as mutable string works with NS ranges, and so I'll use the convenience initializer that we introduced last year to convert to an NS range, and this, I'm calling again attributed strings string property to do the conversion. And then we'll color the first instance of the word brown. And when I go to run this code, I notice it's a little slow. So I profile it. And I see that, to my surprise, I thought all the time would be spent coloring the word brown, but indeed, it's actually computing the indices, and why is that? And the reason for that is that we're actually bridging our string multiple times across languages. Mutable attributed string is an objective-C reference type, and so when we're asking for the string property, we're actually having to convert from an NSString to a string. And we're doing it once here when we calculated the first range, and we're doing it a second time when we convert for the NSRange. You can imagine how expensive this would be if we did this in a loop looking for all the text to color. Now let's look into why this is happening. Every time I call text.string, we start in a Swift execution context. However, the implementation of NSMutableAttributedString is objective-C, and so in order to provide the result, we actually have to consult the original implementation. The original implementation returns an NSString, which is the reference type, and so when return to string, it needs to be bridged, graphing cluster by graphing cluster, character by character. And bridging happens whether it's a return type or a parameter. So now that we know those details, we can make, now we can actually make this a little bit better. Let's just bridge once. And let's remeasure our code and see that indeed we've improved our performance by half. But it turns out this year we can do a little better. Oh, and also, now we're not bridging where we do that. But this year we can do a little bit better. This year, if we actually [inaudible] to an NSString when we ask for the text.string's property, when we get the variable out, no bridging is actually going to occur. And further, by doing so, now that string is an NSString, when we call the range of property, we're actually going to get an NSRange out of it automatically. We won't need to do any of the range conversion between Swift native types and the NS ranges, which is pretty excellent. So let's measure this code and see how it is, and sure enough, this looks pretty good. Much, much faster than the, you know, almost 800 milliseconds that we were consuming previously. However, I do want to point out that we're still bridging here. And it's a teeny, tiny bridge, but we're still bridging. Brown here is a Swift value type string. And every time we call into the objective-C API range of [inaudible] NSString, we're actually going to bridge that teeny, tiny string back to an NSString. This is inexpensive in this case. I'm only doing it once, but you can imagine, if this was in a loop, that small amount would add up over time. And so you want to take care to avoid bridging the same small strings repeatedly. However, before you do such optimizations, always measure. And so now that we've seen the details of bridging, I want to offer a little bit of advice about when to use Foundation collections. You should consider using them explicitly when you need a collection with reference semantics. Don't need to write one of those for yourself, we already have many great ones. You should also use it when you're working with types that you know to be reference types. Things like NS proxies or core data managed objects. And the final time to consider using them is when you're round tripping with objective-C code, but I'd recommend strongly doing this only after you've measured and identified that bridging is indeed the culprit for whatever performance problems you may be seeing. And now, we've reached the end of today's dive into the incredible power of our world's collections in Swift. I want you to use this new-found understanding to review your existing use of collections. Look for places where you can improve your code through more effective use of indices and slices. Measure your code. And look for places where you could benefit by being lazy or by tuning how you bridge. Use the thread sanitizer to help audit your mutable state, and further hone your mastery of collections by apply all the concepts discussed today in Playgrounds and you own apps. Be sure to visit our last few labs today if you have any questions about collections. We're here to help. Thank you so much for you time. Now go out and be effective.  Alright, good morning. Thanks so much for coming out this morning to learn all about what's new in Cocoa Touch. Now normally, Eliza would join me up here a little bit later although this year actually you're just going to be hearing from me. So, she'll be back in later years. Don't worry about it. This morning we're going to talk about things in three main categories. We're going to start with some framework updates including things like performance and security. Then we're going to talk about some API enhancements across a number of different existing APIs in the SDK, including notifications and messages. And then we're going to end with Siri Shortcuts. So let's get started with our first topic: Performance Updates. Now we're going to talk about performance across three main areas: scrolling, memory, and auto layout. Now before we get in to it, it's important to keep in mind one little bit of background information about scrolling. Scrolling on iOS follows a pretty common pattern in most places. We load content to be displayed into the views and then we're just moving that content around. And while we're moving it around, most of those frames are really cheap to generate because we don't have to load anything new. But every now and then, a new view first becomes visible and the one frame when that first happens is quite a bit more expensive to generate than those other cheaper ones. Now, of course, once that one frame is loaded, we're just back to moving content around, so the amount of work we do on the CPU goes back to being pretty small for most of that other scrolling. So what's happening during that really expensive frame that causes that one to be more than all the others? Well, let's take a look from the perspective UI Table View but everything we look at here will really be the same for UI Collection View or really any of your own custom views that you may build that behave in similar ways. So the work in that expensive frame probably starts in your implementation of TableView, cell For Row At index Path delegate method. Now the first thing we're going to do in there is get the cell that we want to display. And so we're going to try and dequeue it ideally from the reuse queue although if one is not available already in the queue, we might actually have to do some memory allocation in order to get it ready. Once we have the cell, we're then going to populate it with your model data. Now how expensive that is will vary depending on your application but it can be including a fairly large amount of expensive operations like reading files, loading data out of databases, or other things of that nature. So you'll definitely want to look at the expense here in your own apps but it tends to be this is where a good portion of it will exist. Now, you may think looking here that that's the end of the expensive work but even once you return from this method, there's actually more work that has to happen in order to get the cell prepared to have it show up on screen. So, of course, next, we have to lay out all of the content in that cell. We need to size all the views and position them in the right spot. Now, this can actually be a pretty substantial amount of the total time that we're spending because it can include other expensive operations like measuring text. Once everything is properly sized and positioned, then it's time to generate any content that would need to be drawn using drawing calls and to do that we have to call draw Rect on all of the subviews within that cell. Again, this can be a pretty large amount of the time because we'll also be doing things like drawing text. So overall, there's a lot of work that has to happen across this whole piece of code and it has to happen in a really short period of time. On our 60-hertz devices, you have 16 milliseconds to complete all this work in order to make sure you don't drop any frames, and maintain smooth scrolling. On our 12-hertz iPads, on the iPad Pro, you have only 8 milliseconds to complete all of that work. So, it really needs to be done as quickly as possible. Now, to help with that, in iOS 10, we introduced a cell prefetching API and the idea with the prefetch API is to take some of this work, populating your cell with model data, and pull it out of this critical section that's happening on demand in that short window, do it earlier, and do it on a background thread so it can happen asynchronously with some of the other work, in particular those cheaper scrolling frames we talked about. Now adopting this is really easy. It's just a UI Table View Data Source Prefetching protocol and it only has two methods, only one of which is actually required. And the idea here is to move some of that expensive work of loading things from files or reading your database into here so that you don't have to do it on demand. You data is already prepared when the cell is needed for display. So this in most cases can be a really big win although while we were looking at some of our own apps in iOS 12, we actually noticed a case where this was causing an issue instead of helping us. So let's take a look at what that looked like. Now here's an example of a trace that we took while scrolling on an iPhone 6 Plus. The vertical bars across the top, those represent frames that we want to display. The alternating light and dark blue colors represent frames that we did swap to the display as they were changing. And that double wide light blue bar, that is a place where we drew the same frame for two full frame durations. So for a customer looking at the device while this was happening, that looked like a dropped frame or a scrolling hitch, which obviously is what we're trying to avoid. So what was happening in this case? Well, here you can see that red bar is representing the time that we're spending in the critical section we just talked about, all the self-[inaudible] index path, layout, and drawing. And here, it's actually taking longer than the 16 milliseconds we had to draw the frame. Now because the device can only swap new frames onto the screen at fixed positions in time, once we miss that deadline, we ended up displaying the same frame for two full durations, which was obviously not great. So why did that happen here? In this case, we're looking at an app where we actually had implemented the cell prefetching method so our data should've been ready. Ideally, this could've been done more quickly. Well, if we look at a little more of the trace, we can see what was going on. The cell prefetching API was being called at the same time that we were requesting the current cell. Now it wasn't being called for the data for the current cell. It was being called for data that we might need in a future cell, but it was getting run at the same time. And so there was now contention for the CPU as we tried to both load the current frame and also load data for a future frame that we don't actually need yet. So because of that contention, it actually caused both tasks to take a little bit longer. Now in iOS 12, we're much more intelligent about scheduling these background prefetch operations so that rather than happening concurrently and causing some CPU contention, they'll now happen serially, shortening the time that you need to take to load the current cell and helping avoid dropped frames in many cases. So once we had that fixed, we kept profiling our apps and we actually found another case where there was a bit of a surprising cause of some dropped frames. Now what we found was that when the device was not under load, there was no background activity, all we were doing was a little bit of scrolling in the foreground app. Counterintuitively, we could actually drop more frames than times when there was some small amount of light background work going on. That didn't really make a lot of sense. And to understand why it was happening, we had to drop down a level and take a look at the behavior of the CPU when it was scheduling our workloads. So let's take a look at another trace. Here, we've got the same situation. Double wide blue bar is causing us to drop a frame or rather is our dropped frame. Now here we can see a graph of our CPU's performance over time. Now during most of those cheap frames, our CPU performance is staying pretty low. There's no background work going on. All we're doing is scrolling things and that's exactly what you would want because when we're not doing expensive work for scrolling, you want to keep the CPU as low as possible to preserve battery life. So that was great. What wasn't great is that it took a little bit of time before it could ramp up. You heard about this yesterday in the keynote. Now when it did finally ramp up, it was already too late to have completed the work to load the cell that we needed to display and so we ended up missing that frame again. Now because we own the full software stack from top to bottom, in iOS 12 we took all the information we have in the high-level UIKit framework about what scrolling is happening and when these critical sections are occurring and pass that information all the way down to the low-level CPU performance controller so that it can now much more intelligently reason about the work that's happening and predict both when these bursts will occur and how much CPU performance will be required to meet the deadline for the historical demand that your app has had. So once that change happens, where the load starts right here, we end up seeing that we've ramped the CPU far more frequently or, far more quickly, to the exact amount of perform it needs to make sure that we hit that deadline and don't drop frames. This has caused a really great improvement across many different scrolling scenarios around iOS. So all of your applications will get all of this enhancements, both of them and a number of others, for free with no additional work on your part, but there are a couple of things that you can do to make sure that you get the most out of both of them. So first of all, if you haven't already adopted that tableView cell prefetching API or the Collection View one, definitely look into that because having your data ready is one of the best things you can do to make sure that loading cells is as quick as possible. Of course, it's also important that you profile your full critical sections of your cell loading and reduce that demand as much as you can. iOS 12 will now try to match the CPU performance to the needs of your application during this period but the best thing that you can do will always remain to reduce the amount of work that you have to do to make sure that you give your customers a really smooth scrolling experience. So that's scrolling in iOS 12. Next, let's turn our attention to our next performance topic: memory. Now, you might wonder why memory is showing up right here in the middle of a performance discussion, but I assure you that that actually makes a lot of sense and, in fact, the reason is because memory really is performance. The more memory that your app is going to use, the more that it will have an impact on the performance of your application. So to understand why, let's take a look at a really high-level overview of what the overall memory on the system might look like in a common situation. So, of course, you can see here, a lot of the system's memory is being used by other applications and the system itself. Your app is using some amount for the moment. And there's some that's being kept free to service new allocation requests as they come in. Now, most of that memory is probably not truly free. It's likely including things like caches or other things that can be quickly thrown away to make sure that the memory is available to satisfy demand right away but in general it's probably actually being used for something, but it is readily available. So let's look at what happens when your application requests some memory. Maybe you'll make a small request, something that can be satisfied by the amount that's currently available in that free pool. Well, if that's the case, it'll be return right away to your app and you can continue on with your work. Now, let's say, though, that your application makes a larger request, and maybe it doesn't need it for a long period of time, so you might not be real worried about it. Perhaps you're just going to load an image off disc and decompress it, perform some quick operation on some of the pixels and then throw it away. So it seems like a quick operation that you don't have to worry too much about. Well, we'll make that big request and now that is more memory that is currently easily readily available to the system. So it won't be able to satisfy it immediately and we'll have to go find that memory from somewhere else. And, in fact, the most likely place to come from is from other applications or things on the system. Now, that might not worry you too much because you're trying to get the memory for your app right now, so you're not so worried about what's happening elsewhere. But, of course, this will have an impact on something else later that your customers will be expecting will be working, but more importantly to you right now, it will have an impact on your app as well because the system has to do work to go get this memory for you. The fact that it's not readily available means that the kernel has to go find it and perform operations on the CPU in order to make it available. And that time it's spending getting that memory for you is time that could be better spent doing whatever your app does best. So finding ways to either chunk these larger requests or just reduce those overall peak demands can actually have both a big impact on the performance of your app while you're using it and also improve the customer experience across other apps later. So there's many ways that you can reduce the total memory usage of your apps and starting with a profile and instruments is a great way to look at that. But for this morning, we're just going to take a look at one new technique that iOS 12 brings along that helps reduce the memory usage of your apps and that's Automatic Backing Stores. So let's say we want to draw this lazy prairie dog in portrait mode on an iPhone X. Now, how big is that going to be? Portrait mode on iPhone X, 375 points wide and, to preserve our aspect ratio, it'll be 250 points tall. So how much memory will that use? Well, 375 by 250 at 3x, with 64 bits per pixel because this is a deep color device, is going to be 2.2 megabytes of memory to draw a prairie dog. That seems like a pretty decent amount, but in this case that's actually probably memory well spent. We're actually trying to represent this full-fidelity image and that's the amount of memory that's needed to draw it into this buffer. So that's probably expected. But now let's say that we were going to draw a lower-fidelity version of our prairie dog, maybe something in black and white that we were going to sketch out with core graphics, maybe it was drawn with an Apple pencil on an iPad. That might look something like this. So how much memory is our low-fidelity prairie dog going to use? Well, it'll actually use the exact same amount of memory. Now here, that clearly is not as good of a use of memory. It's all grayscale. All of that deep color information is not even being used in this image. So hopefully we could do much better and iOS 12 introduces automatic backing store support to help make that exactly be the case. So all of your views now that implement draw Rect will have their backing stores defined by the depth of the content being drawn into them. So in this case where we're using Core Graphics to sketch out only grayscale content will actually automatically use an 8-bit per pixel backing store instead of a 64-bit per pixel one. This reduces the memory demand for that drawn view by an order of magnitude down to 275 kilobytes from 2.2 megabytes. That's a really big improvement across many different applications, both yours and ours. So Automatic Backing Stores are enabled by default for all apps built with the iOS 12 SDK. All implementations of draw Rect on UIView will have this happen automatically as well as all content that you draw with UI Graphics Image Renderer into offscreen bitmaps. Now in the case of UI Graphics Image Renderer, we don't necessarily know what you're planning on doing with the image that you get out at the end. So in cases where you actually know that the automatic behavior is not what you want, from Image Renderer, you can specify the specific backing store style that you want, for example using the new Range API to specify specifically that you want an extended-range image. Now you can learn all about this and many more techniques around UI Image in the Images and Graphics Best Practices Session later this week. So that's our second top for performance. Next, let's move on to Auto Layout. Now Auto Layout in iOS 10 has some really great improvements. The team has been working incredibly hard to optimize Auto Layout for your apps as much as possible. So you'll find that Auto Layout is now faster by default in iOS 12. We'll talk about a number of ways that's the case. But while they were profiling and optimizing Auto Layout, they also were looking across the system at many of our own apps and how they were using Auto Layout and they found a number of cases where there were some common pitfalls that different apps were falling in to. So we'll show you some of the simple best practices that you can follow in order to optimize your app layout as well. So this morning, though, let's look at how Auto Layout is faster by default in iOS 12 by looking at the asymptotic behavior of Auto Layout as we add more views in some common user scenarios. Now, we're looking at the asymptotics because we really want to look at what happens as we dramatically increase the number of views. This can really make performance issues show up quickly when we go to a really like absurdly large number of views. That just helps us see it though. The optimizations really do apply and make things faster even for small numbers of views. So let's start with a really common pattern, the simplest case really, Independent Sibling Views inside of some container. Now, these views are positioned with constraints against the container or other views but not against each other. They're independent of one another. Now in iOS 11, the cost of these independent siblings, as you continue to add more, grew linearly. So that's actually pretty great. That's exactly what you'd expect for a situation like this. Basically, what we're saying is that there's a fixed constant cost for each additional new view that you added into your hierarchy that was positioned independent of those other views. Now because that was already growing with the exponentials that we expected, that remains the case in iOS 12; however, the team has worked really hard to reduce that constant cost to make additional views as cheap to add as possible. So let's look at a more interesting example next. And in this case, we're going to take a look at Dependent Siblings. So this is the case where you have multiple child views and they're now, they have constraints between each other. So their layout is dependent on one another. Now, unfortunately, in iOS 11, you'll find that the asymptotics here weren't quite as nice. In fact, it was growing exponentially. So the more views that you added, the surprisingly larger cost you would find as you added additional ones. Now, the team worked really hard to identify the source of this exponential growth and fixed the algorithm so that is no longer the case. So on iOS 12, these now grow linearly. And, of course, the team's also been working to reduce those constant costs for these cases as well. Now in addition to dependent siblings, there's another common type of layout that you'll find and that's Nested Views, when one view is inside of another and there's constraints out to those outer ones. This is obviously also a pretty common pattern in your apps and, unfortunately, here as well in iOS 11, we found some exponential growth. And again, great news. The team has also made this linear in iOS 12 as well. So there's a number of really great improvements across Auto Layout in iOS 12 and you'll see these improvements in your apps as soon as you start running them on iOS 12 yourselves. To learn more about all of that, get a better sense, develop a good sense of how to get some gut feel for the performance of different layouts and here are some of these topics I was talking about that are common pitfalls. Definitely check out the High Performance Auto Layout talk later this week. So for our final framework update topic, let's turn to Swiftification. As you heard, iOS 12 introduces Swift 4.2. And for Swift 4.2 we really wanted to make sure that UIKit had a really great feel when used in Swift across your app, especially where it interacted with other Swift Standard Library or places that Swift had common patterns. So, we audited all of UIKit and made sure that everything feels like it fits really naturally. Even better, we made sure that all of the changes that we made to UIKit are all automatically migratable so there's no additional work that you should have to do in order to get these updates. Now, these updates fall into really three categories that we'll talk about this morning, although there's actually a ton of improvements and consistency improvements that you'll find as you look at the SDK. But today we'll talk about nesting of types, constants, and functions. So let's first look at nesting types. Now in Swift 4, there were a number of types that were in the global name space, things like UI Application State. For types like this that have a really strong use along with another class, we've now nested them within this class. So we looked at all the enumerations and other global types of this sort and now have moved them to be child types of the relevant class. So this becomes UIApplication.State. This sends a much stronger message about the relationship between these two and makes them easier to find as well. Now in comes cases this can also help improve understandability and remove some confusion. So in this case, let's look at UI Tab Bar Item Positioning. Now do you think that's UITabBarItemPositioning or UITabBarItem Positioning? It could actually be either. Those are both classes. And in Swift 42, it is now perfectly clear that it is, in fact, UITabBar ItemPositioning. So in addition to nested types, we've also nested a bunch of constants. So if we look here at Swift 4, we had NS notifications were all in the global NSNotification.Name namespace and their associated user info keys were actually just global constants that were floating out there. So for consistency with AppKit and to make it easier to find and associate these types together, they've now all been nested under the class that they're used with. So something like did Change Status Bar Orientation is now under UI Application did Change Status Bar Orientation Notification and its user info key moved along with it so that they're all co-located. Now we've also audited all of the other global constants all throughout UIKit and nested all of them in appropriate places. So things like UI Float Range Zero and UI Float Range Infinite have not just become properties on UI Float Range so they're both easy to find and easier to use. In places to take a UI Float Range, you can now just type .zero or .infinite and, in fact, because they're now properties, Xcode can suggest them as auto-completions for you in places where they make sense. Now in addition to constants, we've also audited all of our global functions. So things like UI Edge Inserts and UI Image had some global functions for operating on different types. Now in Swift 4.2, these had become methods on the appropriate type. So it's now really easy to inset Rect or get png Data from an image in a really natural Swift feeling way. Now, here was one other big class of functions that I want to mention this morning and that was all of these string conversion functions for all of the many types in UIKit, CGPoint, CGRect, CGSize, CGVector, all of them, there's quite a lot, both to and from strings. Now, when we looked at these and tried to decide where they should go, we realized that they actually have two different use cases. One is for encoding and decoding. But the other is that they're commonly used to print things when you're just trying to get some debug descriptions. And those are two very different uses but, in fact, Swift has first-class support for both of those cases. And so we've made sure that all of these types will work really great with Swift's built-in support for both. So in Swift 4.2, all of these types will conform to Codable so that you can very easily do things such as encode and decode JSON for all of these different types. Of course, debug printing in Swift is actually even easier than in Objective-C because you don't have to do any additional conversion. The built-in introspection of the types can allow you to print them directly. So in Swift 4.2, you just actually pass these types directly to your print functions if you want to print them out for debug purposes or log them. And then finally, you may already have some existing code that was using the behavior of the old string conversion functions and need a compatible functionality going forward. And so for that we've actually just renamed all of these and moved them to be properties on NSCoder. This really helps to emphasize the fact that the intent of these methods was to be used for encoding and decoding, so it's a pretty natural fit for them to go over there. So these are just a few of the consistency improvements that you'll find across the iOS 12 SDK for Swift 4.2 but you'll find many more as well. Now speaking of encoding and decoding, NS Secure Coding, in iOS 12 there are now new secure by default encoding and decoding APIs. Adopting NS Secure Coding for all of your encoding needs on NS Keyed Archiver is really key to ensuring that you're protecting your customers from both malicious and corrupted data. You'll also find that the older insecure APIs are not deprecated. So you can learn all about that and get much more detail on it in the Data You Can Trust Session on Thursday at 9:00 a.m. And that's framework updates. Next, let's turn our attention to some enhancements to a number of existing APIs and we'll start with notifications. Notifications has a number of really great improvements in iOS 12 but we're going to focus on just three this morning. Interaction within custom notifications, grouping of notifications, and access to your notification settings within your own apps. So let's start with interaction. Now custom notifications have for a while now allowed you to define a predefined set of actions for those notifications. In iOS 12, this set of actions is now no longer static. It can be defined programmatically and you can change them at runtime. In addition to these actions, and even better than that, the notifications themselves can now be made interactive. So for example, here, you can see Messages is now allowing you to reply quickly to a message inline directly in that notification. Now in addition to interaction, iOS 12 now includes grouping of notifications by default and the default behavior will be to group all the notifications for a particular app into a single group. But, of course, your app may have custom needs to have more granular groupings so something like iMessage will group all of the messages from a particular conversation together and separate from all the rest of the notifications for that app. Now you can adopt this in your app as well by just tagging your notifications with a particular thread identifier and then all the notifications for that threat identifier will appear in a single group. Now the UI updates for notifications in iOS 12 also include some new ability for users to customize the delivery behavior of their notifications. But, of course, your apps may also include some existing, more granular controls for notification management within your apps as well. And iOS 12 introduces a new API that makes it easy for your customers to get directly deep linked into your notification settings UI exactly when they're looking for those more granular controls. So you can learn more about all of these notification things in What's New in User Notifications and Using Grouped Notifications later this week. That's notifications. Next, let's talk about messages. Now messages in iOS 12 includes some really new and exciting features in the camera. And you can bring all of your iMessage stickers directly into the camera as well. If you're using the Xcode sticker template, this will work automatically by default with no additional work on your behalf. But if you're building a more custom sticker experience using the MS Messages App View Controller, some small amount of adoption is required. Now there's a new MS Messages Supported Presentation Contexts API that you can add into your user info plist and then specify that you want to appear both in the messages context and the media context. Once you've done that, your apps will appear both in the App Strip and also within the camera. Now if at runtime you need to figure out which context you're in so that for example you want to customize the display of your stickers a little bit, there's a new API for that as well. By checking the presentation context, you can quickly see whether you're in messages or in the camera. Now in addition to these features, iOS 12 also brings a new access for interaction to your messages apps. In compact mode, previously swiping horizontally down in your messages app would switch between apps. In iOS 12, these horizontal swipes and interactions that move horizontally are now available for us by your apps directly so they'll interact with your apps rather than switching to a different app. And that's Messages. Next, let's talk about automatic passwords and security code autofill. Now iOS 11 introduced automatic passwords or password entry into your apps. And in iOS 12, we're taking these a whole step further. But let's go back to the beginning for a minute and talk about the entire experience. So for users that have passwords stored in iCloud Keychain, since iOS 11 it's now been possible to have those automatically get populated into your app in your login flows. Now in iOS 12, these passwords can also be stored into iCloud Keychain from your apps both from your login window flows and also from your password change request UIs. As soon as a user logs in, they'll be prompted to save the password to iCloud Keychain. Now even better, iOS 12 can automatically generate passwords in your new account flows and in your password change flows. Adopting this is really easy. You just make sure that you've tagged your password fields with either the password text content type, if it's a login field, or the new password text content type if it's either a new account or password change field. If your services have a specific requirement on passwords for example if they got required or disallowed characters or if they have other requirements such as maximum number of consecutive repeated characters, you can specify these requirements as well to make sure that the automatically generated passwords are fully compatible with all of your requirements. Now, the final bit of friction during some of your login experiences is when you have to take that two-factor authentication code, get it out of a text message and into your apps. IOS 12 makes this really easy by automatically identifying these notifications, noting the security code in them, and suggesting it right in the Quick Type candidate bar so that it's really easy to get it right into your app. Now the only thing you have to do to make sure this works in your app is to be sure that you're using standard iOS text interaction APIs in order to accept these passcodes. With all of these new features, iOS 12 is enabling a much more secure future with unique, strong passwords used for every service that you never have to memorize or type ever again. You can learn all about this in the Automatic Strong Passwords and Security Code Autofill Session later this week. Now, our final API enhancement topic is actually a bit of a review but now with a little bit more context. So in iOS 11, we introduced Safe Area insets. Safe Area insets are a really great way to make sure that your content is avoiding any overlap from other parts of the user interface such as bars on the top and bottom of the screen. This is really great on iPhones where bars are pretty straightforward, but it's really powerful as well. Safe Area insets give you that safe area coordinate in the local coordinate space of any view in your application, so it scales even the much more complex interfaces, things like iPad Split View which has different height bars on the master and detail side of the Split View. The Safe Area insets in any view underneath these bars will be appropriate for the amount that they're overlapped by the bar on their side of the split. So this is really great on devices with rectangular screens but it also is really powerful on devices that have non-rectangular screens like iPhone X. Now, you can see here we've got our larger bars at the top and bottom than we have on devices that have home buttons. And the Safe Area insets obviously have just grown to accommodate that larger size. Now, unique to iPhone X is that there are Safe Area insets even in cases where no bars are present and this extends the landscape mode too where it can really help you make sure that you've got a rectangular area at all times that's safe to display content and will never be clipped. So I want to thank you all for those of you who have adopted Safe Area insets and updated your apps for iPhone X. It's been a really great experience over the last year and I'm sure most of you have already done that. If you haven't, now is a really great time to do so. Your customers will always prefer apps that are being kept up to date and support for iPhone X is a really visible indicator of that being the case. So if you haven't, definitely go do it now. And to help make sure you have all the information necessary to do that, you can check out the UIKit Apps for Every Size and Shape Session later this week which will tell you both all about Safe Area insets and all of the other related inset APIs all throughout UIKit making it easy to make sure you have apps that scale to every shape and size. So that's our framework updates and our API enhancements. Next, let's talk about Siri Shortcuts. So Siri Shortcuts is an exciting new API in iOS 12. Siri Shortcuts makes it easy to get common actions out of your app and make them accessible via Siri. Now, Siri Shortcuts can be suggested proactively right on the coversheet making it easy to access actions that you would want to access at the exact time and place that you want to access them. Even better, they can also be suggested right on the Siri watch face on Apple Watch. Now not only are Siri actions suggested proactively but they can also be set up to be executed using a custom voice phrase. Now, adding Siri Action Support to your apps is really easy. You can use two APIs. There's NS User Activity, which you may already be using for support for Handoff and Spotlight integration, and there's also support for Siri Intents for more complex scenarios where you have more custom interactions. So let's look first at NS User Activity. Now as I mentioned, NS User Activity is a common API with Handoff and Spotlight and this could be a really great API to use if your Siri Shortcuts should get your customers back to a specific place in your app, for example loading a particular message or document, the same as you would do if you were trying to hand that off to another device. If you're already doing this, it's really easy to add support for Siri Shortcuts. You just set Eligible for Prediction to true. And if you're not, this may still be a great way if your shortcut fits into one of these categories. Now, if your app has other more custom needs or if you just want a lot more control, you can adopt the Siri Kit Intents API. Now Siri Kit Intents provides a number of predefined intents that you can easily adopt on your own. These are the same as the Siri Kit Intents in previous years. Now if your apps have more custom behaviors, though, you can now in iOS 12 define your own custom intents. Custom intents can be really flexible and they do anything that you would want. In this case here, I've created one to help me create my WWDC slides next year. Now the categories that you could put your intents in to are pretty broad already. So here I've used the Create category. But if your intents are even more generic than that, there are even more general ones available such as General Do, Run, and Go options. Now once you've created your intent, you also want to make it really easy for your customers to create these custom shortcuts to get to them. And so there's now an API from right within your app you can allow customers to create a custom voice shortcut. So here I've got a button that will just bring up a new panel enabling me to create a new shortcut right within my app as soon as I finished an operation. So if you're doing something like ordering a coffee in the morning and you notice it's something that might be done again, this is a great opportunity to offer to create a Siri Shortcut to do that next time. Now even better, you can also combine these shortcuts together using the new Shortcuts app available from the app store. So you can learn all about this and much more in the Introduction to Siri Shortcuts, Building for Voice Siri Shortcuts, and Siri Shortcuts on the Siri Watch Face Sessions later this week. So we've talked a lot this morning about what's new in iOS 12, but there are also a number of great sessions that are worth mentioning that aren't necessarily about what's new. So if you're new to creating apps for iOS, there's a really great session you should check out called I have This Idea For an App. So definitely check that out. And if you already have an app, and are just looking to add more polish, there's a couple other great sessions as well, A Tour of UI Collection View and Adding Delight to Your IOS App. So thanks so much for coming out this morning to hear what's new. Look forward to seeing you in the labs and I hope you have a great week. Thanks.  Hi everyone. My name is Jeremy and I'm an engineer on the tvOS team. Today I'm really excited to speak with you about what's new in TVMLKit that we've added in tvOS 12. For the uninitiated, TVMLKit is Apple's high level framework for quickly building content based applications on tvOS. Out of the box, it is compliant with our human interface guidelines. So your applications look beautiful and feel correct. It uses JavaScript to drive application logic and a special XML based markup language which you define that renders into user interfaces on the screen. In fact, some of the apps you're familiar with and use today are built using TVMLKit. Along with that, there are thousands more in the app store that you write. So let's get started with the enhancements we've made to TVMLKit, beginning with three things we want to talk about today. First, we have some enhancements to Web Inspector that allow you to further debug your applications and introspect them at a deeper level. Since last year, we've added a bunch of features and enhancements to the framework and I'm going to discuss three of them with you today. We're going to talk about new Web Inspector enhancements for better debugging, new data binding constructs that are added to make it more powerful, and finally, a new way to customize the playback experience on TVMLKit. Let's get started with Web Inspector. tvOS 11 introduced more support for Web Inspector that allow better introspection into your TVMLKit apps. Since then, we've enhanced this support even further. In tvOS 11.3, in addition to showing install event listeners on an element, you can now temporarily disable them. This vastly helps with debugging as you can now toggle the event handlers off and on at whim. In the network tab, joining at [inaudible] in document resources are image resources. This allows you to see the images that are being loaded as well as information about how long it took and where time was spent. If you are interested in seeing the actual image that comes off the wire, this is now also an option you have at your disposal. However, please note that this only works after you've connected the Web Inspector. It does not show what has already been loaded. Finally, my most favorite feature of all, the inspect button. Clicking this will show you the approximately element that represents the view that's currently in focus. If your elementary is collapsed, Web Inspector will expand it to the exact element and highlight it. To use Web Inspector, either download the latest version of macOS and install it, or use Safari technology preview. More information about how to use Web Inspector can be seen in our talk given last year using Web Inspector with tvOS apps. With that, let's talk about data binding. And before we jump into the new features of data binding itself, let me give you an overview of what it is. Data binding helps you easily transform your data to user interface elements through expressions in your template markup. This is really good because it allows you to decouple your data and your lot -- and your layout logic as well as your application logic itself. Because of this, data bound templates can reduce the amount of JavaScript code you actually have to write to convert your data to TVML documents, since the framework does this on your behalf. In fact, by authoring your documents on your behalf, it is able to do it in the most performing way so you don't have to worry about the right APIs to use. Let's look at a practical example to explain this. Say you want to generate a banner with a title and a description. And this is how you would typically do it without data binding itself. First, you will fetch the relevant data that's supposed to be shown to the user in this case, a title and a description. And upon fetching, you pass it onto a part of the JavaScript code you've already written that processes this data and generates the actual final document itself. Now, with data bindings, you can take out JavaScript processing step and provide binding specifications in your template itself, and TVMLKit will populate the final document on your behalf as directed. Effectively, your application just needs to worry about fetching and massaging data, and not worry about dom [phonetic] editing at all. In a nutshell, that's how data binding works and helps reduce the amount of code you write. Last year, we introduced the concept of data binding with some core concepts that include binding of attributes of an element, binding the text content of an element, and of course, binding items of a section in either a shelf, grid, or list. Let's quickly recap on these concepts with another example. The corresponding data bound template representation of this image element would contain a binding expression with the 'at' symbol, followed by the attribute name and the property you want to bind it to. Next, let's seek the example of generating text content of an element. Like in this case, a title element that has corresponding data of what it should actually be filled with. The data bound template representation for this title element will contain a text content binding and the property it maps to. Finally let's talk a little bit about items binding. It is a slightly different binding and involves a group of data pieces you want to show. It also only works with sections on a shelf, list, or grid. In this example, we have some data with the tree list items as an array. And the final representation should be a section of tree list item lockups. The corresponding data bound template for this section will contain two things; an item's binding and the property it maps to, and also a prototype to convert each data object contained in the array. In this case, it would be a data bound template representation of a list item lockup. So that's the tree binding constructs we've introduced in tvOS 11. For more information, please look at our talk, Advances in TVMLKit from last year's WWDC. This year, we've extended that vocabulary. To begin, we've added a more generic way of binding child elements of an element using the children binding. And to help better organize your DOM, we've added a couple of special elements, namely fragment and rules. We'll go into this in detail, starting with children binding. Children binding is a more generic form of items binding. Items binding is optimized for use cases where you have a section of a shelf, grid, or list in order to work efficiently with large data sets. They can be used outside these elements. For everything else, use children's binding. And it's because of a very simple reason. It works by generating children of a target element itself. And it behaves the same way as items binding does. You require the use of prototypes to define the elements that data should be converted into and this will be used as a template when generating the final DOM itself. Let's take an example to explain how this works. I am going back to the same example of tree items in an array. We have this very same piece of data but in this particular situation, we have three different menu items. And this would be used in a menu bar. This is the final representation we expect to see. Basically a menu bar tree, menu bar items. And this is a very simple way to specify the template for it. As you can see, it's similar to how you would do items binding. It has prototypes that's used to map data to elements and it has a binding expression. The only difference is that the element is expressed on [inaudible] section. Children binding works with any element. Now this works really well as long as you want to generate all the child elements itself. There might be cases where you want to only generate some of those children. Take, for example, an auto streaming app with a now playing menu item. The now playing menu item is a special menu item that should always be in the menu bar but would only appear if background audio is currently playing. However in this case, we still want the menu bar items to be data bound. In order for this work, we need to compartmentalize the data driven and non-data driven parts of the menu bar. And that's where fragments comes into play. So what are fragments? Fragments are invisible elements that the renderer doesn't see and it helps you organize your DOM. But what's special about fragments are that its children are visible. And because it is an element and children binding works with any element, the fragments itself works with children binding. So let's go back to our data in the final form we want and we have something like this. You have the menu bar items. Nice to encapsulate in a fragment and this is great because it allows us to do children binding like so. Now all we have done is moved the data around [inaudible] into fragment while keeping the menu bar intact with the now playing menu item. And because the renderer only sees the children of the fragment, this still renders as a properly formed menu bar. On the very subject of data itself, that you can use to map to user interface elements, this is another likely scenario where you have data that some do not change, and some change all the time. For example in this particular piece of data itself, it is for a video that has a poster image, a title, and a playback progress. In some situations, we want to show different user interfaces based on that information itself. In the case where playback progress hasn't started, it's naturally zero and we are interested in showing the poster image and the title for the video itself. But when we start watching the video, progress will naturally be greater than zero. For that, we want to show the same thing that never change, the poster image and its title. However to make things obvious this video's being played back, we want to show a progress bar that's filled through the playback progress' percentage itself. In the very same vein, we have now two use cases where data is different and we also want to show two different looks of what it is. In the first case, we -- all we have is an image and a title in the lock up. And in the second case, we have the addition of an overlay and a progress bar. Typically you would have application logic that outputs different x amount based on that data itself but with rules, you can have a single statically defined template that would give you either representations of the final document. So what are the rules? Well, rules used data states to redefine your final document which, in turns, refines your UI. It is an invisible element. So the renderer doesn't see it but it affects how the document is authored. Any operations within those rules happen on sibling elements that the rules are a part of. And the best way to show this to you is in another example of how this can be setup. So let's look at the rules that we would use to structure the very prototype that we want -- that we've shown as an example. We will start by defining the prototype as the lowest common denominator of what our user interface should look like. And in this case, we have an image and a title. However, you would notice that we also have a place holder for the progress bar. Placeholders are also special elements that are invisible to the renderer and in this case, would be used by the rules as a target for replacement when data states match. Now let's fill in our rules. A group of rules that act on sibling elements are encapsulated in the rules tag. Individual rules that match a data state are encapsulated as specialized elements, and specialized elements become active when it matches certain data states. And the way that that's matched is based on a query in the state attribute itself. When data state matches, the children of the special element -- specialized element are the list of operations that act on the sibling elements of rules. In this case, we want the placeholder to be replaced with an overlay element and its children. TVMLKit matches the elements to be replaced by looking at the tag attribute on any element. It effectively does replacement by first matching on that tag and then comparing the element name. If the element name is different, it would replace that element hole and in this case, placeholder becomes an overlay. However, if the element name matches, whatever that's new would be appended to what's already existing. And so we have a single template with rules that will generate two different output based on the state of the data that's provided. Effectively you can move your application logic to deal with how things are displayed into a statically specific template that exists within the context of elements that's transformed into user interfaces. Now let's switch gears and talk about playback in TVMLKit. TVMLKit has long provided extension points where you need more customization of your user interfaces, be it individual views or even whole templates. In tvOS 12, we are extending this to our playback pipeline, giving you control over playback as well as its associated user experience. This experience works with all the different playback styles we have, whether it is embedded or in a full screen. You do this by providing a TVPlayer object and its associated user interface as a UIViewController. These have close analogues to our JavaScript APIs. So that would be less confusion in talking to your JavaScript developers. And finally there's a limited JavaScript Bridge that's exposed. It allows communications between your native code and JavaScript itself. Let's talk about TVPlayer and that's the base where you can get your customized playback experience working. TVPlayer is a public AVPlayer adaptor to the Playback Pipeline. What this means is that TVPlayer effectively translates regular AVPlayer callbacks into what JavaScript expects. TVPlayer is also the object that you can use to dispatch custom events to JavaScript and by default, it already handles everything that AVPlayer has as playback events. So anything extra is on you to dispatch. Changes that JavaScript makes to the player are KVO observable. So you know when things are changed by your JavaScript developers. Finally the TVPlayer object plays media in a sequential fashion from the very first [inaudible] item all the way to the very last in its playlist. Whenever a player is needed, TVApplicationControllerDelegate will ask for a TVPlayer and you will have to return an instance in order to participate in the Playback Pipeline. The next step of the Playback Pipeline is to actually show Playback on the screen itself in the form of a user interface. This can happen anytime whether it's full screen playback or embedded playback. It is entirely up to you to create your own user interface. When TVMLKit needs a user interfact, the TVInterface [inaudible] will ask its delegate for a view controller. It will also pass it a reference to TVPlayer that is responsible for playing media in that view. With everything, there are several caveats to using TVPlayer and its associated user interface. The very first thing is you should handle any 'should' events yourself. These are usually tied to interstitials and since that's largely user interface, this should be handled by the native code that you write. If you use FairPlay encryption for your video playback, you need to use AVContentKeySession for secure key loading. For more information about AVContentKeySession, look at our talks from last year, Advances in HTTP Live Streaming, and a talk from this year about AVContentKeySession Best Practices. Finally, if your JavaScript developers use overlay and interactive overlays, this will not work out of the box. They are user interfaces and because you are building your own, you will have to handle it yourself. In summary, the changes we've made to TVMLKit and tvOS 12 are the following two: One is the data binding is now even more powerful, enabling you to build data driven templates in any form. We highly encourage you to check this out. And finally, if you've been long waiting to customize the playback experience itself, you can do it today by implementing your own native playback experience. For more information about this talk, please look at the following url. And thank you for attending WWDC 2018. Thank you. My name is David Duncan. I'll be joined on stage by my colleague Tyler Fox and Russell Ladd. And we're here to talk to you about building apps for every size and shape. So I don't know how many of you were here back when we released iOS2 on the original iPhone. But you had one screen size to work with then. But today, we've got iPhones. We've got iPads. Multiple sizes. We've got the iPhone 10, the brilliant new screen, and a new shape for you to build your apps to. For this talk we're going to be using our Bagel Times app as an example of a design that adapts beautifully to iPhone 10 and iPhone 8. And so these are the three things we're going to be talking to you about today. I'll be speaking about safe area and layout margins. How you can use them in your application to adapt to various screen sizes and shapes. Then Tyler will come up and talk to you about scroll views. How they interact with safe area, layout margins, and other UIKit technologies. And finally, Russell will get up and talk to you about using all UIKit tools to build adaptive applications. And so with that, let's get to talking about safe area and layout margins. So what is a safe area, and what does it look like on the devices that you've come to know and love? Well, on an iPhone 8 with a rectangular screen, the entire screen is a safe area. All of it's there, no overlaps on your content. On iPhone 10, there's a little extra space that's taken up on the top and bottom by the hardware that will be taken away from the safe area for your application. And in landscape, you've got something kind of similar where we've given you a symmetric layout with space on the bottom for the home indicator for you to lay your content in safely. But what other devices might have a safe area that's not the entire screen? Well, Apple TV actually might give you the situation because certain TVs will have a screen that actually extends past what's visible to the user. And those will represent their information based on something called overscan compensation. And that overscan will be represented as a safe area in any apps that you build for Apple TV. So now that we've seen a few examples of what the safe area looks like at the screen of a device, how does that get to your views? And how do you use that in your own applications to build adaptive applications? So let's just pick up an arbitrary view. Whole thing. There are four insets on the top, bottom, and left and right that represent areas that might have some kind of overlay or other thing that might occlude your content if you placed it there. You can access this on your UI views using the safe area insets property. A UI edge inset that has those four values. Now if you're doing layout with Auto Layout, you actually might want to just see the entire rec that's safe. And you can get that by looking at the safe area layout guide. Which is a UI layout guide that represents that information. It has layout anchors for you to do Auto Layout with. And a layout frame if you just want to see the actual rectangle. So now that we've seen how that's represented on our view, let's take a look at how it gets from one view to the next. So we'll just take away the text and add a subview. And this is going to cover most of the bottom edge of that view. Now, how does the safe area get calculated for this? Well, as we saw, that subview that we just added, it intrudes on the unsafe parts of the view on the left, right, and bottom of its superview. And so those are the values that you will see in the UI edge insets represented by the safe area insets property on that subview. And similarly, you'll see a safe area layout guide whose layout frame looks kind of like this. Now once you've seen this, and you've got a view, you might want to actually add additional insets for your UI. You might decide that you want to add controls through your view controllers that will then add to the safe area or subtract from the safe area for your subviews. So we'll go ahead and add another subview here. And along with that, it will have a view controller because a view controller is where we actually have the property that allows you to add additional insets. And that property is called additional safe area insets. We're going to go ahead and inherit the insets we got from our parent. Add those additional safe area insets to this view. And finally construct the final safe area that let-- , layout guide. What other things might you want to know about how safe area behaves in your application? Well, let's take a look at another example. Here we've got one view safely inside of the safe area of its parent. So that view safe area, of course, encapsulates the entire region of that view. And we'll move it closer to one edge and as you might expect, we're not going to gain a safe area because we haven't left the safe area of our parent yet. We'll move a little bit further out, and you'll see that the bottom inset on that safe area now grows a little bit. It takes up a part where it overlaps, where it extends outside of the safe area of its parent. And as we move closer to the edge there, it continues to extend. Now what might you expect to happen if we move this view further off the bottom edge of its parent? Well, how about that? The safe area did not grow any further as the view moved outside of its parent. No matter how far outside of the parent that view might go. And you might be asking yourself, "Why would that, why would we do that? What's the purpose here?" And the answer is animation. In this particular case, we're moving this subview out of its parent. We wouldn't want the content to stay inside of the safe area because then it wouldn't move with its own parent. And as an example, we can see this app. Where we're going to pull up a view on the bottom. If that view had been laid out against the safe area of the parent, or its parent, and that parent's safe area would extend the farther it grew off the bottom of the screen, then this area would have stayed on screen during the entire transition, which would have meant that you would have not seen the background come up with it. And so that's why the safe area never grows larger on any dimension than what its parent provides. So to summarize that safe area part, let's took a look at how you can interact with it. Again, we've already mentioned the safe area insets property and the safe area layout guide property. But if your view needs to react to when safe area insets change, then you can override the safe area insets did change method. Typical thing you might do is just call set needs layout. But if there's any additional logic you need to run, that's okay too. If your view controller needs to respond to its view safe area layout changing, then you can override view safe area insets did change on your view controller. And finally, if you're doing your interface in IB, you can use that safe area property that's shown in the view list to build your constraints against the safe area. And so a safe area is explained. Let's talk a little bit about layout margins. So layout margins are padding. They're a property that, in general, you have full control over and allows you to specify a space from the edges of the view. Just like with safe areas, they're represented by a UI edge insets property, this time called layout margins. Now last year, we also added directional layout margins. These differ from layout margins in that layout margins use a UI Edge Insets value while directional layout margins uses an NS directional edge insets. And the primary difference between those two structures is that directional insets use leading and trailing instead of left and right, which makes it really easy for you to create layout margins that adapt to RTL layouts. So you don't have to do the swapping between left and right when you use directional layout margins. Now, just as with safe area, we provide a margins guide as well called a layout margins guide that you can use with Auto Layout to layout content against that margin. And then we go ahead and put our content inside of that view. Now the next question you might be asking since we're talking about layout margins and safe areas together is how do the two interact? Well let's go ahead and bring those markers back. Oh yeah, I meant to talk about how you can change those [inaudible] layout. So with safe area, by default, we build the safe area, and then we build the layout margins with respect to the safe area. And we do this for the perfectly obvious answer that by default, you probably actually want this. You want your layout margins inside of the safe area because they represent additional padding against the layout that you want to do. But we thought to ourselves, "Hey, some people may not actually want this." And so we made it very easy for you to flip this default. And so if you just change the insets layout margins from safe area property from true to false, then we'll go ahead and move layout margins back to the bounds of the view. As a [inaudible] instead of being encapsulated inside. So what else can you do with layout margins? Well, we already saw that by default, safe area margins propagate down the hierarchy. But with layout margins, they go by default. Because by default, your layout probably wants to be fairly independent with margins as opposed to safe area, which represents a concept that really is for the entire view hierarchy. But if you want to have propagation, you can flip this on view by view by changing preserve super view layout margins from false to true. And we'll go ahead and line up those margins that are smaller than the parents in order to make sure that everything lines up naturally between a child and its parent view. Now in years past, you've probably also tried to change layout margins of your view controller views. And if you look at what we do by default. That this margin that you'll see against that view. Now we additionally added a property last year called system minimum layout margins and these margins are the minimum margins that we've combined against any margins you provide now to create the final margin. So if you wanted to add to the top and bottom of this, you can do that without disturbing the left and right margins that UIKit provides. But again, you might want a little bit more control. And so there's another property called view respects system minimum layout margins. If you want your margins to be completely underneath your control, flip this to false, and we'll use your margins as you describe them, no questions asked. And so to wrap up with layout margins, the properties you have on UI View are the layout margins property, edge insets, left, right, top, bottom. Directional layout margins. Great for your RTL layouts. Top, bottom, leading, and trailing. The Layout Margins Guide that you can use with Auto Layout to do all this there. And finally, if you have logic for when the layout margins change in your view, you can override layout margins did change and do whatever logic you need to do there. In Interface Builder to create a constraint that ties against the margin, just check that constraints to margins box. And so with that, I'll bring up Tyler Fox to talk to you about scroll views. Thanks, David. Good afternoon. And as David mentioned I'd like to talk a little bit about scroll views today. So scroll views are a really key part of the iOS experience. And they show up all across the system. You have them in table views, collection views, UI text view, and of course in all of the custom application in all of your apps. And so in our app, Bagel Times that we've been working really hard on as you can tell, this is our news article screen. And everything as you can see is sort of centered around a scroll view with the article. And this is we want to you know really showcase the high-quality content our writers are putting together. Like, you know, exploring the true inspiration for, you know, Apple Park. And we want to review some of the basics about how you can use scroll views in your apps to understand how they can help you adapt your content to devices of different shapes and sizes. So to do that, we're going to walk through an example here, and we'll start with a full-screen scroll view. And on the left I'm going to show you what things look like on more or less a real device. And on the right, we're going to look at kind of things behind the scenes. Understanding what's happening in a diagram. So everything that we talk about today is going to apply in-- we're going to talk about the y axis of everything vertically. But everything also applies equally to the x axis. And we're also going to use some simple values for illustration. So in this case we'll say that our scroll view has a height of 400 points. Now inside of your scroll view, you'll have some content. And you can think of the scroll view a lot like a metaphorical picture frame. The scroll view size represents kind of the fixed frame of the picture frame. And then inside of that, the picture is your content. And the content can kind of slide around as if it's on ball bearings. Now in this case, our content is vertically scrollable. And that happens because our content has a taller height via the content size than the scroll view's height. And here I've turned off clipping on this diagram on the right so we can kind of peek behind the scenes at what's going on. So scroll views use content offset as a way to represent the current scroll position of the scroll view. Right now, we're scrolled to the very top. So our content offset is at zero, meaning our top edge of the content is aligned with the top edge of the scroll view. Now, if we go ahead and scroll the scroll view down, that will slide our content up. So let's scroll to the bottom and watch that happens here. And as you can see, the content offset increases and goes all the way to 100 in our example when we reach the bottom. That's because the top edge of the sc-- the scroll view is now 100 points below the top edge of that content area. If we set a content offset of zero on the scroll view, that will put us all the way back up to the top. And we're back where we started. So that's pretty much just the basics of scrolling a simple scroll view around. Now let's talk about a very important concept about extending the scrollable area of the scroll view. And we do that through a property called content inset. So content inset is a mechanism for you to provide insets that you can specify from the edges of the scrollable area of the scroll view to the content inside. So adding content inset increases the scrollable area and makes the scroll view able to scroll to a larger area. So let's set a content inset on the top edge here and see what happens. So here we've set a content inset of 20. And as you can see, that extends that top edge of the scroll view so that the scrollable area is now larger. Now when our content offset is sitting at zero, our content is still aligned with the top of the scroll view. But the scroll view could actually scroll even farther towards the top now. So let's scroll around like we did before and watch how things have changed. We'll scroll to the bottom and just like before, we're still here at a content offset of 100. That's because we didn't change anything on the bottom edge. And we still have a bottom content inset of zero. But if we now go ahead and scroll back all the way to the very top, we end up with a content offset that actually goes negative. That's because we are now able to scroll beyond the top edge of our content. And so we can scroll as far as the negative content inset on the top edge. Now starting in iOS 7, this content inset technique became really important and that's because with iOS 7, translucent bars became really common across the system. And the idea was that you would display your content edge to edge. And it would underlap the bars so that you could get these nice colorful blurs with your content through the bars, right? Tool bars, navigation bars, and so forth. So because this was so common, we wanted a way to help automatically set the content inset on your scroll views to make this easier in your apps. And to do this, we had a property on UI View Controller. And this was called automatically adjust scroll view insets. And the intent, intent again was to automatically set the content inset on your scroll view whenever it had overlapping bars coming from a navigation bar or a tool bar. And that was whenever your scroll view was inside of a view controller. Which itself was contained inside of a navigation controller. Now this worked pretty well for some common cases. But if your app had a more custom or advanced usage of UI scroll view, sometimes having your code setting content inset and also UIKit setting that same content inset property could create some challenges. And so starting in iOS 11, we have a much more explicit and powerful way for you to get the same automatic behavior. And the mechanism for doing this is a new property on UI scroll view which we'll talk about now called adjusted content inset. So starting in iOS 11, we introduced a new property on UI scroll view. It's read only. It's adjust content inset. And this basically describes how the scroll view is actually going to behave. Well you're probably asking what's the difference between this adjusted content inset and the content inset that we just talked about? Great question. Here's how it breaks down. Adjusted content inset is equal to the content inset that your app provides plus any automatic insets coming from the system or UIKit. And so because we now separate these two, it's a lot easier to reason about what's happening. Of course you might be wondering where and when would I get a system inset on my scroll view. Well, one most common case will be safe area insets. Let's look at how this works. So we're back to our basic diagram here. But we're going to bring some safe area insets into the mix. So if we go ahead and start out right now we have no safe area insets on any edge. But we'll go ahead and add some safe area insets to the top edge of our scroll view. And what you'll see is that by default, the scroll view is automatically absorbing those safe area insets on its top edge into its adjusted content insets. And that's because our scroll view's vertically scrollable. And so what this does is this automatically increases the scrollable area and makes it so content can scroll out from underneath anything covering up the top edge like a bar or even the edge of the display. So take a look at an example here where we have the same scroll view on two different devices. iPhone 10 on the left, iPhone 8 on the right. And you can see this is a real example, the top safe area inset is larger on the iPhone 10 because of the larger status bar height and the sensor housing. And as a result, the scroll view has a larger top inset-- that's the gray region that's shaded at the top here. So this is one way that scroll view helps automatically adapt to any device that it's running on. Now let's go back to our diagram and talk about something else. When we have a scroll view like this where we have top safe area insets, let's go ahead and add a subview to our scroll view. And we'll put this right in the content area. And so right now this-- this subview is sitting fully inside of the safe area in the scroll view. But what if we start to scroll the scroll view down which will move that content up? Just like this so that part of that subview is now sitting outside of the scroll view safe area? Based on the safe area inset propagation that David just talked to you about, you might be thinking, okay, that means the subview is going to start seeing its own top edge safe area inset. That's actually not what happens. The reason why is because when a scroll view absorbs safe area insets into its adjusted content inset, it will no longer propagate those same insets down to its subviews on that same edge. This is really important. Scroll views use scrolling to move content around and move it out into the safe area. And if a scroll view were to also propagate safe area insets that it was using to extend its scrollable area, it would almost be like double accounting for those same insets in two different places. And so as a result, the subviews on the scrollable axis are kind of completely unaware that there are safe area insets on the edges that the scroll view is absorbing them into its adjusted content inset. So now that we understand how scroll views work together with safe areas, let's cover the options that we've exposed to let you control this behavior. The mechanism that you have to do this is a property on UI scroll view called content inset adjusted behavior. And it also is available on interface builder as you can see here. Now the default value for this is automatic. And most of the time, if not all the time, you're really going to want to leave this at its default value. But we want to walk through the options so that you understand what they do and know that they're available. And so you can make the right choice in your apps. So we'll start with the first one. The first one is the always behavior. This is pretty straightforward as you might expect. Scroll view is going to always incorporate any system inset like safe area insets into its adjusted content inset on any edge. And this works fine in our particular example here. We only have a top and bottom safe area inset on our scroll view. And so it will incorporate those and the content gets to move out from underneath the bars. We don't have any horizontal insets on the left or right. So no problems. But be careful with this one because if you have something like let's say a table view on the iPhone 10 in landscape, there are left and right safe area insets. Using this behavior the table view is going to incorporate those into its adjusted content inset, which increases the scrollable area, which is going to make a table view horizontally scrollable. You're not going to want that behavior. That's why we have the next behavior which is scrollable axes. For this one, the scroll view is going to independently consider things on the vertical axis and the horizontal axis. For each of those, if the content size exceeds the width or the height as appropriate, or if you set the always bounce horizontal or always bounce vertical properties to true, then the scroll view considers that axis scrollable. And it will go ahead and incorporate in the system inset into its adjusted content inset. So in this example right behind me we have a long article that's scrollable. And so we're getting those automatic insets incorporated. But what if we had a shorter article? Let's take a look. Okay. Here's a shorter article. What's going on here? Let's take a look under the nav bar and see what's going on. Ah. Looks like we've lost our system inset because it's not scrollable anymore. So our title's stuck all the way up underneath the status bar. So let's put back the nav bar and talk about how we fix this. Well, one way could be you could set always bounce vertical on this scroll view if that's the behavior you want. That will make the scroll view always vertically scrollable. Or we'll get to our next behavior, which is automatic. And so automatic works basically the same as scrollable axes, which we just talked about. But it has one additional behavior as part of it that is when the scroll view is inside of a navigation controller, the scroll view will go ahead and adjust its top and bottom content inset even if it's not vertically scrollable to account for top and bottom bars. So, even in this case where we have a very short article, it still means that we're getting the right insets. And this is generally the behavior you're going to want. And that's why we have it as the default. Just one quick heads up though. If you are setting the deprecated automatically adjust scroll view inset property to false, that will disable this behavior. And so it's going to behave basically like scrollable axes. Alright, that brings us to our last behavior. Never. Now with this one, as you expect, that means the scroll view is never going to adjust its content inset. However, that has some side effects. For one, that means that the scroll view will end up propagating safe area insets on all of its edges just like a regular view. And as we talked about before, that might end up giving you some behavior you don't really want. For example, if you recall your layout margins are relative to the safe area, which means your layout margins might end up increasing or changing as a result of this. This is also going to disable some very helpful automatic behaviors that scroll view provides like automatic scroll indicator insets. And so if you do a search online and you see a favorite, you know, question and answer website suggesting that you set your scroll view's content inset adjustment behavior to never, consider instead using additional safe area insets to increase the safe area insets. If your goal is to try to express to the system that you've added let's say a toolbar or some sort of other overlay. Or you could consider just modifying the content inset directly, the property that we talked about in the very beginning. That's for you to control. And you can use that to add or subtract from the effective adjusted content inset that the scroll view will see. So with that, I'd like to hand it over to Russell to tell you all about how to put this altogether. Thanks, Tyler. Now we've introduced many adaptive APIs to help your apps over the years adapt to different environments. And safe area is really just the newest of these. So I'm going to review some of these concepts and also talk about how they work with safe area. So let's go to the first screen of our application. And we've got a pretty standard setup here with a tab bar controller, continuing navigation controller, containing our content view controller. Now note that the views of all three of these view controllers are full screen. This is what enables the tab bar to extend underneath the home indicator, the navigation bar to extend underneath the status bar, and the content to extend and scroll underneath all of it. But we need to prevent these elements from overlapping. So let's see how safe area allows these components to do that. So the safe area insets start by flowing through the tab bar controller, which only receives insets on the home indicator and status bar because that's all that it sees. Since the navigation controller is inside of that, it also receives a safe area inside in the bottom that accounts for the tab bar. And the content view controller inside of both received safe area insets that account for both bars. Now what does the story look like in landscape? Similar. There are safe area insets in the top and bottom. But there are also insets on the left and right that account for the size of the screen. And those are propagated all the way down from the screen through the view controller hierarchy. Now I want to stop here and use this example to make a point about how you should think about using safe area when you implement your own views. So this custom view shouldn't know that it's running on an iPhone 10. It shouldn't even know that it's contained inside of container view controllers. This is the idea of encapsulation. If your views only read the safe area insets that are provided to them on all four sides, and are able to adapt to different, to arbitrary safe area insets, that will ensure that your views will be able to, will be modular, can be moved throughout your application and run in different, different environments and still not be occluded. Now let's jump to an article and talk about hiding the status bar. So hiding the status bar is a technique that would reclaim 20 points of vertical screen real estate on rectangular screen phones. And we're just doing this by overriding preferred status bar hidden in our content view controller and we're turning true. And this preference is then propagating up through our contained view controller hierarchy and respected by the root-- the root of the system. Now unfortunately on iPhone 10, preferring the status bar hidden does not also hide the sensor housing. So we can't slide content underneath it. [Laughs] So UIKit will protect you and will not allow you to create this UI. Instead, the behavior of navigation controller on iPhone 10 is that it will always display the status bar when the navigation bar is visible. So if you want to hide the status bar and reclaim vertical real estate, our recommendation is to hide the navigation bar and status bar together. And, in general, when you want to go-- when you want to create an immersive experience, go immersive and just hide all of your overlays and controls together. Not only does this look good and help your users focus on their content, it's also a design technique that will adapt well to all of our devices. Now, speaking of immersive experiences. Let's switch to the iPad and talk about rendering text in a very wide environment. So here you can see we have text, and it's not extending all the way to the edges of our view. The problem with doing that, if we had rendered the text all the way to the edge, is that it can become difficult to read, for your eye to track from one line of text to the next at a given font size. And so the solution is to always render text inside of a readable width, a recommended readable width provided by the system based on the user's currently selected dynamic type font size. Dynamic type being another adaptive element of our iOS. Now you can get the readable width with this API on UIView called the readable content guide. So this is another layout guide just like the layout guide for margins and safe area. And it works just the same. And I mentioned that this readable width depends on the user's currently selected dynamic type size, which means if the user changes their dynamic type size in Control Center or the setting app, the readable width will get smaller or larger to compensate. Now let's switch to portrait and bring in our sidebar with our article list to make the context for our article display much narrower. So here the maximum recommended readable-- readable width is much wider than the space we have to display the article. And the thing to note is that the readable content guide will still not report the maximum readable width necessarily. It'll be clamped to the layout margins, which means you can be confident in laying out your views against the readable content guide. And not worry that they're going to spill outside your margins. Now let's see how this works on a context for a safe area exists. The readable content guide functions just like-- just like layout margins where its insets add to those provided by the safe area insets. Now normally at the regular-- the default dynamic type font size of the system, the readable width is going to be wider than the width of a device of an iPhone in any orientation which means it won't come into play. However, even iPhones, if the user selects a smaller dynamic type font size than the default, it can come into play, so it's still nice for your application to adopt. Now let's jump back out to our article list table view and see how readable width can work here. Now the thing to know about table views is that they use their layout margins, I'm talking about the margins, to layout a lot of their UI elements. So that means the separators, the system accessories as well as the labels in the system cell styles. And any views you lay out inside of your own custom table view cells, if you laid them out against the margins, will play along as well. So that means if you adjust the margins of a table view, you can move all of these elements together. So if you have a table view with a lot of multiline text in it, it makes sense that you would want to adjust the margins of the table view to bring all these elements into alignment and still respect the readable width. So to do this, table view provides an API. Called cell layout margins follow readable width. Now when it's false, table view will use its normal system margins. And if it's true, all the content will inset. Now, something to note is that the default value of this property has changed in iOS 12. It is now false by default. It was previously true. This shouldn't affect the behavior of your appl-- of your applications too much, especially on phones. Our general recommendation is to leave the default alone and set it to true when you know that you have a table view that's going to contain a lot of multiline text where it would make sense. And this property's also adjustable from Interface Builder with the follow readable width checkbox. Now keeping with table views but moving on from readable width, there is something else to know how they work with safe area, which is that the content views of your table view cells will not extend beyond the safe area. However, by default, the background and selected background views do-- of UI table view cell do extend beyond the safe area. So if you have some content you want to place in custom table view cells that you want to overflow outside the safe area and bleed to the edges of the screen, you can either place it in the background or selected background views if the semantics of those views make sense. Or there's a property on UI table view called insets content views to safe area, which by default is true, but you can change to false to get your content views to also extend to the edges. And this property is also configurable from Interface Builder. Now let's rotate back to portrait. And in the main screen of our application, we mentioned this sheet that slides up from the bottom before containing a picker view. Now the safe area of the screen here means that we have to adjust the layout of the picker view to be inside this safe area. And most of the system controls, like UI picker view and other controls, and probably controls of your own many views don't make sense to have to know anything about the safe area. Because it's not clear how they would respond or like re-lay out themselves internally. So we have a recommended technique for handling the layout of these kinds of views, which is to place them inside of the container view. And the responsibility of this container view is simply to position its safe area unaware content inside the safe area by analyzing the safe area insets. And it can also provide the background, which extends beyond the safe area and makes your control feel connected to the edge of the screen in this case. The-- another technique we want to talk about related to safe area and positioning elements close to the edge of the screen is when you have a control or button that on one device you may want to place directly against the safe area and on another device where the safe area insets are zero you may want some padding. And the reason for doing this in design is that the safe area insets can sometimes incorporate some implicit padding. So let me bring-- I want to provide you a single solution that will work for both of these situations. So let's bring in a diagram, and I'm going to give you two auto layout constraints that will produce this result in these two different situations. So the first constraint we need represents the padding that we would other-- we would normally add. So this is just a constant constraint from the bottom of the super view to the bottom of our control. But in this case, we're going to make it not required so that we can break it when the safe, when the safe area insets are non-zero. The second constraint is an inequality constraint from the bottom of our control to the bottom of the safe area. This ensures that our control is always inside the safe area. So if I change the safe area insets to make them non-zero, you can see that the inequality constraint ensures our content is not clipped while still maintaining some minimum padding. Now we've talked about many different adaptive APIs in this talk and in previous talks. Layout margins help keep lots of your elements aligned. Safe area insets protect your views from being clipped or occluded. Readable width keeps columns of text comfortable to-- comfortable to read. Size classes inform when you should make large structur-- structural changes to your applications. And these APIs we use to implement higher-level components in our frameworks. So scroll views, tab view-- table views, container view controllers. The other kinds of things we talked about today reuse these things to ensure that our high-level components can adapt to all of our devices in the simplest way possible. So the takeaway is for you to design your applications in terms of these adaptive primitives as well. The advantage is that-- instead of coding for a specific device. Your code will be simple, flexible, and it'll guarantee that your applications are as future-proof as possible for all of our different environments. For more information, these slides and recording will be available online. Thank you all for coming, and I really hope you've enjoyed the conference.  Good afternoon, I'm Jason Morley, a software engineer on the Health Team here at Apple. And along with my colleague Peyton I'd like to take some time to introduce you to an exciting new feature we have in iOS 12 this year, access to health records with HealthKit. Earlier this year in iOS 11.3 we introduced Health Records and Health. Users can connect to their healthcare institutions and securely download their health records for quick access and safekeeping. We aggregate data from multiple healthcare institutions and present it in a timeline allowing users to search and surfacing important details like lab values and reference ranges really helping users to better understand their health data. As of today we have support for more than 500 US hospitals and clinics covering over 50 healthcare institutions and we are continuing to add more. Of course this wouldn't be possible without the fantastic standards work of the Health Level Seven organization with their fast healthcare interoperability resources or FHIR and the Smart health IT project. For us it all starts with health, we establish a secure connection to healthcare institutions and download users' health records in FHIR direct to their iOS devices. From here we it store securely in HealthKit and aggregate data across multiple institutions. Starting today in iOS 12 we are providing APIs to allow you to access this data and work with it in your apps. First, I'm going to introduce the new sample types that we've added to support this feature. Then we can see how you authorize your application and query for data. After that, I'll take a brief dive into FHIR and show you what's possible here. And finally, we can take some time to think about how you can help protect your users' privacy in this sensitive domain. So let's jump right in. If you've used HealthKit before you may be familiar with HKSampleType and its subclasses. These group similar types of data, for example QuantityType groups data represented by a single numerical value, things like step count and heart rate. While CategoryType groups data characterized by enumeration, things like sleep analysis, am I in bed or asleep or menstrual flow, heavy or light. And to support Health Records we're introducing HKClinicalType which groups the categories of health records that you see in Health, things like conditions and medications. Within each of these sample types the different data types are differentiated by a type identifier. HKClinicalType identifier identifies the clinical types that correspond with the categories of Health Records and Health. We have allergies, conditions, these might be short-term conditions such as pregnancy or ongoing conditions like type II diabetes, immunizations, lab results, these might be my blood glucose measurement for my annual physical. Medications, procedures, a diagnostic procedure or a surgery. And vital signs, things like blood pressure or weight. Alongside these sample types and sample type identifiers, we're also introducing a new HK sample subclass HKClinicalRecord. If you're not familiar with working with samples in HealthKit we have a number of talks that cover this topic and I'll provide details of these at the end. There are a few key properties you'll be working with on HKClinicalRecord. The clinical type, we looked at this earlier and this allows you to determine the type of a given record. I want to know that a health record is for example an immunization. We provide a display name which is a string that we use when showing that health record to users in Health. It's selected from the FHIR resource and it's your opportunity to tie back your app's experience to something users are familiar with. You can see in this lab result that there are a number of alternative representations and names, including a coding in the LOINC coding system. This is an industry standard reference for medical information and it is required by FHIR for observations. You can see here that we've taken that LOINC coding and performed a lookup to provide a canonical name which we will surface to you as the display name. And finally, we provide access to the full FHIR resource, but there's a lot you can benefit from looking at the HKClinicalRecord so I'm going to set this aside right now and we can come back to it later. Like all of Health data, provenance, or the source of that data, is very important. You want to know where this health record comes from. And as with the rest of HealthKit we model this using HKSource which is available on the HKSample superclass. We set the name to be the name of the healthcare institution that they have shared with us and we provide a bundle identifier which is a stable identifier for that healthcare institution across multiple user logins and devices. So now we've seen the new sample types. Let's see how you authorize your application and query for data. As you can see, there's a huge wealth of information in medical records and we really believe that that is a fantastic opportunity for you to provide your users with truly empowering experiences around their health data. But of course health records also contain incredibly sensitive information. A user's health records might contain details of medications that they are taking or conditions that they are living with on a daily basis. This is information they may not be comfortable sharing with a close friend or family member. And it can change over time as a user interacts with their healthcare institution, they might receive a new diagnosis of a condition. So to help our users better understand and manage access to that data we're introducing a new authorization flow specific to Health Records. When your app requests authorization we will present a new permission sheet specific to clinical types. First, we inform users what it means to share that data with your app and just how sensitive it is. Since we will present this new authorization flow, whenever you request authorization you should ensure that you time your authorization requests to make sure your user has sufficient context with which to understand this new dialogue. After this we allow users to select the types of data that they choose to share with your app from the categories that you have requested. We also present a new purpose string and your app's privacy policy in the app explanation section. This is really your opportunity to explain to users why you need access to that data, what you're going to do with it, and how you will protect that data. You should be sure to make sure that what you request is proportional to what you need. Users are really going to be surprised and concerned if they see types here that don't pertain to the primary function of your app. And finally because data can change over time as a user interacts with their healthcare institution, we're introducing a new way to control how new data is shared with your app. We default to asking before sharing new data each and every time. This means we may need to present the new permission sheet whenever new data is available. So when you need to query for data you should always request authorization prior to doing so. Okay, so how do I get started and request authorization? This flow is going to look very familiar to those of you who've worked with HealthKit before. First, I need to configure the project. After that request authorization. And finally query for data. To configure your project you need to add a new health records capability in Xcode after enabling HealthKit. Then you add the new Health Records Usage Description purpose string to your Info.plist. Again, this is your opportunity to explain to users why you need access to this data and what you will do with it. Having done this, you can request authorization. Here I define the types I wish to access, conditions, immunizations, and medications. And then you can call request authorization on the Health Store. Being sure to handle any errors and after that you're good to query. You should be aware that in order to maintain user privacy we don't reveal to you which categories a user has granted access or denied access to. And now to querying. Because HKClinicalRecord is an HKSample you can take advantage of all of the existing sample query infrastructure in HealthKit. Things like SampleQuery, AnchoredObjectQuery and ObserverQuery will all work and if your user has granted continuous access background delivery will also work. Again, you define the type that you wish to query for. We're creating a sample query here and you can execute that on the Health Store. So that's the basic authorization flow. But we've actually introduced some additional mechanisms specific to Health Records. We have a new API Get Request Status for Authorization, which allows you to determine if we would present a permission sheet to the user. This gives you an opportunity to determine whether your user will need to see the authorization sheet and optionally present a UI to provide them with more context prior to authorization. To do this you call Get Request Status for Authorization on the Health Store, handle any errors, and if the request status is should request you can optionally present that UI prior to requesting authorization. Now there may also be some types of data that your app requires access to, to provide a safe experience or perhaps if you have a research app you need access to a comprehensive set of types to avoid skewed results. We have a new required types key in the Info.plist, which allows you to specify these types. And if they're present we will return a new required types error if the user has not granted access to these types. And this allows you to determine how to behave, optionally explaining to the user that you cannot continue if you don't have this data. You can implement this by adding the new required read authorization key to your Info.plist. Here I'm specifying that I require allergies, conditions and immunizations. And then you handle the new error in the authorization request. You should be sure to use this judiciously and try not to limit your app's functionality based on the data that you have access to. You really want to make sure as many users as possible can benefit from your app irrespective of how much data they choose to share. So that's the full authorization flow and there's just a couple of points I'd like to focus on. Firstly, you should be sure to request authorization each and every time that you need access to data, especially now because users may want to see that sheet every time when new data comes in we may need to present a sheet when you need to query. You should also be sure to request only access to the data that you need. And finally, you should consider taking advantage of the new APIs to provide users with better context and allow you to time those authorization requests. And with that I'd like to hand over to my colleague Peyton to see what he can do with this in his app. Thank you. Thanks, Jason. So I'm working on an app for mountain climbers training for their next summit. I've got a HealthKit powered view of all my users walking, hiking and stairclimbing workouts. Now after extensive market research, I've determined that mountain climbers travel to mountains and travelers should know their vaccination status. So by the transitive property of future development mountain climbers should know their vaccination status. So I'm going to help out my users. I've started implementing a new tab, an immunization dashboard. So let me tap on what I have now. Here's my immunization dashboard and to start out with I'm going to display my users a list of their current vaccinations and I'm going to use the HKClinicalRecords display name property. I've already added the Health Records capability to my project and I've created a purpose string to tell my users how I plan to use their immunizations. Now it's time to implement my dashboard. So here I am at the ImmunizationsViewController powering that tab. In HealthKit it's good practice to request authorization every time your user enters the part of your application using HealthKit. So for this app viewWillAppear is a great place. Here I need to first specify that I'm interested in immunization record samples. And next, call my Health Store's requestAuthorization method. In the callback I'm going to check for success and handle errors appropriately. Finally, once I'm unauthorized I'm going to call this query for immunizations method which I'll implement now. Here in query for immunizations I'm going to create a sample query, I'm going to iterate over all the return samples, and I'm going to pass each sample's display name to my data source. So let's do that. Here I've got my sample query and I'm using the humanization record sample type. No predicate because I want all of the users' immunizations and similarly no limit because I want to get all of my users' immunizations. No sort descriptors, it doesn't matter to me in what order these samples are returned. And with that I'm going to check for results and if not handle errors appropriately. Next, I'm going to iterate over the samples and I'll pass them to my data source by display name. With the names in my data source it's time to head out to the main queue and call this reload UI method that I wrote earlier. Finally, I'm going to execute my sample query on my Health Store and that's all it takes, so let's build and run. So here I have my training log, I'm going to tap on my immunization dashboard tab. I see the new Health Records authorization sheet explaining to my users how my app could potentially use my users' health records. I'm going to tap continue and here my user has a chance to allow or deny categories as they see fit. There is my purpose string that I defined in my Info.plist earlier alongside a link to my app's privacy policy. Now users have to explicitly switch for every single category, there's no turn on all button. So for the purposes of the demo I'm going to enable immunizations and to have share current records. For future records I'll leave with the default Ask Before Sharing and I'll tap done. And just like that I have a list of all my users' immunizations. So that's just-- So that's just using display name, but there's a lot more information contained in the clinical records FHIR resources. So to tell you more about FHIR I'd like to hand it back to Jason. Thank you, Peyton. So Peyton just showed us how we can request authorization in our app, query for data, and take advantage of the new display name on HKClinicalRecord to present that information to users in a way they're familiar. So now let's take a look at FHIR and see what you can do with this rich data format. Users interact with multiple healthcare institutions over the course of their lives and these are often running different electronic health record systems that don't always represent data in the same way. This makes interoperability incredibly difficult. So to address this the healthcare community came together in an effort called the Argonaut Project, this uses FHIR, a flexible JSON representation of health records and [inaudible] 2 as defined by the Smart Authorization Guide to allow connections, consistent connections to healthcare institutions and to allow data to be downloaded in a common format and related across those multiple institutions. FHIR itself models a huge universe of health data from allergy intolerance to vision prescription and it organizes this data into different data types called resources. Each resource has a resource type and there is one for each and every kind of health data. Today we are selecting the eight most common resource types and these are defined within the Argonaut Project and we are grouping these into the seven different clinical types that we saw earlier. You'll note that we split observations into lab results and vital signs based on the categorization and we group multiple medication resource types into the medications category. So FHIR represents data as a JSON dictionary and there are some key properties that are present on all FHIR resources. Things like the resource type, here we're looking at an observation and this tells us that in the context of health this is a lab result or a vital sign. We also have an ID present on all resource types. This is a unique identifier for that resource, but you should note that it is only unique within the domain of that resource type for that healthcare institution. One of the most fundamental building blocks of FHIR is a coding. This allows for a reference to a unique identifier or a code scoped within an external coding system. There are a number of coding systems specified within FHIR and you can use these to find out more about the items that are referenced in medical records or to relate specific things across multiple healthcare institutions. Here we're actually looking at a category coding which tells us that this is a lab result. There's also a code in the case of an observation for what is being measured. This is blood glucose as defined in the LOINC coding system and it could take that code and look it up in the LOINC database for alternative names and further information. And of course this is an observation so I have the value that is observed. This is 60 milligrams per deciliter and you'll note that even the units here are coded, so I have a really comprehensive understanding about what's going on. FHIR resources have many additional data elements and I encourage you to go and look at the Argonaut Project documentation to find out more and I'll have details of these at the end. In HealthKit we model a FHIR resources as HKFHIRResource. We provide some key properties like the data, access to the full raw data. We surface common elements like the resource type and identifier. You can access the data using Swift codable or JSON serialization. Here you can see that I've defined my own codable struct, my codable observation and I'm using that to pull out the value quantity that we saw earlier. You can have confidence that the data is structured correctly here as we only share valid FHIR resources through the API. We also introduce some predicates to make it easier for you to work with HKClinicalRecord. We have a predicate to allow you to query by resource type, here I'm looking for prescription or in FHIR a medication order. We also have a predicate to allow you to uniquely identify records by their source, resource type and identifier. This can be incredibly useful as FHIR resources can reference other FHIR resources. For example, a medication might reference the condition that it treats. Now there are some additional considerations you need to be aware of when working with FHIR. Firstly, FHIR resources may contain their own dates. For example, prescriptions may contain the order data of that prescription while a condition may contain the onset date, abatement date and the date that that prescription is first observed. You should therefore use the FHIR resource directly to access these dates for each resource type. Start date and end date that are available on the HKSample superclass are set for the date that we first add that resource to Health. You should use source, resource, type and identifier when uniquely identifying a health record. This will allow you to identify a health record across the course of its lifetime as it is updated by the healthcare institution. And finally, you should be aware that FHIR and JSON don't fully agree when it comes to numerical precision. FHIR ascribes significance to the number of digits following the decimal place and this can be lost when using Swift codable or JSON serialization. If it's really fundamental to your app that you have full access to this information Swift codable and JSON serialization may not be appropriate tools. And with that, I'd love to hand back to Peyton and see what wonderful things he can do with this new API. Thanks, Jason. So back to the immunization dashboard I've been working on. I'm using HealthKit to fetch my users' vaccinations, but right now it's tough for a user to tell what vaccinations they have and what they might need to get. So I'm going to solve that by implementing an immunization checklist. Users can see a summary of their immunizations in plain language. To populate this list I need to know that this combination vaccine induces immunity against chickenpox and against MMR. Health institutions speak different languages so I can't rely on name alone, so that's where the interoperability of FHIR comes into play. FHIR gives a way to use predefined codes rather than natural language to specify what things are. So after taking a look at the FHIR documentation here's the algorithm I've come up with to identify a chickenpox vaccine. First, I'll take the immunization records JSON, I'm going to deserialize it and pull out the vaccination code in a system called CVX. CVX is a commonly used coding system for immunizations, but there are many other coding systems for different areas of medicine. Finally, I'm going compare the CVX code I found in my immunization record to a list of vaccines I know induce immunity against chickenpox. If there's a match I'll show a checkmark in my UI next to chickenpox. So let's talk about the immunization record JSON. The structure starts with a resource type immunization. There's a vaccine code object and contained therein is an array of codings. There may be multiple codings, so I need to search by system to find CVX. In this example, the first system is NDC so I move until I see CVX. With that in hand I look at the code and I look it up and I see that I have MMRV. So that's the structure of these immunization records JSON. Now it's time to implement. So here we are back in my immunizations view controller. I've done some work ahead of time to implement the UI of the checklist and I instantiate it with a showChecklist property. Now it's time to populate my checklist with data. Here I am in the query for immunizations method. When I iterate over my users' samples I'm going to add a call to a new method matchCodedVaccine and I'll pass it if the sample is FHIR resource, so let's implement that now. Here I am in matchCodedVaccine from resource and what I need to do is deserialize the FHIR resources data, pull out the vaccination codes, search for CVX, and compare it against lists of CVX codes for different diseases. I have a lot of freedom in how I choose to deserialize the FHIR resource, so today I'm going to use Swift codables. I need to define a codable for exactly the keys that I'm interested in. So I've done that now. An immunization resource has a vaccine code, a vaccine code has an array of codings and a vaccine coding has a system and a code. With that I instantiate a JSON decoder and I use it to decode the FHIR resources data as an immunization resource. All that's left is to iterate over each coding, check to see whether the system is CVX, and then pass the code to this method to mark it in the data source. All this method does is compares the code against lists of codes per disease and if there's a match marks in the UI that disease for the found disease. So with that I'm going to build and run and take a look at my immunization checklist. So here I am on my training log, I'm going to tap on the Immunizations tab and just like that with a few lines of code I have an immunization checklist. So that's an example of solving a problem using FHIR. Now for a word on granting consent for your users I'm going to hand it back to Jason. Jason. Thanks Peyton. So Peyton has shown us how we can take that rich FHIR data and take advantage of the coding that's provided to really provide our users with a very meaningful and comprehensive experience around their health records. And now I'd like to think a little bit about how you can help protect your users' privacy in this domain. As you've seen this is incredibly sensitive data. We've built privacy in from the ground up, establishing secure connections direct to healthcare institutions. Downloading data straight to users' iOS devices. Storing it securely in HealthKit. And providing comprehensive management tools around access to that data. Your users will expect you to treat their data with that same level of care and attention. Every choice that you make here can have a direct impact on individuals. This can be a truly positive impact and it's why we are choosing to provide this API today, but it can also be a negative one. If users feel that this data is out of their control or worse still, it is shared without their consent. You can show respect for your users and help establish a relationship of trust by providing them with clear policies around that data and informing them what you do with it. This starts with your application, your purpose string, and your privacy policy and it continues every step of the way as you work with that data and as you move it around. For example, you should make sure you give users details about how you manage their data. You should publish clear retention policies and you should provide them tools to allow them to delete that data should they choose to stop sharing it with you. And finally, you should make sure that you always request data proportional to your use case. You should never be asking for more data than you need. I encourage you all to go to the Privacy Talk, Better Apps Through Better Privacy on Thursday at 4 p.m. and this covers the whole ecosystem, so it's a great opportunity to find out more. So we've seen the new sample types that we're introducing to support Health Records. We understand how to configure entitlements and privacy strings in our app, to request authorization and to query for data. Hopefully I have shown you just enough to whet your appetite to go and find out more about FHIR. And we've seen how you can do more to help protect your users' privacy. We continue to be amazed by the fantastic apps that our developer community creates and how it really empowers users when accessing their health data. This is a journey that we started with Health and HealthKit and it's one that we have continued with ResearchKit and CareKit. And now today with Health Records and HealthKit we are thrilled to be continuing on that journey with you. We can't wait to see what amazing things you do. You can find the documentation that I was talking about earlier at the URL behind me and we have some labs. We have a health and fitness lab immediately after this talk where we're very happy to get you started with this new API. Our colleagues Anherika and Kareem have a talk tomorrow morning introducing new ways to work with workouts. And we have a health fitness and research get together tomorrow night where it's an opportunity for you all to get to know each other and talk with us as well, so please do come along to that. And with that, thank you very much and have a fantastic conference.  Alright, good morning. My name is Chad Woolf. I am a performance tools engineer here at Apple and today's session 410. We're going to talk about creating custom instruments in Instruments 10. Today's session looks like this. We're going to talk a little bit about why you might want to create custom instruments. We're going to go over the architecture of Instruments. And we have a lot of content today, so we have three sections: Getting Started, Intermediate, and Advanced. And then on the way out, we'll talk about some best practices, some of the things we've learned along the way, writing instruments on our own. So the first one, why would you want to create custom instruments? Instruments already ships with a lot of really powerful tools, for example here we have System Trace where you can see how your application is interacting with the scheduler and the virtual memory. We have a new game performance template this year that combines System Trace and Metal System Trace to help you spot glitches and missed frames in your application. And if you're on the network portion of your application, we also have the Network Connections Instrument, which can show you TCP/IP traffic coming in and out of your app. And then, of course, a lot of you are familiar with the Time Profiler. Time Profiler is a great way to see where your application is spending its time, whether that be the networking layer or the game engine or some other portion. Now the common thing here is that these are all very useful if you know the code that you're profiling, right. So if you know those IP addresses, you know what those mean, and you know what the different functions mean and the call stack of the Time Profiler, it makes it a lot easier. But what if someone is profiling your application and they're not familiar with their code, right? What if they just want to see is the application spending a lot of time in the networking layer, and, if so, what is it doing? Well, a good use for a custom instrument would be to try to tell the story of what your layer or what your application is doing in a way that someone who doesn't understand the code can understand and appreciate it. Now in the advanced section, we're going to show you how to take advantage of the Expert System Technology that built inside of Instruments so that you can create an instrument that's actually able to look for bad patterns and spot anti-patterns in your code even if you're not there. Alright, so let's take a look at the architecture that makes this possible. And to do that, we're going to have to start here, back at the beginning. So in the beginning, Instruments works about the same as it does today. There's still a library. You still drag instruments out and drop them into your trace document and then you press Record and it's like running a bunch of performance tools at once. Now the major difference between then and now is that back then the infrastructure of Instruments didn't really do a lot to help us write instruments quickly. And at the time, that was okay because we had already inherited quite a few assets and performance tools that we already had. They all had their own recording technology and their own analysis logic and all we had to do was build a custom storage mechanism to get the data in the trace and a custom UI to help integrate it with the rest of the app. Now over time, the maintenance costs of Instruments and maintaining this model shot up. And the reason for that was every time we wanted to add a new feature we had to modify seven custom UIs and seven custom storage mechanisms and that's not the model we wanted you guys to inherent. We didn't want you to inherit this kind of maintenance costs. So before we even talked about doing a custom Instruments' feature, we needed to solve that first and I think we did. So in the new version of Instruments, instead of having custom UIs and custom storage mechanisms, we have two standardized components and that's the Standard UI and the Analysis Core. Now the Standard UI is what implements the entire user interface of a modern Instrument and it's tightly coupled with the Analysis Core. The Analysis Core you can think of as a bit of a combination between a database and an expert system. And the two these are optimized to work on time series data, which makes them a great foundation for building instruments. Now when you build an instrument with the modern architecture, really what you're doing is essentially creating a custom configuration of both the Standard UI and the Analysis Core. Now if you look at some of the screenshots of the powerful instruments that I showed in beginning, we have the System Trace and we have the Game Performance template and the Network Connections template and Time Profiler. All of the instruments in all of those documents were built completely out of the Standard UI and the Analysis Core. So you can do the exact same things that they can do. And in Xcode 10 and in Instruments 10, we're giving you the exact same tools to build your instruments. So the only difference between an instrument that ships with Xcode and one that you build is just simply who built it. Now your instruments will show up here in our library and you can see like Activity Monitor at the top. Just like that, you can drag and drop your instrument into a trace document and take a recording. And what happens here is the Instruments fills in the Analysis Core with data and the Standard UI reacts to create the graphs and the table views. Now Instrument has two ways of showing data. It's got the graph view at the top here, which we call a track view, and an instrument can define more than one graph, if it would like to. And the way that you choose between the graphs that will define your instrument is there's a small control here attached to the Instrument icon and we can change this from say CPU to Networking. Now each graph is allowed to define a certain number of lanes. So here we've defined three lanes, graphing three different types of CPU utilization. And each one of these lanes is bound to a different table in the Analysis Core or it can be bound to the same table but you're looking at a different column in the table. Now the other portion of the instrument is the lower portion, which is equally as important. It's called the Detail View. And that's where you can see the event-by-event lists and also any sort of aggregations and summaries of your data. Now just like the lanes, oh, I'm sorry, just like the graphs, you can define a number of details for your instrument and you can select which detail is active by clicking this portion of the jump bar and then selecting the title of detail that you define. Now just like the lanes in the graph, all of the details are bound to again a table in the Analysis Core and that's where they receive the data. The recording happens. The tables fill in. And UI reacts and there's no special code needed on your behalf. Now from the perspective of the Standard UI, everything in the Analysis Core appears to be a table. So let's talk a little bit about tables and what they are. Tables are collections of rows and they have a structure that's defined by a table schema. Right, so it's very similar to a database application. The schema defines the columns and the names of the columns and also the types. Now the Analysis Core uses a very rich typing system called an engineering type and that both tells us how to store the data and also how to visualize it and analyze it in the Standard UI. Now in addition to or while the schema describes the structure of a table, you can use attributes which are key/value pairs to describe the content. So that kind of helps us describe what goes into the table. You can think of schemas as like a class in Objective-C or Swift whereas the rows are like the instances. And so it's important that your schema names are singular, just like we have class names in Objective-C that are singular, like NSString instead of strings. So this will be more important when we get to the advanced section but I wanted to call it out now so we can know what we're looking at. Okay, an example of the schema here is tick. This is one of schemas that comes inside of Instruments and it's used to hold a table of synthetic clock ticks that we'll use later for statistical computations in our modelers. Now it is very simple. It has one column that's defined and that's time and it's using the engineering type sample-time. And it also defines an optional attribute that can be attached to that table instance called frequency. So if you create a table with a frequency equals 10 attribute here for our tick schema, then the provider of that data knows that it needs to fill that table with ten timestamps per second, right. So that's a way to communicate what you want filled into the table. Now with that, I think we have enough information to help us get started. So we're going to show you how to create your own Instruments package project in Xcode and we're going to show you how to create your very first instrument that graphs these ticks and shows these ticks in the detail view. And to do that, I would like to call up my colleague Kacper to give you guys a demonstration. Thank you, everyone. Now I will show you how to start with creating and running your first custom instrument. You're going to be using tick schema presented by Chad to craft instruments during ticks with constant frequency. You will learn how to describe your package, iterate on it using Xcode, and tested in Instruments. Let's get started. You create your new Instruments packets project just like you used to in Xcode. You go to a New Xcode Project, select macOS platform, and Instruments Package. You need to fill out your product name, which will become default name for your Instruments Package. Let's call it ticks. Hit Next and Create. Xcode has created project with package target and one file, package definition. Let's look in to it. Packages are described in XML-based syntax. At the beginning, each package contains identifier, title, and owner. These fields will be visible when someone attempts to install your package. Usually, you would start by defining your own schema and optionally modeler but because here we are going to be using predefined tick schema, let's remove these guides. To import tick schema from base package, all you need to do is to specify import schema element and first name of the schema, tick. Now it's ready to be used by our Instrument. To make defining more complex elements easier for you, we've deployed a number of snippets in Xcode. To use them, just start writing your element name, like instrument, and hit Return. You need to fill out your unique identifier for Instrument and a few properties that later appear in Instruments Library. It will be Instrument drawing ticks every 10 milliseconds. Now it's time to create a table that will be instantiated when this instrument is dropped from library to a trace document. Table identifier has to be unique within this instrument definition. Let's call it tick table. In schema-ref, we need to reference schema that we previously imported, tick. Now we need to define what will appear in [inaudible] view and detail view for our instrument. I will use graph element. We need to fill out title for our graph. I will call it ticks. And title for our lane. I need to reference table by identifier that was previously created here, so I will reference tick table. And now we will specify plotting for our graph. I will use plot element. And in its most basic form, it requires you only to pass mnemonic of the column that contains value to be graphed. We will be graphing time. :11 I would like all of my timestamps to be visible in a table. To do this, I will use list element. We pass title for a list that will appear in the [inaudible] of Instrument, table ref, just like for lane element before, and columns that we would like to see. Now our package is ready to be built, and run in Instruments. To do this, you will use Xcode scheme run action. Let's do it. You can see that build error appears. You have full ID support when building instruments packages. Here, error appears in line and says that column timestamp could not be found in schema tick. Oh, that's right, because it's not timestamp. It's supposed to be time. I will fix it and run it again. You can see it running because this new copy of Instruments to appear. You can recognize the special copy by having different icon. It loads your package temporally only for this run session. It allows you to iterate on your package more easily. To be sure that your package is already loaded, we can check it out in New Package Management UI. You can find it in Instruments Preferences and Packages [inaudible]. You can see our newly created package here along debug batch, which means that it's loaded only temporarily. You see also all of the system packages here. You can use and link against them using subtitle, visible here. Our ticks package contains ticks instrument. So let's test it now using blank template. I will switch my target to my MacBook and we'll search for my instrument in Instruments Library. I will fill the ticks and it appears here with all of the properties being filled out from the package definition. Let's drag and drop it into a trace and record for just a second. You can see the bottom pane was propagated with data generated every 10 milliseconds. Detail and graph are coordinated with each other. When I click on rows, you can see inspection head moving here. I can also zoom into a graph using Option and Click and Drag. Here you can see the ticks are indeed being drawn. That's how you create your first Instruments Package. Now back to Chad who will tell you more about Standard UI. Alright. Thank you, Kacper. Okay, so we've seen how to create a very basic instrument. We see how to get started with creating your first project in Xcode. Now let's talk about the different kind of graphs that we have and the different kind of details we have and how we can potentially do this with real data. Starting with graph lanes. So you saw how Kacper was able to define a graph and a lane using what we call the plot element. Now the plot element is a way to tell the Standard UI that we should be taking the entire contents of the table and trying to plot it in that particular lane. Now the way that the plot element determines how to graph this, what the graphing treatment should be, is by looking at both the schema and the column that was targeted to take the value from. If the schema is an interval schema, meaning that it has a time and a duration, or if it's a point schema, which means it just as a timestamp, they're treated differently. And if the column being targeted has a magnitude, meaning that it can draw a bar graph out of it, it will draw a bar graph like this. An alternative here is our Life Cycle lane where it's still an interval schema but we're targeting a column that's a state and the state does not inherently have a magnitude. So it doesn't make sense to draw a bar graph there. So the Standard UI will automatically pick a state style treatment which involves drawing these intervals with a label in a rounded rectangle style so you can tell it apart from just a flat bar graph. Now it's really important that the Standard UI be able to pick these treatments for you because that's what keeps Instruments UI consistent. So if you define a state graph and we define a state graph, the Standard UI will enforce that they look the same way, which makes it a lot easier for user of Instruments to move from instrument to instrument. Now if you want to create graphs or the number of lanes dynamically, based on the contents of the data, you can define what's called a plot template. Now a plot template is defined very similarly to a plot except there's an extra element in there that allows you to choose a column in the table and it will create a separate row for each unique value in that column. Now if you're looking for just spikes or periods of activity, we have what's called a histogram and what you can do is break the timeline over certain size buckets, let's say 100 milliseconds, and then use functions like count or sum or min or max to sort of drive up the magnitude of those buckets as the different points or intervals are intersecting. So it's a great way to look for spikes in activity such as here in the System Trace where we're looking for spikes of activity in context switches or virtual memory. Now let's talk about details. Details are on the lower half of the UI. And you've already seen the first one, which is the List. That's very simple mapping between the Table and Analysis Core and a table view in the UI. We also have aggregations. And aggregations are nice when you want to try to subtract out the time component and you want to look at your data in aggregate. You want to apply some statistics to everything that's in that table. And so when we define an aggregation, what we're doing is the columns this time are now functions. So you can use functions like sum, average, count, and several other statistical functions to help you create the aggregation view that you want to create. Now the nice part about aggregations is that you can define a hierarchy as well. So here we've defined a process thread in virtual memory operation hierarchy, so we can see these totals broken down by the process and then by each thread that's in that process and then by each type of operation that's in that thread, in that process. So aggregation is a really nice, powerful way to look at a lot of data in summary. Now another type of aggregation is called Call Tree. Now the Call Tree is useful when you have a column that is a backtrace and you got another column that's a weight. You can create weighted backtrace or a weighted Call Tree view using the Call Tree like you see in Time Profiler. Now another style is called a narrative. And the narrative is used when you want to convey information that's just best left to technical language, such as the output of an expert system and that works hand-in-hand with the narrative engineering type. Now the last type of detail here is called a time slice. The time slice looks very much like a list except the contents are filtered to include only the intervals that intersect with that blue line you see on the graph. That's called the inspection head. So as you move the inspection head over the graph, the contents of the list will be filtered to match what intersects with that inspection head. Now all of these UIs are bound to tables in the Analysis Core. And when you hit Record, the data comes in through the Instruments app and fills in the data in the Analysis Core. So let's talk a little bit more about how that process works. The first step before you can press Record is the Analysis Core will be taking the tables that are created within it and it'll be mapping it and allocating storage for it in the core. Now if a table has the exact same schema and the exact same attributes, then by definition it's the exact same data, so it's going to map to the exact same store. Now for each store, the second step is to try to find a provider for the data. Now sometimes we can record that directly from the target through the data stream and sometimes we have to synthesize the data using a modeler. Now modelers can require their own inputs and those inputs can be the outputs of other modelers or they can be recorded directly from the data stream and that's how we synthesize the rest of the data that we don't know how to directly record. Now once we've got data sources for all of the stores in the Analysis Core, that's what's called the binding solution. And so the third step is to optimize the binding solution. And here you see Instruments is visualizing its own binding solution for what we call the thread narrative. Now the next part about the binding solution is that it's trace-wide and so as you're dragging and dropping instruments into the trace, Instruments is computing the best possible recording solution to try to minimize the recording impact on the target. Now when you create your own tables or when you create table instances, you have to give them a schema. And Instruments already has over 100 schemas defined. And all of these schemas are available to you and are contained in the packages that you saw in the Package Management UI. You can just simply import the schema into your own package. Now if that schema is contained in a package that's not the base package, you have to also link that package as a build setting in Xcode for Linked Instruments Packages that you can set so that we can find that extra package that you're referring to at build time and do some type checking. Now because all of these schemas are defined in other packages when you hit Record, all the tables with those schemas will fill in because they either have modelers defined or we know how to record it from the data stream. So these make excellent building blocks for your own instruments but even better they make excellent inputs for writing your own modelers. Now you write a modeler or you define a modeler in your Instruments Package with the modeler element and you can also create a custom output schema for that modeler. You can use the point-schema for just a single point in time or you can use the interval schema if you have a point and a duration. Now the modeler is able to define what inputs it needs and this is what tells the binding solution how to fill out the rest of that data flow graph. And so your modeler will snap right into the binding solution. Now modelers are actually miniature expert systems and they are written in the Clips language, which means that they're very powerful but they're also pretty advanced. So we're going to save the details on how to create modelers for the advanced section; however, it is really important that you be able to define your own schemas and we have a new os signpost API this year, which is a great way to get data into Instruments. So we've created a little bit of a shortcut. Inside your package, you can define what's called an os signpost interval schema and what that does is both define schema and also give us enough instructions to be able to generate a modeler on your behalf. Now inside there, you can capture the data that you recorded in the metadata of your os signpost calls and you can use that captured metadata and expressions to define how we should fill out the columns of your schema. So we'll look at a really simple example. Let's say we're going to do JSON decoding and we have a signpost that we mark the beginning of that decoding activity and the end of that decoding activity. And in the beginning, we'll also capture some metadata to indicate the size of the JSON object that we're about to try to parse. Now in your Instruments Package definition, you can create an os signpost interval schema and you define the name of your schema here. You select which signpost you would like to have recorded, including the signpost name, and then here you can use a syntax to capture the different pieces of metadata from your start metadata message. And here, we're going to take that captured value and we're going to use that as the expression to teach us how to fill in the column for data size that we just defined in line here. Now in Session 405, which is Measuring Performance Using Logging, I demonstrated the Trailblazer application and also showed you an instrument that you guys could write based on the signpost inside that. And now that we know a lot more about how to write custom instruments, I'd like to invite Kacper back on stage to give you a demonstration and walk-through of how we created that package. :15 Thank you, Chad. So Trailblazer app is an iOS app that displays lists of popular hiking trails near you. As an UI component, UI table View. Each cell loads image for a trail asynchronously. To prevent glitches and as an optimization when cell is reused, we cancel the download. To visualize my flow of downloads, I wrap every download in os signpost call. Let's take a look at it. When my [inaudible] cell is displayed, start Image download method is called. We create downloader signpost ID, which takes os log handle and downloader object. We then grab address of UI table View cell and call os signpost begin with os log which is coming from signpost.networking. Let's take a look at it. This log takes our app identifier as subsystem and networking as category. We pass background image name, previously created signpost ID and message format, which includes image name. Here, we wrap it in public specifier because it's a string and caller, which is address of a cell. Our download could complete in two ways, successfully. Let's take a look at it now. When it completes like that, delegate method it's called. We create signpost ID just like before and call OS signpost end. This time we pass status and size. Value for status is completed. And size is set to the image size. Next let's take a look at our prepare for use overwrite. When there's running downloader in progress, we cancel it. We create signpost ID and call our signpost end with the same format string but now value is canceled and size is zero because download didn't succeed. Let's take a look at our os signpost interval schema definition and how we captured those signpost end package. We define our signpost interval schema with unique identifier and title. Then we define our subsystem and category, which corresponds to the one that we passed when creating os log handle. We create name element, which corresponds to the one that we passed in os signpost call and start pattern and end pattern. These both correspond to the one that we passed in os signpost begin and end calls. Message element is the same as the format string you passed but instead of format arguments, you pass variables here to capture the values that you passed when calling os signpost. Let's take a look at how we fill out those values in our columns. Here, you can see status column. It's type of string because it's either completed or canceled, and we fill it out with the value of status variable. Because expression element could take arbitrary Clips expression, we could also do more sophisticated things in it. Here, we could compute event impact by looking at size. If it's greater than 3 and 1/2 megabytes, we say that impact is high, else impact of operation is low. That's our definition for os signpost interval schema. Now let's take a look at table creation. For schema ref, we pass identifier of our os signpost interval schema and create unique identifier for this specific table. Then, we can reference it in our UI definitions. For graph, we create a single lane. It takes our table and this time it graphs by using the plot template. Plot template is dynamic way of creating graphs. It looks at the table, at the column that was passed in instance by element and for each unique value of this column, it creates plot. Label format element allows us to create format title for this plot. Here it's img column and the value from image name column. We pass image name as a value of our plot. Each of our planes will be colored with column of impact and label on our plane will be taken from image size. Next, we have a list. You already saw this one in ticks example. Here, we pass all of the columns that you would like to see. Next, aggregation. This aggregation will track all of the completed downloads. Because our table contains both completed and canceled downloads, we need to apply slice element to filter some of the data. In slice element we specify column that slice will be applied on and predicate value that has to be matched. Here, we want to take only completed rows from this table. We define hierarchy, which is only one level hierarchy with image name and columns that will be visible. For each image name, we will specify count and image size. So we will be summing sizes of an image. Next, we have image, time slice, sorry. We specify all of the columns that will be visible. And to use our instrument more easily, we can specify our custom template. Now let's try and build, and run our package. You can see the template appears here. I can choose it, and target my iPhone and Trailblazer app. I will record for just a while. You can see that track view was propagated with data. Each plot was created for each image name. You can see that label format matches the one that we passed in package definition. And if download is higher rather than 3 and 1/2 megabytes, our plane is colored in red. Size appears on the plane. Next, we can take a look at all of the details. Firstly, we have list downloads. This is just a list of all the downloads that happened. We can choose our aggregation, which divides all of the downloads by image name. You can see that it on top we downloaded 12 images. And image for location seven was downloaded two times. Next, we can take a look at active requests. Here, you can see that when I'm grabbing my inspection head, data in detail view changes. We can track multiple active requests and see what was the duration at the time of current inspection head. If you would like to take a look at your data from different perspective, you would like to take a look at your stores and modelers, we give you this by using the Instrument inspector. This is a way to debug your custom instruments. Here, you can see that I selected Stores step and I see store for os signpost being created. It looks at networking category and com apple trailblazer subsystem and we gather 24 rows here. Then, we can see our created table image download, which has 12 rows. In bottom area, you see whole content of this table. Next, we can jump to modelers and we can see that we have auto-generated os log modeler here. It took 24 rows and outputted 12 rows. On the right, you can see binding solution here. So our generated os log modeler took data from os signpost table and put it into image download table. Then it was consumed by our instrument. So that's how you capture your os signpost invocations, create UI, and look at your data using Instrument inspector. Now let's go back to Chad who will tell you more about advanced modeling. Alright. Thank you, Kacper. Okay, so now we've seen how you can combine os signpost data with custom instruments. And we think that you'll be able to take this, I think you'll be able to take these two, this combination pretty far. Now, we can talk about some of the advanced topics, specifically how you create and define modelers. Now a modeler conceptually is very simple machine. It takes a series of inputs. It does some reasoning over those inputs and produces outputs. The inputs of the modelers are always perfectly time ordered and so if you request several different input tables, those tables are first time ordered and then merged into a time-ordered queue, which feeds the working memory. So as we pull these events off one by one, they're entered into what's called the working memory of the modeler. And as the modeler sees the evolution of this working memory, it can draw inference. And when it sees a pattern that it wants to produce an output for, it simply writes it to its outbound output tables. Let's walk over like a really kind of playful example of how you might use a modeler. So let's say you define a schema called playing with matches, right. This is an os signpost interval schema and it's for an os signpost that you've defined where you're going to do some sort of dangerous operation in your code. And we define another schema called app on fire, right. It's also a signpost schema but these signposts mean that the application has entered into a bad state and we really want to know why. So you create an output schema, which is a point schema, that's going to hold the object that was playing with matches and the time at which the fire started. We are going to call that the started a fire schema. Now the modeler's world looks like this. So we have all of our inputs set up in time order ready to go and this dashed line on the left is what's called the modeler's clock. Now when we grab the first input and we enter that into the working memory, the modeler's clock moves to the start of that interval and then we grab the next input, the modeler's clock again moves to the beginning of that interval and we enter that into the working memory. Now the modeler sees both of these in the working memory and it can see that if playing with matches started before the app is on fire, it doesn't really make much difference, if it's the other way around, it's already on fire, then we can draw a logical conclusion here called the cause of fire and we can enter that into the working memory. Now as we grab this third input, you'll notice that the modeler clock has moved and it no longer intersects with our first two inputs. And so those are removed from the working memory. Now if the cause of fire had what's called logical support, it would also be removed from memory. Now to recap, the clock is always set to the current input timestamp. And for an input to remain in the working memory, it must intersect with the current clock in the modeler. This is what helps us establish coincidence. It allows us to prune out the old data and it also allows us to see if there are inputs that are possibly correlated in time. Now the way that a modeler reasons about its working memory is defined by you through what's called a production system. Production systems work on facts in the working memory and they're defined by rules that have a left-hand side, a production operator, and the right-hand side. The left-hand side is a pattern in working memory that has to occur to activate the rule and the right-hand side are the actions that will happen when that rule fires. Now the actions could include adding a row to an output table or include asserting a new fact into the working memory as the modeling process progresses. So facts come from two sources. One, they're from the table inputs that you saw, so will automatically assert these as facts using the rules that I showed you with the modeling clock, and they can also be produced by assertions from the right-hand side of a production. Now if you're going to create your own facts, Clips allows you to find what's called a fact template, which allows you to provide structure to your fact and do some basic type checking. So let's take a look at some rules in Clips. Our first rule that we're going to look at is called found cause. And what that says is if there is an object who's playing with matches at t1, and the app is on fire at t2, and t1 happened before t2, then on the right-hand side of this production, we can assert a new fact called cause of fire with the object that started the fire. Now that will be entered into the working memory. Now we come down to our second rule, which is called record a cause, if we have an app on fire at some start time and we know the cause of the fire and we have a table that's bound to our append side, that's the output side of the modeler, and that table happens to be the schema that we define called started a fire, then we can create a row in that table and then set the time and who started the fire to the values that we captured up here in the pattern. Now with that, we basically created our very first expert system to look for bad patterns in our application with these two rules. Now you may have noticed that the rules were prepended by either modeler or recorder. Those are what are called modules in Clips and they allow you to both group rules but also control the execution order of the rules. So for example, if you kept all of your output, all of the rules that produced output the output tables in the recorder module, then you can be sure that you won't write an output while you're in the middle of the reasoning process in modeler because all the rules in modeler have to execute before any of the rules in recorder can actually execute. Now I mentioned the term before logical support. What logical support is usually tied to what are called peer inference rules and those are rules that you say, well, if A and B, then C. Right. So by adding logical support to your production, what you're saying is if A and B are no longer present in working memory, then C should automatically be retracted. So what we're saying is C is logically supported by the existence of A and B. Now this is important because it limits working memory bloat which helps with resource consumption but it's also important to remove facts from working memory that are no longer valid. And if A and B are no longer valid, then you should really remove C. So to add logical support to your production, your rule here, you just wrap the pattern with the keyword logical and then anything you assert on the right-hand side of the rule will be automatically retracted when these move forward. And you'll notice these two rules, I'm sorry, these two facts here are the names that come from our schema. So those are inputs and so when the modeler's clock move forward, those will automatically be retracted. Okay, so now we know the basics of how to create a modeler in our package and we've seen some of the Clips language and rules. So let's take a look and see if we can add an expert system to our networking instrument to find bad patterns and potential misuses in our networking layer. And to do that, I'd like to invite Kacper up for one last demo. So now with the existing logging, I will try to write modeler to detect some anti-patterns in our app networking behavior. I was playing with my Trailblazer app and it seemed that if I'm scrolling pretty fast, there are some glitches visible here. So image is replaced multiple times, so I suspect that our cancel doesn't really work. I would like to write modeler that detects that. So let's take a look in our package definition. We will start by writing modeler element. Modeler has identifier, title, and purpose. These fields will be extracted to your documentation. We specify production system path which contains all of the logic for our modeler. Then, we define output of our modeler. It will be downloader narrative schema. Required input for our modeler will be os signpost table. This table contains begin and end events. Now let's take a look at definition for downloader narrative schema. This is point schema that defines two columns, timestamp which tracks the time of logging that diagnostic message and description that has information about what's gone wrong. Then, we can create this table in our instrument definition. We pass downloader narrative schema ref and unique identifier. Then, we could use it in our narrative element definition. Here, we define narrative. We pass table ref for the table we previously created, define time column, and narrative column. Now we are ready to define logic for our modeler. To do this, I will create file that I previously referenced in modeler definition. To create Clips file, you go to File, New, select macOS platform, Other section, and Clips file. I will fill out the name and create. So algorithm for detecting whether one cell is doing more than one request at a time will be as follows. We'll be tracking every request as a fact in working memory. Firstly, we need to create template for this fact. So every fact will be storing time, caller address, which is cell address, signpost id that we captured, and the image name that we are requesting. We will call this fact started download. Then, you'll write modeler rule that creates this fact in working memory. This rule looks at os signpost table. We specify subsystem, name, and event type begin and we capture all of the information that we want to have. So we capture image name, caller address, time, and signpost identifier. Then, we assert new fact to working memory. To clean it up after download finishes, we need to retract this fact from working memory. Here, we are looking at the same table but we are looking at only event of type end. We capture identifier of the signpost. And here, we are using the fact that signpost begin and end has to have the same identifier. We are looking in working memory for a fact that has a signpost identifier that we captured and retract this fact. Then, we can write our recorder rule that will generate all of the narrative data. This recorder rule looks at all of the started download facts and captures them. We captured time, caller address, and image name. If that's true and there is another started download which has the same caller address, you can notice that variables referenced here are the same and happened before the first fact. We notice that there is some anti-pattern and there is overlap in our request. We can then check whether we have access to downloader narrative schema, create new row in it, set time column to the time of the first fact, and set your narrative description. You'll output some information about the problem so that someone could debug it later. Now I can run Instrument against our app. Let's run it again. Choose Trailblazer Networking in template again, and record. I will try to perform some fast scrolling here and take a look at my narrative table. You can see that narrative table contains lots of diagnostic messages being outputted. So we can see that there are some problems and we can later investigate it. You can see that narrative is interactive detail. You could for example check all of the arguments being passed and you can filter. So we can add this caller address as a detail filter and have this detail filter. Now, let's back to Chad who will tell you more about some best practices when developing instruments. Alright. Thank you, Kacper. So we've seen how we can create some basic expert systems in Instruments. Alright, so let's talk about some best practices that we've learned along the way. And the first one is to write more than one instrument. Now I don't mean get practice writing instruments. What I mean is that if you own an instrument already and you want to add some features to it, sometimes it's really tempting to just add them, add extra graphs or details to your instrument, but you should really be thinking, you know, can this be its own instrument. And the reason for that is if you create finer-grained instruments, you give the users of Instruments a lot more choices. They can drag just the instruments that they want out of the library and that will minimize the recording impact on the target. If you focus on one instrument with lots of features, it's kind of like an all-or-nothing proposal there. Now, if you want to create a combination of instruments that are going to be targeted at a certain problem, you want to see all these instruments used together at the same time, rather, then what you can do is create your own custom template like we did for the networking. And so what you do to get that started on that is create a document, drag the instruments in the way that you want to see them, configure them, go to File and then say, "save as template." And then you can use that template inside your package using the element that Kacper had added to our Networking Template. So writing with more than one instrument is a lot better way to use tool. The second one is immediate mode is hard. Immediate mode refers to the recording mode of Instruments where we're visualizing the data as it's coming in, in near real-time, and the reason it's hard, well there's really two reasons it's hard. The first one is it requires some additional support that as much as we wanted to cover today, we just couldn't. We just didn't have the time. And so we're going to be working on the documentation for that. But the second reason, and this is the more important reason, is that, well it's interval data, right. So intervals can't be entered into the tables in the Analysis Core until they're closed, meaning that we've seen both the begin and the end. And so when you're looking at a recording live, you have a bunch of what are called open intervals. Now if your modelers require these as inputs, which is totally feasible, then what you'll notice is that if there's an open interval upstream, well all of the modeler clocks downstream have to stop until that interval is closed because remember, the modeler's vision is all in time order. So it can't move that clock forward until all those intervals upstream have closed. So if you have some intervals that have a long run, what you'll notice is that the output of your modeler appears to stop. And when the user hits the stop recording button, well then all open intervals close and everything processes as normal and the data fills in. But that's not a great user experience. So if you hit that, you have one of two options. The first one is to opt your instruments out of immediate mode support and you can do that by adding a limitation element to your instrument and the second is to move off the interval data as input to your modeler, just like we did in our demonstration here for our expert system. We were actually using the os signpost point events rather than using the intervals. So I know we make it look easy but immediate mode is a little tricky to implement. And then third, one of the things that's really important if you're creating instruments that are going to be targeting high volumes of input data is that the last five-second recording mode is by far the most efficient. Now the way you switch that is in the recording options of your trace document, you'll see that you have a choice between immediate, deferred, and this last end seconds mode. That is going to be a lot more efficient because what it allows the recording technology to do is use buffering to improve performance so that it's not trying to feed the data to Instruments in real time. Now this can have a profound effect and it can have a huge effect on signpost data where it can be up to ten times faster inside five-second mode. Now of course the trade-off is that you're only seeing the last five seconds of data but for instruments that produce high volumes of data, that's usually a good thing. So this is the common mode for a System Trace and a Metal System Trace and Game Performance template. And if you're targeting one of those kinds of applications, I would also opt your instrument out of supporting immediate mode just so that your user experience is not terrible or Instruments gets way behind on trying to get the data or you run into that problem with the intervals. That is the end of the session. So we did a lot of work here to create the Instruments feature and we're really, really excited that we were able to get it out to you guys this year. And so we can't wait to see what you guys are able to accomplish with it. So if you'd like to see us and talk to us about custom instruments, we have a lab in lab eight today at 3:00 and also Session 405 goes over in detail how to use os signpost API which is a great way to get data into Instruments. So enjoy the rest of the conference.  Good morning. Welcome to Create Your Own Swift Playgrounds Subscription. We hope you had a great time at the bash last night and we know you're all feeling bright eyed and bushy tailed and ready to learn about Swift Playgrounds. My name is Grace. And my name is Holly. I'm a software engineer on Swift Playgrounds. And I'm a software engineer on Xcode Source Editor. Swift Playgrounds is an app that allows anyone to code on the iPad using the Swift Programming Language. As authors, you can share your Playground Books with others by hosting your own subscription. Today we'll be talking about how to make a subscription starting from making Playgrounds and Playground books and moving all the way through hosting your content online as a feed for you subscribers. Today's session will be broken up into four parts. First we'll talk a quick look at the Playground book format and new Swift Playgrounds Author Template for Xcode 9.3. Then we'll look at the Swift Playgrounds subscription format and walk through how to host your own feed online. And I'm going to go watch you from offstage so I'll see you later. Grace and I have been working on a set of Playground books for all of you to explore the core image APIs. Today we're going to build another Playground book so that all of you can explore the CI filters, some of the built in ones. And here you can see me having a lot of fun going through the chapter of our book that teaches about the distortion filters, which are my personal favorites. Playground books are a mechanism for you to construct a guided interactive exploration of a particular topic. Using a Playground book you can guide your learners through a sequence of chapters, which are then further subdivided into pages. Each page contains a source editor, which you see here on the left. And you can also choose to implement an optional, always on live view, which you see here on the right. Live views are used for visualizing the results of running the code in the source editor on a page. To bundle all of this into a single file, we use a package based file format. This means that a Playground book is itself a file hierarchy. It contains a subdirectory for each chapter and nested within each chapter is a folder for each page in that chapter. And each Playground page folder contains all of the resources that are required to run the code in the source editor and to display the live view for that page. Each level in the Playground book hierarchy also contains a property list file for metadata that Swift Playground uses to configure the structure of the book and to construct the table of contents. And finally, in addition to the content that is shown to your learner in a page, Playground books may contain additional Swift files called auxiliary sources. And these are stored in folders names Sources. Auxiliary sources allow you to provide additional Swift code like classes or helper functions that can be used throughout the pages in your Playground book. You can use auxiliary source files to achieve many different tasks. And by utilizing these sources, you can greatly simplify the code that you need to write in each page in your book. Let's look at an example of using auxiliary sources to simplify communicating with the live view. You can send values to the live view by calling the send method from the Playground Live View Message Handler protocol, which is declared in the Playground Support framework. Messages are sent to the live view as Playground Values. Playground Value is an enum that is also declared in the Playground Support framework. To simplify converting objects to Playground values I wrote a protocol called PlaygroundValueConvertible in a file called LiveViewSupport.swift. And I include this file in the book-level sources folder of every Playground book that I write. Then for any type that I want to send to the live view, like a CI filter for example. I can extend that type to conform to PlaygroundValueConvertible and implement the asPlaygroundValue method. This method will convert the original object to a PlaygroundValue. I also wrote a helper function called sendValue, which takes in a PlaygroundValueConvertible and sends it to the current pages live view as a PlaygroundValue. And finally, when I have an object in a playground page that I want to send over to the live view, I can simply call sendValue and send in that object as the argument from a hidden-code block in that pagescontents.swift. So as you can see the code that I wrote in my auxiliary source file greatly simplified the code that I needed to write in my Playground page for sending a value over to the live view. For more information on the Playground book package, I recommend you take a look at the Playground Book Format Reference, which you can find on developer.apple.com. Now, let's take a look at the Swift Playgrounds Author Template, which you can use to get started authoring your Playground books. The Swift Playgrounds Author Template is a starter Xcode project that will help you create, debug, and produce a Playground book. Using the template you can step through the code for your live view as if it were an app so that you can identify bugs more easily and develop an efficient workflow for developing your Playground books. Here you can see me stepping through the code for the live view of the CI filter Playground book that I showed your earlier. The Swift Playgrounds Author Template is available for you to download from developer.apple.com. This Xcode project is compatible with Swift Playgrounds 2.1 and Xcode 9.3, both released earlier this year. This project includes some frameworks that were built using the Swift 4.1 Compiler since this is what Swift Playgrounds 2.1 currently uses. So it's really important that the version of Xcode that you use for this project also uses Swift 4.1. The Swift Playgrounds Author Template includes three different targets whose products will help you author your Playground books. The first target is called Playground Book. The second is called Book Sources. And the third is called Live View Test App. The Playground Book Target produces a Playground book as its output. And you can see this in the products group in the Xcode Project Navigator. All of the files for this target are in the PlaygroundBook group in Xcode. And the template contains the starter files for a playground book with one chapter and one page. The template also includes two book level auxiliary source files that provide a default implementation of the always on live view for the Playground book. Next, the Book Sources target compiles all of the book-level auxiliary sources. And you can see all of the files for this target in the sources group inside of the PlaygroundBook group in the Xcode Project Navigator. Compiling these sources allows you to develop your book-level sources with full editor integration. This means that you'll be able to use all of the Xcode source editor features available in any other Xcode project like code completion or quick help. And finally, the LiveViewTestApp uses this target for displaying the live view. So you're implementation for the live view of your book must be in your book-level sources if you want to use the test app for debugging. Lastly, we have the LiveViewTestApp Target. You can find all of the files for this target in the LiveViewTestApp group in Xcode. This target produces an app that displays the live view of your Playground book similarly to how it would be shown in Swift Playgrounds. As I mentioned before, the implementation for your live view must be in your book level auxiliary resources. The LiveViewTestApp allows you to debug your live view in a full screen or in a side-by-side mode without first having to export your Playground book to Swift Playgrounds. The LiveViewTestApp works on iPad and in the iOS Simulator. The Swift Playgrounds Author Template comes with three supporting frameworks. The first two are the PlaygroundSupport and the PlaygroundBluetooth Frameworks from Swift Playgrounds. Including these frameworks in the project allows the book sources and the LiveViewTestApp Targets to take full advantage of the APIs from these frameworks. And the third supporting frame work is called LiveViewHost. And this framework is used by LiveViewTestApp for displaying the live view. To implement your live view, you'll need to add all of your code in the book level auxiliary sources for your Playground book. Included in the template is a view controller for your live view in a file called LiveViewController.Swift. Also included in the template is a helper function for loading an instance of your live view in a file called LiveViewSupport.Swift. You can add any other files that you need to implement your live view and you can use any of the Playgrounds frameworks in your book sources code. Also included in the template is a storyboard for you to configure the UI for your live view. You can add any other resources that your Playground book needs in the private resources group. And finally, to configure your live view test for the test app, you'll need to implement a method in AppDelegate.Swift. Specifically, you'll need to implement the setUpLiveView method. By default, this method loads an instance of LiveViewController from LiveView.storybaord from the helper method that I mentioned in LiveViewSupport.swift. And this is called instantiateLiveView. Now I want to invite Grace back up to the stage and we'll take a look at the Playground book that we've been working on using the Swift Playgrounds Author Template. OK, so here we're looking at my Xcode project for the test out CI filters Playground book that I showed you a sneak peak of earlier. And the idea for this book is that a learner can create and manipulate core image filters in the Source Editor and these filters will be sent over to the live view and applied on top of live camera output for them to see what those filters look like. Here we are looking at the LiveViewController.Swift folder and I've implemented one of the methods that came in the template and below that I've added the rest of the code that I need to display the live camera output in the live view. As I mentioned, I also want to apply core image filters on top of each frame of the live camera output, so I've added a few more files in my book level sources and implemented a filter renderer for applying this filter on top of the live camera output. So now if I select the LiveViewTestApp Target, I can actually run this code and see what my live view will look like. And I've configured this app to show the live view in a side-by-side mode. So where we can see what the live view will look like in a side-by-side mode similarly to how it would be shown next to the Source Code Editor in Swift Playgrounds. And to simulate sending filters over to the live view, I've added a couple of buttons, which you see here on the left. And these are implemented in my test app code. So now if Grace taps on one of the filters, we should see that filter applied to the live camera output. So this looks great. I want to try out some of the other filters, so Grace I really like the CIPointillize filter. So if she taps on that this looks pretty close but if you can -- if you notice the pink filter from before is still around some of the edges and the pointillize filter is a little smaller than we want. So I'm going to head back over to Xcode because I think we have a bug in my filter renderer that I wrote in a function called Render. So I'm going to navigate to that function using Open Quickly with Command Shift O. And I'm going to set a breakpoint right before this method returns. And since each frame of the live camera output is getting a filter applied on top of it, we should hit this breakpoint right away because this method is constantly called. OK great. So we've hit the break point and now I can inspect any of my variables here using the debug console. So specifically I want too look at the size of the source image and the size of the filtered image and see how those differ. OK, so as we can see the size of the filtered image is larger than the size of the source image. And on line 59 I'm rendering the filtered image to the size of the filtered image, but really what I wanted was the size of the source image. So I'm going to fix that. Disable the breakpoint and run it again. Let's head back over to the iPad. And let's have Grace take the same steps as before. So let's tap on CIColorMatrix and then tap on CIPointillize and we see that the Pointillize filter is the correct size, it's the same size as the original live camera output, so now I think all of my code is correct so I actually want to package up my Playground group, copy it over to Playgrounds and see what this live view actually looks like. So if I head back over to Xcode and select the Playground Book Scheme, I can press Build, and then in the Project Navigator my Playground Book is under the Build Products Group. So at this point, you'll want to copy your Playground Group over to Swift Playgrounds using Airdrop or iCloud, but for the sake of time I've all ready done that. So if we head back over to the iPad and have Grace open Swift Playgrounds, we'll see that my test out CI Filter Playground Book is all ready there. So let's have Grace open it up and we'll see what our live view looks like. So as you can see we have a CI filter in the Source Editor and we've manipulated some of the input values for that filter. So if she taps Run My Code, we should see that filter applied on top of the camera output. OK, I really like this filter. I love this color. And hey Grace, do you know what we look like here? What's that? I think we look like engineers. Yes! Yes [laughing]. So you just saw us using the Swift Playgrounds Author Template to write and debug the live view for our Playground book and we hope that these new workflows will help you be more efficient in authoring your Playground books. Now I'm going to hand it back over to Grace and we'll learn about these Swift Playground Subscription. Awesome. Thank so much, Holly. So now that we're refreshed on the Playground Book format and have learned about the Swift Playgrounds Author Template, let's take a look at how to take those documents and put them into a subscription. So first off what is a subscription? You should think of a subscription like Podcasts or like a magazine subscription. So when new content is published by the author, subscribers will become aware of it. Subscriptions show up here along side Apple content. When you publish a new document to your subscription, Swift Playgrounds will fetch it and display it to the user as a another document to check out. Users can subscribe to your subscription by visiting your website. Subscriptions live online as a feed of downloadable content that you publish. The way that you publish your content will be through publishing JSON files that have all of the data that Swift Playgrounds needs to download your content. In case you haven't used it before, JSON is a techs-based document format for storing and exchanging data. The goal is to communicate to the consumer of the JSON object what value your content should have for each key. So for example in our subscription the key title has the value, WWDC Photo Filters. The Playground Feed Format is in two parts. One part to determine some of the overarching characteristics of your subscription and then one part the documents to tell us about teach of the documents within your subscription. So let's take a look at the Playground feed format that you'll use when creating your subscription. Our first key is Title. This value should be a string, the title of you subscription. Here's what it should look like in JSON and here's where the title shows up in your app within your subscription cell within Swift Playgrounds. The next key is the name of the publisher, and that's you. This keys value should also be a string and it will be the name of the publisher of the subscription. It will show on the detailed view of each of the documents within your subscription. Next is the feed identifier. This should be a reverse DNS string for the domain that's hosting your feed. We'll get to domains later in this session and reverse DNS just means that it will be that website backwards. So for example, if I'm hosting my feed at developer.apple.com, my feed identifier would become .apple.developer. The next key is contact URL. And this what will be used to report issues with your feed. Next is the feed format version. This will be the version of the feed format to which your feed conforms. This feed format that we're talking about right now is Version 1.0. And once updates are made to the feed format that you'd like to incorporate with your subscription, you can update your feed.JSON file with the changes and bump this format version up within your feed.JSON file. And remember that this is a string so it should go in quotes. And lastly we need our documents key. So this is the beginning of the second section of the Playground feed format. The value for this key should be an array of objects that represent your Playground Books within your feed. So next we'll talk about the format of these objects. Documents is an array of objects that represent documents with key value pairs. Let's go through what you need to represent a document and then we'll put it all together. Title and overviewSubtitle are strings. They should be the tile and the subtitle for each of your documents. And they show up within your subscription cell and also within Detail Views for each of your documents. Detail subtitle is an optional string that you can use to create a different subtitle to be shown in Detail Views like here. If you don't provide one, we'll just omit that section. Description is text that should describe the purpose of your document. And it will also show up within Detail Views and Swift Playgrounds. contentIdentifier is another reverse DNS identifier. It must be unique to the book and it must match the contentIdentifier within your books manifest.plistfile. It should start with the feed identifier of the feed that contains this book. So in this example io.github.WWDCPhotoFilters. You should host your content at the same place where you host your feed.JSON file. But if you don't that's OK, you'll just need to include a secure hash of your Zipped Playground Book as another key. And you can learn about this more in the feed format online. And there's the contentIdentifier. Content version is the way that you can do versioning with your book. You need to increment this when you update your book, and it always must match the content version in your book's manifest. So next is the URL. This is very important because it tells Swift Playgrounds where to download your book when your book when your user wants to download it. This should be a link to the zipped copy of your Playground Book. The additionalInformation array allows you to store additional metadata about your document. And you can store whatever you want here. This is structured as an array of objects with each object having key and value pairs. This example name, made for, and value WWDC would show up like this in Swift programs within the detailed view. You'll need to include the publish date and last updated date of your content. These dates need to be ISO 8601 formatted and should reflect the day that you publish this piece of content and the day that you last updated this piece of content. And lastly, there are three keys for images used in your subscription. The thumbnail URL would show up here within Swift Playgrounds, as well as within the detail view for any of your documents. Preview Image URLs would show up here in the detail view for each of your documents and can show users a little bit of extra information about your Playground. And in older versions of Swift Playgrounds, we used this as the banner image URL. So to support users that are using older versions of Swift Playgrounds, you'll also need to include this image. So OK, now that we have finished up with the feed format, let's jump into publishing. So you might be wondering, how do I publish all of these files that I've made? You'll need to put both your feed.JSON file and all of your documents that you've written on the web. For this, you'll need a web host. Publishing your subscription requires a web host. There are a number of different ways that you can do this like using GitHub pages, using Squarespace, and others, but for today we'll be working with an example of GitHub pages. GitHub pages is a tool that lets you create a website for you and your project and it's hosted directly from a GitHub repository. It uses Git to manage content. So let's walk through some of the steps that you'll need to take to use GitHub pages. First you will need a GitHub account. You can create one through GitHub's website. Next, you'll have to make a repository called username.github.io. It must be named in this way or else it won't work. And this will be the repository where you'll store your content. Once you've cloned that repository to your Mac, you can create an index.html file to serve as the homepage of your website. This one here will just display Hello WWDC. To learn more about source control workflows in Xcode, check out this session from earlier this week. After you've created that file, make sure it's in your repository, commit your changes, push, and then visit your website at username.github.io. As you continue adding more files and folders to your repository and continue to commit and push them up to your website, it will update your repository and therefore GitHub's website. So right now the folder only has one file, index.html. This will serve as your landing page for your website. But as we talked about earlier you'll need a couple more files to support your subscription. So as Holly mentioned, we have been working on a subscription called WWDC Photo Filters that has some really cool photo filtering content using Core Image within Swift Playgrounds. I'm going to walk you though the levels of our repository to show how we've organized it as an example. The first file that needs to be included is your feed.JSON file. We've put ours in our repository at the same level as our index.html file. And next you'll need to store all of your documents. Each playground that we've published is stored in a folder to keep our repository clean. We have a folder for each of our pieces of content. So ImageTransitions, IntrotoCI and so on. So at the top level of our repository we have our index.html file, our feed.JSON file and a folder for each document. If I open one of the folders for a document I see four things -- a zips.playgroundbook file and three images. These three images are a banner image, one preview image, and a thumbnail image. The zipped Playground book will be the Playground book that a user downloads. So let's look at how this compares with our feed.JSON file. At the top level we can see that the feed identifier is the reverse DNS of our website, io.github.wwdcPhotoFilters. In the documents array, there's an element for each of the four documents that we've posted. And if we look at our first document the contentIdentifier matches the feed identifier and adds the first book's title. For the URL of the book we have our website /IntrotoCI/IntrotoCI. playgroundbook.zip. And again this is really important to get right because it's where Swift Playgrounds will go to download your content. The URLs of your images can be relative URLs. So you only need to include the folder underneath where your feed.JSON file is stored and downwards. So here I just have IntrotoCI/thumbnail.png. And each of the documents in your subscription should mirror this format. Once everything is online, anyone can subscribe to your feed. They could do it by manually typing in the URL that points to your feed.JSON file, or you can use universal links so that a user can tap on a link from within Safari and get redirected to Swift Playgrounds. To do that you'll need to combine the URL for your feed and the Universal link prefix for Swift Playgrounds. So here's the code for the link in our index.html file. I've combined the universal link prefix, https/developer.apple.com/ul/ -- you get it [laughs], with our subscription URL. Then you can imbed that link in an anchor element with an href attribute so users can simply tap subscribe and be directed to Swift Playgrounds. So next I'd like to demo the steps that you should take to add a book to your subscription. OK, so like I said, we're working on this photo filtering subscription that I think is really cool. So I've actually just finished another book about content aware resizing. Content aware resizing is a way that you can resize your images without losing the most important parts of your image. So I have this photo of Monument Valley that I think is really beautiful, but I'd really like to trim it a little bit and make the width a little bit smaller. But I don't want to scale it down and I don't want to crop it because I don't want to miss out on the beautiful sunset and the monuments. With content aware resizing, I can take that photo and look through all of the different pixels in the photo, find the least important vertical seams, and remove those one-by-one so that the width gets a little smaller each time, but I still have the really important monuments and the sunset, the road, and everything is still there. So I've written all that code within a Playground book that I've called Content Aware Resizing. And it's stored within my Content Aware Resizing folder. So this is almost done. All I need to do is zip it up, so I'm going to select File, CompressContentAwareResizing. PlaygroundBook. So now I have a zipped file of my Playground book and I'm going to go ahead and move this to the Trash. And lastly, I'm going to need to move in a couple of images from the thumbnail and the banner image, the two required images within the feed format. So luckily I have two images on my desktop called Thumbnail.png and BannerImage.png. Adjust these two. Perfect. So I'll go ahead and drag that into my folder. So if I look within my WWDC Photo Filters Repository, I have three folders here right now and each of them have an image, a zipped Playground book, and then another image for the thumbnail. So this looks pretty good. I think my folder mirrors that perfectly. So I'm ready to put it up and publish it. So I've created a workspace for my repository. So I'm going to take this folder and drag it over, put it within my repository, and now I have another folder for ContentAwareResizing. Double check that everything is still there and it looks good. So the last step that we'll need to take is to update our feed.JSON file. So here's that file and you can see in My Documents array I have my Image Transitions object, my Intro to Core Image object, and my Test Out CI Filters Object. So I need to add one more object for ContentAwareResizing. And luckily I've made a snippet for it and here we go. So I've all ready filled most of this out because of my nails [laughs], so all I need to do is type in the title and I'm going to call this ContentAwareResizing. I've decided to omit the detailed subtitle and I just have my overview subtitle try out ContentAwareResizing with Swift. I have a description and my content identifier is io.github.wwdcphotofilters. ContentAwareResizing, the title of my book. It's the first time I'm publishing it so my content version is 1.0. And my URL maps to my website and then I have my folder name, ContentAwareResizing -- /ContentAwareResizing. PlaygroundBook.zip. My publish date is right now at 9:40 [laughs]. And my last updated date is also at 9:40 since publishing it counts as an update. I've also included my thumbnail URL and my Banner Image URL. For additional information I have English as a language. And my Preview Image URLs I've decided to leave blank for now. All right, so this all looks good and now all I need to do is going to Xcode Source Control, Commit, make sure all four of these files are selected. So I've added three files and I modified feed.JSON. I'm going to commit and say Adding Our Demo Book. Select Push to Remote. Cross my fingers. Invite Holly back on stage. And, great. It worked. Cool. So now I'm going to have Holly go to our website and subscribe from there. So awesome, we have our website here and you can see we have a nice subscribe button. So if Holly taps that she'll be navigated to Swift Playgrounds, Subscribe-- Restart QuickTime -- There she is [laughs]. Well, anyway, it's a really cool book [laughs]. And you can all download it actually as well because we've published it to our subscription now. It's online at WWDCPhotoFilters.GitHub.io and so you can download all of the four books that we've published. One is Intro to Core Image. One is Image Transitions, using the built in transition filters within Core Image. One is Combining Filters within Core Image. You can add them on top of each other. And then the last one is this Content Aware Resizing book. [ Cheering ] Thank you. And check it out [laughs]. Well, in a second. There we go. There's our subscription [laughing]. So now if Holly downloads Content Aware Resizing -- And opens it up. Cool, it worked. So, in the live view on the right I have this beautiful picture of Monument Valley and I'm going to have Holly crop it a little bit and then run the code and we'll see what it produces. So what this algorithm does is it looks at all of the vertical seams in an image, decides which of the seams are least different from the ones around it and removes those one by one. So now we can see it's a little bit cropped and it looks beautiful. That's all we have for today. For more information please come visit us in our lab Creating Content for Swift Playgrounds today at noon. And for more information on this session and on our subscription, you can visit the Session 413 website on developer.apple.com. Thanks for joining us today. We can't wait to subscribe to all of your published playgrounds. Thank you.  Thank you. Good afternoon. And welcome. My name is Michele. And I work in the iOS User Notifications team. Just a few minutes ago, my coworkers Kritarth and Teja [phonetic names] introduced you to group notifications. And in this session, we're going to explore a little bit more detail in-- this feature in a little bit more detail. And we'll learn how to use it and how to improve the efficiency of your notification and how to make them better organized. We will start by a brief overview of how the feature works and how the different parts of the UI work. Then we'll look at the default grouping, app grouping. We will see how to create custom groups for your notifications so that you can adapt the groups to the content of your apps. And finally, we will learn how to create custom summaries for your new notification groups. Let's start with a brief overview of the UI of the feature. Notification groups collect sets of different notifications together and in a group, and group them in a small stack. This helps making Notification Center better organized and more efficient for the user so that he can see more notifications at the same time. Without any app taking over the entire screen and preventing them from seeing some of the content. Each notification group shows the most recent notification that was delivered to that group at the top. And we call these the leading notification. Right underneath each group shows a brief summary that gives a little bit more details about what are the notifications included in the group. Gives you a number for, so you have a measure of how many notifications are inside. And in some cases, also some details about the content of those notifications. Notification groups also help in triaging notification. Because for example you can clear them, all the notification in that group with just one slide. And clear them all. But it's just as easy to expand the group and see all the notifications that are inside. And read one of them at a time and maybe clear one. And then when you're done, you can clear all of them, all of the group using the group buttons at the top. This was a very brief overview of the feature just to get more accustomed to the different parts of the UI and the terminology. So now allow, now let's see how the groups work and how they are created. The default behavior is app grouping. Each app gets its own group. And all the notifications from that app go to that group. This is the behavior that you get if you don't do anything. And you just keep sending notifications without adopting any new APIs. In many situations, app groups are sufficient, and they work very well. For example, as you see here in the Podcasts app, all the notifications are in the same group because Podcasts sends notifications that are very similar. They're notifications for new episodes of [inaudible] the shows. So there's no specific notification that is more interesting or something that is more important to show differently. As I said, as we said, it's very easy to tap on the group, expand it, and see the details inside the group. So app grouping is good. It keeps notifications more organized and Notification Center more compact and easier to use. But many apps have more specific content. They may need different organization to make notifications more efficient. And so we can create custom grouping. So in this session we will see first the API to create custom groups for your notifications. And then we'll look at a few examples of apps in iOS 12 to explore some of the patterns that they applied, and how they tried to balance the amount of content that was visible to the user. And the amount of content that was grouped together to help with the organization of Notification Center. So this is how you create a group for your app. You just set a custom thread identifier in the notification content. After you've done this, all the notifications that you send with the same thread identifier will be grouped together in Notification Center. The thread identifier can be any string. Doesn't matter what you said there. But all of the notification will be grouped together. You just need to be unique. You just need a unique string that is identifier's group. And that is it. You created a groups for your notifications. But given that it is so simple, we need to pay attention at how we create these groups. And also, if you're familiar with notifications API, you notice that this is not a new API. This is an API that we introduced in previous iOS releases. So it's a really existing and current iOS. We introduced it to, for [inaudible] notifications and for private notifications. So if you already adopted this API to support these two other features in previous iOS releases. Your notifications are already grouping today in iOS 12. You may want to review how the groups are created since now the context is a little bit different. So it may be useful to adjust them a little bit. So in this example, you see how to create a thread, set a thread identifier on a local notification. And of course you can set it on a push payload for push notifications. Okay. No, now let's go over a few examples from iOS 12. The most important thing to keep in mind as we go through these examples is the goal on notification grouping. When grouping-- by grouping notifications we want to make notifications more efficient for the user. And improve the organization in Notification Center. I'm sure you're familiar with the situation where one app has sent many notifications. Maybe you're in a chatty, messages conversations and someone is sending a lot of messages. We want to improve that situation by organizing notifications a little bit better so that the user can use them more effectively. Our first example is Calendar. Depending on how you organize your life or where you work, Calendar may send you a lot of notifications for event changes, event invites. But not all of them have the same importance. There are some notifications that are sent by Calendar that are more important than others. And they are event alerts that you set up when you create an event. Or time to leave notifications. These are more important because are notifications that I need to act on right now. If I receive a notification that I have a meeting in 15 minutes, I need to start walking, because I have to go to the meeting. But many of the other notifications are not something that need to be responded to immediately. There may be updates to share Calendars. There may be time changes in other events that are later in the day or some other day. And so it's useful to separate them. So what Calendar is doing is using the default group, the app group, for the bulk of the notification that it sends. And it does that by not setting any thread identifier on a majority of its notifications. Mail is a default value. So if you don't set it, that's a default group. And then Calendar sets a specific thread identifier just for these notifications that are more important and that it wants to call out. So the result is that most of the notifications from Calendar that are information updates that are not something that I need to react to immediately. Or that I don't need to have in reference later. For example, to know where the event is. They're grouped together in one group. And the other notifications that are the more important ones, more urgent or the ones that I need to reference maybe later to find the meeting room, they're separate than this other one. So this is-- just expand and I see what else is in my group. And this is the pattern that we learned from Calendar. Separate important actional notification from informative updates. Our next example is Messages. Messages is maybe the most straightforward notification group's implementation. It's pretty obvious. Messages has conversations. And each conversation has its own group. But we still, we can still learn something important. We can still learn a good lesson from Messages. Why is it that Messages separate all these groups in different conv-- different conversation in different groups? Well notifications that Messages sends are generally from people. From your friends, from your family. They're notifications that we care about, that are important. And second they're notifications that usually are short-lived. Because usually you respond pretty quickly at a Message. And it will go away from Notifications Center. So what Messages does is it creates a thread for these one-to-one conversation. And it creates a separate notification group for these group conversation. Just generates any group identifier for that thread and they get grouped together. It can expand and see, still see all my messages. So what does-- what is the lesson that we learned from Messages? Create groups for meaningful, personal communications. These notifications from Messages are usually important. And they live very shortly in Notification Center. So we can create many separate groups for all of them. Our final example is Mail. And Mail shares some of the same characteristics that Messages has. So they're often direct communications with people. They have a same, a similar concept of threads. Conversations. But there is some major differences between Mail and Messages. Mail can have an even higher volume of notifications than messages. And on the other hand, they can also be more long-lived. Mail, email is usually used for slower communications. Something that I don't expect to respond to immediately. And so organizing Mail threads, organizing Mail notifications by thread will not be ideal. It will create a lot of threads in Notification Center. And the UI to see in those thread is not very efficient. Mail provides a specific dedicated UI to displaying mail threads. So how is Mail organizing its notifications? Well, first of all, Mail provides some features to already organize and prioritize the emails that arrive in your inbox. Mail provides separate accounts. You can have many different accounts set up in your device. Provides VIP. You can set any contact as VIP. You can create-- you can have favorite folders. And you can turn on notifications for specific threads inside your mail client. And so it seems like the user is already telling us which notifications are more important. Which email messages are more important and which are less important. And that is what Mail does. Mail organizes notifications first by starting with the account group. It creates one big account for all the emails that arrive in a specific account. If I had multiple accounts, each one would get its own group. But then if a contact that is in my VIP contact sends an email to that account. That email is separated out to a different group because I said that's a VIP person. That's a contract that I want to be informed about immediately when it sends an email. And if I turn on thread notifications for some specific threads inside of Mail, also, those also get separated out in their own group. So you see that while Mail shares some of the same characteristics as Messages, the approach of grouping notification is a little different. Because the use of the notification and of the app and the content is different. So what is Mail telling us here? Respect the user's priorities and organization. Mail provides feature to organize email and to prioritize the content. So we can use that to organize notifications that Mail sends. So now that we've seen how to create as many groups as we want, we'll see how to make these groups a little bit more clear. By providing some bits of information in the group. That describes the content that they contain. As we see in these example of my Notification Center from last week. Each group has a little short summary at the bottom. And it explains a little bit of what that group is about. For example, you see Mail says that I have a few emails from in my work account. Podcasts is telling me that I have a bunch of new episodes that were delivered to my Podcasts app to listen. And News tells me I have nine more notifications from National Geographic. Let's explore this one a little bit. If your nine notifications said default message that we set in groups if you don't customize your summary. But you can better describe your content. For example, if you have an app that sends messages. You may want to see, you may want to say there that you're sending nine more messages. So how you do that? The first thing you have to do is to pick a summary format. And this is a format string that just describes your content with a numerical placeholder. As we see here, number, more messages. And then you set this format string in the notification category that you're going to use to send a notification. If you notice, the summary format is set on the category. Whereas the thread was set on the notification content. This is because categories as you know are types, groups, of notifications that you send that are similar. For example, messages has different categories for one-to-one conversations. And group conversations. And this means that they can set different summaries for those two categories. That is why the summary format is set in the category here. And while you're here updating your notification category for notification groups, you can also set this hidden previous placeholder property which is very similar to the summary that we used for notification groups. The major difference between these two is the context in which they're used. The hidden previous placeholder is not a new feature. It's a feature that we introduced last year in iOS 11. And this customizes the text that is displayed instead of the notification when the user sets the notification to be private. If I set a notification to be private, I say nine more messages. But then when I authenticate and I unlock my device, I can see the expanding notification and the summary underneath that says eight more messages. And that is why the summary format has more messages. And the preview placeholder has only the number of messages. So that was the setup for a basic summary. But Messages for group conversation has a more interesting summary, where it says the number of messages. But also the people that sent those messages in that thread in that conversation. And we cannot do that with the formula that we just specified because it only has a numerical placeholder. So what we do is we create a different summary format that contains a numerical string-- a numerical placeholder and a string placeholder. As you see here, number, more messages from a string. And we will replace the second placeholder with a list of names that are in the notifications. Still set the summary format in the category. And then we have to collect these names. To send these names, you set them in the notification content again. Because each notification can be sent by a different person. And they can be different names. So we will collect all of those names, build them up in a string and replace them in the summary. Oh, this is the push payload of course also supports summary arguments. And this is how the summary looks after we collected all the names and formatted them in the summary argument. The names don't need to be unique. You can send many notifications with the same name. And in this, in the Mail case, for example, all the notifications have the same name because they're all in the same account. We will handle de-duplicating them and will display only one instance for each single name that is visible. Next example is Podcasts. Podcasts shows us another small detail about this API. What is special about this notification? In this notification, Podcasts is saying that there are two new episodes available in my leading notification Podcasts. And the summary says that there are seven more episodes from a number of shows. And what happens when I expand this group? What happens is that there are only three external notifications in the group. Not seven like the summary said. And in previous examples, the number in the summary matched the number of notifications that were contained in the group. So what is happening here? Well, since Podcasts does these bundling on notification and they try to limit the number of notifications they send. When a show releases multiple episodes at the same time. They have these notifications that say, "Two new episodes are available, three new episodes are available." And if you sum the number, the number of episodes in the bottom three notifications, there were the ones that accounted for the summary, you can see that the total is seven. So let's see how the API for this works. It's just another property on the notification content and it's called summary argument count. And this count expresses the number of items that summary argument account, accounts for. In the summary. We will collect again as before all the names. We will sum up all the counts. And we will create the summary. And again as we can see, these notification would have summary argument count three, one, three, total is seven. And that's the summary that we displayed. And as everything else, of course you can also set it in the push payload. The summary argument count is optional. Defaults to one so you can only need to set it if you're doing this type of bundling on notifications. You don't need to set it always. Now, we are playing with words and we are building sentences. And when you do that, you always need to account for different languages and you need to account for plurals in this case. Because we have numbers. Again, another Podcast notification. For example, in this case. It says, "Four more episodes." But if I only had one more episode, that string would need to be different. But the API only allows us to set one string. And now we need two. You need two in English. But if you want to start to localize your app or if your main language is not English, you're writing up for a different language. Some languages have different rules for plurals. They may not have two cases. They may have three. And they may have cases that apply with different rules. So iOS in the foundation frameworks provides support for localizing these kind of strings. So you don't need to learn all the rules. And you don't need to know all the languages that you're going to translate your app to. And it's very simple to adopt and translate each string to have the correct plural form. The first thing that we need to do to adopt this 'fix our problem with plurals' is to replace the literal string that we use for the summary format, with a localized string. And remember here that you need to use our special notification API for localized strings, because we need to store these localized string and set it aside. Because in the case that the language of the system changes later. If you send notifications, we send them with the correct localization after the system localization change. So after you set a localized string here, you need to localize the string. And you localize the string in a strings dict file. A strings dict file is a property list file that describes a performance string and a configuration dictionary. In the configuration dictionary here at the bottom, you see the two versions of the string for singular and plural in English. But as I said, all you need to do to support other languages and different pluralization rules is change this stings dict file. You create a new one. And you see here Hebrew has three different cases for plurals. And Russian has three different ones and they're different from Hebrew. And you don't need to know which one to use when. If you use the foundation API that we provide. This is for the simple summary format that we just saw. But of course we can also format summaries with arguments. You see we define the format string at the top. We need to match the configuration dictionary by key. And underneath we specify the two strings for the two different versions. With the numerical placeholder and the placeholder for the list that we're going to provide with names. So since we're exchanging these format strings between apps and the system. These are effectively part of the API. So there's only a limited number of formats that we can support. And we need to agree on those. And they are the ones that I just showed you in these examples. The first format that you can use is the one with one numerical placeholder. When you don't need the arguments and you need to specify an unsigned number. And the second format that you need, you can use is the one with the numerical placeholder and the string placeholder. We will automatically detect which string you are using and format them correctly. Now we're all done with the features of notification grouping. But before you leave, I want to give you a couple more tips about notification grouping and these API that will help you finish the last bit of polish in your app. The first step is about combining different summaries. Since we saw that you can set different summaries in different categories. And the thread group are defined in the notification content. That means that you can mix in the same group notifications that have different summaries. And what happens when you do that? So there's two main cases. If none of the summaries in that group have any arguments, we'll try to combine all the summaries in the list and display them like this. But if any of the notification summaries in that group have any arguments, we're going to have to fall back to the default message. The second tip is about enriched notifications and groups. So we introduced enriched notifications a while ago in iOS. And they continued working in with group notifications. And when the user presses on a group to see the enriched notification that loads your content extension for that group, what happens is that your notification will be loaded with the leading notification. The one that is displayed at the top, and you receive it in notification using the usual [inaudible] notification API. Once the extension is loaded, you can load additional notifications. For example, the same ones that are in the group or if you want to display different content, you can also load it with your own API. Then while your extension is open and is running, if additional notifications are delivered to the same group, so they have the same thread identifier, they will be delivered to your content extension using the same did received notification method that you receive at the beginning. And lastly, if you display, if you already displayed additional notifications to the user by loading them from the delivery notifications. Or by loading them from your own API, you should remove them from Notification Center. To continue keeping Notification Center organized and more efficient. I want to recap a little what we talked about. It was a long presentation, and there were a lot of details. But I only have two very important things that I want you to remember here. The first one is the goal of notification grouping. The goal of notification grouping is to better organize Notification Center and help your users be more efficient in when they're using your notifications. This will help them triage them, and will have them receiving important information more quickly and react better. And the second thing I want you to remember is to add custom summaries to improve clarity. As you see in my examples, for example in podcasts or in Mail, just that little bit of text underneath the notification group will help to see what else is new in that group. And I don't need to open the notification group just to see everything about the content. The summary already gives some information. We will have a lab shortly right after this session, downstairs. We will have another lab tomorrow morning, and we look forward to talking to you. And on Friday, there is another session about notifications that talks a little bit more about the design aspects. About how to think about your notifications globally across all the user devices and across different systems. Thank you very much, and I'll see you later.  Good morning. My name is Keith Rauenbuehler, and I'm from the HomeKit Engineering team. Now, today we're here to talk about HomeKit with a focus on three specific areas. First, we'll give an overview of HomeKit and the Home app. Then, we'll outline what it takes to build a HomeKit accessory. And finally, we'll look at some HomeKit APIs and how to use them. So let's jump right in. So the best way to really understand the power of HomeKit is to take a look at the Home app. It's a single app interface to all of your HomeKit accessories. Now, there are a ton of capabilities packed into the Home app, and we'll use this session today to talk about some of the most important ones. And the Home app currently exists on iPad, iPhone, and Apple Watch. And the Home app has three basic tabs. There's the Home tab, which is where you'll find your favorite HomeKit accessories and your scenes. There's the Rooms tab, which is a great, it's a great tab to dig into a specific room and view all of the accessories associated with that room. And the Automations tab, which is where you'll be able to set up and manage your smart home automations. Now, controlling accessory in the Home app is simple. To toggle the state of it, just tap on the tile. So if you want to turn on a light, all you need to do is tap on the tile for it. Now, if you want finer-grain control, you can use 3D touch to bring up our quick controls. And so if we go back to that same light and we want to adjust the overall brightness -- say it's a dimmer switch -- we can 3D press on the tile, and we can bring up our quick controls. Now, we've also added scenes to the Home app, and scenes are a great way to allow you control-- allow you to control multiple accessories with a single tap. A scene is our simple interface for tying multiple accessories together, whether it's kicking off your morning routine or setting up your living room to the perfect movie-watching environment. Now, throughout the day, your home is always changing. People may come and go. Lights might turn on or off. Blinds can open and close. And scenes allow you to basically specify a desired state for all these accessories. So you can have a good night scene that turns off the lights, closes the blinds, locks the door, and sets the thermostat. Then, when you're ready for bed, you simply tap on the tile for "Good Night," and HomeKit prepares your house for the evening. Now, in iOS, we also have quick access to Home shortcuts via Control Center. This is a great way to view and control your favorite HomeKit accessories, and it even provides access to your scenes. Now, this is very-- really convenient, but another great way to control HomeKit accessories is through Siri. You can tell Siri good morning, and that will kick off your good morning scene. You can also use Siri to control individual HomeKit accessories. So you can use Siri to turn on your light in the kitchen or to make sure your door is locked. You could even use Siri to bring up your favorite HomeKit camera stream. Now, up until now, HomeKit has only existed on iOS, watchOS, and tvOS. But this year, we're excited to announce that we're bringing the Home app to the Mac. Now, we're really excited about this, as it's a great way to control your home using your computer, all without taking your phone out of your pocket. It's also a great way to monitor your home because you'll get your HomeKit notifications right on your Mac. And we've even built HomeKit Siri integration on the Mac as well, so you can use all of your favorite Siri triggers to control your accessories directly from your computer. Now, one of Apple's newest products, the HomePod, is my favorite way to control my HomeKit accessories. Not only is it a great music speaker, it's also a great way to control your HomeKit accessories because anyone in your house can use it using just their voice. This hands-free Siri interface allows you to quickly and easily control these accessories in your home. It's really simple to get started, and I love using my HomePod to control the lights in my house. It's especially convenient when I walk into the kitchen with my arms full of stuff, and I can use my voice to tell Siri to turn on the lights. Now, we've also gotten a lot of feedback from users saying they love using their HomePod to trigger their good night scene. They simply tell Siri good night as they head off to bed. Now, the HomePod also plays a much more powerful role in the home, as it helps control remote access and automations. Now, remote access is one of my favorite features in HomeKit, as it allows you to monitor your home when you're not there. So you can have peace of mind, and you can check to make sure your garage door is closed or make sure your thermostat's turned off when you're not at home. And you can do this all while being remote. Now, this feature requires a home hub, but HomePod, Apple TV, and iPad have the home hub functionality built right in. So with remote access, you can keep tabs on your home while you're away. And you may also want to share some of this support from this control with other users. With HomeKit, you have control over granting remote access to some users and then specifying other users to only control accessories while locally. So let's talk a little bit about how remote access works. So when you're in your home, your Apple device communicates directly with your accessories. But when you leave your home, you'll need a home hub -- again, this is a HomePod, an Apple TV, or an iPad -- and your iPhone will communicate with your home hub, and your home hub will talk to your accessories directly. You can look at the latest state of these accessories. You can view the stream from one of your cameras. You can even lock your door if you've forgotten to secure your house before you left for the day. Now, we saw earlier how easy it to leverage scenes to get devices to work together. Now, scenes are great, but automations allow you to run scenes automatically. So you can do so without launching the Home app or using your voice. Automations allow you to control individual accessories or entire scenes based on some key triggers -- things like someone arriving home or having it run at a specific time of the day. You can even use other accessories in the house to trigger automations. Automations can be as simple as turning on the back porch light when the back door opens or turning down the air conditioner when someone leaves. Now, we also support more complex automations so you can open your garage door, turn on your lights, open your shades, and set your thermostat to just the right temperature all as you pull into your driveway. Now, this is one of my favorite automations, as I'm frequently juggling groceries, or kids, or even finishing up a phone call as I get home. This allows me to get in my house without needing to fumble with my keys as well. Now, one of the key things that HomeKit provides to the customer is an incredibly easy way to add an accessory to your home and to your home network. Other services often require downloading an app, following custom instructions, and jumping through a lot of hoops to get an accessory on your network. Now, if you've ever set up a non-HomeKit accessory, you know just how frustrating of an experience this can be. With HomeKit, the add accessory flow is really simple. All you do is launch the Home app and scan the accessory code. HomeKit handles the rest. It's really that easy. And accessories can also use NFC to make this setup even easier. So you can launch the Home app, tap on the accessory with your phone, and you're done. Now, the great thing about HomeKit is that it's both secure and private. All accessory communication is encrypted, and we create a new shared secret for each session. The data is also private, so Apple doesn't have access to your camera streams. Apple can't listen in on your conversations. Apple can't even tell if you have a light bulb turned on or off. We know your home is private, and we want to keep it that way. So you can monitor your home, control accessories, execute scenes, and even use full Siri support all in a secure and private way. We want this communication to be super secure, and so all messaging is done directly between the Apple device and the accessory. So your accessory never needs to talk to the cloud. In addition to this being secure, it's also really fast because all the messaging is done locally and there's no need to round trip to a server. Now, our HomeKit Accessory Protocol is the communication channel for secure messaging between an accessory and your Apple device. These messages are encrypted so that only the trusted devices can read them. This means that if someone else, something does intercept the message, they can't decrypt it and understand what it says. And the HomeKit Accessory Protocol works over Wi-Fi and Bluetooth Low Energy. And by IP, we mean Wi-Fi and Ethernet. And Wi-Fi's great because it provides reliable, fast, long-range support, so you can position Wi-Fi accessories anywhere in your house. And we also support Bluetooth Low Energy. This is also great because -- especially for accessories that are battery operated. So for things like sensors, you can use Bluetooth Low Energy, and you can position these sensors in convenient locations because they don't require a power cord. Now, we want everyone to experience home automation, and so we provide lots of accessory categories, and we have some of the top manufacturers in each category. Now, we have an ever-growing list of categories, and we support multiple manufacturers in these categories. So you can find the one that's right for you. With HomeKit, you can even combine accessories in the same category from different manufacturers and have them work together. HomeKit makes it easy for you to choose the accessory that's right for your home. Now, with AirPlay 2, we've added support for adding speakers to the Home app. And in iOS 12, we're also releasing support for remote control systems. Now you can create a remote control accessory that's able to control your Apple TV, including support for Siri. Now that we've given a quick recap of the benefits of using HomeKit, let's dive in and see what resources are available for making a HomeKit accessory. So the easiest way to get started with HomeKit and building an accessory is to become an MFI licensee. It's a great program with lots of valuable tools that I can't wait to tell you all about. Now, if you just want to learn about the HomeKit Accessory Protocol, we also have that available for developers. Now, this is for noncommercial use to explore and experiment with the HomeKit Accessory Protocol. So this is great for learning about the HomeKit Accessory Protocol, but if you're serious about building a HomeKit accessory, we really encourage you to join the MFI program because it's the best way to get off to a quick start. So let's talk a little bit about, what are the benefits of being part of the MFI program? So becoming an MFI is really easy, and you'll get lots of tools and resources. And one of the newest tools that we're really excited about is the HomeKit Accessory Development Kit. This is an easy way to get started on building a HomeKit accessory. Now, this Dis a set of code that runs on the accessory to help abstract away a lot of the implementation details that you previously have to worry about. Now, we also have tools to help with certification, and we also provide access to documentation to help you get off the ground. You'll also get a copy of the specification that has all the commercial support, including how to build authentication right into your accessory. Now, the ADK makes it easy than ever to build a HomeKit accessory. Previously, you would need to worry about building your accessory logic, adopting the protocol, implementing crypto, adopting WAC, all these other steps. With the accessory development kit, we've abstracted all this away for you. So we provide the platform logic for your specific accessory, and the HomeKit Accessory Development Kit handles a lot of the details. This means you only need to worry about that accessory logic, which is the part that's unique to your accessory and is the part of the code that you care about. Now, the HomeKit ADK is new, but we've already gotten a ton of great feedback from our partners on how simple it is to use. MFI licensees are finding it easier than ever to get started, and we're seeing that the ramp-up time for new partners is faster than ever before. And of course, the ADK is reliable and secure, as we've tested it thoroughly internally, and it's been reviewed by our security folks. Now, the really nice thing about the ADK is that we'll continue to update it throughout the year, so whenever there's an iOS update that releases new support for accessories or to the spec, we'll also rev the ADK so that it'll stay in sync. So how much faster is it to use the ADK? Well, with the ADK, you can build a functional prototype and have it up and running within just a week. But let's look at how that impacts the overall timeline. So previously, our partners were spending about 6 months for development, and then they'd have to do integration, certification before they could ship their product. We were finding this would often take about a year to complete. So with the ADK, we're seeing that timeline get dramatically decreased. Based on feedback from our partners, it's now possible to ship out HomeKit accessory within just 3 months. The ADK makes it easier than ever to build a HomeKit accessory. To learn more about it, join the MFI program. You'll have access to all the ADK information that our licensees are already using today to build great accessories. Now, I mentioned authentication before, so let's talk a little bit about that. So this is an important part of the MFI program. Now, our customers trust when they see that Apple logo. They trust that they're buying a high-quality product. So when they buy an accessory that works with Apple HomeKit, they trust that their accessory has been tested thoroughly and reviewed by Apple. Now, this process is similar to the App Store review process, so on the HomeKit side, we have a way of reviewing accessories. This is so they, we ensure that they meet our high bar of quality and that they're ready to be shipped to customers. Now, once an accessory is certified, it uses our authentication stack for onboarding. This helps guarantee that a customer is getting that high-quality product. Now, the authentication itself can be done using our hardware chip or via our new software-based authentication. Now, as an MFI licensee, you'll also have access to manufacturing partners that can help get your product to market. So if you're sitting in the crowd today with a great idea for a HomeKit accessory, this is your big opportunity to bring it to the home. Now, we've always supported hardware-based authentication, but we have also recently began supporting software-based authentication. Now, customers can be guaranteed the same level of quality and trust, and the integration support can be rolled out via a firmware update. Now, this is great because this means you can roll out HomeKit to accessories that are already shipping to customers. Now, there are three other great tools that I want to talk about this morning. The first is a resource for accessory developers, and this is available for MFI licensees, and this is the HomeKit Accessory Tester. Now, this is an app that allows an easy interface for helping you test your accessory. This is a great way to validate that your accessory is ready to be submitted to the review process. Now, the second tool I want to talk about is the HomeKit Certification Assistant. Now, this resource is also available to MFI licensees, which is another great benefit of being part of the program. And this is built on top of that HomeKit Accessory Tester. Now, this allows you to self-certify your accessory and even has support for automating some of that testing process. And the third tool I want to talk about is the HomeKit Accessory Simulator. Now, this is a tool that's created for app developers, and so it will simulate various accessories. Now, this allows you to test your app and make sure it plays with all the HomeKit accessory categories. Now, the really nice thing about this is it means you don't need to go out and buy each one of these accessories to do your testing. You can get this tool available through Xcode and also through the MFI program. And this is actually one of the tools that we use internally, as it's a great way to quickly and easily set up a complex home with lots of accessories. Now, of course, you'll also want a test with real-world accessories as well. Now, we talked quite a bit about making a HomeKit accessory, but what about making a HomeKit app? So let's switch gears and talk about what it takes to build a HomeKit application. So there are lots of apps out there on the App Store. How do you make yours stand out? Well, for HomeKit, it's important to think about your target audience. So that might mean extending custom functionality, like tuning or calibrating an accessory, or maybe it means you live in a region where there's some functionality that's specific to your locale that doesn't already exist in the App Store. And if you're an MFI licensee writing an app for your accessory, you may what to be able to support custom services and characteristics. Now, with HomeKit, there's a lot of APIs that I'm going to talk about today, but before I get into the details, I think it's important to take a look at a high level at the object hierarchy. So the first class that I want to tell you about is this core object, which is the HMHome. Now, this is corresponds to the overall home, and within the HMHome, you can also optionally have HMZones. Now, these are things like upstairs and downstairs. Now, you could also have HMRooms, and these are the rooms that are normally associated with the home -- the kitchen, the living room, the dining room, the bedroom. And within those rooms, you can have HMAccessories. And these are the lights, the fans, the cameras, the locks, all the normal accessories we support. So your home, the HMHome contains HMZones. The HMZone contains HMRooms. And the HMRoom contains HMAccessories. Now, the home has a handful of properties on it, but the home is really used as the central organizing object for HomeKit. So the home is how you add accessories, configure rooms, create scenes, and build automations. But some users may have more than one home, and so you'll need to use the HMHomeManager to help manage these homes. So if you want to add a new home or remove an existing home, that's done through the HMHomeManager APIs. Now, the great thing about HomeKit is you can extend home control to other users. We also have access control capabilities so you can give a spouse or a roommate full administrative privileges. Now, as an administrator, they, these users can add accessories, edit scenes. They basically have full control over the home. But you may only want to give some users normal access to the home and only allow them to control accessories and not edit them. And those are non-administrators. Now, the important take-away here is that if you're building an app, you want to make sure that if your HM user is not an administrator that you disable some key functionality so that it doesn't fail when they go try and use it. Again, all this user management is done through the HMHome object, and you'll quickly see that this is a common design pattern we use where the HMHome exposes a lot of the basic functionality. So let's take a look at some sample code and see what this looks like. So here we have a class where we will instantiate an HMHomeManager. Now, we want to add a home to that home manager, so we'll use the addHome APIs and specify the name of the home. Now, we want to be sure to set ourselves as the delegate, so we'll get the appropriate delegate callbacks. And finally, if we want to configure the users for the home, we'll call manageUsers on HMHome. Now, I mentioned HMZones earlier, so let's take a little peek at that. So this is a way of grouping multiple rooms together, like upstairs and downstairs, or you can group all the bedrooms together in a single zone. So it has some basic properties on it like name and access to the rooms, but it also has this unique identifier property. And this could often be overlooked, but this is a way that you keep track of which zone you're, you care about. So when the name changes or the rooms change, you still have an access to that specific zone. Now, on the HMRoom object, we also support exposing the name and the list of the accessories, and it also contains this unique identifier property so you can keep track of rooms as well. Again, the rooms and zones are managed through the HMHome APIs, so if you want to add or remove a room or if you want to add or remove a zone, you do through, do that through the HMHome APIs. So if we go back to some sample code, let's take a look at this [inaudible] in action. So here we are adding a room to the home with our roomName. And then, we're adding a zone to the home with a zoneName. And then, we associate that room to the zone. Now, the next level of object hierarchy that's important is the relationship between accessories, services, and characteristics. Now, this can sound confusing, as accessories contain services and services contain characteristics. So it's best to look at a real-world example to give you an idea how, really how this fits together. So let's say you have a ceiling fan fixture. Now, this is a single accessory, and it has a light, and it has a fan. So this maps to a light service and a fan service. Now, these services have a set of common characteristics that are shared across all services, but they also have characteristics that are unique to their service. So the light service has characteristics like power, brightness, and saturation. And the fan service has characteristics like power, rotation speed, and rotation direction. Now, some of these characteristics are optional. As you can imagine, not all lights support saturation and color. But the HomeKit APIs expose these as a way to elevate these services to your user. So we look back. We have the ceiling fan fixture, which is the HMAccessory. We have the light service and the fan service, which are the HMServices. And we have the corresponding characteristics, which are the HMCharacteristics. So we look, if you look at the API for some of this, you could see we have HMAccessory, and this allows you to learn about the accessory that you're controlling. So this exposes properties like the name, the list of services it supports, and the firmware version it's running. Now, this is just a subsample of all the properties on the accessory. I just wanted to give you an idea of the sense of what types of properties are on it. So if we take a look at HMService, we'll see that it exposes the list of characteristics, and there's a setter method for updating the name. Now, updating the name is actually pretty important. So why is it important? Well, it's important because this is how Siri and the Home app will interface with your accessory. So you'll want to choose a good default name. So we, you don't want to use serial numbers, or MAC addresses, or model numbers in your service name because that'll make it challenging for the user to talk to your accessory through Siri. You also don't-- want to avoid special characters and numbers, emojis, punctuation, things like that also because it gets confusing to the user on how they want, when they want to address their accessory. Now, we support over 40 predefined service types. So things like light bulbs, thermostats, garage door openers, outlets, and all kinds of sensors, to name a few. We also allow the ability to specify custom service types as well. Now, when it comes to adding the accessory to the home, you'll want to go back to the HMHome helper methods. These APIs are pretty simple to use, but we'll walk through an example shortly here anyway. So the addAccessory API, this API is what invokes the Home UI view service, which is that familiar addAccessory flow that I mentioned earlier, which is how you can do quick QR code scanning or use NFC to add an accessory to the home. Now, I want to call special attention to the last method here, which is addAnd SetupAccessories with payload. Now, you can use that payload to pass in a HomeKit accessory code for in-field activation cases. So this API is especially valuable for software authentication. So this is the case where your accessory has left the factory without a HomeKit code on it. So you've updated the accessory via firmware update to provide HomeKit support, and you don't want the user to have to memorize the HomeKit code or to write it down. And so this API allows you to pass in that HomeKit code so that this accessory can be seamlessly added to the home. Now, this API is available to MFI licensees. For more information, contact your MFI representative. So if we look at some sample code on how we would add an accessory to the home, you just need to call addAnd SetupAccessories on the home object, and HomeKit takes care of the rest. Now, I mentioned scenes earlier, but it's a pretty powerful concept, so I want to go a bit deeper. Now, this is how you can integrate different types of accessories together to help provide a cohesive experience. We have some suggested scenes, but we also allow full customization by the user. Now, one of the great things about scenes is that Siri understands what scenes are. And so speaking the name to Siri execute, speaking the name of a scene to Siri will execute the scene directly. Now, we don't call them scenes in HomeKit. We call them HMActionSets. So if you want to create an HMActionSet -- for example, arriving home -- you'll need to create a list of HMActions, and then you'll associate those with the action set. So the action set, the HMActionSet contains all of the individual HMActions. Now, we support some predefined scenes, like wake up, go to sleep, leave the house, and return home. Now, this not only gives the user some context about what types of action sets are possible, but this is also what we've found to be some of the most useful. Now, of course, we also support user-defined action sets, and these are great for things like creating your perfect movie scene lighting. So again, to add them to the home, we go back to the HMHome APIs because this allows you to add these action sets, execute the action sets, and the HMHome APIs also expose the way of accessing the built-in action sets. So if we look at some quick sample code, we'll see that if we want to create an action set, we call addActionSet withName on the home, we pass in the name of the action set, then we'll want to iterate through our list of actions and add each one to the action set. Then, later, if we want to run the scene, all you do is call executeActionSet on the home. Now, running scenes via the UI or via Siri is great, but sometimes you want things to happen automatically. Now, we have several options for automatically triggering scenes and automatically controlling accessories. These can be geofence based, so you can turn on the lights as you pull up to your house. Or they can be time based, so you can turn up the heat before you get out of bed. They can be accessory based, so you can leverage a motion sensor to turn on a light. And they can be presence based, so you could turn off your lights when you leave for the day. Now, event triggers themselves are comprised of action sets, and the APIs on HMEvent or HMTrigger are mostly around naming, adding, and enabling. Now, with automations, you can set them up to run once ever, or you can have them be recurring. So you can them just be, happen that one time, or you can have them happen every day, or maybe you want them just to happen every weekday, or even just every Friday. So with automations, you have a lot of control with the frequency and how often you want automations to run. Now, let's take a look at the creation of an HMTrigger and how we add it to the home. So let's say we want to create an automation that will run every night at 7 p.m. So first, we create our date components and we set the time to be 7 p.m. Then, we'll create a calendar event using that component. Then, we'll generate a trigger using that time event. Now, we need to add that trigger to the home. And we need to add the action set to the trigger. So this allows you to run an automation every night at 7 p.m. So let's say it's turning on a light every night at 7 p.m. Now, that's great, except for sometimes, some parts of the year, it's not dark at 7 p.m., or sometimes, parts of the year, it gets dark before 7 p.m. Sometimes it gets dark after 7 p.m. So turning on the light at 7 p.m. every night isn't really what you want. So we also support what we call significant time events. And this is sunrise and sunset. So if we look at that previous example, we could change that to say, turn on the light every night at 7 p.m. We also support relative time offsets, so you can have the light turn on, say, 30 minutes prior to sunset. So let's go back to the code we had before and make some changes so that we can have it run every night at 30 minutes before sunset. So the first thing we need to do is change our date components to be 30 minutes. Now, you'll see that it's negative 30 here because we want it to run 30 minutes prior to sunset. Then, we'll create a significant time event for sunset and pass in that offset. Then, when we generate our trigger, we pass in the sunset event. Then, we execute the code we had before, which is adding the trigger to the home and adding the action set to the trigger. But sometimes you don't want these automations to run. You probably don't want the heat to turn on in the morning if no one's home. You probably don't want the light to turn on when the door opens if it's during the day. So we also support what we call conditions. And these can be time based, they can be presence based, they can be accessory state based, and they can be based on significant events. Now, I have an automation in my house which turns off the lights whenever someone leaves. Now, that's great, except that I live with my wife, and so whenever she leaves the house, all the lights turn off and I'm left in the dark. So with presence-based conditions, you can update this so that the lights only turn off when the last person leaves. Now, to set these conditions, we use predicates. So let's take a look at some sample code. So here we can see that we're using our same sunset time offset from before. Now, we're going to create a presence event that's when users are at home. We're going to use that presence event to generate a predicate. And we'll pass that predicate into our trigger generation. Then, we'll go back to the same code we had before where we add the trigger to the home and we add the action set to the trigger. So now, we've gone from having an automation that runs every night at 7 p.m. to an automation that runs every night 30 minutes before sunset, only if someone's home. Now, we also have support for HomeKit cameras. And there are some HomeKit APIs that allow you to show live streams, show still images, and to configure camera settings. You-- this support also includes the ability to control the speaker volume and microphone. So let's take a quick look at some sample code for that as well. So here you could see we have a function where we want to start the camera stream for our accessory. Now, we want to be sure to set ourselves as the delegate so we can get the proper delegate callbacks. Now, we'll use this start stream API to start the stream, and we also want to be sure to create a camera view and to add that view to our view hierarchy. This is so we can display that camera stream in our app. Then, later, we'll get the delegate callback. Again, this is the HMCameraStream ControlDelegate. And we'll receive this cameraStreamControl DidStartStream delegate callback. And all we need to do there is set the camera source to be the camera stream. Now, this is the end of the sample code for today, but there's plenty of documentation available for these APIs online. So we hope that this gives you a fresh look at learning what, how HomeKit works. We encourage you to join the MFI program to take advantage of all these great resources and powerful tools. It's amazingly easy to become an MFI licensee. Come visit us in the lab to learn more about it. If you're already an MFI licensee, contact your MFI representative to get access to the latest tools. With the new ADK, we've made it easier than ever to bring a HomeKit accessory to market. Whether you're building HomeKit through, [inaudible] your accessory through a firmware update or creating a new HomeKit accessory for the first time, use these tools to start integrating with HomeKit so you can get native series support and to integrate with other HomeKit accessories. We can't wait to see the great accessories we know that you'll come up with. We also encourage you to use these APIs to provide a unique experience for your customers and to take full advantage of all that HomeKit has to offer. For more information or to learn about anything we've discussed today, check out this link or come visit us in the lab. We're going to be down in the Technology Lab 12, which starts shortly. Thank you all for coming. Hope you all had a great WWDC.  Thanks, Mike. Hello everyone. Design is hard but design presentations are way harder. I still find them daunting. And today, a presentation about how to present design work at WWDC. The struggle is real. In any presentation, we have to take this big and complex topic and explain it precisely. Take it right down to its nucleus. And if we don't do that well, those ideas might die. All that hard work you did resigned to the depths of your hard drive. Presentation matters. As a designer at Apple, I have to present design work all the time. Unfortunately, this year I can't show you too much of that, but I'm going to try and transmit to you the ten most important things I've learned about presenting design work in the next nine minutes or so. It's going to be punchy. Now when you really think about it, there are three fundamental parts to a presentation. There is me, the presenter, the emotional designer consumed by this problem and the prettiness of it. And there is you, the audience, detectives of design crime, guardians of good sense, Draculas of deadlines, and captains of code. And between us, there is this central idea. The thing we need to agree is good for our company, good for our customers, something that makes the world a slightly better place. Let's call it a toaster. And really a presentation is just a mechanism to exchange information about that idea, to facilitate decisions and action one way or another, to make this thing fly or crash right back down to Earth. So dear presenter, these are the key things you need to understand. What are your objectives for this presentation? Clarify your objectives. Are you trying to get this work approved by your superiors or successfully communicate with engineers? Maybe you're pitching a new idea. Try to imagine what succeeding looks like and work backwards from there. Identify the questions you need your audience to help you answer. State these and your goals up front. And understand the objectives of your audience. What do they care about? Remember, your objectives include addressing theirs. Try not to dive in to details and minutia before you clarify the bigger picture. Next is a big one: feedback. As designers, we feel vulnerable and exposed, painting our finest pictures and teeing them up to be torn to shreds. Try to entertain the mindset that there is no failure, only feedback. Take all of it seriously even when you disagree. If you are presenting, you are asking for feedback. It's the whole point and clarify the problems the audience identify. Sometimes people offer solutions that are unworkable, but the problem they are addressing is totally valid. So when you have exhausted all possibilities, remember this, you really haven't. People in your audience will know things you don't. So be willing to change your position, especially when new information has been presented. Don't pretend to understand unclear feedback. Clarify the problem this person is trying to address. Don't respond like that. And don't be dismissive or get overly defensive. It really won't help your case. Seek advice. Use the wisdom of your colleagues. Don't be afraid to ask for help. They know how this works. Their input is priceless. Practice with them. Sense check your deck. Do a full practice run if it merits it. Do not deliver it blind. The feedback will make a huge difference. And honestly, don't try to do everything yourself. Teamwork makes the dream work plus it's really hard to keep track of too many disparate things as one person, let alone try and explain them to others in a single presentation. Use your own voice. Be genuine. Have fun. Make terrible jokes. Quote Bruce Lee. You wouldn't be giving this presentation at all if you didn't have the credibility. You've earned it. Express your opinion. You are the expert here. The audience wants to know what you think and that doesn't mean you have to have all the answers. Show your passion. Why do you care about this? And explain your process. Show the journey you've been on. How and why did you reach these conclusions? Don't be overconfident or insincere. Sometimes designers can adopt a posture of being super confident, bordering on arrogant and avoid that. It's just not cool. And don't say what you think people want to hear. Your vote counts for a lot. The audience. It's not a presentation without an audience. Make sure you show them that respect. You need their cooperation to give life to your ideas. And if you've presented to this audience before, you'll get -- start to get an idea of what they respond to. Tailor to them as much as you can and always have an agenda. Here's what we're going to talk about in this order. This is also your chance to assert the point at which you'd like to field feedback. It almost certainly won't work that way but still do try. And if you've met previously, recap what happened. Like when you're engrossed in a series but then life happens and when you pick it up again, you really grateful for that 30 second refresher. Hey, Zach. Here's a study of the character that you asked for. Good feedback. I'm not sure about those emoji either. Integrate their feedback. Let them know how it affected your designs. Show that you're listening and responding. Terrible. Make no assumptions, you know -- people know what things mean or why decisions were made. You are much further along this path than your audience. Anticipate their questions. Pay attention to their reactions. And be willing to explain. Do not show up without a plan or try to wing it. It's not a good look. And also showing your working files, not good. These things are lazy and disrespectful of people's time. And, of course, don't be dismissive of -- again, don't be dismissive of anyone's input. Try to be patient and objective. Make it relatable to your audience. If not, they may well start to tune out. Let people see themselves in your story. If you can elicit an emotional response, excitement, worry, or even fear, it will enhance their sense of commitment. Ellis goes crazy for that elderberry jam. Use what you know about your audience to your advantage. Describe what you see and feel. Don't talk about users like they are a distant third party. Flow diagrams can be really useful with engineers but when communicating concepts, there's a big disconnect. Instead, speak in the first person. Here I am at my desk working on my e-mail and ding, a handy reminder it's time for breakfast. I hit the notification and bam, straightaway I can load my toast without even stepping away from my desk. Just one swipe down and we're toasting. And a few minutes later I know to be on my way, perfect timing. And how cool is that? I can even see my stats. Clearly, this is more powerful if you can demo a prototype. And engage in dialogue. A presentation should be a dialogue. Engage with your audience. Design for the aha moment. If you can lead people through your story, their minds will try and predict the ending. This will keep them engaged. Ask them questions. Keep them dialed in. You're here for their feedback after all. Do not deliver a monologue. This is boring and naive. Think of it as a discussion more than a speech. And finally, the idea. It is crucial to explain why. Define each problem in a single sentence. It's a great exercise to really get to the heart of the matter. You'll need to find agreement on those problem statements with your audience. Otherwise, you'll be barking up the wrong tree. Continue to revisit, refine, and refer back to those statements as you progress and, of course, where you can, show evidence that these problems exist. Boom. Cold, burnt toast. Tragic problems in this world. Avoid using subjective reasoning for your decisions. I chose this color because I like it is not remotely persuasive, especially if your audience don't share your preferences. And the truth is that people really won't remember your slides. They'll remember how your story made them feel if it resonated with them so sketch out the narrative of your presentation early. The most successful designers I know do this. It's another thing that will help you focus and use your time effectively. So where you can, incorporate a story with a distinct beginning, middle, and an end. People intuitively get stories. And it'll really help them follow along. Contrast today's reality with a better future. Nancy Duarte, a master of presentations, discovered in her research that some of the greatest and most effective presentations follow this structure. And the key thing here is providing the contrast between what is today's reality and what could be, what the future could look like if we make the ideas we're presenting our new reality. Today, people are frustrated, cold, joyless toast. It's an epidemic. But imagine happy, flawless toast at the top of the button on your iPhone. But don't get too carried away. That is a rookie mistake. Keep it simple. The main thing is to keep the main thing the main thing. I love this quote. And I always find myself coming back to it. It's such a perfect expression. If I had more time, I would have written a shorter letter. Relevant not only for presentations and writing e-mails but as a philosophy toward design. Don't overload people with too much information. Strip away everything that isn't essential to your mission or your message. And finally, don't forget to summarize. Know your objectives, otherwise what's the point? Embrace feedback. It's what you're here for. Seek advice. Your colleagues are smart. And use your own voice. It's the best one you have. Respect your audience or they won't respect you. Make it relatable, otherwise they'll forget it. Engage in dialogue to keep them present and explain why. It's the million dollar question. Utilize storytelling because it works. And keep it simple even though it's not. Good luck with your presentations.  Good morning everyone. My name is Karol Gasinski, and I am a member of GPU Software Architecture Team at Apple. We will start this session with a brief summary of what is new in [inaudible], in terms of VR adoption. Then, we will take a deep dive into a new Metal 2 features, designs specifically for the VR this year. And finally, we will end this session with advanced techniques for developing VR applications. Recently, we introduced new iMac and iMac Pro that have great GPUs on board; iMac is now equipped with [inaudible] based GPUs and have up to 80 gigabytes of video memory on board. While iMac Pro are equipped with even more advanced and even bigger based GPUs, with up to 16 gigabytes of video memory. That's a lot of power that is now in your hands. But we are not limiting our service to iMac on their own. With recent announcement of extended GPU support, you can now turn any Mac into powerful workstation that gives you more than 10 [inaudible] of processing power. And that's not all. Today we are introducing plug and play support for HTC Vive head mounted display. It has two panels with 1,440 by 1,600 and 650 pixels per inch. That's 78% increase in the resolution, and 57% increase in pixel density compared to Vive. And with support for better panels comes support for its new dual-camera front-facing system, so developers will now be able to use those cameras to experiment with pass-through video on Mac. And together with Vive Pro support comes improved tracking system. So, you might be wondering how you can start developing VR application on Mac OS. Both HTC Vive and Vive Pro work in conjunction with Valve SteamVR runtime, that provides a number of services including VR compositor. Valve is also making open VR framework that is available on Mac OS, so that you can do the map that works with SteamVR. We've been working very closely with both Valve and HTC to make sure that Vive Pro is supported in SteamVR runtime on Mac OS. So, now let's see how new Metal features that we're introducing can develop a [inaudible] Mac OS Mojave can be used to fiber-optimize your VR application. As a quick refresher, let's review current interaction between application and VR compositor. Application will start by rendering image for left and right eye into 30 multi-sample textures. Then it will resolve those images into iOS surface back textures that can be further passed to VR compositor. VR compositor will perform final processing step that will include [inaudible] distortion correction, chromatic aberration, and order operations. We can call it in short warp. Once the final image is produced, it can be sent to the headset for presentment. It is a lot of work here that is happening twice, so let's see if you can do something about that. See, now with VR application it wants to benefit from multi-sample [inaudible], it needed to use the dedicated textures per i, or single shared one, for both. But none of those layouts is perfect. The dedicated textures require separate draw calls and passes, as we just saw. While straight textures enable rendering of both eyes in single rendered and results pass, they are problematic when it comes to post-processing the effects. [Inaudible] textures have all the benefits of both dedicated and shared layouts, but currently they couldn't be used with MSAA. This was forcing app developers to use different rendering part-time layouts, based on the fact if they wanted to use MSAA or not. Or use different tricks to work around it. So let's see how we can optimize this rendering. Today, we introduce new texture types to the multi-sample already textured. This texture type has all the benefits of previously mentioned types without any of the drawbacks. Thanks to that it is now possible to separate from each other rendering space, which simplifies the post-processing effects, views count, so that application can fall back easily to monoscope rendering and control over anti-aliasing mode. As a result, application can now have single rendering files that can be easily adopted to any situation, and most important, that can be rendered with single draw and render pass in each case. So here we see code snippet for creation of mentioned 2D multi sample [inaudible] texture. We set up sample count to 4, as it's an optimal tradeoff between quality and performance, and at the same time, we set up our other length to 2 as we want to store each image for each I in separate slice. So let's see how our pipeline will change. We can now replace those 2D multi-sample textures with single 2D multi-sample [inaudible] one. So now application can render both I in single render pass and if it's using instancing, it can even do that in single draw code. So that already looks great, but we still need to resolve those 2D multi-sample array texture slices into separate iOS [inaudible] faces before we pass them to compositor. So let's focus on our way, application shares textures with compositor. So now, for sharing textures, we use IOSurfaces. They are sharing textures between different process spaces and different GPUs, that we've got [inaudible] comes a price. IOSurfaces can be only used to share simple 2D textures, so if you have a multi-sampled one, storing [inaudible] or having [inaudible], they couldn't be shared. That's why today we introduce shareable Metal textures that allow your applications to share any type of Metal texture between process spaces, as long as these textures stay in scope of single GPU. This file features [inaudible] advanced view of these cases. For example, sharing depth of your scene with VR compositor. But, of course, it's not limited just to that. Now, let's look how those textures can be created. Because shareable textures allows us now to pass complex textures between processes, we will create 2D array texture that we will pass to VR compositor. As you can see, to do that, we use new methods, new shared texture with this creator. And while doing that, you need to remember to use private storage mode, as this texture can be only accessed by the GPU on which it was created. Now, we see a code snippet showing us how our VR application would send IOSurface to VR compositor in the past. We will now go through this code snippet, and see what changes needs to be applied to switch from using IOSurfaces to shared Metal textures. So we don't need those two IOSurfaces anymore, and those two textures that were backed by them can now be replaced with single shareable Metal texture that is over 2D array type. We will then assign this texture to both texture descriptors from open VRSDK, and change its type from IOSurface to Metal. After doing these few changes, we can submit image for the left and right I to the compositor. Compositor will now know that we've passed shared Metal texture with advanced layout, instead of IOSurface, and if we check, if its type is 2D array or 2D multi-sampling array. If it is, then compositor will automatically assume that image for the left i is stored in slice 0, and image for right i is stored in slice 1. So your application doesn't need to do anything more about that. And of course, sharing Metal textures between application and compositor is not the only use case for shareable Metal textures. So here we have simple example of how you can pass Metal texture between any two processes. So we start exactly in the same way. We create our shareable Metal texture, but now from this texture, we create special shared texture handle that can be passed between process spaces using cross-process communication connection. Once this handle is passed to other process, it can be used to recreate texture object. But while doing that, you need to remember to recreate your texture object on exactly the same device as it was originally created in other process space, as this texture cannot leave scope of GPU. So now let's get back to our pipeline and see what will change. Application can now replace those separate IOSurfaces with one 2D array texture, storing the image for both i's. This allows further optimization as original 2D multi-sample array texture can be now resolved in one pass as well to just create it shareable through the array texture. But that's not everything. Let's look at the compositor. Once we have simplified rendering parts on application site, there is nothing preventing compositor from benefiting from those new features as well. So compositor can now use those incoming 2D array textures and perform work for both i's in single render pass as well. And as you can see, we've just simplified the whole pipeline. So let's do recap of what we've just learned. We've just described two new Metal features. Shareable Metal textures, and 2D multi-sample array texture type. And the way they can be used to further optimize your rendering pipeline. Both features will be soon supported in upcoming SteamVR runtime updates. So now, let's focus on techniques that will allow your application to maximize its CPU and GPU utilization. We will divide this section into two subsections-- Advanced frame pacing and a reducing free rate. We will start with frame pacing. And in this section, we will analyze application frame pacing and how it can be optimized for VR. So let's start with simple, single-threaded application that is executing everything in serial monitoring. Such application will start its frame by calling WaitGet pauses, to receive pauses, and synchronize its execution to the frame rate of the headset. Both Vive and Vive Pro has refresh rate of 90 frames per second, which means the application has only 11.1 milliseconds to process the whole frame. For comparison, blink of an eye takes about 300 milliseconds. So in this time, the application should render 50 frames. So once our application receives pauses from WaitGet pauses, it can start simulation of your trial [inaudible]. When this simulation is complete, and state of all objects is known, application can continue with encoding command buffer that will be then sent to GPU for execution. Once GPU is done, an image for both i's is rendered, it can be sent to VR compositor for final post-processing, as we talked about a few slides before. After that, frames scanned out from memory to [inaudible] in the headset. This transfer takes additional frame as all pixels need to be updated before image can be presented. Once all pixels are updated, [inaudible] and user can see a frame. So as you can see from the moment the application receives pauses, to the moment image is really projected, it takes about 25 milliseconds. That is why application receives pauses that are already predicted into the future, to the moment when photons will be emitted, so that the rendered image is matching the user pause. And this cascade of events overlapping with previous and next frame is creating our frame basing diagram. As you can see, in case of the single-threaded application, GPU is idle most of the time. So let's see if we can do anything about that. We are now switching to multi-threaded application, which separates simulation of its visual environment from encoding operations to the GPU. Encoding of those operations will now happen on separate rendering threads. Because we've separated simulation from encoding, simulation for our frame can happen in parallel to previous frame encoding of GPU operations. This means that encoding is now shifted area in time, and starts immediately after we receive predicted pauses. This means that your application will now have more time to encode the GPU [inaudible] and GPU will have more time to process it. So, as a result, your application can have better visualize. But there is one trick. Because simulation is now happening one frame in advance, it requires separate set of predicted pauses. This set is predicted 56 milliseconds into the future so that it will match the set predicted for rendering thread and both will match the moment when photons are emitted. This diagram already looks good from CPU side, as we can see application is nicely distributing its work [inaudible] CPU course, but let's focus on GPU. As you can see, now our example application is encoding all these GPU [inaudible] for the whole frame into a single common buffer, so unless this common buffer is complete, GPU is waiting idle. But it's important to notice that encoding of GPU operations on a CPU takes much less time than processing of these operations on the GPU. So we can benefit from this fact, and split our encoding operation into a few common buffers while a few common buffer will be encoded very fast, with just few operations, and submitted to GPU as fast as possible. This way, now our encoding is processing in parallel to GPU already processing our frame, and as you can see, we've just extended the time when GPU is doing its work, and as a result, further increase amount of work that you can submit in a frame. Now, let's get back to our diagram, and see how it all looks together. So as you can see, now both CPU and GPU are fully utilized. So [inaudible] application is already very good example of your application, but there are still few things we can do. If you will notice, rendering thread is still waiting with encoding of any type of GPU work before it will receive predicted pauses. But not all [inaudible] in the frame requires those pauses. So let's analyze in more detail to pick our frame workloads. Here, you can see a list of workloads that may be executed in each frame. Part of them happen in screen space or require general knowledge about pause for which frame is rendered. We call such workloads pause-dependent ones. At the same time, there are workloads that are generic and can be executed without knowledge about pauses immediately. We call those workloads pause independent ones. So currently, our application was waiting for pauses to encode any type of work to GPU. But if we split those workloads in half, we can encode pause independent workloads immediately and then wait for pauses to continue with encoding pause-dependent ones. In this slide, we've already separated pause independent workloads from pause dependent ones. Pause independent workloads is now encoded in [inaudible] common buffer, and is marked with a little bit darker shade than pause-dependent workload following it. Because pause-independent workload can be encoded immediately, we will do exactly that. We will encode it as soon as the previous frame workload is encoded. This gives CPU more time to encode the GPU work, and what is even more important, it ensures us that this GPU work is already waiting for being executed on GPU so there will be exactly no idle time on GPU. As soon as previous frame is finished, GPU can start with the next one. The last subsection is a multi-GPU workload distribution. We can scale our workload across multiple GPUs. Current Mac Book Pro has two GPU on board, and while they have different performance characteristics, there is nothing preventing us from using them. Similarly, if each GPU is connected, application can use it for rendering to the headset while using Mac's primary GPU to offload some work. So we've just separated pause-independent work and moved it to a secondary GPU. We could do that because it was already encoded much earlier in our frame, and now this pause-independent workload is executing in parallel to pause-dependent workload of previous frame. As a result, we further increased the amount of GPU time that you had for your frame. But, by splitting this work into multiple GPUs, we now get to the point where we need a way to synchronize those workloads with each other. So today we introduce new synchronization parameters to deal exactly with such situation. MTL Events can now be used to synchronize GPU work in scope of single GPU across different Metal cues and MTL Shared Events extends this functionality by allowing it to synchronize workloads across different GPUs and even across different processes. So here we will go through the simple code example. We have our Mac, with attached eGPU through Thunderbolt 3 connection. This eGPU will be our primary GPU driving the headset, so we can use GPU that is already in our Mac as secondary supporting GPU. And we will use shared event to synchronize workloads of both GPUs. Event initial value is zero, so it's important to start synchronization counter from 1. That's because when we would wait on just initialized event, its counter of zero will cause it to return immediately, so there would be no synchronization. So our rendering thread now starts encoding work for our supporting GPU immediately. It will encode pause-independent work that will happen on our supporting GPU course, and once this work is complete, its results will be stored in locker memory. That's why we follow with encoding brief operation that will transfer those results to system memory that is visible by both GPUs. And once this transfer is complete, our supporting GPU can safely signal our shared event. This signal will tell eGPU that now it's safe to take those results. So our rendering thread committed this [inaudible] common buffer, and supporting GPU is already processing its work. At the same time, we can start encoding command buffer for a primary GPU that is driving the headset. In this command buffer, we will start by waiting for our shared event to be sure that the data is in system memory, and once it's there, and the shared event is signaled, we can perform a brief operation that will transfer this data through Thunderbolt 3 connection, back to our [inaudible] GPU and once this transfer is complete, it's safe to perform pause-dependent work, so a second command buffer will signal lockout event to let pause-dependent work know that it can start executing. After encoding and submitting those two command buffers, rendering thread can continue as usual, with waiting for pauses, and later encoding pause-dependent work. So now we have a mechanism to synchronize different workloads between different GPUs. But as you can see, our secondary GPU is still a little bit idle. That's because in this example we decided to push through it, pause dependent workloads that have dependency with pause dependent ones. Excuse me. But of course there are types of workloads that have no dependencies, and they can happen at lower frequencies, the frequency of the headset. One example of such workloads can be, for example, simulation of physically based accurate [inaudible] or anything else that requires a lot of time to be updated. Such workload can happen in the background, completely asynchronously from rendering frames, and each time it's ready, its results will be sent to primary GPU. It's marked here with gray color to indicate that it's not related to any particular frame. So, of course there are different GPUs with different performance characteristics, and they will have different bandwidth connections. And your application will have different workloads in a single frame with different relations between them. So you will need to design a way to distribute this workload on your own, but saying all that, it's important to start thinking about this GPU workload distribution, as multi-GPU configuration are becoming common on Apple platforms. So let's summarize everything that we've learned in this section. We showed multi-thread application to take full benefit of all CPU codes. And split your command buffers, to ensure that GPU is not idle. When doing that, if possible, try to separate pause-independent from pause-dependent workloads, to be able to encode this work as soon as possible, and even further, splitting workloads by frequency of update so if your application will execute on multi-GPU configuration, you can easily distribute it across those GPUs. And while doing that, ensure that you drive each GPU with separate rendering threads to ensure that they all execute asynchronously. Now, you switch to reducing fill rate. Vive Pro introduces new challenges for VR application developers. To better understand scale of the problem, we will compare different medium fill rates. So, for example, application rendering in default scaling rate to Vive headset, produces 436 megapixels per second. And most advanced [inaudible] against that [inaudible] HD TVs have fill rate of 475 megapixels per second. Those numbers are already so big that game developers use different tweaks to reduce this fill rate. Now, let's see how Vive Pro compares to those numbers. Vive Pro has a normal fill rate of 775 megapixels per second, and if you add to that four times multi-sampling [inaudible] or bigger scaling rate, this number will grow even more. That is why reducing fill rate is so important. There are multiple techniques there and new techniques are made every day. So I encourage you to try them all, but today we will focus only on a few still as they are the simplest to implement and bring nice performance gains. So we will start with clipping invisible pixels. Here, you can see image rendered for left eye. But due to the nature of the lens work, about 20% of those pixels are lost after compositor performs its distortion correction. So on the right, you can see image that will be displayed on a panel in a headset before it goes through the lens. So, the simplest way to reduce our fill rate is to prevent our application from rendering those pixels that won't be visible anyway, and you can do that easily by using SteamVR Stencil Mask. So we've just saved 20% of our fill rate by applying this simple mask, and reduce our Vive Pro fill rate to 620 megapixels. Now, we will analyze implication of this lens distortion correction in more detail. We will divide our field of view into nine sections. Central section has field of view of 80 degrees horizontally by 80 degrees vertically, and we have surrounding sections on the edges and corners. We've color tinted them to better visualize the contribution to final image. So as you can see, corners are almost completely invisible and edges have matched less contribution to the image than in the original one. In fact, if you see this image in the headset, you wouldn't be able to look directly at the red sections. The only way to see them would be with your peripheral vision. So this gives us great hint. We can render those edge and corner sections and a reduced fill rate, as they are mostly invisible anyway. We render the central section as we did before. But then we will render vertical edges with half of the width and horizontal sections with half of the height. And finally, we will render corner edges at one-fourth of the resolution. Once our expensive rendering pass is complete, we will perform cheap upscaling pass that will stretch those regions back to the resolution at which they need to be submitted to compositor. So you are wondering how much we've gained by doing that. In case of 80 by 80 degree central region, we reduced our fill rate all the way down to 491 megapixels per second. But you remember that we just talked about clipping invisible pixels, so let's combine those two techniques together. By clipping pixels combined with multi-resolution shading, you can reduce your fill rate even further to 456 megapixels per second, and that is not a random number. In fact, that's a default fill rate of Vive headset, so by just using those two optimization techniques, your application can render to Vive Pro with much higher resolution using exactly the same GPU as it did when rendering to Vive headset. Of course, you can use those techniques when rendering to Vive as well, which will allow you to bring visualize of your application even further and make it prettier. There is one caveat here. Multi-resolution shading requires few render passes, so it will increase your workload on geometric [inaudible], but you can easily mitigate that by just reducing your central vision by a few degrees. Here, by just reducing our central vision by 10 degrees, we've reduced fill rate all the way to 382 megapixels per second. And if your geometry workload is really high, you can go further, and experiment with lower fill rate, lower regions, that will reduce fill rate even more. In case of 55 by 55 degrees central region, 80% of your [inaudible] eye movement will be still inside this region, but we've reduced our fill rate by more than half, to 360 megapixels per second. So of course there are different ways to implement multi-resolution shading. And you will get different performance gains from that. So I encourage you to experiment with this technique and try what will work for you best. So let's summarize everything that we've learned during this session. We've just announced plug and play support for Vive Pro Headsets, and introduced new Metal 2 features that allow you now to develop even more advanced VR applications. And I encourage you to take advantage of multi-GPU configurations, as they are becoming common on other platforms. You can learn more about this session from this link, and I would like to invite all of you to meet with me and my colleagues during Metal 4 VR Lab, that will take place today at 12:00 p.m. in Technology Lab 6. Thank you very much.  Good morning, and welcome to this talk. My name is Guillem Vinals Gangollels. And I work at the GPU Software Performance Team here at Apple. Good developers like you make iOS an excellent gaming platform. And we at Apple obviously want to help. So this year we reviewed some of the top iOS games and found some common performance issues. We analyzed a lot of data, and as a result of that investigation, we decided to put this talk together. So this is going to be the main topic today. Develop Awesome Games. But I will only be providing technical directions here. So we'll [inaudible]. Before we begin, please let me thank our friends at Croteam. They are the developers behind The Talos Principle, which is a really awesome game. You will see it featured in these slides and in two of the demos. Notice that it has stunning visuals but it does really not compromise in performance. And that's what this is all about. So let's do a quick run through of the agenda. I'll start with an introduction to the tools. This is a very good place to start. And then we'll talk about the actual performance issues. Around frame pacing, thread priorities, thermal states, and unnecessary GPU work. Even though all these issues seem unrelated, they will compound and aggravate each other. So it's important to tackle them all. Let's start with the tools. This is the most important message. You should profile early and do it often. Do not ship your game unless you've profiled it. And for that you will need to know your tools. Today, I will focus on two of them. First, we have instruments, which is our main profiling tool. You will want to use it to understand performance, latency, and overall timing. Second, we have the Metal Frame Debugger, which is also very powerful tool, which you will want to use to debug your GPU workload. So where do we start? This is a question we often get. Well, this year we are making it easier for you. We are introducing a new instruments template, which will be a great starting point. The Game Performance Template. It is the combination of already existing instruments such as System Trace, Time Profiler, and Metal System Trace. We configured it for you so it records all the CPU and GPU data that is relevant for your game. So you can make it smooth. So how do we launch it? How do we get there? Well, just open Instruments and you will see it right there in the center. After you choose it, you will be able to configure it same as every other template. Once you start recording, you will do so in windowed mode, which will allow you to play your game for as long as you like, and only the last few seconds of data will be recorded. And this is how this last few seconds of data will look like. There's a lot of information so let's have a quick high-level overview. First, we have System Trace and Time Profiler, which will give you an overview of the system load as well as your application CPU usage. For example, user interactive mode will record all the active threads at a given time. In this case, the orange color you can see means that there are more runnable threads available than CPU cores. So there is some contingency. These will offer a great view of the system. There's a couple of great talks that talk about this instrument in more depth. Please follow-up on them. Next on our list is Metal System Trace, our GPU profiling tool. It offers a great view of the graphic stack. All the way from the Metal Framework down to the display. In particular, we will want to pay close attention to the GPU [inaudible], which is split in vertex, fragment, and compute if your game uses it. Notice as well that the display track will be the starting point of many of our investigations. We will identify a long frame or a starter and we will work it all the way up from there. So it's a very natural place to start. There is a lot of information about the tool because it really is a very powerful tool. And I encourage you all to catch up on it. These are a couple sessions that will provide you a great starting point. Okay. So next on our list we'll have a thread states view which we introduced this year. This view will show you the state of every thread in your game. In this case, each color represents a possible thread state, such as preempted which is represented in orange. Or blocked which is represented in red. We designed this view specifically with you, game developers, in mind. Because we know the threading systems in modern games are very complex. And we hope this really will help you. Also we have a track for each CPU core. It will show the thread running on that core as well, as well as the priority of that thread, which is color coded. By using this, you will be able to see at a glance how easy the system really is. That was a short but a quite wide introduction to the tools. So it's about time we move to the actual performance issues. The first one will be around frame pacing. And let's visualize it first. For this we used the modified version of the Fox [inaudible] demo. That will help us illustrate the issue better. Can you guess which game renders faster? Well, some of you may not have guessed it. The game on the left is trying to render at 60 frames per second. But it can only achieve 40, so it's inconsistent, and it seems jittery. The game on the right on the other hand is targeting 30 frames per second, which can consistently be achieved. That's why it looks smoother. But that's a bit counterintuitive. How, how come the game that renders faster doesn't look smoother? Well, this issue's known as micro stuttering or inconsistent frame pace. It occurs when the frame time is higher than the display refresh interval. For example, our game may take 25 milliseconds to render or 40 frames per second. And the display may refresh at 16.6 millisecond or 60 frames per second. Same as the video we've just seen. These will create some visual inconsistencies. So how did we get there? What have we done to be in this situation? Well, we didn't do much really, and that's kind of the whole point of this. After rendering the frame, we requested the next drawable from the display link. And as soon as we got the drawable, we finished the final pass and presented it right away. We explicitly told the system to present that drawable as soon as possible, at the next refresh interval. After all, we are targeting 60 frames per second, right? There's also another class of problems that will cause micro stuttering. And some games are already targeting lower frame rate. But we have also identified many of those games that are using usleep on their main or random thread. This is a very bad practice in iOS, so please don't do that and just hang, hang here for the next few minutes. And I'll tell you the actual correct way of doing this in iOS. Now, let's have a deeper look into what happens in the system for micro stuttering to be visible. In this case, we see here a timeline of all the components involved in rendering. And we'll start rendering our game normally. Notice this is a three-point buffer case, which is quite common in iOS. In this case, every drawable is represented by a letter and a color. And also notice the premise here. Rendering to drawable V takes longer than one display refresh interval, which is the time between vsyncs. In this case, could be 25 millisecond to render to V and 16.6 millisecond in between display refresh intervals. So since that is the premise, this means that we will need to [inaudible] on the display for the next interval to give time so we can finish. And we will do so. And that during that interval, we will actually B, B will actually finish. And we will be ready to present it but notice that we have just hid the issue here. During this interval, we have also finished rendering to C. And we are ready to present it right away. So we will [inaudible] an inconsistent frame pacing from that moment onward. We are stuck in this pattern. Every other frame will be inconsistent. And the user will see micro stuttering. Now this may appear in different shapes and forms in the real world. So what we'll do now is a quick demo and I'll show you an instruments trace of the Talos Principle. And we will use to see if we can identify micro stuttering in the real world case. Okay. So what we see here is the same lot of information I've shown you before. This has been captured with the Game Performance Template by default. Notice all the same instruments I talked about here displayed on the left. And all the game threads here in the middle. In particular though, we are looking now at micro stuttering. So this quite intuitively will bring us to look at the display track because micro stuttering by definition is frames presented inconsistently. In this case, we have the display track here. Notice as well that there are some hints in the display track. We [inaudible] and these are the hints here. They will show you when a surface has been displayed for longer than we would expect on a normal rendering. So maybe this is a great place to start looking at it. There's some clusters of them. So let's zoom into one. To zoom, we will hold the option key and just drag the pointer to the region of interest. And in this case, if we keep looking at the display track, it's kind of evident already that we are micro stuttering. We can see that every display has a different timing. So in this case for example, we have 50, 33, 16, back to 50, and back to 33. So when we see this pattern in an instruments capture, it means that we are micro stuttering and we should correct it. So let's just do that. Back to the slides. Okay. We've just seen the problem, how it occurs in the real world. The pattern is basically the same. So how do we go about fixing it? The best practice here is to target the frame rate your game can achieve. So at the minimum frame duration there is longer than the time it takes to render. For that, there's a bunch of APIs that can help you. For example, MT Drawable addPresentedHandler will give you a call back once that drawable is presented. So you can identify micro stuttering as it is happening. The other two APIs will help you to actually fix the problem. They will allow you to explicitly control the frame rating-- the frame pacing. In this case we have present afterMinimumDuration and present atTime. What we want to do here? We set the minimum duration for our frame longer than it takes to render. And we'll do just that. Let's see how that looks. Notice that when we start rendering, we are already consistent from the get-go. Our frame spends on display more time it takes to render. Every frame will be consistent. The user will see also being consistent. And that's great. Also notice that there's a side effect. The frame rate will be lowered. We went from 40 frames per second to 30 frames per second. So that also gave us some extra frame time to play with. So how did we do this? How did we fix the-- the frame pacing? Well, really it's just a couple of lines of code. We have the same pattern as before. We rendered the scene. We get the next drawable. We do the final pass. The only difference here is that we specify a minimum duration for our frame. And present it with that minimum duration. That's all it takes. That will allow us to set the minimum duration for our frames. And they will all be consistent. And after doing so, you may be thinking well, what about maximum duration? What about the concept of priority of our work? Or how long a thing could take? Well, that's actually the next issue on our list-- thread priorities. Let's visualize it first, same as we did before. Again, with the modified version of the Fox II demo. You may be thinking and you would be right that there are many things that could cause stuttering such as this. Maybe you are doing some resource loading or [inaudible] compilation. Today we will focus on the more fundamental but also incredibly common type of stutter. That caused by thread stalling. If the work priority is not well communicated to the system, your game may have unexpected stalls. iOS does plenty of stuff besides rendering your game. Thread priorities are used to warranty the quality of service in the whole system. So if a thread does a lot of work, its priority will be lowered over time so other threads can run instead. That's the concept known as priority decay. Also you see on the slide behind me priority inversion. This is another class of problems that manifests in a very similar way. In this case, priority inversion occurs when the render thread depends on the lower priority worker thread from your same engine in order to complete the work. Let's see how that looks like in the same timeline as we've seen before. In this case, we start rendering at 30 frames per second so we are cool. But then there is some background work. iOS does lots of stuff. Maybe now it's checking the email. And the problem here is that the [inaudible] thread is not well configured. You may get preempted by that background work. You may not finish scheduling all the work onto the GPU. And there is no such thing as maximum duration for a frame. So that could potentially go along for hundreds of milliseconds. The user will see a stutter. This is also the theory behind it. And in practice it shows in different ways that follow the same pattern. So let's do another demo. I'll show you another instruments capture of the Talos Principle. That will show you how to identify this problem. So in this case, what you see here is again a capture taken with the Game Performance Template. But this time we have already zoomed into the frame we are interested in, which is this very long frame. It has a duration of 233 milliseconds. So that's likely a very good stutter that we should investigate. By-- by looking at it at a glance, we can already tell that the GPU does not seem to be doing much. It's idle during this time, so this means that we are not fitting it. Now we can look at the CPU, of course, and they seem to be fairly busy down here. Right? They are really-- all of it seems quite solid. But notice what you see here is the time profiler view of our application. And it does not seem to be running. Why is our game not running and how come that causes a stutter? Why? Well, we can switch to the new view I talked to you about, the new thread states view. To do so you will go into the icon of your application and click on that button here and that would pull out the track display. And in this case, you can switch to thread states. And that will hope-- hopefully already help you to see there is something wrong here. It is highlighted in orange, and it's already telling us that the thread has been preempted for 192 milliseconds. So that's the actual problem here. A render thread is not running. Something preempted it. If you want to know more, you can expand information at the bottom, which will contain also the thread narrative. And by clicking at the preempted thread, you will see here an explanation of what's going on. In this case, your render thread was preempted at priority 26, which is very low. It's below background priority because the App Store was updating. So that's something we do not want. We want to tell the system that to our user, our game is more important than an App Store update at that particular moment. So let's go back to the slides and see how can we do that? So the best practice here is to configure your render set. We recommend the render set priority to be fixed to 45. Notice that the [inaudible] OS and macOS priorities have ascending values. So priority 31 has higher priority than priority four. Also, we need to opt out of the scheduler's quality of service in order to prevent priority decay which could lower our priority as well. Let's see how a well-configured render thread looks like. In this case, we configure just how I told you. We start rendering normally. We also have some background work going on. Otherwise it wouldn't be fair. And this background work could be updating the App Store just as we've seen in the demo. But notice that vsync after vsync, our render occurs normally. We are preempting the background work of the CPUs so we can run instead. The user does not see the stutter. Your game can run at 30 solid frames per second, even though the system is under heavy load. That is technically awesome, and that's what this is all about. So let's see how we make this happen with a little bit of code. And it literally is a little bit of code. It is only like a couple lines. In this case, it's just about configuring the pthread attributes before we can create the pthread. We need to opt out of quality of service, set the priority to 45. And that's it. We can create the pthread with those attributes, and it will work just fine. It is simple and technically awesome. What's not so simple though is the next issue on our list. That about dealing with multiple thermal states. The message is very clear. Design for sustained performance and deal with the occasional thermal issues. So let's see how we go about that. iOS devices give you access to an unprecedented amount of power. But [inaudible] in a very small form factor. So more apps use more resources on the device, the system may begin enacting measures in order to stay cool and responsive. Also the user may have enabled a low power mode condition, which will have a very similar effect. Okay, so the best practice really is just to adjust your workload to the system state. You should monitor the system and tune the workload accordingly. iOS has many APIs to help you with that. For example, use NSProcessInfo thermalState to either query or register for notification when the device thermal state changes. You should also check for the low power mode condition in a similar fashion. Also consider querying the GPU start/GPU end time from the MTL Command Buffer in order to understand how system loads may impact the GPU time. Let's see how we do that with a simple code example. This comes straight from our best practices. A tip score is a very simple switch statement when every case corresponds to a thermal state. We have nominal, fair, serious, and critical. And that is all very good. So now we know that we are in a thermal state and thse command's telling us to do something about it. So how can, how can we actually help the system stay cool? Well, I can give you some suggestions, but it's up to you game developers to decide what compromises to make in order to help the system. You know what's best for your game to keep being awesome under stress. Some recommendations I'll give you though are to target the frame rate that can be sustained for the entire game session. For example, stay at 30 frames per second if you cannot sustain 60 for ten minutes or more. Doing the GPU work is also super helpful. For example, consider lowering the resolution of intermediate render targets, or simply find the shadow maps, loading simpler assets and even removing some of the post-processes altogether. Wherever, whatever fits your game the best. You should decide that one. And this will bring us to the next issue on our list. That about dealing with unnecessary GPU work. For that, please welcome my colleague Ohad on stage. He's going to tell you all about it. Thank you, Guillem. Hey, everyone. My name is Ohad, and I'm a member of the Game Technologies Team here at Apple. In the previous slides, Guillem showed you how important it is to adapt to the system. Responding to states like low power mode or the varying thermal states will require you to tune your GPU workload in order to maintain consistent frame rates throughout an entire game session. However, for many developers, the GPU is a bit of a black box hidden behind the curtains of a game engine. Today, we'll pull back those curtains. Wasted GPU time is a very common problem and it's one that often goes unnoticed. But I want you to remember this. Technically awesome games don't only hit their GPU budget. They're also good citizens to the system, helping it to stay cool and save power. All the popular game engines provide a great list of best practices to follow. We won't cover those. Instead we'll focus on how to tell if something is expensive to render. And as we've done with the CPU several times today, the best practice here is profile your GPU as well. The power of our GPUs can hide many efficiencies in either content or algorithms. You will want to time your workload, but also understand each rendering technique that you enable. And only keep those that add noticeably to the visual quality of your games. But how do you find these inefficiencies? How do you determine which parts of your pipeline are flat-out excessive? This of course brings us back to tools. As always, your first stop should be Instruments. Here we're looking at Metal System Trace. It'll provide you accurate timings for vertex, fragment, and compute work being done. But by measuring your GPU time, you're only halfway there. Next you want to really understand what each of your passes is doing. And for this, we're added a new tool to the Metal Frame Debugger this year. It's the Dependency graph. The Dependency graph is a story of a single frame. It's made up of nodes and edges and each one of these tell a different part of the story. Edges represent dependencies between passes. As you follow them from top to bottom, you'll see where each pass fits into your rendering pipeline. And how they work together to create your frame. Nodes on the other hand are the story of a single pass. They're made up of three main components. First, the title element will give you the name of the pass. Now I really want to emphasize this. Label everything. It'll help you not only in the Dependency viewer, but throughout our entire suite of tools. Secondly, it'll allow you to quickly tell what type of pass you're looking at. Render, blit, or compute. Here from the icon we can see that it's a render pass. Next, you have a list of statistics describing the work being done in this pass. And finally to the bottom, a list of all the resources that are being written to during this pass, and each of these also comed with a label, a thumbnail allowing you to preview your work, and a list of information describing each one of those resources specifically. And all that together allows you to really understand each of your passes. Okay, so now we know how to read the graph. Let's jump into a demo and see how it all fits together. Okay. So I have the Fox II demo running on my machine here. It was built in Scene Kit, which allowed me to add all sorts of great effects. As you can see, I have cascading shadow maps, bloom, depth of field, and all of it comes together to create a beautifully rendered scene. Let's use the dependency viewer to see how it all works. First, we'll go to Xcode and we'll capture a frame using the capture GPU frame button in the bottom. And we'll select the main pass on the left. And we'll also switch to automatic mode which will give us, will give us our assistant on the right. Now notice that the same pass that I selected in the debug navigator is also the one that's showing-- is selected, and focused in the main view. And this is a two-way street. So as we interact with the graph, select, selecting different passes or textures or even buffers, both the navigator on the left and the assistant on the right will update to show your selection. So this is a really fantastic way to navigate your frame. Now as I zoom out, the first thing you'll notice that the statistics hide and the focus goes away from the individual passes onto the frame as a whole. And I can zoom out even more to see a great bird's-eye view of my entire frame. Now the really cool thing to notice here is that since dependencies drive the connectivity of the graph, each logical piece of work is grouped together in space. So let's zoom in and see what I mean. Here I have a branch of work that's creating my shadow maps. On the left, I can see three passes that are rendering the shadows. So this is really fantastic because I'm not just getting the story of my entire frame. But there's another story in between these two layers. One of how each rendering technique is built up. And this is something that isn't always entirely obvious when you're using a game engine to turn these on. For instance, when my shadow maps, I may not have known that cas-- that each cascade would require its own pass. If I considered each one of these individually, they wouldn't really stand out. But now I see that I have to consider them as a group. And that gives me the insights that I need to make informed decisions on any compromises that I make while tuning my GPU workload. So that's the Dependency viewer. I'll switch back to the slides. And please help me welcome Guillem back onto the stage for his final thoughts. Thank you. Thank you. That was an awesome demo [inaudible]. Cool. So Ohad had just shown us how a frame looks like through Dependency viewer. And that is great for you to inspect your GPU workload. For example, oftentimes we may go from a very small and simple pipeline such as this one to a very complex one with post-process, multiple shadow maps in HDR. And all of these can be done by adding, you know, a couple properties to the common object of your favorite game engine. You see that the code complexity of those changes is minimal. But the-- but the rendering complexity may have increased tenfold, which will really bring us back to the beginning right where we started. Profile. It is very important that you understand what your game does. You spend tens of thousands of hours developing a game, you should consider spending some of that time profiling as well. Everything we have seen today can be found within minutes. The best part? You don't need to know what you're looking for. Just record the stutter, get the long frame, and work it all up-- all the way up from there. It's that simple. The tool will give you all the information you need to identify the problems. But you will need to use the tool. And that is really the takeaway. So we have seen a bunch of common pitfalls followed by some best practices. All of these issues can be found through profiling. That's how we found them. We analyzed a ton of games, found the common issues, and decided to put a talk together. Now, if you have access to the engine source code, make sure that both thread pacing and thread priorities are well configured. It's just a couple lines of code really. But regardless, your game should always adapt to thermals and do not submit unnecessary GPU work. By making sure to follow all these best practices, you too will be developing technically awesome games. And that's what this is all about. For more information, there is a-- a coming lab at 12 PM. We will be there. I'll be there and now we'll be more than happy to ask any questions you may have after this session. Or maybe you just want to sit down and let us profile your game. Also there, there were two great talks [inaudible] about Metal for game developers and our profiling tools. Thank you very much, and enjoy the rest of the day. And have a great one.  Good afternoon and welcome to Getting the Most out of Playgrounds in Xcode. My name is Tibet Rooney-Rabdau and with me today are my teammates Alex Brown and TJ Usiyan. I love that every time I have a new coding idea I can jump right into a playground to try it out. In today's session we will share with you our favorite work flows. This is a great session for those not too familiar with playgrounds in Xcode, as well as those diving deeper. We're going to start out todays talk with playground fundamentals, a brief overview of playgrounds, markup, and imbedded resources. Then we'll show you how to run step-by-step in a playground. We'll discuss tips to give you more control of your code execution. Lastly we'll finish with advanced techniques including the wonderful world of Custom Playground Display Convertible. We will use this protocol to customize your inline results. We will also discuss how to use workspaces to exercise code in your frameworks. Playgrounds provides an interactive place to explore Swift, create learning environments or others, and prototype parts of your app. Let's get started with a quick tour of Playgrounds in Xcode. When you first open a playground, Xcode shows you the standard editor. This is the basic editor mode you use in Xcode with a couple additions. What is unique about this editor when you're in a playground is the result sidebar on the right hands side displaying the result of each expression in your code. You also have the ability to add these results as inline results. This is done by selecting the rectangular button beside your result in the result sidebar. This will then show your result inline with your Swift Code. In this example you can see this demonstrated where "Hello, playground" is added below our string variable declaration. Let's take a look at the assistant editor mode. What is interesting about this mode in Playgrounds in Xcode is that you can visualize your results in more detail by using a live view. In this example I've created a live view using UIView that shows a welcome message. But you could also use UIViewController for iOS and [inaudible] playgrounds and for MacOS playgrounds NSView and NSViewController. Now let me show you a short snippet that demonstrates how to show Live View. First you need to import PlaygroundSupport. PlaygroundSupport is a framework provided by Xcode to allow playgrounds to interact with Xcode, including support for showing live views. You import this playground support framework so you can use its API. Once you've created a viewController using the standard UIKit or AppKit APIs, you need to hand it over to the PlaygroundSupport framework. You do this by setting the Live View property of the current playground page to your viewController. This signals to Xcode that it should show your viewController in the assistant editor. To give your playgrounds an extra bit of polish, you can include some markup text. It is especially great when you're making a playground to share with others. With markup you can include stylized text, images, and video in your playground. Now let's take a quick walkthrough what is possible with markup. Here I've included some markup comments with text for a poem I'm writing. A markup comment is like a regular comment but with a colon after the forward slashes. The rest of the comment is then treated as markup text. If you have multiple lines of comments next to each other they form one block of markup text. You can also use multiline comments by putting a colon after the first asterisks. Here's our markup in Xcode now. It is showing the raw markup of the poem I wrote. To show it in its rendered form, select the button in the top right corner of the window, which shows Xcode's Inspector. Then select Render Documentation under Playground Settings. The lines in my poem are now rendered. Other interesting things you can do with the markup are adding headings to a Playground peach. You can use headings to create structure in your playground. You can use number signs to supply up to three levels of heading. In this example the title of my poem "Roses are Red" as a first level heading, the subtitle, "An Ode to Markup" as a second level heading, and the byline as a third level heading. Remember to add at least one space between the number sign and the heading string, otherwise you'll have a number sign connected to your heading when rendered. This is what the headings look like rendered when included with the lines in my poem. You can see that the first level heading is largest, followed by the second level, and then the third level heading. You can also format the text in your markup content. You can wrap a string of characters on a single asterisk on each side to italicize the text between the asterisks. You can also use backticks to display text with Code Font. Finally, if you use two asterisks instead of one the text will be bold. Let's take a look at this rendered. You can see that red and blue are italicized. Markup is in Code Font and fun is in bold. Let's take a look at using lists in markup. If a markup comment starts with a number followed by a period, it will create an item in a numbered list. In this example, I've added the lines of my poem to a numbered list. Here you can see the lines of my poem rendered in a number list, each line representing an item in a numbered list. You can also create a bullet to list in markup. A bulleted list is like a number list except each line starts with an asterisks instead of a number. This is how my poem in rendered in a bulleted list with each line in my poem represented as a bulleted item in a bulletin list. Markup can also contain links. In this example I've created a link to roses and violets. To create a link you wrap the text in brackets and then place the destination of the link in parentheses after. Another way you can create a link is by using a reference. In this example I've created a reference named 1, but this can be any string. It doesn't have to be a number. When creating and using a link reference you wrap the name in brackets. When creating a reference you add a colon and then the destination to the link. Here are the links represented in rendered form, Roses, Violets, and Fun are shown in blue to represent a link. Up to this point we treated a playground as a single location, but playgrounds can contain multiple pages, each with their own markup and code. To create a new playground page, select the playground and then open the file menu. Select New, then Playground Page. You could also create a playground page by control clicking on the playground and then selecting New Playground Page. You can create links in your markup, which navigate between pages. To go to the previous page, you can create a link where the destination is @previous. To go to the next page you can create a link where the destination is @next. Finally, if you want to navigate to a specific page, you create a link where the destination is the file name for that page without the extension and by replacing any spaces or special characters with their percent in coded form. You can also imbed some additional content to make you playgrounds even more powerful. You can add additional Swift files to the sources folder, which is at the top level of the playground. Each page also has its own sources folder. Sources are compiled as separate modules where they are automatically imported where visible so you don't have to handle import statements. Since these are compiled as separate modules, you can use access control to control what is exported from your axillary sources. Anything you want to use as your main playground source should be marked as Public. A great example of things to put into the sources folder is helper code such as classes or extensions that are outside of the main playground. Playgrounds can also contain other resources. Resources are any other file that you'd like to use in your playground such as images, audio, video, storyboards, and ZIPs. Just like sources, there's a resources folder at the top level of the playground as well as for each page. You can use these resources that you've added to your playground both in your markup and code. Similar to how you create a link in your playground, you can use this highlighted syntax to imbed an image in your markup. You specify the image name, in this case MyPicture.jpg. You also specify alternate text describing the image along with hover title text that shows my pointer is over the image. Both alternate text and hover title text can also be used for accessibility to benefit voiceover users. And here we access the same imaging code using the standard UIImage or NSImage APIs. Similarly, to images you can also imbed videos in your markup. This is done by using a syntax similar for that images with additional support for specifying a poster image along with the width and height of the video. For other resources that you want to access in code, such as this video, you can use the standard Bundle APIs for finding resources. This example uses a URL for resource with extension API to ask the main bundle for URL for Mvideo.mp4. Resources in your playground are automatically treated as resources of the main bundle. For more information on the markup you can use in your playgrounds and the playground support framework we discussed earlier, please visit developer.apple.com. With that please welcome my teammate Alex Brown to the stage so he can tell you more about exciting new changes to playgrounds in Xcode 10. Thank you. Thank you, Tibet. I'm Alex Brown and I'm a Core OS Engineer. I want to ask you a question. Have you ever woken up with a great coding idea but then struggled to set it down before you reach your code editor? Perhaps it's your inbox that gets in the way or perhaps you haven't yet set up your project within Xcode. Whether you're a beginner just getting started with Apple APIs, a seasoned engineer with a deadline, or a data scientist building machine learning models, I want to tell you that Xcode playgrounds are the fastest way to get started coding against Apple's APIs. In Xcode 10, playgrounds are faster and more responsive than ever allowing you to execute your code in a step-by-step fashion. First I'd like to get you familiar with a new UI for doing this. If you're familiar with playgrounds, your eye should immediately jump to the blue line over the not line numbers. There's a new play button which tracks your pointer. A blue line over the line numbers mean these lines are ready to execute and when you click the Play button this means run all of the blue lines up to and including the one with the play button on it. Let's see what that looks like. There we go. You can see just the first three lines of the playground are executed. The results are visible on the right-hand side. You can also see that the play button has now gone gray. This indicates that these lines of code are no longer ready to execute. You just executed them. There's a second reason that they play button might be gray. This means you're hovering over some code, which is not a top-level line of code. This includes code inside function brackets and inside of fall loop. If you wish to execute a fall loop you need to move your pointer to the closing brace where the play button will go blue then you'll be able to execute it. There's also a great keyboard shortcut, Shift Return. This is just like pressing Return at the end of typing a line of code, but it also means execute this line, and it move the cursor to the next line ready to write some more code. Blue code also has a second interpretation. It means code that it's safe to edit without resetting your playground. Why is this important? If you edit code above the blue line this is modifying code that you've already executed. So you'll have to resent your playground to take account of your changes. When you edit code above the blue line, the playground resets automatically. Sometimes you'll need to reset your playground manually and you can do this using the stop button in the Debug bar at the bottom of the screen. So why run in a step-by-step fashion? Why have we added this feature? Firstly, executing just one more line is fast. It's way faster than restarting a playground and waiting for it to catch up with your thoughts. Secondly, it allows you to respond to live data. Write a line of code, execute it, see what the results are, and this should lead you naturally to the next line of code to write. And thirdly rerunning a playground every time can give you different values every time. This can happen, for instance, if you're accessing a network resource or you're generating random numbers. By executing step-by-step your data model stays stable and easier to understand. So let's take a look at a simple example that you can try yourselves. I like games. I like card games and board games and computer games. I even like writing games. Now I'm not too hot at it so I like to keep my games small. This is a really small game. It's the game of roshambo. It's the schoolyard game of rock, paper, scissors. I've written the rules as a simple function check, which tells us if we've won. I've also written a computer player, which simply makes one of the moves at random. By executing step-by-step we can execute line six where the computer player plays and see what the result is before making our move. In this case the computer player has played "rock" and we've played "paper". Paper beats rock and we win the game. This might sound a little bit like cheating, but it's an extremely powerful technique. It's an extremely powerful technique when you're trying to learn a new and unfamiliar API or exploring a data structure whose types and values you don't yet know. So that's a very simple example. Now we're going to look at something a little bit more magical with a demo. So we started off with one game, I'm going to move on to another. This is the game of Tic-tac-toe. I've already been working on this a little while. I've built the game engine and I've made a first version of the UI. I've moved all of that code aside into the auxiliary sources. This gets it out of our way and allows us to focus on the next steps -- actually playing the game and refining the UI a little. I'm also going to use a live view, but first let's look back at the blue control over the line numbers. You can see that it tracks the mouse pointer. So let's get started by loading the game board. Let's rotate it so we can see it. This is the Tic-tac-toe game board. You can see I've been able to execute the first half of the playground and I've still got some lines left in the playground to execute. These include my first move. Let's run that. There we go. So you can see we're going to be playing the game in code and viewing the results with the live view. Let's let the computer play and have a turn. Few, that's great. The computer player is completely random. So it's just possible it's going to beat me today. So I've shown you how we can execute some of the lines of code and then step-by-step execute lines of code which are already present in the playground. But we can do more than that. It's always safe to add new code at the end of a playground. So let's make another move. There we go. Now, I can click the Play button to execute this line of code and I think now we'll let the computer have another go. That's great. [ Laughing ] You should notice a couple of things here. Number one, in the beginning the board was flat. I had to rotate it so it was in a position where we could view it. As we execute more lines of code, it doesn't rotate back to its original position. This is pretty good because I could write lines of code to rotate it to the correct position, but this would involve something called [inaudible], and I don't think I'm quite ready to learn that yet. The second reason it's good is because if the computer player was allowed to play again, every time we executed new code it would effectively get a do-over, which would be a little unfair. Let's finish this up. I think I can take the game home from here. Is that the right move? That's great. You can see the game has highlighted my winning move by placing three red circles. That's not too fancy. I'm looking for something which really makes it feel like a celebration. So I've been working on a new version of the UI. In a traditional development environment, you would have to set up a complex harness to automatically play you deep into the game or whatever the API was you were using so that we could test out something that happened later on in the program. But in a playground, because I'm able to execute step-by-step, I can play it manually to the end of the game and then write new lines of code to update whatever's going on at that point. Let's do that now. I've prepared a new effect for the end of the game using a particle system. Let's use Shift Return to execute it. Boom. And that feels much more like a win. So let's review what we've just seen. By running step-by-step we can explore an idea a line at a time. This enables us to have a conversation between code and data. Every time we learn a new fact we can write the next line of code to explore it a little more. We can use Shift Return to keep our hands on the keyboard and our brains in the zone. And finally, by adding a dynamic live view to a playground, we can create a second view of our model and we can seamlessly shift between manipulating it in a graphical environment and using code. So whether you're a beginner, just learning about new Apple APIs or an experienced programmer trying to sketch out the bones of their next great app, the next time you have a coding inspiration we invite you to get started with a playground. Just in case you're still looking for ideas I've got three right here. Number one if you're creating your own API, one of the best ways to showcase it is to create a follow along tutorial for your API. Users can execute the code step-by-step and see in real time what it does. Number two you can download and explore some publicly available data using the playground step-by-step to drill down into your data. You could get this from maps, from local government, or perhaps a class project. Finally, like I did, you could create a game or an animation. Start off simple using SpriteKit or SyncKit and line by line enhance it until it's completely awesome. So I hope we've shown you today that playgrounds are not a toy. They're fun but they're serious fun. They allow you to explore code and data interactively. This is great whether you've downloaded an unknown snippet of JSON using a rest API from the Internet, or if you're dealing with hundreds of thousands or millions of lines of data for a machine learning application. And to find out more about that use case I encourage you to download and watch the Create ML sessions in your WWDC application. Secondly there's really no better way to learn how to use Apple APIs. Whether you're just getting started or your trying out a new API, which you've learned about at this WWDC. You don't just have to use Apple's APIs. You can also import your own frameworks into playgrounds. And one of the best ways to showcase this is to create custom representations of your data types allowing developers to view the most relevant information at a glance. And to talk more about these two advanced concepts, I'd like to invite TJ to the stage. Thank you, Alex. My name is TJ Usiyan and I'm an Xcode Engineer. By the time I finish my section, I'm sure that all of you will agree that every librarian framework that you ship will and can be improved with the addition of a playground. Playgrounds provide a richer experience when providing readme's, tutorials, and general API documentation. One of the ways that they can do this is by, as Alex said, providing your own representation in playgrounds. I'm going to cover all of these things starting with Custom Playground Display Convertible, which will allow you to provide custom representations of your types in playgrounds so that users can get a nice summary. I'll follow that up with how to import your frameworks into playgrounds. And finally, I'll cover a little bit of troubleshooting tips for when things don't go exactly as you'd expect. Let's start with Playground Display Convertible. So as you know, when a user types in a line of code values show up on the right hand side in the result sidebar. For types that are no optimized for playgrounds there are two ways that playgrounds will generate this description. For types that do not conform to Custom String Convertible, we will create a structured representation using the Swift type. For types that do conform to Custom String Convertible, we will use the result of calling descriptions. When this isn't enough sometimes your users will tap on the Quick Look button on the right-hand side. They will also tap or click on the Inline Results button. When they do this a custom representation will show up. Often a textual representation. And for many types this is perfectly enough but sometimes maybe you would like to return a number. This is still text though and there are times when text isn't going to be enough and you would like to return something graphical, maybe a picture. The way to control what you return is to implement Custom Playground Display Convertible, a new protocol introduced in Xcode 9.3 and Swift 4.1. It replaces Custom Playground Quick Lookable, which was deprecated in the same versions. Let's take a look at conformance right now. You can see here that conforming involves only returning one property, Playground Description. Playground Description is of type Any, which means you can return anything that you feel best describes your value. Now, there are certain types that have specialized representations already and those are provided by Apple. Here's a list of the types with specialized representations as of Xcode 9.3 and Swift 4.1. The types on the left have a particularly textual representation. The types on the right have a graphical representation. I invite you to try each of these and decide which best represents your value in a playground. Once you've done this you'll want to ship all of your types, and your values, and your API to your users. I'm going to show you how to import your own custom frameworks into playgrounds alongside Apple frameworks. Typically, when you build a single framework in a project, that framework will end up in the Built Products Directory. When you want to import it into a playground, that is where the playground will work. The simplest way to make sure that your playground will see your framework is to add the playground to your project. This is the strategy that I suggest for simple projects where you have access the project and are willing to edit it. This is what that would look like in the Project Navigator. Once you've added this project, or once you've added this playground to your project, I have a little bit of advice. Remember building is very much like walking in that you have to remember to build before you run. Now, sometimes you have more than one project, more than one framework, maybe you have two, three, four, and in this case that Built Products Directory situation that I just described is actually multiplied. There are multiple Build Product Directories if you have separate projects. This can be a problem when you want to import your code into a playground. The simplest way to address this is to add each project to a workspace, a single workspace, and then when you build each project, those frameworks will end up in that one Built Products Directory for your workspace. After that just add the playground to that workspace and everything should be fine. This is what that should look like in the project navigator. Now, let's say you've done this, you followed my instructions and things don't work out how you expect. You say, "TJ lied to me." You're going to want to check the Built Products Directory to make sure that everything ends up there as you expect. Here's how you can do that. You're going to go to File, Project Settings, and then we're going to click the Advanced button. On this screen in light gray we'll see that there is a Products destination. You'll click on that and that should bring you directly to the Built Products Director for the project that you have open. This is what that should look like in Finder. Now let's see all of this brought together in a demo. Now those of you who know me know that I have a pretty enduring interest in music. I'm actually so interested in music that I have found this new notation, well new to me notation, called Helmholtz notation. And I feel that many, many people should know about this and it should be much more widely used. So I've created a framework to share with other developers so that they can use it in their code. You can see here that I've included a playground and I've also, as the result of extensive research on Wikipedia written up a little playground that describes the notation. Follow it, followed by a few values so that we can see the results on the right hand side with the notation. If I render this documentation this will all end up in Pros with the links highlighted in blue just as Tibet spoke of. And before I build, or before I run I'm going to build. Then let's run our playground, yay. You can see here that on the right-hand side in the results I have this interesting notation. I really think that this notation is interesting because the lower pitches are represented by uppercase letters. The higher pitches are represented by lowercase letters and the exact octave is indicated by either commas or apostrophes, the count thereof. Now, hopefully you all get this looking at this notation, and hopefully I've explained it very well, but I've actually had some trouble with some of the people I'm tutoring. They're not taking to it as well as I'd like. So I actually have a separate framework that I've created with a keyboard visualization that I think will really help them ease into this notation. Since I want to use that, I'm actually going to create a separate playground in a workspace where I can import both frameworks without changing either of the frameworks. So to do that I'm going to quickly close this, I'm going to create a brand new workspace by going to File, New Work Space, and I'm going to name this Tutoring. Just put that on the desktop. Then I'm going to add each framework by going to File, Add Files. I'm going to start with Helmholtz, my framework that I just had open, then File, right there, Keyboard. Then I'm going to add a new playground by going to File, New, Playground. A blank playground is fine. My Playground is a fine name, a fine as name as any. I'm going to start by importing Helmholtz, and My Keyboard. After that I'm just going to quickly do a little test to make sure that pitches show up as I expect. I'm going to build each of my frameworks. I'm going to make sure to choose each one from the schemes. Later on I might want to make a separate scheme that builds both of these projects in one go, but I don't have time on stage right now. And then I'm going to run My Playground in it. Everything works as expected. But again this visualization is not helping. I have some younger students who have only seen a piano, so I have a little snippet here that quickly creates a view in playground description, configures it, and actually I have one more line that I wanted to write on stage. Custom text equals description. And after I configure my view I'm going to return it. Simple as that. Made much simpler by the fact that I happen to have a framework already on hand. Now, I want to run this line, line 4 again, but with my conformant. So I'm going to reset this playground by clicking on the stop button down here. And then I'm going to rerun this entire playground after I add my inline result. And you can see here -- You can see here that I have a nice visualization of a keyboard that my students can recognize along with the notation that I'm trying to teach. Now let's cover or let's go back over what we've learned today. Tibet covered the fundamentals of playgrounds, the layout and playground markup syntax. Alex explained the power of running step-by-step in playgrounds. And I covered Custom Playground Display Convertible and importing your own code. So hopefully, now that I've given this session and all of us have -- and we've explained playgrounds and the power that you can get, we hope to see that every project has a playground next year. If you have any questions please feel free to visit us in the labs and we'll see you next year.  Hello, everybody. Welcome. I'm really excited to be here today. So let's get started. Shaders are important part of graphics, and they help you create great-looking games and apps. Given the massively parallel execution environment, working with shaders is not easy. There are a number of things that can go wrong during development, and even when you get it right, they have to perform really fast. My name is Alp and today I am going to show you great tools and workflows to help you debug and profile Metal shaders. Let's talk about Metal first. Metal is not just about the framework and the language. We have Metal kits, Metal Performance shaders, and we have great tools to make your development process efficient and easy. With Metal system trace, fully integrated into instruments, you get visibility into how your CPU and GPU work in parallel and the runtime performance of your app. It's a great first tool to start profiling your apps, and seeing your bottlenecks. But today, my focus is going to be on Metal frame debugger. Metal frame debugger lets you capture Metal work with a frame of a game, or computer upload, and step through your course. Inspect state and resources. And it provides great profiling features, giving you access to GPU counters and pipeline statistics. It is fully integrated into Xcode, providing you one place to debug and profile your Metal workloads. And this year, we have some great additions to Metal frame debugger. With the dependency viewer, now you get visibility into how your encoders are working together and connected through the use of resources. It's a great tool to navigate your workload and just understand how your [inaudible] are working together to create that frame. Tomorrow at 10:00 a.m., we have Metal Game Performance Optimization talk, and part of it, we will be covering the dependency viewer. But today I will be focusing on geometry viewer, shader debugger, and enhanced shaded profiler. And I will be showing you workloads that will help you in your day-to-day shaded elements. So, let's start with the geometry viewer. Vertex Stage is the first stage of the graphics pipeline. And when you get it wrong, it can mess up your entire scene. So it is important to rule out some of the vertex issues before moving on to debugging your shaders. In Vertex Stage, there are several things to check. Mainly, your vertex inputs, your indices, and your vertex outputs. Last year, with Xcode 9, we introduced input attribute view where we can see all this data combined in a table view format. But it is still hard to debug such visual data without really correlating back to 3D. So that's why this year we combined all this data and created one place you can visualize and inspect it. Now, I would like to introduce you to geometry viewer. Geometry viewer is a new tool that visualizes the post-transform vertex data in 3D so that you can actually see what your vertex output is. Alongside, it provides you your data input indices and output, so that you can actually correlate that back to 3D. It is available per-draw call just like your attachments and bond resources, so you'll have one place to investigate all your vertex issues. Now, let's take a look at three common vertex problems, and let's see how you can use geometry viewer to tackle them. So this is the first problem, and I am going to draw call, drawing the tree, and as you can see, some of the vertices of the triangle is off screen, causing this triangle to get skewed. Just using the geometry viewer, you can click on the visibly wrong triangle. And just using the viewer below, you can see all the data that these vertices are taking in and outputting. And in our second case, I'm on the same draw [inaudible] but this time, the tree is completely missing. Just using the geometry viewer and the free fly camera, you can see that the tree is drawn completely outside of the Frustum. So it makes sense that it is not rendered on screen. So it is really easy to use geometry viewer to detect cases like this, and on to our last case, sometimes it's not even possible to form triangles to visualize in 3D. And these are tricky to debug. You might have degenerated triangles, where you have multiple of your vertices going into the same location. Or you might have infinites and [inaudible] in your vertex output position, which is undefined behavior for GPU. Thankfully, geometry viewer detects these cases and provides an issue button for you. With a single click to that, you can get a list of issues it finds. In this case, it found several degenerate triangles. Clicking one, you can directly see the data. And just looking at this data, I can clearly tell that two of my vertices are going to the origin, so it's a degenerate triangle. And so as you've seen in all these examples, geometry viewer is a powerful tool to quickly investigate all your geometry-related issues, and if the problem is in your inputs, then you have to go back and check your model. But if the problem is in your output, then you have to debug vertex shaders. Which brings us to our next topic. Debugging shaders, using the new Shader Debugger. Shaders are hard to debug. They have math-heavy code, with lots of vector operations. And they are highly parallel. The shader code you are writing gets executed millions of times, every single frame. Here is the scene from Unity's Book of the Dead demo, that we've seen in the keynote and State of the Union earlier this week. In a single frame of this scene makes 10 million vertexing locations, and rendering into 60 million triangles, every single frame, in all the different passes. So when you have a problem with your shaders, it's not going to be easy to find where the problem is, which shader it is, and what part of the shader code. So to help you debugging your shaders, now I'm really excited to introduce to you the new Shader Debugger [applause]. Shader Debugger is a new tool for debugging all Metal shaders, providing you rich visualization across thousands of threads at once, and it shows you the real data from GPU, not from emulator. It provides you flexible way of setting, so that you can follow your execution easily, without needing to have breakpoints at all. And it is fully integrated into Metal frame debugger, so that you have everything you need to debug your shaders. Now, I would like to bring my colleague, Xavier, to the stage, to show you Shader Profiler in action. Xavier? Thank you, Alp, and good afternoon everyone. For today's demo, I wanted to create a cool-looking water simulation, but as you can see, there are some geometry problems. My name is Xavier, and I am really excited to show you how to use the new Shader Debugger and geometry view ware to identify and fix geometry issues. So we are going to start by taking our friend capture. You can easily do this by clicking on the UP cutter button here in the debug bar. Now that we have captured our frame, we can start debugging our issue. Since we are having geometry problems, we are going to use the new Geometry Viewer. The Geometry Viewer allows you to navigate and inspect the geometry of your draw call. The water surface should be smooth, but as we can see, there is a steep cliff. So let's pick one of the triangles that looks wrong. For example, this one. And now, let's select the broken vertex that is the one here at the bottom. You can easily do this, by using this table here. So now that we have found our broken vertex, let's take the input positions. As we can see here, all of the input positions are very similar for all of the vertices of this triangle, and in fact, the white coordinate is exactly the same. So this looks correct, and it was inspected. Seeing the input is correct, but as we can see here, in the 3D view, the output is wrong, then the problem has been our vertex shader. So let's use the new Shader Debugger to debug and fix this issue, just by clicking on the Debug button that you can find here in the bottom right corner. And just like that, we are now in the Shader Debugger. It is that easy. The first thing that you will notice in the Shader Debugger is that alongside your source code, you have access to all the variables that were modified on its line, and if you want more details about your variables, you can just open the detail views by clicking in this detail view button here. The detail view will show you in language of source code the full value of the variable. And not just that, but also the value of the variable across all the vertices of the triangle that you are debugging. Here in the Debug Navigator, you have access to the [inaudible] history, which shows you what your shader did from the very beginning to the very end, and as you can see, it's really easy to step through your code by just using the arrow keys. As you may have noticed, as I navigate through the navigator, its position in the source code is highlighted. And this works in both directions. You can select one line in your source code and its position in the Debug Navigator will be selected. And at any point, you have access to all the variables that are alive in a scope by just taking the variables view, here at the bottom. So now that we are more familiar with the Shader Debugger, let's debug our issue. The water is calculated by generating a displacement vector that is later added to our final position. So let's get more information about this displacement vector just by opening this detail view. As we can see here in the detail view, the right coordinate for the vertex that we are debugging is a big, negative number, compared to its more positive one in the other vertices that were correct. Then the issue must be in how we are calculating this displacement vector. So let's use the debug navigator to go through the function and investigate what is happening. And here, without even having to read the code, we can already tell that there is something going wrong. One of our variables has a value that is not a number, and this is likely corrupt in further math. So let's check why this is happening. If we check here in the debugger, it turns out that we are doing this operation on this negative number, and this is what is causing the [inaudible]. So now that we have found the cause of our issue, let me quickly fix this just by swapping those two lines, so that we don't do operation on a negative number. And now that we have done our changes to the shader, let's try out those changes just by clicking in the reload changes button, here in the debug bar. Reloading the shaders will run your full [inaudible] using the new shaders, so that you can check the results right away. And as we can see here now our water surface is smooth. But let's check it in action, just by clicking on the continue button here in the debug bar. And, as we can see [applause begins] now our water simulation is correct. In this ex-- in this demo, we have seen how to take a frame cocktail, how to use the new geometry viewer to find broken vertices, and how to use the new Shader Debugger to debug and fix your vertex shaders. Thank you, and back to Alp. Pretty cool, huh? Thank you, Xavier. Now that you have seen Shader Debugger in action, let's take a closer look at how you can use this tool for your debugging needs. First things first, though. Let's talk about how you can get into the Shader Debugger. Here I already captured a frame, and am on the last roll call, and then you will have draw or dispatch call selected, the debug shader button on the debug bar will be enabled for your easy access to the Shader Debugger. Clicking that will show a view for you to select the thread you're interested in. And selecting threads means different things depending on the type of shader you are debugging. For fragment shader, you will be using the pixel inspector to select the pixel you want to debug. For vertex shader, you will be using the geometry viewer we have just seen to select the vertex you want to focus on. And for compute, you will be using the controls to select the thread you are interested in. And then, just click the debug button, and that's it. You are in the Shader Debugger. And whilst you're in the Shader Debugger, it is so easy to inspect any variable you are interested in. You just need to go to the source line you have the variable, and that is all you have to do. You don't need any break points, you don't need to step through your code. The values of the modified variables in the line are going to be available on the side bar. And as you have seen in the demo, we highlight [inaudible] and infinite values in there, so that it's really easy to spot common mistakes. You can just use the button on the side bar to enable the detail views, and get full details about your variables. This is pretty handy, especially if you want to inspect the complex [inaudible] and want to dive into different components. Or you can just hover on any variable you have in your source code, and you will instantly see the value of the variable at that point. And this is partially useful if you are sampling from a texture, like in this example, and you want to quickly check whether this is the right texture. And just like in CPU debuggers, you have access to your variables view, showing you all your variables in scope. So inspecting variables is a core part of debugging your shaders, but that is only half of the story. You also want to know what is the order of your code getting executed? And Shader Debugger makes this really easy as well. When you get into Shader Debugger, in the Debug Navigator, you will see all your source line execute by your shader for your selected threads. And you can select any line you want and directly navigate to that. This also provides a unique stepping experience where you can just use your cursor keys to step through your code. Functions are groups in the navigator, so that you can just expand them and step into them. This also enables backward debugging, which is a really powerful way of debugging your shaders. Just find the variable that has the wrong value, and just work your way backward to see where it got wrong, and why. Also you can use the filtering. Filtering is a great way to focus, and you can filter for anything you are interested in your shader code. Filter will be matching with function calls, variables, resources. And your shader code might span across many different files, but if you use filter, it will only match with what your shader executed, so it's a really great way to focus. So far, what we have seen is great for debugging a single thread, but what about other threads? Shader Debugger also gives you access to other threads based on the initial thread you selected. What this means is, for vertex, you'll get the primitive of your selected vertex. For fragment, you'll get direct angle area, and on your selected pixel. For compute, you'll be getting the full thread group of your selected compute thread, so that you have access to all variables interacting with the thread group memory. Now, let's look at how accessing other threads can help you debug fragment shaders. So while your code executes millions of different pixels, seeing a single variable for a single pixel may not help you in all the cases. Here, I have an example, I'm calculating a gradient, and I can't see the single gradient value. But I can't really tell whether this gradient value is good for my pixels, or bad for my pixels. With a single click, to bring up the detail views, I can see this gradient value across thousands of other pixels around my pixel, and now I can tell this gradient is not correct because I can clearly see it's not smooth. As you've seen, detail views help you understand your shader, and identify good and bad values for your shaders. You can quickly hover into any other pixel to instantly see the value of that variable for that pixel. And you can also use this view to switch threads. Switching threads is great for comparing cases where you can clearly see good and bad pixels. You can quickly switch between them and see what the back pixel is doing. Once we switch threads, we update the execution history and the variable views, so that you can fully debug your newly-selected threads. This gets even better. Detail views also help you understand your divergence. So whenever you enable detail view, the mask on the right-hand side will always show you what other threads executed the same line of code. In this example, it's conditional, and just looking at the mask, I can clearly tell that roughly half of my threads executed inside this-- if conditional, in a diagonal shape. So now, to show you how you can use detail views to fix problems in fragment shaders, I would like to invite Xavier back to the stage. Xavier? Thank you, Alp. After fixing our geometric problems in the first demo, I added highlights as a process and effect. But the results were not as suspected. So I took a frame capture to debug the issue. It was here in the [inaudible] view, we can see that our highlights are actually generating dark areas in the bottom parts. So let's use the Shader Debugger to debug and fix this issue. By not pressing, I'm bringing up the pixel inspector, which allows you to select the pixel that you want to debug. So here, we are going to select one of the pixels, for example this one, and to start the Shader Debugger, you have to click the debug button here on the bottom right corner. And now, we are debugging the frame and shader. As we can see here, the highlights are generated in three different steps. So let's use the detailed views to quickly understand what is happening. Let's start at the beginning on this close first one. As we can see here, we are just sampling from our original color, and this looks correct. So let's see what is next. Here, we are accumulating highlights in one of the directions, so let's take the results by opening the detailed view. And as we can see here in the detailed view, there is already dark area in the bottom part, right here, so then the problem must be in how we are accumulating those highlights. As you can see, detailed views are a powerful tool to understand your shader without even having to read the code. So let's use the Debug Navigator to go to that first one and investigate what is happening. As we can see here, this function is mainly a loop that iterates a few times, accumulating samples from [inaudible] map. Loops are highlighted just in the Shader Debugger, which means that you can navigate through your iterations using the Debug Navigator, and you will get, alongside with your source code, the value of the variables at each one of the iterations. If we focus here, we can see that as I navigate through the different iterations, the values change. But what is really powerful is combining this with detailed views. So here, we are going to bring up the detailed view for final color, that this is where we are accumulating our highlights. And now let's iterate using the Debug Navigator. As we can see, as I iterate, we are starting to accumulate highlights in our final color. But as I continue iterating toward the end of the loop, we are actually seeing this, there is a pattern. So let's investigate why this is happening. As we see here in the Debugger, we can actually see that our weight is negative. And this is what is causing the color to be distracted rather than accumulated in our final color. So now that we have found the cause of our issue, let me quickly fix this. And now, let's check the results, just by checking in the reload changes button, here in the debug bar. And as we can see here, now there is no more other carriers, but let's see it in action, just by clicking on the Continue button. And indeed, now our highlight looks correct. This is just an example of how you can use the Shader Debugger and its powerful detailed views to debug [inaudible] Shaders. Thank you. And back to Alp. Thank you, Xavier. Now that you have seen everything about Shader Debugger, let's recap what we've talked about so far. Shader Debugger is specifically designed for debugging Metal shaders, taking into account of the highly parallel nature of GPUs, and as you've seen, it's great for fixing bugs with the highlights to nans and infinites, it is so easy to spot common mistakes, and also it's great for understanding your shader. With the detailed views, you don't even need to read the code to understand what your shader is doing. And also it provides excellent environment to develop your shaders in. Just get into Shader Debugger and start editing, and read out to quickly iterate all your results. Shader Debugger supports iOS, Mac, OS and tvOS with the recent hardware. And it is yours to enjoy with Xcode 10. And now that we've seen how you can use geometric viewer and the Shader Debugger to debug your shader problems, now let's focus on how we can make your shaders run faster. So optimizing your use of GPU is important, because you want your apps to run fast, and consume less battery for a great user experience. But before starting to optimize, it is important to know what to optimize. If you are GPU bound, typical Metal workload has many different passes. And knowing which pass to focus on is quite important. The good news is we have great profiling tools built into Metal frame debugger to help you with this. GPU counters gives you high level performance [inaudible] of your app. That you can see, time [inaudible] or draw call, and also providing you different counters showing bottlenecks. And from here, once you know your expansive pass, you can just move to pipeline statistics. And here you get great compile for your shaders, giving you the type of instructions you have. And using the Shader Profiler, you get visibility into timings for draw call or per-pipeline states. So today, I'll be focusing on Shader Profiler, and I will show you work flows that will help you optimize your shaders. So once you are in the Metal frame debugger, going to read by performance navigator, you will see all your pipeline states sorted by the time it takes to execute them. This is a great place to see all your expansive pipelines that we are optimizing. And here, you also have access to your draw-calls, using these pipeline states, so that you can directly navigate to one and see what it is drawing. And if you're in iOS or tvOS, just going to the Shader Source, you'll get the per line execution cost. This is a great place to see where you are spending the most time in your shaders, per line basis. And once you are in your shader source, you can just start editing your code, and reload to see if it makes any difference in timing. Or you can just get into Shader Debugger. This is a great workflow if you are trying to optimize an algorithm or when I just get through this control flow, and when I understand whether you can. So Shader Profiler helps you a lot trying to optimize your shaders, but we made this even better for A11. Now, A11 [inaudible] being the completely [inaudible] design GPU, now we have hardware support to give you even more insight in what your shaders are doing. Now with A11, you get this chart, per line, showing you the cost breakdown of the executed line. So Shader consists different execution units, and if you are using one of them heavily, you might be bottlenecked by that. So seeing ALU and memory breakdown gives you visibility into like time spent in different execution units. We did a lot of work for how precision instructions to be much more efficient in A11. So using health, overflows, might bring significant things on your GP budget. And another category now you have visibility into is synchronization, which is an important category to look at because you want your Shader core to be busy doing work, not waiting for synchronization. And in your shader code, if you are reading from texture, buffers, or using atomics or barriers, you might have stalls waiting for synchronization. And also now we provide you ability to see cost of in-line functions, you are calling from your shaders. This way you can directly navigate to your most expensive function, and start optimizing that [applause]. Welcome to the Apple designed A11 bionic chip. We can provide better profiling data than ever. This year, we are introducing per line timing information and instruction categories, even in inline functions. My name is Max, and I am going to help you max out your Shader performance [applause]. So I've captured the same water simulation again, but this time, on my iPhone. Let's begin by change to view frame by performance. We now see a list of pipelines being used in our frame, ordered by time, and we can easily see that the noise pipeline is taking a significant amount of time. Disclosing this, we can see that we spent nearly all the time in our fragment Shader. But this year, we can go deeper. We can see the time we are spending each inside of each of the functions that this shader is calling. Just clicking here goes to the source code file, and directly jumps to the right line. And that is the start of our investigation. So what we see here right now on the right-hand side is a percentage of time we are spending inside of this function, and further down the percentage of time we spend in each file-- each line. Be aware the Shader Profiler works on optimized code, so the compiler can reorder the instructions and there may be no timing information for some lines. Next to the timing information, we have the instruction categories. Let's take a look. We can see that we spend time in ALU doing math operations, we are reading from memory, from textures, but what raises my concern is the time we are spending in synchronization, specifically, waiting on memory here. That means our GPU could not read data fast enough, and it could not hide this latency by doing ALU work in other threads. So let's figure out why. Just following the performance numbers, we will arrive at this line, where we spent over 50% of our entire shader time. Taking a look into the instruction categories again, we can see that we spend a significant amount in synchronization. But why? We are using a color value here that we have read from a color texture here using an offset. This offset was read from a noise texture here. So we have a dependent texture read. The GPU could not process until data from the texture read became available. So what can we do about this? Well, instead of reading from a noise texture, we could just simply calculate a noise value in our GPU. Let me do that. Now, let me update my Shader. The Shader is now sent to the device as we compiled, and then the entire frame is rerun and reprofiled, because changing a single Shader or pipeline might influence your entire GPU timing. Once that is done, let's take a look. We can see that we now spend just three milliseconds in the shader from down over to 20. But let's also take a look into the instruction categories. Here we can see that we nearly eliminated the entire time in synchronization, and we keep our GPU busy doing work for us. So, as a summary, when you detect the bottleneck in your shader, the per line timing information tells you exactly where your problem is, and the instruction categories help you to understand why. That's it for me. Back to my colleague, Alp. Thank you, Max. And before closing, I would like to talk about one last thing to make it easier for you to work with these new great tools. Hopefully all of you are already compiling your Shaders offline, so that you are not paying the runtime cost of online completion. And if you are compiling your shaders offline, now we have a new compiler option to save Shader sources into the built Metal [inaudible] so that you have access to your shader sources from anywhere using these tools. If you are compiling your shaders using Xcode Project, you can just go to Build Settings, and Enable this option. Or if you are compiling using command line, you can just pass dash MO option to the Metal compilers. But please remember to only enable this in debug builds so that you don't mistakenly ship your shader sources alongside your app. All right. Let's recap what we talked about today. We started with geometric viewer, providing you one place to investigate all your vertex related issues, and debugging visual problems, please remember to check with geometric viewer first, and make sure that your geometry is right before moving on to debugging your shaders. And then we have seen shader debuggers, powerful tools to debug all your Metal shaders. Great for fixing bugs, understanding your shaders, and just developing your shaders. And within [inaudible] profiler, now you get even more visibility into what your GPU is doing with the power of A11 bionic chip. For more information, please check the link. Tomorrow at 10:00 a.m., we have a great talk on Metal game performance optimization, where we will be talking about common performance problems that we found in games and tools available for you to detect and fix them. Thank you [applause]. Good afternoon. My name is Pete Hare and I'm an engineering manager on the App Store team here at Apple. We're here to discuss the best ways to build your app and server infrastructure to support the subscriptions. I'm going to go over a few topics here today. Firstly, we'll discuss the best ways to actually build your app and server architecture. We've got some tips and tricks around how to enhance the in-app experience for your user. My colleague Michael will get up and talk a bit about reducing subscriber churn. And then finally, we've got some new announcements around analytics and reporting tools available to you. But first up let's talk a bit about how to build your app and server infrastructure. Let's start off by asking a simple question, what is a subscription? Well a subscription gives a user access to your content or service by them repeatedly paying you some amount of money. When you look at this at the engineering layer a subscription is really just a set of repeating transactions each of which unlock some subscription period. To use subscriptions in your app there's a couple of things that you need to do as a developer to be able to handle these transactions as they come in. Let's go through each of these steps. Firstly, it starts off with your app receiving a transaction. Once your app has received a transaction you need to go ahead and verify that it's an authentic transaction, that money really has changed hands. Then it's up to you to update and maintain that user's subscription state for ongoing access to your service. So let's go into each of these in a little more detail. Firstly, let's talk a bit about receiving that transaction in your app. Now whether it's the initial purchase of a subscription or a renewal transaction for a prescription your app is set up to handle subscriptions and transactions using the StoreKit framework. Now when you are set up to handle transactions using StoreKit the App Store will make these charges on a user's credit card in the background. And anytime a transaction occurs it informs your app of these transactions via a thing called the SKPaymentTransactionsObserver. This transaction observer object is really the central piece of in-app purchases when it comes to your application. It's actually just a protocol in StoreKit, it looks like this and you can set it on any object. In this example we're just setting it on the AppDelegate itself, but the really important thing is that you add a Transaction Observer to the default payment queue as early on in the application lifecycle as possible. Once you've got a Transaction Observer registered on the default payment queue you're ready to start receiving transactions as they occur in the background. You receive transactions on a callback in the Transaction Observed called updatedTransactions and it's here that StoreKit will inform your app of a set of transactions for you to process. They can come in various different states that we're not going to go completely into in this talk, but keep an eye out for a transaction in the purchased state. That's a transaction that StoreKit is telling your app is ready for verification and unlocking. Once you've got a transaction the purchase state you're ready to go with that next step, which is to verify that it is an authentic transaction. So when it comes to checking for authenticity how can you know that money really has changed hands? You use a thing called the App Store receipt. The App Store receipt is just like a receipt you'd get in a department store, it's a proof of purchase that a user has bought something that they say they've bought. In this case it's a trusted record of the initial app download and any in-app purchases that have occurred for this app. This is a digital document, it's stored on the user's device, we provide an API for you to access it and it's put there by the App Store. We also sign this document using certificates so that you can check to make sure that it is an authentic document that's actually issued by Apple. And finally, the document is for your app on that device only. So if you've worked with subscriptions before you'll notice that maybe one user with multiple devices has receipts that look slightly different on each device. When it comes to actually verifying this transaction that your app's been told about the first step that you need to do is to actually verify that the document in question, the App Store receipt, is authentic. So how do you do that? You can do this in two ways. Firstly, you can use on-device validation and this is where directly on the user's device you can use a series of checks to check the certificates used to sign this app and verify that it's authentic. Or you can use the technique called server-to-server validation. This second technique is where you take that binary encoded receipt data, send it up to your own server, and then from your server over to the App Store for processing. And the App Store will actually do those checks for you. You can use either of these techniques, but whichever one you choose it's important not to use online validation directly from the user's device, this is not a secure way of actually validating this document is authentic. But let's compare each of these two approaches in a little more detail, especially around subscription management and order renewable subscriptions. Both of these techniques give you a way to validate that this is an authentic document. They also give you access to the contents of the receipt, any transactions that have occurred for this particular user. But when it comes to auto renewable subscriptions there's a few key advantages that server-to-server receipt validation actually gives you over on-device receipt validation. Firstly, we include some additional subscription information in the response to that validation check. You can use this, Michael will talk about a little later in this talk. Your server is always on to handle those renewal transactions in the background. This is really important if you've got a service with maybe multiple platforms. Your server is not susceptible to a device clock change. If you're using on-device receipt validation for subscription management on the user's device there's actually nothing stopping the user from winding their clock back and putting themselves into a valid subscription period, maybe a free trial that they've actually lapsed their subscription from. And finally, it's just a little simpler. The server-to-server validation you're dealing with JSON API, you don't have to build OpenSSL or use ASN.1 decoding. So with all these things in mind we're really encouraging more and more people to actually adopt server-to-server validation when it comes to maintaining an auto renewable subscription state. If you've got a simple utility app that doesn't need any kind of networking you can still use on-device validation for subscription management. And if you're interested in finding more about that I'd invite you to check out the video from last year's events StoreKit Talk where we went into more detail about on-device receipt validation. But for the purposes of this talk we're really going to focus more on the server to server techniques outlined here. So let's go back to our example and see how we can use server-to-server validation for this transaction that we're processing. Back here in our Transaction Observer you can access that binary receipt data using the App Store receipt URL API on the main bundle. Once you have this URL you can pull out the binary data located at that spot on the file system and you can pull out that receiptData and base64Encode and this will give you a nice string that you can send up to your own server for processing. You might provide some in-app networking API for the current user. When you're sending that data up to your server for processing firstly you'll do this securely obviously. You can send it up to maybe a process transaction endpoint on your server. In this endpoint you might have a user ID associated with the current user for an account on your own system. You can send this receipt data up to your server and then once you get that on your server you can establish a secure connection to the App Store's verify receipt endpoint. And you can send over that receipt data to the App Store. Here you can include a password field, this is just a shared secret between your app and the App Store. You can set this up in App Store Connect and store it on your server. And when you send this receipt data to the verify receipt endpoint the verify receipt endpoint will respond with JSON payload that looks a little like this. The first thing to check when you're verifying that this transaction is authentic is this status field. This is an indication that Apple has actually issued this document in the first place. Once you've verified that the status is zero like this you can check the contents of the receipt portion of this payload. This is the decoded version of that binary data that you just sent to verify receipt endpoint. So in here you can do things like verify that the bundle ID in this receipt matches your app's bundle ID. And then you can inspect the in-app array, this contains a list of transactions for this particular user for your app. And you can verify that the product ID associated with this receipt is the one associated with your app. So assuming that these all match you're determining that this receipt entitles this particular user to your subscription product you're ready to go ahead now with that third step, updating the user's subscription state. Now in the same way that each subscription period starts with a transaction it also ends with an expiry date. And the response from verify receipt tells us each of these expiry dates for each transaction. So looking back at this response from verify receipt you'll notice that there's this expires date field in the transaction and the response. Let's bring up the user table now that you might be saving this data on your server. You can take this expires date from this transaction and actually populate this into a field on your own server, maybe the latest expires date field for this particular user. And this field is going to become the new source of truth on your server for whether or not this user is a subscriber. While you're here you should also keep track of this other field, original transaction ID, and you can save that in the field called original transaction ID against this current user as well. Well come back to this one in just a moment as to why that's important. But once you've saved these two bits of information against this current user on your server you're ready to go ahead with the last step, which is to tell the device that this transaction has actually passed your verification checks. And then when your device gets this callback it can call Finish Transaction back down in your Transaction Observer. This is a really important step because finishing the transactional will actually clear it out of your payment queue. And if you don't call Finish Transaction it might reappear there the next time the app launches for processing. So make sure you finish every transaction that begins in StoreKit. Once you've finished the transaction you've got an updated subscription state in your server, the user is now free to enjoy that subscription period on your service. Now let's take another look at that user table I mentioned that you're storing on your server. Each user who purchases a subscription using this setup will be assigned a unique original transaction ID, that's that field that you saved from the transaction response. And this identifier essentially becomes that user's subscription ID. And it's important because it shows up in all subsequent renewal transactions. Let's take a look at how this works. So let's say that you're verifying a renewal transaction, this happens in just the same way you might use the same process transaction endpoint on your own server. When you do those techniques of verifying the transactions a valid one, and you get up to that stage of updating the user's subscription state you'll observe here that there's now multiple transactions since this is a renewal transaction. Now according to your existing server-side logic this latest expires date is now a date in the past, so the user is not currently a subscriber and you need to figure out are they still a subscriber based on the data in this receipt. So how can you use this receipt data and make that deduction? Well for discovering whether or not the user has an active subscription you can pull out the transactions associated with that original transaction ID. And then you can find the transaction that has the latest expires date. Now if you find a date in the past that's an indication that this user is no longer a subscriber anymore. But if you find a date in the future that's an indication that this user is in a valid subscription period. So let's look at how this works in the example that we're going through. Grab that original transaction ID associated with this user and pull out all the transactions associated with this subscription. Then sort those transactions by that expires date field and grab the one that has the latest expires date. Now you can take that expires date and update that latest expires date field associated with this user. And when you do that you're effectively extending that user's subscription by another length of time and your server-side logic now knows that the user is in a valid subscription window. Of course when you're dealing with renewal transactions like this that have come through StoreKit you still need to inform the device that it passed those validation checks. And have your device, your app call Finish Transaction back down in StoreKit again. So let's say you have this set up and working correctly. The App Store is charging the user's credit card in the background and you're using StoreKit to process these transactions as they come in through the app. And then your server is updating and maintaining this latest expires date field on your server. So you've got that server-side source of truth as to whether or not a user is subscribed. Let's now introduce a slightly more complex example though to the mix, which is maybe that you offer your service through a website as well. Now when the user accesses your subscription service through a website you know based on that latest expires date field that the user is a subscriber. But as much as we'd like to think that people use our apps all the time let's say that the user doesn't use your app for a number of days. And then during this time the App Store actually successfully renews a user's subscription in the background. When the user tries to access your servers through your website that latest expires date is now out of date because your server hasn't learned about that new transaction. So how can your server know about this transaction that's occurred on the App Store? You use a technique for this called status polling, this allows you to discover these transactions directly from your server. In order to get set up to be able to status poll from your server you just save a latest version of that encoded receipt data that you send up associated with each user. And what you can do is you can treat that encoded data just like a token. The reason why it can treat it like a token is every time you send up that encoded receipt data to the verify receipt endpoint the verify receipt endpoint doesn't just respond with a decoded version of that receipt data, it also includes any new transactions that have occurred for this particular user's subscription. It's located in a field called the latest receipt info field in that JSON response. And you can use that info to unlock those new subscription periods for this particular user without the app having to launch at all. Let's see how this works. So when you're verifying transactions just like we saw before you're sending up that receipt data. Now once you've determined that this transaction in question has passed those same checks that you did before you can take that receipt data and store that in a field called latest receipt data against the current user. And now that you have that latest receipt data stored, that's a base64Encode string with the user. When it comes time to answer that question, does my user have an active subscription you can just take that latest receipt data from your server directly and send it to the verify receipt endpoint. You can also include here an optional flag that's the exclude old transactions flag, this is just telling verify receipt that you don't even want to know about that decoded version of the receipt you just want to find out about any new transactions. Verify receipt will respond with that particular object, the latest receipt info object. And inside this object is those new transactions that have occurred before this receipt data was actually generated. And you can take the expires date directly from the response of the latest receipt info object and update it against the current user, again extending them access to that next subscription window. And so the user who is on your website trying to access your content can now access that next subscription period all without the app having to launch with that new transaction. If you are using this technique though status polling it's important to remember one thing and that's that when your app does come back online again transactions will still appear through StoreKit in the updated transactions callback. And you still should handle these transactions passing them up to your server for verification and finishing them back down on the user's device again, even if your server already knows about them through a status poll. We'd encourage you to use that as an opportunity just to send up that new latest receipt data for storage against the current user on the server. Now status polling works great when the user's credit card is able to be charged, but what if some subscription period the user's credit card has some kind of billing issue and the App Store isn't able to charge it for the next subscription period. Is this user destined to remain unsubscribed involuntarily? Not at all. When reacting to billing issues like this you can do three simple steps. Firstly, you can observe that there's been no renewal transaction for this particular user, their subscription has now lapsed. Secondly, you can direct that particular user to go and update their billing info. And then the third step, when a renewal transaction occurs unblock that user immediately after it happens. Now, steps one and two are pretty straightforward using the status polling techniques that we just went through, but step number three here uses a feature that we actually launched last year server-to-server notifications. Let's walk through this example, let's say that one subscription period the App Store has an error when trying to charge this user's credit card. And then you find out about the fact that there is no new renewal transaction for this user through a status poll. Your server correctly makes the calculation that this user is not a subscriber anymore and so when the user comes along to access your service through the website you give them some appropriate error message about the fact that their subscription wasn't able to be renewed. And you can direct that user to go and update their billing info in the App Store. Now when the user does update their billing info, maybe they just need to update an expiry date or something, two things happen. Firstly, the App Store will actually immediately charge that user's credit card and make a successful transaction. And when the App Store does do this the second part of what it does is it sends your server a direct HTTP post upon the successful renewal. And then the payload of this post includes the new transaction info for the transaction that's just occurred. You can use that original transaction ID field located in the payload to locate which user this notification is for. Once you find out which user you're talking about, well you can grab that latest expires date and update it for this new user giving them access again to that next subscription period. And then the user who's probably still sitting on your website trying to get access to your content will be able to immediately be unlocked since your server received that push straight from the App Store. It's really important to unlock users in a speedy manner when this sort of thing happens, especially when they go to all that effort for updating their credit card info manually and waiting for access to the servers. But there's one thing to note here and that's the notification is only sent if the subscription actually lapsed like we just saw. To discover successful renewal transactions you still need to rely on status polling techniques we just outlined before. But it's really easy to set up to use server-to-server notifications. All you do is enter a URL in App Store Connect. This is just an endpoint on your own server and if you enter it into App Store Connect the App Store will begin to send your server HTTPS posts for these status change events. And as we saw included in the post is that latest transaction info for the transaction that triggered it. You do make sure your server is ATS in order to receive these, but it's a really simple step that you can do to give a bunch of users a much better experience. So those are some tips and tricks around how to build your app and server architecture. Let's talk about three tips that you can use in your in-app experience to really enhance the user's experience. Firstly, we made the assumption earlier that the user was signed in to an account on your own service. In order to keep track of each subscription ID you need this particular user table located on your server. Now when it comes to actually creating accounts we think it's best to offer in-app purchase before an account creation step. Why is that? Well as the user it's a much better experience, you can just open the app for the first time and buy the subscription immediately getting access to the content you're after. For you it's better because you get higher conversion rates, users don't have to go through funnels of entering emails and passwords to be able to get access to giving you money. Now you can use the techniques that we just went through by using an anonymous account in these instances. And you can rely on the original transaction ID if you need to associate multiple devices. If you're using anonymous accounts like this, when it comes time down the road to actually give the user an account creation flow, you can just simply take them through a deanonymization step where you update those email fields and maybe other personally identifiable fields. So that's tip number one. Tip number two is around selling your in-app purchase. When it comes to selling your subscription, you can use a feature that we launched last year Introductory Pricing. Now there's an important step with Introductory Pricing which is that you need to know at runtime whether or not a user is actually eligible for an introductory price. And the reason you need to know this is you have to know which price to render to your user, whether to render the normal StoreKit price or the introductory offer that you want to offer a user to get them in the door. Now you can set yourself up to actually know this ahead of time by monitoring the transactions that are occurring in the background as they come in. Let's see how this works. When you're verifying transactions like we just saw earlier keep an eye out for these two fields. The is trial period field and the is in intro offer period field. If either of these fields are true that's an indication that an introductory offer or a free trial was used for this particular transaction. And if it was you should keep track of the product ID in question against this current user. You might store them in a field called consumedProductDiscounts. Now if you're keeping track of which products were used with introductory offers, when it comes time to render the price of some new subscription product that you want to show your user this is how you can do it. You can take those consumed product discounts saved against the current user and execute an SKProductsRequest with them. And the reason why is that the response from SKProductRequest now in iOS 12 includes the subscription group identifier so you know which subscription group this particular product is from. And now armed with this subscription group identifier you can keep track of that in a set of consumed group discounts for this particular user. So you know which subscription groups this user has used offers for. Now when you want to render the price string of say product A it becomes a simpler check. You can check to see if this user's list of consumed group discounts contains the group identifier for the one you want to sell them, in this case product A. And if it does, that's an indication that this user has actually used an introductory offer here before, so you can render the normal price string to this particular user. But if not, they're still eligible for that introductory offer so you can use that introductory price located on the SKProductObject. Now when rendering the price string nothing's really changed here it's the same techniques that you use for rendering any in-app purchase and I'm not going to go into too much more detail here, but I'd encourage you to check out the video of the session just before this one where they talked about rendering these price strings a little more dynamically. For more information about setting up introductory offers I'd also recommend you go to the What's New in App Store Connect session on Wednesday at 5 p.m. in hall three. So that's tip number two around introductory pricing. The third tip here is around subscription management. You can offer the user upgrades and downgrades between subscription tiers right in your app's UI. And to do this you can actually treat it just like selling an initial subscription. Now if the subscription you're selling the user is part of the same subscription group, so it's a different tier than the one the user has already subscribed to you can basically just create an SKPayment just like you would if you were selling them an initial subscription. And when you do this StoreKit actually handles the fact that it's an upgrade or downgrade for you. So you don't have to worry about that user being subscribed twice. Now if you don't want to provide your own UI for upgrades and downgrades in your app you can also just provide a link out to the App Store subscription management screen. We provide a link for you to be able to get to this screen directly from your app and here the user can upgrade, downgrade or even cancel their subscription. Now your app is often the first place a user will go for subscription management, to be able to upgrade, downgrade or cancel. So it's a really good idea to give some kind of link maybe in your app's settings for a user to be able to do this. To actually get to this screen there's a link available on our In-App Purchase Programming Guide and here it is if you enjoy writing down links. So these are some simple techniques you can implement in your app to give a user a pleasant experience using subscriptions. Next, I'm going to hand it over to my colleague Michael who's going to go over some great techniques for reducing your subscriber churn. Thanks. Good afternoon, my name is Michael Gargas and I'm a technical advocate on the App Store operations team. Today I want to talk about reducing subscriber churn inside your applications by using some of the tactics and methods that Pete just walked you through. Today we'll cover involuntary churn and voluntary churn, the two types of churn you'll see inside of your subscription applications, as well as some ways to win back those subscribers that you potentially may have lost or will lose. First, let's talk about involuntary churn. Involuntary churn is the loss of a subscriber due to a failed payment or billing issue on the platform. Now last year at Dub Dub DC we walked you through what we're doing in order to minimize involuntary churn inside of your applications. We announced our updated Billing Retry service where we expanded our retry duration from 24 hours to up to 60 days. We also implemented new retry strategies and we tuned them over time to recover more and more of your subscriptions. A date to remember is July 13th, 2017 because this is the day that Apple actively started recovering subscriptions for you. If we look at the performance of Billing Retry since launch we can see that our recovery rate has more than doubled. And when we look at involuntary churn we've cut this by over 2% platform-wide. Now if we look at how our tuning has impacted the recovery of subscriptions we can see that quarter over quarter we've been able to continue to recover more and more of your subscriptions. Now the net result of this has been 12 million of your subscriptions recovered since the launch of Billing Retry. So that's what Apple's doing to minimize involuntary churn for you. But there's also some tactics that you as a developer can do to minimize voluntary churn inside of your subscription applications. You can leverage some of the subscription specific receipt fields that Pete mentioned earlier. You can implement grace periods and during that time you can deploy some effective customer messaging. So let's take a look at an example subscription. Here we can see that our subscriber was set to renew on April 26th, however, they encountered a billing issue. So in order to let you know that Apple is actively attempting to collect funds from that user via the Billing Retry service we are going to surface a field in the JSON response aptly titled is in billing retry period. A value of one signifying that we're attempting to collect funds for that subscriber. If we go back to our example of subscription you can see that this has been added to JSON response. And when you see this in conjunction with the expires date this is your signal as a developer to implement what we call a grace period. You may ask yourself what is a grace period. A grace period is free subscription access while in a billing retry state, however it's before you have lost that subscriber, before they've churned out. The goal of the grace period is to improve recovery. So let's take a look at how we can do that leveraging some of the information in the receipt response. If we flip back to our example subscription you can see that our subscriber is in a billing retry state and was set to renew on April 26th. Here we want to add some server-side logic to use that expires date field and the is in billing retry period in order to add a period of time, in this example three days where that user will continue to have access to the service and technically stay subscribed. Now why would you do this? Well it's an opportune time to deploy some effective customer messaging to contextually communicate with your subscribers and let them know that there may be an issue with their subscription. You may want to do things like ask them to update their payment method or have them restate the value proposition of the subscription offering that they're subscribed to. And during this time you can offer limited service as well, such as a browse but not watch experience in an entertainment app. Here we can see Peak, a subscription developer on the App Store. Peak is leveraging the Billing Retry status fields in order to surface a contextual message to their subscribers letting them know that there's been an issue with their subscription. When engaged upon they're taken to an additional screen which clearly states what the issue is and how they can resolve it. But it would be really effective if from this screen you could drive that user or subscriber directly to our systems to update their payment details. So I'm excited today to announce that we have two new URLs coming shortly after Dub Dub DC this year, one to drive users directly to update their billing information and the others to take them to manage their subscriptions as Pete alluded to earlier for upgrades, downgrades and crossgrades. With that being said, a lot of developers will ask well when are we seeing our users be recovered. And on average we see the majority of users recovered within the first seven days of entering a Billing Retry state on the platform. This might be a time to offer a full access grace period because we see a lot of users self-recovering during this time. You may want to deploy that customer messaging towards the tail end of this to bring in some of those subscribers that might've taken longer to come back into your application. Let's flip back to our example subscription. What happens if this is effective and we're able to recover these customers? When a retry attempt is successful the date of the retry or recovery becomes the new subscription anniversary date moving forward and this will be reflected in JSON response when validating that successful transaction and finishing it. But we're not going to stop there we're also going to deploy our server-to-server notifications so that you can immediately unlock access on all platforms and close the loop with your customer letting them know hey you're all set. So that's involuntary churn where the customer didn't technically make a choice to unsubscribe from your application. But what is voluntary churn? Voluntary churn is the loss of a subscriber due to customer choice, either cancellation or refund requests. To be clear, this user actively made the choice to leave your subscription offering. So what can you do as a developer to minimize voluntary churn inside of your applications? Well Pete walked you through how to status poll earlier and you can implement that to get some key subscription details about your users and when you get that information you can use it to offer attractive alternative subscription offerings to potentially save that user. So let's talk a little bit more about status polling. With the release of server-to-server notifications there's really only two key reasons that you still need to status poll. The first is understanding will my subscriber churn in the subsequent subscription period and the second being has my subscriber renewed. We often get asked when should I status poll as a developer, when should I try to catch those users and see their subscription state changes. The most effective times that we see are doing that status poll at the beginning or end of a subscription period. By deploying this responsibly you're most likely going to catch most users that may want to voluntarily churn from your subscription offering. But when you status poll you'll also get access to some additional subscriber status fields. And you may want to take those fields and save the decoded JSON response from the verify receipt call in the user tables on your databases. Or alternatively, you can parse out specific fields such as the Billing Retry Status in order to segment your customers and maybe understand those that are in a retry state and those that are not. But the signal to understand if a customer will voluntarily churn is shown via a field called auto renew status. Auto renew status will let you know with a value of one that that subscriber will return in the subsequent subscription period and a value of zero letting you know that they will voluntarily churn at their next subscription anniversary date. Let's see what this would look like in our example subscription. Here we have a subscriber purchased on March 26th and they've made the choice to disable auto renew via the manage subscription setting screen. Now coincidentally, we status polled shortly after this happened and we were able to see via the receipt response that the auto renew status has changed to zero. It's at this point that you can update your user tables on your database or on your server and segment that customer as a potential voluntary churning customer. If we go back to the example subscription we've status polled, we've captured this customer might leave, so what should we do? As a developer this is your opportunity to present an attractive downgrade offer potentially in the same subscription group. Here we can see Peak trying to save that user by potentially having them take a lesser term or a lesser price subscription of a different duration or maybe a different offering. If that subscriber decides to engage with this the same way that Pete showed you by surfacing upgrades and downgrades within your applications we want to let you know what product they will renew on in the subsequent period. We do this via the auto renew product ID field in the JSON response. So this differs from product ID in that this is what the next offering will be after that subscriber renews. Here we can see in the example subscription our subscriber has elected to downgrade instead of churn out. We've changed the auto renew status to one and we've added auto renew product ID. It would also be beneficial to be notified of this change immediately, so for this we'll also send a server-to-server notification letting you know that your subscriber did change their renewal preference. This is useful if you want to maybe state the differing service levels between what they're currently subscribed to and what they will have in the subsequent period. Now it's impossible to run a 100% retention subscription business, so it's important to understand how you could possibly win back some of those subscribers that you might have lost. A win back is reengaging subscribers after they've churned by showing them resubscription offers or potentially surveying them to understand why they've left. If we look at our example subscription let's see what a voluntary cancellation would look like and how we can leverage that inside of our app. Here our user has elected to cancel via AppleCare. In order to let you know we're going to surface a cancellation date field in the JSON response. This is your signal to understand this customer has contacted AppleCare and either canceled or requested a refund. But as a developer you would want to know this information immediately. For this we'll deploy a server-to-server notification. This is important because you'll immediately want to shut off access to those users on all platforms and potentially prompting them to see alternative subscription offers. Now after this user has churned it would be important to easily segment between those that have voluntarily chosen to unsubscribe and those that have involuntarily been unsubscribed due to a payment issue or billing issue. So for that we have the field expiration intent in the JSON response. Now to be clear, this will only show up after the subscription has lapsed. And we really want to focus on two key values, the first being value one which signifies voluntary churn, the second signifying involuntary churn with a value of two. If we flip back to our example subscription where our customer canceled via AppleCare you can see that we've added the expiration intent field to the receipt response with a value of one. So what do you do as a developer when you see your subscribers in this state after they've already churned and you've segmented between those that have voluntarily and involuntarily churned? Well for voluntary you may want to survey those subscribers who have opted into an account on your system. You can ask them maybe why the service wasn't appropriate for them and what you can do to tailor it moving forward to provide a better experience for them or other users. Additionally, you can always just show alternative subscription products potentially within the same group because if they re-subscribe you want to continue accruing your time towards that 85/15 revenue share. Here we can see Peak when a user has voluntarily churned being shown a resubscription offer, in this case a 60% discount. For involuntary churn since the user did not actively make the choice to unsubscribe it's appropriate to just show the same or alternative subscription products. You may want to leave some persistent messaging if that user is logged in inside the application experience letting them know that they have lapsed, but that they can always come back. And you may also want to deploy a limited subscription experience, such as a browse but not watch experience in an entertainment app. Here we can see Tinder, when users interact with pro level or subscription level features they are continually prompted to subscribe. So in summary, if you take anything away from this section on reducing subscriber churn it's that you should be leveraging these subscription receipt fields effectively. You can then implement status polling to understand when your subscribers may voluntarily churn. You can then use that status polling to deploy some targeted and effective customer messaging. And then to close it out, presenting contextual subscription offers to these users to hopefully win them back or prevent them from churning in the first place. So with that I'd like to hand it back to my colleague Pete to discuss analytics and reporting. Thank you. Thanks Michael. If you haven't implemented handling of these JSON fields yet we highly recommend you try it and watch the great effect it has on your retention rates. It's not often as engineers we get the chance to make such simple architectural tweaks that can have such a big effect on a business' revenue, so take a look. Now we've got some great new updates today in the areas of analytics and reporting. Located in App Store Connect the sales and trends section contains a huge amount of useful information. Now you can get even more insight into your app's performance. This existing subscription summary dashboard now includes monitoring for subscriptions that are in those Billing Retry windows. This is great for gaining insight into your subscriber behavior and to determine the most effective amount of time to offer those grace periods for like Michael just talked about. This year we're also introducing a brand-new dashboard for subscription retention. This page offers a glance at how your introductory prices are going, as well as how many of the users are subscribed to the higher proceeds rate, that's that 85 to 15% split you get when a user has been subscribed for a year. The dashboard includes new graphs to help you quickly identify which subscription cohorts are the highest performing and you can monitor your subscription performance over time and compare your app's performance across different territories. Now all this new information is not just available inside the App Store Connect report, but it's now available through the new App Store Connect API. The report data here is available to you daily and you can script your own setups to import this maybe into your own data warehouses for further analysis. We're not going to go into any more information on the App Store Connect API here, but I really recommend you check out the automating App Store Connect session in the hall three on Thursday at 3 p.m., there's some really exciting enhancements in the area of automation. Now we've talked a bit about what you can get from receipts and what you can get from these App Store Connect reports. So as a bit of a summary, App Store receipts are useful for validating those StoreKit transactions and updating user subscription states, maintaining the state on your server. And you can also use them to understand individual subscriber behavior just like Michael showed you. For App Store Connect reports it's for a slightly different reason, they're better at that macrolevel analysis, maybe understanding subscription pathways of users of your app and maybe most importantly, understanding how much money is going to get deposited into your bank account for your subscriptions. Now we've covered a lot of topics here today, but as a bit of a summary remember that server-side state management offers you much more flexibility when it comes to managing subscriptions. If you haven't done it yet add that URL to receive notifications from the App Store. Consider offering an introductory price in your app, it's a great way to get users in the door to your own subscriptions. Add some simple messaging to reduce subscriber churn, using those fields that Michael walked us through. And for users that have actually lapsed offer some alternative subscription options maybe to win them back. Finally, remember to check out these new reporting tools available in App Store Connect. For more information on this session and for the video, this has been session 705. We're also going to be in the labs this week right after this and also on Thursday at 9 a.m. we'll have engineers from the StoreKit team and for App Store Connect ready to answer any questions that you might have about engineering subscriptions. Thanks a lot.  Good afternoon. I'm Geoff Coffey. I'm an engineer on App Store Connect and I'm here to talk to about automating App Store Connect. And I got to be honest with you, I'm really excited about what we're going to talk about today. So let's start by talking about where we are with automation in App Store Connect today. As I'm sure you all know, you can use Xcode to upload your builds to App Store Connect and to download your crash reports and we have the Transporter tool that lets you automate uploading the metadata.xml and also automate your build uploads. And then we have Reporter, the command line tool for downloading your sales and financial reports. And these tools are great. A lot of you use them but we've heard from you that you want even more. You want access to more areas in App Store Connect and you want to integrate with more diverse workflows. And so we're really happy to introduce the new App Store Connect API. We're excited too. So this is a standards-based REST API for App Store Connect. And if you ask me, that's the best part about it, it's a stock standard REST API with JSON responses, so it's going to feel familiar to a lot of you right out of the box. That also means you can use this API from any platform, almost any programming language, and using the tools that you're probably already using today. And of course, the API needs to be secure. So we use industry-standard JSON web tokens for authentication. And that just means you don't have to pass usernames and passwords around, and you don't have to have your code tied to any specific person on your team but you still maintain control of who can access your data and what they can do. From an ease-of-use standpoint, we focus really closely on consistency with this API. We have a single unified REST resource model, which just means that you can take what you learn in any one part of the API and apply it to every other part. And we've also built discoverability into the API itself, in simple ways like when we return JSON data to you, it's nicely formatted and indented so if you need to, you can dump it out to the console and read it right on the screen and we include links and those responses to related information to help you figure out how the parts fit together. And, of course, the API will be fully documented. If you haven't seen this already, the documentation platform that you already use at developer.apple.com and in Xcode was extended this year to also include Apple's REST APIs. So the same format you're familiar with your framework docs will apply to the App Store Connect API as well. Now we started with a focus on the areas of App Store Connect that don't already have an automation story and that you have to visit over and over again and in particular we're starting with these four areas you see here. So let's look at what all is included. First, we have comprehensive coverage of TestFlight. You can manage your testers and groups. You can submit your builds for review. And if you saw the session yesterday, the What's New in App Store Connect Session, you know we just announced a new public links feature for tester acquisition. And this works hand-in-hand with the API. You can use the API to manage your public links and you can use the two of them together to run your beta test program in whatever way works best for you. On the Users and Roles side, you can add and remove users and keep the permissions on your users in sync with your actual organization. And as we announced yesterday, this includes your complete unified user base across developer website and App Store Connect. So you have one set of users and one set of roles. We have provisioning APIs that let you add development devices and register bundle IDs and manage your certificates and profiles, and then we have reports APIs that let you download the sales and financial report files. And if you use the Reporter tool today, this will feel really familiar to you because the parameters you send to Reporter are very similar to the ones you send to the API. So it's really easy to make the switch. And I said there were four things and this last part isn't really part of the API itself but I wanted to bring it up because it's an important part of our automation story. We've made a couple of important changes the Transporter to help smooth all of this out for you. First of all, Transporter will officially be supported on Linux. And also, you can take the API tokens that you use for the new API and send those through to Transporter and it can use those for authentication as well. It just makes the process easier to manage for you. Okay, we have a whole bunch to get through. We're going to go kind of fast today. I apologize for that but we have a lot to cover. We're going to talk about getting data and changing data with the API. We'll talk about relationships which is how the different parts of the API fit together and how you work with those. We'll talk about how to handle errors, how to get access to the API and authenticate your request, then we'll wrap up with some best practices. So, you ready to get started? So like I said, we're going to start with getting data and that means we start right here, api.appstoreconnect.apple.com. This is the home base of the API and from here we want to construct a URL. First we will add a version. Now all API endpoints have a version and right now that's always v1. But inevitably as App Store Connect grows and changes over time, we'll have to make changes to the API that might cause your code to stop working. So when that ever happens, we'll change this version number and the old version will keep working for a period of time so you have time to adjust to our changes. Now after the version, we have to add something called the resource type name. This is a really important concept in the API. The resource is like the fundamental unit of the API. Conceptually, you can almost think of the API as a big collection of resources that you operate on. We have a whole bunch of resources in the API. Most of them map to things you're familiar with in App Store Connect and we're not going to talk about all of those today. We'll focus on just a few and we'll start with Users. And so here we have a complete API URL: api.appstoreconnect .apple.com/v1/users. This URL represents all the users on your team. And, of course, you can go get this URL and you'll get back a JSON object. And the first thing you'll notice about this subject is it has a data property. Anytime we send you a successful response with data in it, we include this data property. In this case, it's an array of user resources. Now, you can only see one user on the screen right now but if the screen were much, much taller, you'd see that all the users in your team are in this array. Now I want to focus in on some of the data we return to you with each of these resource responses. Every resource has a type just telling you what it is and an ID that uniquely identifies this resource across all of App Store Connect. Then we have the attributes. This is probably the stuff that you're most interested in. For users, it's things like their first name, last name, and email address. The values of these attributes are usually simple types like this, strings, numbers, dates and times or Booleans but they can also be complex types sometimes, like arrays and objects. After the attributes, we have relationships. We're going to skip those for now. We'll come back and look at them later. And then we have links and in particular what we call the resource self link. Now this is a URL that uniquely identifies this particular resource, in this case the first user in the result here. Every resource we send back to you includes this resource self link. It always looks like this: api.appstoreconnect.apple.com and then the version, v1, and then the resource type, users, and then the identifier of that resource. And you can go get it and you get back data that looks almost exactly like what we just saw. The only difference here is we're seeing just this one user now instead of all the users on your team. So that's two ways to get data. You get a list of resources or an individual resource, but of course you're going to want to change data as well. And to do that, we lean on common REST conventions, so this will also feel familiar to a lot of you. You've already seen how to get resources. To create a new resource, you'll use the http POST method. To change a resource, you'll use PATCH, and to delete a resource, you'll use the DELETE method. So let's try this. Let's say we want to add a new user to your team. Now a quick aside. We can't add the user directly. Just like in App Store Connect itself, you have to invite the user to your team and then they can accept that invitation. So we're going to create a user invitation resource. It looks like this. We do a POST because we're creating. And the URL is the user Invitations resource URL. And then we have to send in the data for this user. Now this data looks a lot like what you've seen when we fetched users before. But there's one important difference I want to point out. We have a type user Invitations but we don't have an ID. Now Apple will assign an ID to every resource you create for you. So you don't include that in the POST. We've also left off links because they're not required for creation. And we've left off relationships because they're not relevant in this particular case. If we run this request, we get a response. In particular, this is a 201 CREATED response. That's just standard REST behavior to tell you that the resource was successfully created. And we also include the full resource information in the response data and this is important for two reasons. First, it gives you the ID and the resource self link of the resource that was created, which is stuff you're probably going to need later so you can come back and operate on this resource. And also if you look at the attributes, you may see attributes in this response that you didn't include in the POST. For example with user invitations, Apple will assign an expiration Date to every user invitation you create. Now you don't set that, so you don't include it in the POST. And that's why this response data is really important to you. It shows you the complete picture of this resource after it's been created and all of our rules have been applied to it. Now if this user accepts, they'll become a user on our team. And suppose we want to come back later and modify them in some way, like for instance you can see from the attributes here this user has the developer role. So let's change that so they have the developer and the marketing role. We're changing an existing resource. So we do a PATCH request. The URL this time is the resource self link of the user we're changing. And if you look at the attributes, we're only including the roles attribute. This request says change this user so that they have the roles of DEVELOPER and MARKETING. We didn't want to change anything else about the user, so we specifically didn't include the other attributes. If we run this request, again we get a successful response and we get the full resource representation of the resource with our changes applied. Now the last thing you probably want to do with the resource is delete it and this is the simplest operation of them all. We just do a DELETE request to the resource self link and we get a 204 NO CONTENT response. If you're familiar with REST, you know that any status code in the 200s means success. So this is the API telling you that the user has successfully been deleted. And we didn't need to include any additional data this time, so we use the NO CONTENT version of a success response. Now before we move on to the next segment, I'd like to invite my colleague Sehoon Shon up to show you some examples of how to use these resources in real-world scenarios. Sehoon. Thanks, Geoff. Hi, everyone. My name is Sehoon Shon and I'm a software engineer in TestFlight team and I'm here today to show you the live demo on App Source Connect API. We'll look at one use case and see how we can apply the API on users resource. Let's say someone in our team has left our company and we'd like to find this user and remove this user from the team in App Source Connect. So let's go see a demo. We'll begin by getting the list of all the users in our team by sending GET request to the users resource, which is GET v1/users. And this returns all the users we have access to in our team. Now let's try to find the user we're looking for and we can search by the email of the user by applying the filter parameter. So this filter parameter of email specified that we would like to find the users that matches JohnAppleseed@mac.com. So let's send this request. And we get a response with a user that contains the email of John Appleseed. Now let's use the ID of this user to get the instance of this user, which is users/ID and this should return the instance of the user with a matching ID. And we get a response with the username John Appleseed. So we found the user that we're looking for. Let's try to remove this user by sending DELETE request to self link of this user. So we'll replace GET with DELETE. And this should remove the user with the matching ID. And we get a 204, which means the deletion was successful and NO CONTENT since the content of user is no longer available. Finally, let's confirm that the user is indeed removed by sending GET request to the self link once again. And now we get 404 NOT FOUND and it looks like the user is indeed removed. So in this demo, we saw how to get collections of user, how to find instance of user, how to search user by applying filter, and how to remove user by sending DELETE request. And this concludes the demo on how to use App Source Connect API with the users resource. And I'll give the stage back to Geoff. Thank you. Thanks, Sehoon. So now you know how to create, read, update, and delete all these various resources and it and might feel like that's basically it, that's all you need to be able to do but isn't the whole story and that brings us to relationships. Sometimes it's not the individual resources that you're most interested in but the connections between them. It helps if I give you an example. We have a resource called Beta Groups. These represent all your groups in TestFlight. We have another one called Beta Testers that represent all of the people who can test your applications. And as you know, you can put those testers into groups. So how do we model something like this in the API? Let's start by getting Beta Groups. We go a GET v1/beta Groups and we get back an array of Groups. And if we look at the first group and in particular the relationship section, we see that this group has three relationships: app, beta testers, and builds. We're only going to talk about Beta Testers today. So we'll pop that one open. And again, we see another links section. These are links associated with this beta testers relationship on this first group in the list. You'd have a section like this in each of the groups in the array. There're two links here, the first one we call the relationship self link. And it's a URL that represents the relationship itself. And we use this URL to operate on this relationship. Now let me explain that. We said we want to add testers to this group, right, but I want to be clear. The testers are already in TestFlight and the group is already in TestFlight. So we want to take these existing testers and put them into this existing group and that's a operation that doesn't fit with any of the things we've learned about already, if you think about it, right, like we're not creating or editing or deleting testers and we're not really editing the group either, at least not the attributes of the group. So conceptually, you might say that we're creating new connections between this group and its related beta testers. And this is where the relationship self link comes in. It looks like this. We do a POST request to the relationship self link. And if you look at the data we're sending, it's these type and id pairs of the two testers in this case that we want to add to this group. So this request says take these two testers and put them into this group. We don't have to include more tester information because the testers are already in TestFlight. If we run it, we get a 204 NO CONTENT response again. That's the API telling you that the testers have been added to the group. I'm sure some of you are wondering "What if I want to take testers out of the group instead?" And the answer is you do it in exactly the same way. It's the same URL. It's the same data format. You'll just use the DELETE method instead of the POST method. Now let's take a look at Beta Groups again and look at that second link. This is what we call the related link. This URL represents the actual related data, in this case the testers in this group. If we go and get this data, we get back an array of beta testers. The format here looks exactly like what you'd get if you did a Get of v1/beta testers. The only difference is we're getting the testers in this group rather than all the testers in your TestFlight program. And I want to pause there and make sure I'm being clear. We're getting the testers in this group and when I say "this group," I mean the group whose identifier is in that URL. So this URL get the testers in one group. If I want to get the testers in a bunch of different groups using this mechanism, I'd have to make a bunch of different requests, which can sometimes be inconvenient. So we do have one more way to get related data. It's called the include parameter. It looks like this. We do a GET of v1/beta Groups and then we add this query parameter: include = beta testers. And this tells the API while you're bringing me back the beta groups, also include information about the related testers in each group. Here's what it looks like. We get our array of beta testers, of beta Groups, excuse me, and if you look at the beta testers relationship in the first group, you see this new section called data. And this has the type and id pairs of the testers in this group. Now if you could see the whole response here, you'd see a data section like this inside each beta testers relationship inside each group that's in this array, right. But this is just the type and IDs. I'm sure you're wondering where the actual tester information is. And if we scroll all the way down to the end of this response, we also have a new section called include. And this is an array of beta testers with the full tester information. More generally the include section has a full resource representation of every resource that you have included by way of relationships. And then we can match by type and id, figure out which testers belong to which group. Now you might be wondering why we do this. We have the data section with all of our groups and then we have the include section with all of our included testers. And we do this for an important reason. We have this test on the screen John Appleseed and that tester might be in multiple groups, right. The way we have this structured, the data for John Appleseed is only in the response one time, no matter how many groups they appear in, in the response. Make sense? Okay. Sehoon is going to come back up now and show us some more real-world examples with relationships. Sehoon. Thanks, Geoff. So for this part of the demo we'll look at TestFlight and see how we can create new beta groups and add some external testers and we'll see some relationship between testers and groups. So let's go see the demo. We'll start by creating a new beta group by posting to the beta groups endpoint which is v1/beta Groups. When we're creating a resource, we also need to provide the payload that looks like this. We have the data with the type of beta Groups and a set of attributes. And since the name is the only required field when we're creating a group, let's give it a name and we'll simply call it Test Group. And I'll send the request now and we get a 409 CONFLICT response. So let's look at the details. It says, "You must provide a value for the relationship app with this request." So in TestFlight, you cannot create a beta Group that does not belong to any app and, thus, we must create a relationship to an app while we're creating beta Group. And how do we do this? We can include the relationship in the payload like this. So now we have the relationship in the payload of an app with a data type of apps and the id specifies which apps the beta Groups should be linked to. So this will create beta Groups with the name of Test Group and link to an app with the id. So let's resend this request. And now we get a 201 CREATED response. In the response, we have the id that is generated. So let's copy this id. So we just created a group called Test Group. But what if you don't like the name of the group we just created? We can modify the existing group by sending PATCH request to the self link of this group. We'll send PATCH and then beta Groups/the id and we'll also need to provide the payload that looks like this. This looks a lot like payload for posting but we also need to provide the id to make sure that we're modifying the correct data. And let's paste the id of the beta Group. And the only information we'd like to change is the name. So let's rename it to WWDC Group. And I'm sending the request and now we get a 200 response and it looks like the name has been updated. So now we have the group that we wanted. Let's start adding some external testers. To create tester, we send POST request to the beta testers endpoint, which is v1/beta testers. And we'll also need to provide the payload that looks like this. It has a type of beta testers and a set of attribute. So we'll create tester that's called Kate Bell and this also has a relationship to the beta Groups and this will create Beta Tester and also assign to the beta Group at the same time. So if you paste the id of the beta Group we just created, this will assign the tester to the WWDC Group. And we get a 201 CREATED response and it looks like the tester is created and also assigned to the group at the same time. While we're here, let's add one more tester and all we need to do is simply replace this attribute portion with a different person's name and this will create John Appleseed to the same group. And again, we get a 201 CREATED response. So we just added two testers to the group and now let's confirm that these two testers are indeed inside the group by sending GET request to the related link between beta Groups and beta testers. So this link, /the id of the group/betaTesters, this will return all the beta Tester that are assigned to the beta Groups with the specified id. And as you can see, we have John Appleseed that's added earlier. And this response we have a lot of information including list of attribute as well as relationship. But what if you only care about maybe the email of the tester? We can trim down the response by applying the special parameter called fields and this will allow us to trim down the body and only look at what we would like to see. If you do fields of beta testers equals email, as you can see now the response only contains the email of the user and nothing else. And we have the two testers that are added to this beta Groups. So in this demo, we saw how to create beta Groups and beta testers and so some relationship between testers and groups. And this concludes the demo on How to Use App Store Connect API with TestFlight. And I'll give the stage back to Geoff. Thank you. Thanks again, Sehoon. That was awesome. So that's getting data, changing data, and relationships. It kind of covers the basic features of the API. But as you saw a glimpse in Sehoon's demo, sometimes you make mistakes and we can't process your request. So we're going to shift gears now and talk about how the API communicates these errors back to you. Anytime a request fails, you'll get a response that looks like this. Now the first thing to notice is that we'll send back an appropriate HTTP response status indicating what went wrong, usually something in the 400s. And this is often all you really need. Most REST client libraries have a Did Succeed or it Is Success function and you can call that and it will correctly tell you if the request was successful or not. But if you want to know more about what went wrong, you can look down into the response. When a request fails, instead of that data property we've been seeing all along, we'll give you an error's property. This is an array of error objects. But there can be multiple of them, each one representing some problem with the request you sent. They have an id that uniquely identifies this particular error of this particular response. You might log this away and if you think there's a problem on our end, send it to us. It'll help us track it down. More useful for you is the title and detail. These together give you an English language explanation of what went wrong. From the title here, I can see that I have something wrong with my parameters. And from the detail, it tells me that I'm filtering by email and right away I see that I spelled email incorrectly. So these are great values to log and use in your troubleshooting and learning the API, but you don't want to use these in your code. You don't want to try and interpret these with your code. I don't make any promises that we won't change the wording on these messages over time without warning. For programmatic error handling, you want to use the code property instead. This is a stable machine-readable string that represents what went wrong. It's a hierarchical value with dots between levels of specificity. So in this example, I can see that I have a parameter error, generally, and more specifically one of those parameters is invalid. Now these codes can get very long and very specific and sometimes you don't care about all that specificity. In fact, usually you don't. So we structure them this way so that you can do a prefix match on the code and be as generic or a specific as you need to be for your particular use case. But if you need to be specific, say so you can report back to your own users with clarity about what went wrong, we try to give you enough information to do that. And we help with that also with the source parameter. And whenever possible, we track this error back to the place in the request that caused the problem. Here I can see that it's the filter bracket email parameter, specifically, that produced this error. The source can be a parameter like this or it could be a JSON pointer pinpointing the spot in the JSON data you sent us where the problem originates. And that's basically everything there is to know about the App Store Connect API except how to get to the App Store Connect API and successfully send requests. So Julie Richards is going to come up on stage now and help you with that. Julie. Hi, everyone. I'm Julie and I'm an engineer on the App Store Connect team. I'm here today to talk to you about access and authentication. So at this point you've seen a bunch of different examples of endpoints that will be available. And when you're ready to start testing out some of these new features, you might start by sending a simple GET request like this; however, if you were to simply curl this endpoint or type it into your browser, you'll end up with a response that looks like this. So as Geoff mentioned, we're still missing one very important step stat and that is authentication credentials. Now this step is necessary for two reasons. First, authentication credentials give context to your request. After all, you don't want all apps. You want your app. And most importantly, these credentials secure the API and ensure that no one but you will have access to your data. So to add these credentials to your request, you'll need to first create an API Key. You'll then use that key to generate tokens and those tokens will need to be sent with every request. So let's start with API Key. Each key is actually a public key, private key pair. The private key belongs to you and will be used to add a unique signature to your token. The public key will be used by Apple to verify that signature and ensure that it was in fact signed by the associated private key. To create one of the keys, you'll need to log in to App Store Connect and navigate to a new API Keys tab. Your admin users will be able to manage your team's API Keys. You can create new Keys and revoke them when they're no longer needed. Each key will need to be assigned a level of access and this will determine what API services the key can be used for. Once created, new keys will appear in this list and the private key file, that's the part that belongs to you, will be available to download. Now it's important to note that each private key can only be downloaded once. That is because these keys are not stored by Apple. In fact, your key is not even generated until the moment you decide to download it. So you can think of these keys like real keys. They belong to you and they must be managed and protected by you. Also much like real keys, these keys do not expire, so if the key is lost or stolen it will continue to have access to your data until it is revoked through App Store Connect. So for this reason, it is very important to keep your keys safe and secure. Once you have your private key file, you'll be ready to start generating JSON web tokens. Now each of these tokens will need to contain a few pieces of information. First, you'll need to add your Issuer ID, that's your account identifier, and this ID can be found at the top of that new page. You'll also need to find or you'll also need to add the ID of your key. This ID can also be found on this new page and it is specific to the key that you'll be using. Now each token will also need an expiration timestamp. As I mentioned before, your keys do not expire but these tokens can only be used for up to 20 minutes. And the last two pieces of information are simply constant. By that, I mean they will remain the same across all tokens and for all App Store Connect APIs, the first is the Audience for the token and that will always be App Store Connect. And finally, you'll need to assign or you'll need to add the algorithm that was used to sign the token. And for this, we have chosen to use ES256. Now this algorithm corresponds to a JWT-supported algorithm that we have chosen to have you use but don't worry, you don't need to implement this algorithm. Fortunately, JWT provides multiple libraries across virtually any language that makes creating and signing these tokens as easy as possible. This example behind me is written in Ruby and, as you can see, all I need to do is pass in these few pieces of information along with my private key and this encode method returns to me a complete and signed token. That token can then be added to my request by simply placing it inside of an authorization header. So now that we know how to create these keys and how to use these keys to add tokens to our requests, let's go ahead and give it a try. Okay, so here's that new API Keys page and I don't have any keys yet. So I'll go ahead and create one. I'll call it demo and I'll have to assign a level of access. So here, if I were to choose something like finance, I'd get a key that has access to things like financial reports or sales reports but it won't have access to things like beta testers or builds. I can add on levels of access or I can choose admin and I'll have a key that has access to all APIs. So I'm going to stick with admin. And when I generate my key, I can see that my private key is available to download. As I mentioned earlier, my private key can only be downloaded once. If I lose my key or accidentally delete it, I can't come back here to re-download it. So for that reason, I'm prompted to make sure that I'm ready to download this now. And I am, so let's go ahead and download. Now that I have my private key in my Downloads folder, I can pull up that script we saw earlier. And I'll need to copy over my ISSUER ID. That's the one that's shared across your API Keys. And I'll also need to copy that ID of the private key I just downloaded. And once I have those IDs in place, my private key will be loaded in from my Downloads folder. And I'll generate a new token. So if we pull up the terminal, I can curl that apps endpoint. And as we expected, we get that 401 response. So let's call my script. And I get a new token. So I can take that token and add it to my request by putting it in an authorization header. Make sure I spelled that right. And I get back the list of my apps. So as you can see, with just a few easy steps I was able to get access to the API and start getting back real data. Thank you. Back to Geoff. Alright. Thanks, Julie. Pretty cool, huh? So that's access and authentication. And now in the few minutes that we have remaining, I want to talk about some best practices when using the API. We're going to start with those keys. Now as Julie said, those keys belong to you and it's your job to protect them. Anybody who has the keys can access your data. So ideally, you're going to put those keys in some kind of a secure key store and your code will check them out, use them in memory, and never store them anywhere, like in a database or on disc. If you do have to store the keys on disc, make sure you check your file system permissions very carefully. And, of course, if you have any reason to think a key has been compromised in some way, immediately log in to App Store Connect and revoke that key. I also want to talk about the tokens that you create from your keys. Now there's no reason that you have to send a different token with every request. In fact, you'll get better performance in your code and on our end if you reuse those tokens over and over again. And usually this is really easy, right. You just generate a token at the top of the script, send it with every request until you're finished. And we let you control the expiration time because you're the best person to know how long it should last, long enough to complete that process and not significantly longer than it needs to be. Of course, you might have processes that run for longer than 20 minutes or run continuously and when that's the case you'll want to structure your code a little differently. Maybe you'll generate a 20-minute token and use it as needed and then throw it away and issue a new one before the old one expires, say every 18 minutes. That way, you're getting maximum token reuse for the best performance and you're never sending expired tokens. The next think I want to talk about is the links that we include in the responses. Now we've talked about these today as a sort of nice form of self-documentation, a way for you to look at the data and see what else is available, but that's not the only reason they're there. They're actually intended to be used by your code. When you're doing a multistep process, whenever possible take the link we gave you out of the response and use it for the next part of the process. This will help you in two ways. One, it makes your code more generically applicable to different parts of the API. And two, when that inevitable v2 API comes along, this will reduce the amount of work you have to do to adjust to the changes. And finally, I want to talk about the documentation. We've talked about the consistency of this API today and we really focused on that. And so any time a resource can do something, it will do it in the same way as all the other resources but of course not every resource can do everything. You already saw an example of this with users. We had to create an invitation. There's no API to create a user. So the documentation is how you figure this out. It tells you what the resource can do, what operations are available, what parameters it supports, and so forth. And that's the App Store Connect API. It's a consistent standards-based REST API. We're really excited about it. And one thing that we love about this API is that we designed it to be really flexible so that you can take App Store Connect and you can put it into your workflow into the way that you work and using the tools that you like best. We're really excited for you to start doing that. It'll be available to all app developers this summer. And I know you guys have questions, so come and see us in the App Store Connect Lab. It's actually happening right now. So we'll head down there as soon as I stop talking and we can answer those questions. We have a lab tomorrow as well at 1:00 p.m. and also check out the What's New in App Store Connect Session if you missed it yesterday. It had some more about the API and about App Store Connect itself. And bookmark this link because we'll post the documentation there just as soon as it's available. Thank you so much. We'll see you in the labs.  Good afternoon, everyone. Welcome to our talk on Metal for Accelerating Machine Learning. My name is Anna Tikhonova, and I'm an Engineer on the GPU Software Team. The Metal Performance Shaders Framework is built on top of Metal. And it provides GPU accelerated primitives that are optimized for both iOS and macOS. We provide primitives for image processing, linear algebra, and machine learning. We talked extensively about inference in our past WWDC sessions, so I just want to highlight them here. And this year, we've also added support for training on both iOS and macOS. Thank you. We've also added support for accelerated ray tracing on our platform, and we had an entire session on this topic earlier this week. So, it was titled Metal for Ray Tracing Acceleration. And the video for the session will be available online shortly. In this session, I will talk primarily about machine learning, specifically training. So, I mentioned training and inference. Deep learning algorithms consist of these two phases. The first phase is the training phase. So, let's use an example where we want to train a model, to categorize images into classes like cats, dogs, giraffes, etcetera. So, in order to train a model to recognize cats, we need to feed it a large number of labeled images of cats. And same for the rabbits and all of the other animals you want your model to be able to recognize. Training is a computationally expensive and time-consuming, iterative process. The result of training are trained parameters. The trained parameters are required for the next phase, the inference phase. This is when your model is presented with a new image, that is never seen before, and it needs to classify it based on the trained parameters. This is a cat. We now provide GPU acceleration for both the training and inference phases. But before I talk about training, let me tell you about some enhancements to CNN Inference we've added this year. We now have support for FP16 accumulation for the convolution and convolution transpose primitives. This new feature is available on devices with an Apple A11 Bionic GPU. We find that using FP16 accumulation for inference workloads is more than sufficient in terms of precision for many commonly used neural networks. FP16 accumulation offers better precision and significant power benefits. So, please take advantage of it in your inference workloads. And this is an example of how you can enable FP16 accumulation for a convolution primitive. You just need to set the accumulatorPrecisionOption property. And now, let's talk in depth about the main topic of this session, training neural networks. And let's start with training convolutional neural networks. So, here we have a simple, handwritten, digit recognition network that takes an image of a handwritten image as input, and assigns it to one of 10 classes, from 0 to 9. In this example, we are correctly classifying this image as an image of a digit 7. For inference, we initialize our network with trained parameters. So, in this example, the trained parameters add the weights for the convolution, and fully connected primitives. The goal of a training algorithm is to compute these trained parameters, so the network can use them to map inputs to correct outputs during inference. When we start the training process, we don't have any weights. We need to compute them. So, the first step is to initialize the weights with small, random numbers. And now we are ready to train this network. So, let's take a look at all the steps involved in a training process. Training is an iterative process, and each iteration of training can be divided into four steps. The first step is the forward pass. This is when we take an input and pass it to our network to produce an output. It's very similar to inference. And next, we need to compute loss. So, loss intuitively measures the difference between the network's output and the ground truth. The objective of a training algorithm is to minimize loss. Our next step is the gradient pass. This is when we propagate this difference when the network's output and the ground truth, back to the network to update weights. The idea is that as training continues, our network is becoming better trained, so it's better able to map inputs to correct outputs, which in turn helps to minimize loss. So, this is an overview and now let's take a look at each one of these steps in more detail. The forward pass involves propagation forward to the network, to compute an output. As you can see, during this first situation of training, our network is not doing so well. It's outputting a result that's clearly wrong. So, why is it doing so badly? Well, this is expected. We just initialized our weights with some random numbers. We haven't trained the network to do any better yet. So, now, we need some weight to quantify how well or how badly our network is currently doing. So, we can use this information to improve our weights, so that hopefully after more iterations of training, the network can produce a better result. But in order to know how well we're doing, we need to know what the right answers are. So, the ground truth, which I will call labels from now on, is an input to the network along with the input image. So, in this case, it's a vector of 10 values, where we have 1 for the correct class, Class 7, and zeros for all the other classes. The output of the network is our 10 probabilities. One per class. So, as you can see in this first situation of training, the network is producing a very low result for the correct Class 7, and it's assigning the highest probability to a Class 9, which is why the network is returning 9 as the answer. So, now we take all of this information and we pass it to our loss primitive. And as I mentioned previously, loss measures the difference between the network's output and the ground truth. And the objective of a training algorithm is to minimize loss. And now, we also need the second half of the graph. The second half of the graph contains gradient primitives for each responding forward primitive. The gradient primitives compute gradients that are needed to update weights. So, the loss primitive computes the first gradient, which is a derivative of a chosen loss function with respect to its inputs. And then we take this gradient and back propagate it, backward through the network, backward through the first gradient primitive in the backward direction. In this case, it's the SoftMax gradient primitive. And we use the Chain Rule to do that. So, the Chain Rule allows us to back propagate gradients, backwards through the network. And we're computing these gradients so that we can update weights. So, we're computing very small deltas to apply to the weights, in each iteration of training. And then we can use these updated weights in the next iteration of training, to ideally produce a lower loss value, which is what we're trying to minimize. In practice, any situation of training, we're not going to operate on a single image. We're going to operate on a group or a batch of images. For example, a batch of size 32 or 64. And we need a corresponding batch of labels, for loss computation. So, in this case, we have a batch of labels, was 1 for the correct class and zeroes everywhere else. And in each situation of training, we're going to use a different batch of images and a responding batch of labels. So, let's now run through several iterations of training. For the first batch of images, we're doing a forward pass, computing loss, and doing a gradient pass. And updating weights. So, what happens with the second batch? Exactly the same process. The forward pass, then we compute loss, to the gradient pass, and update weights. And as we go through iterations of training, we want the loss for our network to decrease and the accuracy of the network to increase. And we continue training until the loss falls below a particular threshold and the accuracy of our network reaches a desired level. And then we know that the network is fully trained and now we can use the computed trained parameters for inference. And now, let's take a look at the steps necessary to train a neural network using the Metal Performance Shaders Framework. Neural networks are often described using graph abstraction. So, in MPS, we enable you to describe your networks as a graph. So, the first step is to create a training graph. Then we need to prepare our inputs. We need to specify weights. And then we execute the graph. So, we run the forward paths, compute loss, do the gradient pass, and update weights. And training is an iterative process. It can take many iterations to train a network. So, we'll also need to know when we can stop training. So, let's now discuss each one of these topics in more detail. Let's start with creating a training graph. So, as I said, in MPS, we enable you to describe your networks as a graph using a neural network graph API. So, here we again have a visualization of our handwritten, digit recognition network. But in this visualization, you can also see the image notes. They're the small white notes. The image notes are for your data. For your input, your outputs, and all of the intermediate results. They describe how data moves between different operations. And then, operations on the data, like convolution and pooling, are described with your filter nodes. We support all of the nodes necessary to create commonly used neural networks. And now, let's take a look at how easy it is to use the neural network graph API. So, here's an example of how you can create an MPSImageNode using the neural network graph API. And this is how you would create a convolution node using the graph API. And now, for every forward node, we support a corresponding gradient node for training. It takes a single line of code to create a gradient node from the forward node. So, here is an example of how you can create a gradient convolution node from the convolution node. And now, let's build an entire graph. So, here we have a very small network. We have a convolution node followed by a pooling node, followed by another convolution node. So, how do we connect these nodes into a graph? So, that's easy. We take the result node of -- the result image of one node and pass it as a source image to the subsequent node. And here we have an entire connected graph. And now, let's build a training graph. So first, we need to add a loss node to the graph. And now, let's add some gradient nodes. So, as I said, it takes a single line of code to create a gradient node from its corresponding forward node. And then we connect them as previously, and now you have a complete training graph. So, as you can see from this example, the graph API is very simple to use. The graph does a lot for you automatically. It manages all the intermediate results, and even the output image. It minimizes the memory footprint of your networks, by aliasing memory for all your intermediate images, using Metal heaps. It can also fuse graph nodes. For example, it can fuse batch normalization and neural nodes. And it can optimize away nodes. For example, it optimizes the way you can cut nation nodes. The graph also automatically handles padding and manages state objects for you, which we will discuss later in this session. So, please take advantage of the graph API. So, now that we know how to create a training graph, let's now take a look at the inputs we need to pass to the graph. And for this, let's take a look at the encode call we will use to encode the graph to the GPU. So, as I already mentioned, we're not going to send in one image at a time for training. We're going to operate on groups or batches of images. So, one of the inputs to the graph, is a batch of images. And as you recall, for every batch of images, we also need a corresponding batch of labels for loss computation. The labels for loss computation are passed to the graph as states. So, the code call also takes a batch of states as input. And now, let's talk about these batches and states. What are they? Let's start with batches. So, batches are just arrays of images or states. We've added them this year specifically to support training. There are two new MPS types for you to use: MPSImageBatch and MPSStateBatch. So, here's an example of how you can create a single image, using our API. So, here we're creating one from an existing Metal texture. And this is an example of how you can create a batch of images, using our API and append a new image to the batch, so you can pass it to the graph. And now, what are state objects? An MPS state is an opaque data container. In training, it's frequently used to capture a state of a forward node, when it's called. And so, it can later be used by the gradient node. So, the graph manages all of the state objects. So, as a developer, you generally don't need to worry about states. But it's nice to know how they work. So, let's use a specific example. So, let's go back to our handwritten digit recognition network. And take a look specifically at the drop-out and drop-out gradient nodes. The forward drop-out node drops, or it zeroes out, values in its input, with a certain probability. And then, the dropout state object captures information about the forward drop-out operation, so it can later be used by the drop-out gradient node because it used to zero out values in its input gradient at the exact same locations as was zeroed out by the forward operation. So, as I said, you don't generally need to worry about states, because the graph manages them. But because the labels for loss computation are passed as states to the graph, and because they require user input. So, that's your ground truth or correct results. You need to create a batch of labels for loss computation and pass this batch as input to the graph. So, this is an example of how you would create a single label for loss computation. You first need to create a loss data descriptor which describes how the label's data is laid out in memory. And then you need to create an MPSCNNLossLabel object, with this descriptor. And then you create a batch of these for training, and when the GPU's done running the graph, your batch of labels will contain the per image loss values for the batch. And you can inspect these values, or you can compute a single value across the batch and inspect that value. So, now that we have a training graph and we talked about how to provide inputs to our graph, let's talk about how to provide weights to the graph nodes that require weights. The only way to provide weights to convolution fully connected, batch normalization and instance normalization nodes, is through data source provider protocols. This is an example of how to create a convolution node, with a data source provider. You need to implement a class that conforms to the protocol. We call it MyWeights in this example. Data source providers are very useful in many ways. For example, if you have many convolution nodes in your network, the overall size of the weights for the network can be quite considerable. And we do not want the weights for all of your convolution nodes to be in memory all at the same time. We want to keep the memory footprints of your networks as low as possible. And data source providers come into play here because they provide just in time loading and purging of weights data. So, we load the weights for one convolution kernel, when we process it. And then we purge them before moving on the next convolution. So, here's an implementation of MyWeights. You need to provide an initialization method that is responsible for pulling in memory and making it ready. And then the graph will call the load function. And then when the purge method is called, you can release the weights. Data source providers are also essential for training, and we will discuss this later in this session. So, now that we have a training graph and we've prepared our inputs and specified weights, we're ready to execute the graph on the GPU. To change the [inaudible] graph on the GPU, we first need to do the usual Metal setup. We need to initialize our training graph. So, we have prepared our inputs. And now, let's train a network on the GPU. Training is an iterative process. So, we want to set up a training loop. And we usually want to execute our graph over a number of EPOCHS. The number of EPOCHS is the total number -- is the number of times we want to iterate over our entire data set. And we want there to be multiple iterations in each EPOCH. So, the number of iterations is the total number of images in your data set divided by batch size, like 32 or 64. And now, let's take a look at each training iteration. In each training iteration, we encode a batch of images for training. But we don't want the CPU to wait for the GPU to finish running one run of the graph, with one batch of images before the CPU can start encoding commands to the command buffer for the next run of the graph. We want the CPU and the GPU to work concurrently. And for this, we're going to use double buffering. So, in this setup, we're going to create a counting semaphore with an initial value of 2. It's because we want only two encodes to be in flight at the same time. And then when we enter the training iteration function, we're going to call weight on the semaphore. That's decrementing it. So, if the value of the count has already been decremented to zero, we wait. Otherwise, we continue. And then we encode our graph, and the encode call returns immediately. And a user specified callback is called, when the GPU is done running the graph. So, now we know. The GPU is done running the graph, and the CPU can continue encoding more work to the GPU, work that was previously waiting on the semaphore. So, why are we using double buffering? Why not encode more runs of the graph, to the GPU concurrently? Well, it takes a lot less time to encode commands to the command buffer, than to run the graph. So, we don't want to encode too many runs of the graph concurrently to minimize memory footprint. Okay, we've talked about executing the graph. When we execute the graph, we do the forward pass, we compute loss, we do the gradient pass, and the graph will also update weights. So now, let's talk about weight updates. As I mentioned, data source providers are essential for training. All of your weight updates need to happen through an optional update method on a data source provider. The graph will call the update method automatically. So, what does the weight update step actually involve? Let's take a look. So, recall that we're computing gradients during the gradient pass that we can apply small deltas to the weights, in each situation of training. How these deltas are applied to the weights, is described by an optimizer. It's just a function that takes the old weights, the computed gradients as input, and produces updated weights as outputs. You will use an optimizer in the update method of your data source provider. And we support a number of different variants of the weight update step on the GPU, including Adam, Stochastic Gradient Descent, and RMSProp. And you can even define your own custom update weight step if you prefer. So now, let's take a look at how to use an optimizer in MPS. So, recall that your data source provider has an init method. This is where you want to create your optimizer because you only want to create it once. And now, let's take a look at the implementation of our update method. The update method receives the source state and gradient state as inputs. So, the source state contains the old weights, the gradient state contains the computed gradients, and now we can encode our optimizer with this data, and the last step is to return the source state, which now has the update weights. So, pretty simple. And now we have just one more step to discuss. So, as I said, training is an iterative process. It can take many iterations to train a network. And you will need to know when to stop training. So, let's now discuss how can you make this decision in the context of your training loop? So, here we again have our training loop, where we're training a neural network for a number of EPOCHS. To check whether you can stop training, you need to have a test set of images. So, a test set of images contains images that are not used for training. They're only used to evaluate the accuracy of your network. So, after each EPOCH, you can optionally wait for the graph to -- for the GPU to stop running the graph, and then you can use the current trained parameters to initialize an inference network. And then you can run this inference network on your test set, and you can optionally stop training when the accuracy of your network on this test set, reaches a particular level. So, now that we've discussed all of the steps that are necessary to train a neural network in MPS, it's time for a demo. So, as was already mentioned in the platform State of the Union, the Metal Performance Shaders Framework powers Core ML, Create ML, and Turi Create. To Turi Create is an easy to use, flexible, high-performance tool set for creating Core ML models for tasks such as image classification, object detection, recommendations, and more. For more information on Turi Create, we want to refer you to the A Guide to Turi Create Session Video. We've prepared a demo where we will be using an -- we'll be training an object detection network in Turi Create powered by MPS. As was mentioned in the platform State of the Union, this is nine times faster than without MPS. An object detection network draws bounding boxes around recognized objects. So, in this demo, I will be using a MacBook Pro, with a connected external GPU. I will be running Turi Create on the MacBook Pro and I will use an external GPU to train the network with MPS. This is a great example of how you can use an external GPU to enhance the computational power of a MacBook Pro. The external GPU we're using is an AMD Vega GPU. So, in this demo setup, I've already imported Turi Create, and preloaded the object detection network, and a training data set. So, now let's train this network for 10 iterations. And now, the entire object detection network, all the primitives, the optimizer, the weight update step, everything is running on the external GPU. Okay, so we're already done with 10 iterations of training, it would take more than 10 iterations to train this network, and we're not going to do this on stage. But what I'm going to do right now, is to load a network that we pretrained in advance, run it on a test set of images and visualize some of the results. So, let's take a look. Okay, so here we have a banana, that's correctly classified as a banana. And we have a bounding box, and now we have a perfect breakfast of a cup of coffee and a croissant, and very, mean-looking egg. Okay, so that's it for the Turi Create demo. Thank you very much. And now, let's switch gears and talk about training recurrent neural networks. But first, let's do a recap of what are the recurrent neural networks? One of the disadvantages of convolutional neural networks is their inability to remember anything that happened previously. They can take one input, such as an image, and generate a single output, such as a set of probabilities of what is depicted in the image. RNNs on the other hand, have memory. And they're good at operating on sequences of inputs and outputs. For example, they can take one set of probabilities, so what is depicted in an image, which is an output of a CNN, and generate a sequence of outputs, which is a sequence of words that make up a caption for this image. They can also take a sequence of inputs, such as a sequence of words that make up a sentence and generate a sequence of outputs which is a same sentence but translated to a different language. For example, to ration or finish. With support, a number of different variance of RNNs. The most commonly used one is the Long Short-Term Memory RNN, or LSTM for short. In our last year's WWDC Session, we talked extensively about the gates inside LSTM and walked through a LSTM inference example. So, please refer to that session for more information on LSTM inference. This year, we've added support for training, for all of these variants of RNNs. And in this session, I'm going to talk about training LSTMs. So, let's use a specific example. So, here we have an activity classifier network which takes motion sensory data as input. For example, reading some sensors like an accelerometer or a gyroscope. And then the network uses this data to identify a physical activity performed by the user. So, for example, we want to know if a user is cycling, skiing, or walking. As you can see, this network is set up in an interesting way. So, it contains a series of CNN primitives, followed by LSTM primitive, followed by more CNN primitives. So, why is it set up this way? Let's take a look. So, even though our input is sensor data, it's represented by a batch of 1D images with six feature channels. So, one feature channel for access in the accelerometer and gyroscope readings. And each 1D image has 2,000 pixels. And you can think of them as samples in time because the activity we're trying to identify, occurs over time. And then we pass these images through a 1D convolution primitive which compresses these 2,000 samples, to just 20 samples. But it expends a number of feature channels, because -- so, we're not losing any features in the data. And then, this new representation of the data, is passed to LSTM primitive as a sequence of lengths 20. And we ran LSTM for 20 iterations. So, our LSTM is operating on a sequence of lengths 20 instead of 2,000, so it's operating on a higher-level feature representation of the data. And then we have additional CNN primitives that we find high-level features in the data. And the last primitive in this network is the SoftMax primitive which generates probabilities for the different activity classes, which is the output of the network. And now, let's take a look at how to train this network. So, we again need a loss primitive, which takes the output of the network and the labels as input. And then we need the second half of the graph. So, in the second half of the graph, we again have gradient primitives for the corresponding forward primitives, including the LSTM primitive. And now, for training, we do the forward pass through the network, then we compute loss, and we do the gradient pass to compute gradients that will be used to update weights. So, this is a very similar setup that we have for a CNN training. And the last step is of course to update the weights and as you know, the LSTM also has weights, so they need to be updated as well. And now, let's take a look at how to train this network in MPS. But first, let's take a look at how we can create LSTM layer for training using our framework. So, first, you need to create LSTM layer descriptor. And we initialize the descriptor with initial training parameters using data source providers. So, these initial training parameters are, use smaller random number or some checkpoint values. The descriptor setup for training, is exactly the same as it is for inference. And we discussed the layer descriptor setup in our last year WWDC session in a lot more detail. So, I want to refer you to the session for more information on LSTM layer descriptor setup. Once you have the descriptor, the next step is to create LSTM training layer with this descriptor. MPS will populate training weights using the data sources specified in the descriptor. And we also need to have some matrices to hold the computed gradients. You will use the createWeightGradientMatrices API on the training layer to create these matrices. And then, the training weights will be used in a forward and gradient passes and will be passed to an optimizer along with the computed gradients, job, to date, weights. And now we need to prepare some inputs and outputs for training our LSTM. So, here's an example of how you can create the matrices to hold the input and output sequences for both the forward and gradient passes. You will need 20 matrices for each one of those. And here is how you would initialize these matrices with data. And now, we are ready to train our activity classifier network in MPS. So, in this code example, I will be highlighting only the LSTM filter in the interest of time. So, in the forward pass, we ran a sequence of 20 matrices forward through the LSTM training layer. And then in the backward pass, we ran a sequence of 20 matrices though the LSTM layer to compute gradients. And now, you have the training weights, and you have the computed gradients, and you can pass them to an optimizer to update weights. So, there's just one more thing I'd like to mention. [Inaudible] neural networks operate on images and LSTMs operate on matrices. And we'll provide convenience kernels in the framework to make it easy to convert between images and matrices. So, in order to copy an image to a matrix, you need to use the MPI's Image Copy to Matrix Kernel. So, this is how you can create one, and this is how you can encode one on a batch of images. Here, each row in a destination matrix, will contain one source image. And to copy from a matrix to an image, you need to use the MPS Matrix Copy to Image Kernel. This is how you can create one and this is how you encode one to the GPU. So, we just showed you how to train CNNs and RNNs using MPS. We also showed you a demo of Turi Create which is now powered by MPS. And now it's time for one more demo. We have been working with Google to add support to the Metal Performance Shaders Framework to TensorFlow to accelerate machine learning on macOS, and we would love to show you a demo of that in action. Specifically, we want to show you a demo of training the InceptionV3 Object Classification Network, using TensorFlow, powered by MPS. So, for this demo, I will again be using a MacBook Pro with an attached external GPU. So, I will be running TensorFlow on this MacBook Pro, and I will use an external GPU to train a network using MPS. So, in this demo setup, I've already imported TensorFlow and preloaded the InceptionV3 Network and a training data set. So, now, let's train this network for 30 iterations. So, you can see how fast this is going. Again, the entire network, all of the primitives, the optimizer, and the weight update step, everything is running on the external GPU. And we're already done. And as you can see, the training rate is approximately 100 images per second. So, as was stated in the platform State of the Union, training the InceptionV3 Network, in TensorFlow powered by MPS, is up to 20 times faster than without MPS. So, this is it for the TensorFlow demo. Thank you very much. And now, let's summarize this session. This year, we've added a FP16 accumulation for the convolution and convolution transpose primitives to improve the performance of CNN inference. We've also added GPU accelerate primitives for training neural networks. These primitives are optimized for both iOS and macOS. We've also added the neural network graph API for training. It makes it very easy to train neural networks on the GPU and enables us to provide the best performance across different GPUs. For more information on this session and links to related resources, please go to our developer website. We have the Metal for Machine Learning Lab tomorrow at 9 a.m. So, we would love to talk to you. So, please come talk to us. And thank you for coming and have a great WWDC. Ladies and gentlemen, please welcome Vice President of Software, Sebastian Marineau-Mes. Good afternoon, everyone. Welcome to the afternoon session of WWDC 2018. Now, we had a really, really great session this morning. I think you all enjoyed the keynote? Lots of great things were presented. And I think you saw that 2018 is a year with a strong focus on the fundamentals across our entire ecosystem where we pushed the boundaries in key technology areas. We're introducing numerous APIs and capabilities that enable new experiences covering a broad spectrum that ranges from machine learning, augmented reality, high performance graphics and of course, new development tools. Now, many of the improvements in the APIs apply to all of our operating systems so they all move forward together. And iCloud provides the fabric that enables a unified and consistent experience across all of our devices. In iOS 12, we've seen a huge number of incredible new features including these great new capabilities in AR, the camera effects in Messages, multi-way FaceTime, usage data with Screen Time, richer photos and of course, a great focus on performance. And with macOS, we're really excited to introduce Dark Mode, New Finder on desktop features, new apps like news and stocks, a redesigned Mac App Store, and a strong focus on security and privacy, watchOS 5 brings customizable interactive notifications, support for your app content and shortcuts on the Siri Watch face, background audio mode and improved workout API. And in tvOS, we're adding Dolby Atmos support so that video apps can deliver immersive audio experiences. We heard that this morning, really amazing. Secure password sharing from iOS devices so it makes it really easy to slide into through Apple TV apps, VPP support and enhancements to UIKit and TV ML Kit to make it even easier for you to build native apps that look and feel great. Now our great products are platforms and all of your apps truly impact the world. And when you think of the breadth and the scale of our ecosystem, it really makes us an essential part of our users' life. Be it helping them explore their creativity, connecting with the people they care most about or transforming the way that healthcare is delivered, we together focus on what's most important to our users and we deliver these great experiences. Now we think technology is most powerful when it empowers everyone. And so we work to make every Apple product accessible from the very start. We provide great capabilities that make our platforms and all of your apps accessible and we want to encourage you to take, keep taking advantage of these because it's really important to those users. Now, our users also entrust us with their most precious data. And so at Apple, we think deeply about privacy and security. And I'd like to invite Katie up on stage to tell you more about this. Katie? Thanks, Sebastian. When we think about privacy, we think about how to build privacy into all their products and services. And there could be a lot of details to think about. But it's important to think of the big picture, trust. Now it's up to all of us to ensure that users can protect, can trust us to protect their most sensitive data. From financial data to communications to location and photos, trust is crucial as technology becomes more and more integrated into our lives. So how can you build trust with your users? We focus on four key pillars and let me show you an example of each. Now, we don't require users to sign into Maps but instead we use rotating random identifiers that can't be tied to an Apple ID to enable relevant results. We use on-device intelligence to enable powerful features like search and memories in photos without analyzing photos in the cloud. We designed Face ID so all Face ID data is encrypted, protected by the Secure Enclave and doesn't ever leave your device. And when we collect users' data or allow a third party to collect data like photos, we make sure we do so with the user's consent. So let's dive a little bit deeper into transparency and control. You have all seen these alerts when you request access to location or photos. And this alert includes a purpose string. Now this is what you provide in order to explain why you're requesting data and how you will use that data. Now a good string includes a clear explanation of what features it will enable and what functionality it will improve. Now, the more specific you are with your users, the more likely they are to grant you access. We think it's critically important to understand and for users to understand how their data will be used. So app review is paying closer attention to these purpose strings. So if you have a purpose string like this, which you know, it's clearly not valid, you may get dinged by app review. Now this string technically explains how data will be used. But it lacks detail so it's really hard for your user to make a decision. Now some users may have concerns about granting your app microphone access but it may be key to your app's functionality. So that's why it's important to have a clear purpose string like this one that explains exactly how you are going to use the data. Now great features don't have to be at the expense of privacy. But instead can support them by making it clear to users how you're going to protect their data and how it's being used. Now, we care deeply about security. And in order to protect all the sensitive data that resides on the device in apps and in the cloud, we think about security holistically. And we provide technologies to make it easy for you to build secure apps. Here's a few examples of the technologies that we provide. On iOS, we automatically encrypt app data by default. Over the network, App Transport Security means you never have to patch client networking libraries again. Now in CloudKit in the cloud, CloudKit securely stores and syncs data across devices. Letting you focus on building a great experience for your users without having to worry about managing account state or your cloud credentials. And it enables you to take the best, advantage the best in-class security including built-in two-factor authentication. Since its launch three years ago, more than two-thirds of Apple ID accounts have adopted two-factor authentication. This is a huge success compared to the rest of the industry where we see less than 10% of accounts protected by two-factor authentication. But this is very important to us. And we work continuously to make our users' accounts more secure so you're the only person who can access your account even if someone else knows your password. And new in iOS 12, we're making using passwords more convenient and more secure for you and your users. We all know that a secure password is critically important to keeping your information and your identity secure. But they can be hard to remember and it's tempting to use weak or reuse passwords. And this creates problems for you as a developer as well. Now, users may abandon account sign up and you have to deal with password reset requests. But worst of all, is the potential for compromised accounts due to weak passwords. So we have a solution -- iOS 12 makes it easy for you and your users to always use a strong unique password by creating, storing and AutoFilling the password. But the really great thing is it will also work automatically in your iOS app too so they always get a strong password no matter where they create an account and it syncs to all of their devices. Now it couldn't be easier to offer automatic strong passwords. In fact, you may not need to make any changes within your app. So to ensure it just works, you need to associate your app with the domain. You may have already done this if you have adopted universal links. Then you need to label your user name and password fields. And if the passwords don't meet your app requirements, now you can even customize them. We've also made it easier for your users to get to their passwords. They can just ask Siri and once they've authenticated, they're taken right to their password list. And on top of that, to help clear up old password since, we're making it really easy to tell if any of your passwords have been reused across your existing accounts. Your iPhone would flag these passwords and take you right to the website where you'll be able to replace it with a strong password. We're also make it easier to deal with those one-time passcodes that are texted to you and your users much more convenient. They'll automatically appear right in the click tap bar and you can fill them in with just a tap. We're also creating a new extension point for third-party password managers to enable them to supply passwords for AutoFill and apps in Safari. Now these features work across iOS, the Mac and even Apple TV for a great experience across your Apple devices. We care deeply about privacy and security. And they're foundational to all of our products. So we provide the ability for you to build on this foundation to protect, secure and earn your users' trust. And now, handing it back to Sebastian. Thank you, Katie. Isn't -- aren't these new password features amazing? Really, really great. That was great. Thank you. Now, ultimately, we also promise our users great experiences. And we usually think about great experiences as being great innovative features. But equally important is not to compromise that delight with unpredictable and slow software. This is top of mind for the Apple Engineering Team. We develop tools and practices that help us with this. And then we work to bring these same tools to all of you so that you can apply them through applications. Available to you are a number of tools and techniques to help you make your code more reliable and robust. It's important for your app to be predictable. And of course, making your app run fast is critical. And for that, we have a number of performance tools at your disposal. Now we understand that optimizing performance across complex systems and applications is challenging. And this year, we worked a lot on this. We've developed a lot of new tools and techniques and want to bring you some of these powerful new capabilities. So in Xcode 10, we've extended instruments capabilities and enabled you to take it even further with your own custom tools and work flows. Now this all starts from a legacy API. Some of you may know this and have used it. I know I'm guilty of it -- printf, it's like the Swiss Army knife of APIs. We use it to debug and trace through our code but we all know that it's slow. And so two years ago, we brought you this new API called os log. It's an efficient and performant API that captures logs and tracepoints across all levels of the system. It's fast and lightweight and if you've not adopted it already, you really should. It's great. And our newest addition this year builds on top of os log and it's called os signpost. It's a powerful technique that provides rich, contextual data for your application in a format that instruments can interpret. So you could use signpost to trace through your code and you can also use it to bookend critical sections of your functions. And once you have the data, the real power comes in the built-in custom instruments visualization. Now, we have this new Custom Instruments support and the best way to convey the full power of this, I think, is through demo so Ken will show us what our tools can do. Ken? Thank you, Sebastian. So I'm working on my Solar System Exploration app here. And I've noticed I've got a little bit of a performance problem. So every time the app goes to update its data, you know, when it launches or when I press command R like that, you can see the UI, it gets really choppy. The planets, they kind of stutter as they move around their orbits. And then once the update completes, well, it's pretty smooth. So I want to figure out what's going on here. Now back over in my code, PlanetUpdateService.swift -- this is the file that handles that planetary update. So I want to add some logs, some signposts to help me understand what's going on in my code. So I'm going to start by adding a log handle. So I'm going to use the new pointsOfInterest category. Now this is a special new category. Anything that I log with it is automatically going to show up right inside instruments. Now, the first thing I want to see is when we kick off this update. And that happens in this method. So I'm going to add my first log statement here. I'm going to say requesting planet data so that we could see that. And then what I really want to know is how long is it taking to process and parse all the data that I'm doing here? So right here is where that happens. And to help me visualize this, I'm going to add a couple of signposts. So the first signpost is going to be a begin-type signpost here, just before I start doing the work. Then I'm going to add another signpost right here after I finish doing the work. That's an end-type signpost. So this is going to create a time interval for me, automatically calculate the delta and surface that right up through instruments. So let's profile this in Instruments and see what kind of data we get. So I go to Product, select Profile. Xcode's going to build my app, launch Instruments and then we'll start to see, well, we'll start seeing data stream in here. Now right here, you can see the pointsOfInterest track. So everything that I was logging with the pointsOfInterest category, that shows up here so this is my data. I want to zoom in. So I'm going to hold on Option and click and drag so we can get a closer look. And we can see this little flag right here that says requesting planet data. So that's a result of the first log I added in my code. Then these blue bars right here, this is where I'm processing and parsing data. So those are the results of the signpost I added. Now, as I look at this, I think I see what the problem might be right away. So every time I go to process data and parse it here, I can see a corresponding spike in the CPU use on the main thread. And to me, that is a bright red flag that I'm probably parsing and processing this on the main thread. Not a recipe for a smooth UI. So with just a log statement, a couple of signposts, you could see I can start to get some really great insight into the performance of my app. Now the new tools, they let you do way more than that. So with Xcode 10, there's a new template that lets you create a fully customized Instruments package. Now, one of my team mates, he's gone ahead and built one based on some network, some signposts that he added to our networking framework. And I've got the latest version he sent me here in my downloads. So let me open that up and when I do, Instruments offers to install it for me. So I'll say install. And now you'll see, I've got a new template. Here are my templates. These are called Solar Systems. I'm going to double click that. And then we'll start recording data again. Now, just like before, I have the pointsOfInterest tracked so that is on the data that I wanted to see. But now, I've got much more detailed information about the networking request that I'm making here. So again, let me zoom in so we can get a closer look. Now, this custom Instruments package here is giving me a great visualization into how I'm using this framework. So it's showing me things like for example here, how many network requests am I making on average every 10th of a second. Then down here, this track is showing me detailed information about each and every network request. How long did it take? It's even highlighting duplicate requests in red. So these are places where I'm asking for the exact same data more than once. It looks like I'm doing that maybe even more than 50% of the time. So I'm just leaving a ton of performance on the table and it's exactly these kinds of insights that I need to help me use this framework more effectively. So signposts, Custom Instruments packages, two really great new ways for you to visualize your data right in Instruments. And that's a look at the new performance tools. Sebastian? All right. Thank you, Ken. That's a really, really amazing demo. Really great tools that all of you can use to make your apps run even faster. Now, to recap, we just reviewed a lot of great tools and best practices that we can use to ensure that we delight our users and keep their trust. Now, I'd like to turn our attention to the Mac. OS X was launched 17 years ago and we've constantly pushed the platform forward. 64-bit support in Leopard, MacOS Mountain Lion introduced Gatekeeper, a key step forward in Mac security. And one of our key missions is to always push the Mac forward by extending its capabilities to take advantage of the latest technologies. But as we push the platform forward, we sometimes have to deprecate legacy functionality to ensure that we're not holding it back. Last year, we announced that High Sierra was the last MacOS release to fully support 32-bit apps without compromise. And this year, we're announcing that MacOS Mojave is the last release to support 32-bit at all. So as we remove 32-bit support next year, these 32-bit only frameworks will also be removed such as the QuickTime framework and the Apple Java framework. Next, let's look at security on the Mac. Gatekeeper has done a great job at avoiding large-scale malware attacks and this year, we want to push it even further. We're extending user consent, enhancing run time security and we're launching a new Notary Service. So let's look at these in more detail. As you heard this morning, we're extending the protections afforded to sensitive system resources. We've added camera and microphone and we now require user consent for API and direct access to all these resources. What does it mean in practice? Well, it means that your application has to gracefully handle those calls potentially blocking or failing as the user provides consent. It's also a really great idea, as Katie has pointed out, to provide meaningful purpose strings so when the user is faced with one of these dialogues, they understand why your app needs access. We're also going further in protecting sensitive user data. And only specialized apps like backup tools require access to this kind of data. And so we'll protect these locations by requiring user consent directly in the security and privacy preference pane. Next, we're introducing enhancements to run time protections. Now, a number of you have requested a way to extend the zip protections to your own apps. And our new enhanced run times, there's a new security baseline that requires risky capabilities to be opted in. So beyond strong code validation, if for example also protects your apps from code injection. The enhanced run time is fully backwards compatible. It's opt in through a simple switch in Xcode. And finally, we're introducing the concept of notarized apps. This is an extension to the Developer ID program for apps that are distributed outside of the Mac App Store. And it has two main goals. The first is to detect malware even faster than today before it gets distributed to our users. And second, provide a finer-grained revocation capability so that we can revoke a specific version of a compromised app as opposed to revoking the entire signing certificate. Now here's how it works. You develop the bug and build your app as before. And you sign it with your Developer ID Certificate. But before distributing it to your users, you submit to the Developer ID Notary Service. Once notarized, you distribute the app through your existing channel. Once your user runs the app on their system, MacOS Mojave will check with the Notary Service to make sure the app is properly notarized and is not known to be malicious. Now, the service is not app review. There are no new guidelines being imposed on Developer ID apps as a result of the Notary Service. It is used exclusively to analyze apps for security purposes. A future version of MacOS will require all Developer ID apps to be notarized by the service before they can be installed so we want you to get ready. It's available in beta today. We encourage you to try it out and give us feedback. And those are the enhancements to Gatekeeper in MacOS Mojave. Let's now switch gears and talk about the MacOS user experience. And to do that, I'd like to invite Kristen up on stage. Kristen? Thank you, Sebastian. I'm excited to be here. We have a lot of great features in MacOS Mojave including improvements to Finder, SnapShots and desktops docs. I'd like to focus on one in particular that you, as developers, can take advantage of. And that's Quick Actions. With Finder Quick Actions, we've embedded the tools you need right where you need them in the Finder preview pane. You can perform common actions on your files without ever leaving Finder. And there's different actions for different file types. As you can see here with video and here, with a PDF. And it's not just built-in actions. We know pro users especially like to create their own. And those actions are shown here in Finder as well. New developers will be able to provide custom actions from your applications using app extensions. And as an end-user, you can also combine shell scripts, AppleScripts and Automator Actions in Automator to create an action bundle. And these action bundles will be shown here in Finder as well based on file type. These Custom Actions get some prime real estate in Finder and even more so in Touch Bar. Touch Bar is great when customized. And you can customize Touch Bar to show these actions all the time or on the tap of a button. Moving on, in the keynote this morning, you got a sneak peek at another technology we are really excited about. An easy way to bring iOS apps to the Mac. We are in the midst of developing this technology in the context of these four apps, News, Stocks, Voice Memos and Home. These apps utilize UIKit and this is a new way to delivery great Mac apps. Of course, AppKit is our primary native framework and it takes full advantage of all the Mac has to offer. And in no way are we de-emphasizing that. However, we note that a lot of you have iOS apps and you don't have a native Mac experience. And for these cases, we want you to -- we want to give you an easy way to bring your apps to the Mac as well. So how are we doing this? These UIKit apps are running in a native environment on top of a native stack. And if you look closely, you'll see that the stack below the UIKit app has a lot in common with the stack below the AppKit app. In fact, these environments were built on a common foundation which in some cases has drifted apart over time. So we're taking this opportunity to rationalize the substrate which is great news for you developers independent of this technology because it makes it easier for you to write portable code. These apps get all the typical Mac features and I'd like to show that to you now. You've seen the new Stocks app for iPad. I'm running a Mac version of this app built from the same sources. Mouse events are mapped to UI events so I can click on a ticker symbol in the watchlist to see more information. I can move my mouse over the interactive chart to see the price at a point in time and I can click and drag to see the price change over a period of time. I'm going to click on an article to open it right here in app. Now since this is a Mac window, I can resize it as I would like and I can also take it full screen. I can navigate using two-finger scroll which is another example of event mapping. And if I want to copy some text, I can select it, pick it up and drag it and drop it in my Notes app. Now in this Note, I have a link to a news article so I'm going to click that to open it directly in News. And we've populated the menu with items for this application. So for example, I can go to the file menu and I can follow this channel. And notice how ESPN appears directly in my transition sidebar. Another Mac touch can be seen in the toolbar here where there's a red color contribution coming from the content underneath it. Now we have controls for the window in the toolbar including the share button so I can click on the share button to show this article with a friend. So that's a quick look at UIKit apps on the Mac. Now, thank you. We are continuing to develop this technology and we are working to fully vet it before making it available to you and your applications which we are planning to do next year. Next, Dark Mode -- you've seen that Dark Mode is a big thing for MacOS Mojave and we think it looks stunning. Let's take a quick tour. The window background is dark making the content pop. The sidebar is translucent and the content is blended vibrantly which preserves contrast with whatever may be underneath the window. And in a few cases, we found it valuable to change the icons slightly so you can see a slight darkening of this photo icon and a new dark trash can. But there's something very subtle here. The window background is actually picking up a slight hint of color from the desktop. To show you what I mean, here is a window on top of two very different desktop pictures. On the left side, there's a slight blue tint in the window from that slightly blue desktop picture. And on the right side, there's a slight orange tint from the predominantly orange desktop picture. This is not translucency. We're actually picking up our average color from the desktop and blending it into an opaque background. And we do this so that your window looks harmonious with a variety of desktop pictures. Let's look at what you need to do in your apps to support Dark Mode. Because we want to make sure to preserve compatibility with your applications, we are not automatically opting you in. You need to build against the MacOS Mojave STKit. For example, this is how Keynote looked when we first ran it after building on Mojave. It got a dark toolbar but it didn't otherwise adopt to Dark Mode the way we wished. The [inaudible] part is drawing too light of a background. The toolbar controls are faint and hard to read. The sidebar is the wrong material so it's too translucent. And in the selected segment in control, we have a white glyph on a white background. The good news is these issues were all easy to fix. We have simple API that support all the needs of Dark Mode. And in fact, most of these have existed for years and we just had to augment them a tiny bit. There's NSColor. There's Container Views with background color properties. There's Visual Effect View in materials. There's Template Images and a new way to colorize your content. So we updated Keynote with these APIs and this is the result. It looks great. These were pretty simple changes. We invite you to try this today. If you're already following the best practices of using asset catalogs in system colors, you could be pleasantly surprised at how close you already are. And since these techniques are available on previous releases, you can adopt and easily back-deploy. It, of course, depends on how many custom controls you have in your applications but for a few of our apps, it was as little as a day of work. We give you some useful tools for it as well. Well, I'd like to welcome Matthew to the stage to show you how Xcode 10 supports adoption of Dark Mode and much more. Thank you, Kristen. Our Xcode release this year is focused on the productivity. Work flow improvements, performance improvements and new feature support in all of our STKits. And of course, when running on MacOS Mojave, Xcode has a whole new look and feel. So let's start by taking a sneak peek at how Xcode can make your Mac apps look great in Dark Mode too. So here we are back in our solar system application. We've been converting it over to Dark Mode and we've made great progress so far. There's a couple of items left I need to finish here. There's a darker version of this globe my designers have provided to us. And there's these two hard-coded boxes that have colors that I need to change. Xcode's asset catalogs makes this easy. Let's start with this image. I'll change over to the tab with my assets and we can see, I've already defined dark variance for all of my colors. I'll select the group with all of my images and here's the planet image I'd like to add a dark variant for. That's easy. I'll select it. Go in the Inspector and add a dark variant. And my designers have sent me the assets here so I can just pull them out of my downloads folder and put them into the catalog. That's it. You'll see when I go back to my interface now, the globe is updated to match the appearance of the interface builder canvas. Now, I've already specified all the color variants that I need. So to update these boxes, I'll just select both of them, go to the Inspector and change the fill color to one of my catalog colors. We'll take the badge background color. Great, so now my interface is looking pretty good. Now when designing interfaces, I often like to check the other appearance as I'm going along to evaluate my progress. Interface Builder makes this easy. Down the bottom here, there's a new appearance bar that allows me to toggle between appearances. I'll just select the appearance on the left and now I'm seeing my application in the light appearance as well. So I can easily evaluate my progress. Let's run our application and see how we've done. We'll update our assets and we'll launch our application. And we'll see here that the application launches. And great, it's looking pretty good. Now the application launched in the dark mode to match my system. But while I'm developing, I can change the appearance. Down here in the debug bar is a new appearance toggle that's also in Touch Bar, and it gives me access to all the appearances. I can select the light mode, the dark mode, even high-contrast modes to evaluate accessibility. So I'll select the light mode. We'll load those assets, and there's my application in light mode as well. So very simply, with asset catalogs, interface builder, and our debugging tools, it's really easy to make your apps look great in Dark Mode, too. Now I know many of you have wanted a dark mode appearance in Xcode for a long time. It's been one of our most popular requests. In fact, just a couple weeks ago, there was a posting in the App Store about this feature. It was from a user name Ronnie Bo Bonnie. This is true -- I'm not making this up. But I just wanted to take a moment and say Ronnie, if you are out there, no charge. Now we also have some other improvements to our design tools to share with you today. Form-based UIs like preferences and inspectors are common in Mac apps. And Cocoa's NSGridView is the perfect system for laying them out. So we're bringing the power of NSGridView right into Interface Builder where you can now design your column- and row-based UIs just like working with tables in a spreadsheet. Drag-and-drop content [applause] -- yes. Yeah, you can clap for that. Spreadsheets can be cool. You can drag-and-drop content, use contextual actions, and you get system access to things like right-to-left layout. Now when designing your interfaces, the library is an important tool, and we have a whole new workflow for you because the library is now separate from the inspectors. You can now take the library and reposition it wherever you want. you can adjust the size to match your layout. And you can keep the library up while working or have it automatically dismiss when you are done. And the library works great with all of our content types, including media and code snippets. And finally, with our design tools, you'll notice they're just snappier, with faster document loading and more responsive canvas interactions. Now we've also spent time focusing on our source editing tools, keeping them fast, fluid, and informative. We started with performance, where you'll now see the editor loads large documents much faster, all while keeping smooth scrolling at 60 frames a second. Next, we double down on stability in SourceKit and enhance the robustness of our language integration. So now more of your colorful comments will stay inside of the editor rather than being about it. Co-completion and navigation are two essential workflows, and we've improved on both. Co-completion now provides more targeted results and limits completions to high-confidence matches. And when navigating, with jump to definition, the destination list will now provide contextual details like file and line information to help you easily get to where you need to go. And you'll see the same contextual information in the new callers option in the action menu, which is a seamless way to move throughout your projects. Now last year, we introduced refactoring for all languages, including Swift. And you, the Swift community, embraced the opportunity and added a number of new actions. These actions all streamline common programming scenarios and are built right into Xcode now, just a click away. Now refactoring is just one of many ways you can modify the source in your project. And to make it easier to keep track of your changes, we're introducing a source control change bar. The change bar is on the left side of the editor and highlights lines of code which have changed since your last checkout. The style and color of the indicator reveal the type of change, making it easy for you to see at a glance changes you have made, your team members have made, and those which might be in conflict. Now this feature is -- yes. I agree. I think this feature is pretty awesome. And I'd actually like to show it to you in a demo now. So we're going to go back to our solar system application, and I have some changes I'd like to make in one of our source files. It's our scene view controller here. So I'll scroll down in the editor to the place I'd like to make changes. And here we can see on the left -- just to the left of the line numbers -- the source control change bar is indicating there's some upstream changes a team member has made. In fact, if I had already made the changes to this line, you'd see the indicator turns red, highlighting a conflict. If I put my cursor over the indicator, you'll see it highlights the ranges of characters which have changed and are in conflict. And if I click on the indicator, it brings up an action menu with both a description of the changes and some actions I can take I see my team member has added more descriptive comments here. I think we'll take his change, so I'll use the action menu to discard my change, and go up under the source control menu to pull his changes in. So here's his changes -- very descriptive comments. I can scroll to the bottom of the editor, see if there's anything else I'd like to look at here. Here's another new feature of Xcode 10 -- our editor supports overscroll now. So going back to the lines of code I'd like to change, I'd like to convert these hard-coded functions into properties that pull the colors from the asset catalog. Now there are three of them that I'd like to change, and they're a bit spread out now because of all these comments. Well, no matter, with Xcode 10, we've improved code folding. Basically, you can now code fold anything you want. And we've brought back the code folding ribbon. So just to the right of the line numbers, I can click -- -- to collapse the code away. And we have this nice, svelte presentation of the collapsing now. Now this is the first function I'd like to change, and I see that all of these functions are very similar, and it would be great if I could make all of these changes all at the same time. Well, I can do that now, too, with multi-cursor editing. The key to multi-cursor editing is two fingers, control and shift. So I'll hold down those two keys and just click at the beginning of each of the other functions. We'll use range selection, and we'll just change that to VAR. We'll change those into colons. And we're pretty good so far. Now I happen to know that I've named my colors in the catalog the same name as my properties here. So we'll just select those names and copy them. And now let's go to the implementation and change that. So we'll drop three more cursors, and we'll just select all of this, type in named, paste in those colors, and we've made all those changes. It's like three times faster. Now multi-cursor editing also works great with column selection. So here I have all of my IBOutlets defined with week. If I hold down the option key and I select all of these in here [cheering] -- oh, yeah. Oh, yeah, so let's just convert those into [applause] unowned. And just like that, I can make my changes, and then use the source control bar to make sure that I got the changes I want. So those are some of the great new editing features you'll find in Xcode 10. So additions like the source control change bar and multi-cursor editing alongside performance and stability improvements. Xcode 10 continues to raise the bar on our source editing experience. Now in addition to the source control change bar, we are also extending our source control integration. We started first by unifying our conflict resolution system with Git, making the results more accurate, more predictable, and significantly faster. Next, we've enhanced the pull action to support rebase. So you can replay changes between branches [applause] -- yes, it's okay to clap for that. You can replay changes easily between branches without the unnecessary merge commits. And to keep your connections secure, Xcode will help you create SSH keys and upload them directly to your service accounts. And this is the perfect accompaniment for our service integrations, because in addition to GitHub, we're adding two new services this year, support for Atlassian's Bitbucket cloud and Bitbucket server -- -- and support for GitLab.com and self-hosting. There's a lot of source control love here. And both of these work great because their web interfaces will check out directly into Xcode. Now as Sebastian mentioned earlier, we are passionate about giving you great tools to debug and optimize your apps. And this year, we focused on the usability and performance of our tools. We started with LLDB, our lower-level debugger. Which now has faster startup and more precise access to your variables in the console and Xcode's variables view. Next, we've made downloading debug symbols five-times faster. So now it's more like seconds rather than minutes. We've enhanced our memory debugging tools to have faster loading, and saving of documents, and a new compact layout to help you visualize even more of your application at once. And earlier this spring, we introduced energy diagnostic reports. They're like crash logs, but for energy usage. These reports are automatically collected on iOS for test flight in App Store apps and surface details for foreground and background usage. These reports show up in the organizer and include stack frames to illustrate the issue. And just like with crash logs, you can open these reports in your project to navigate your code and find and fix the issues. Oh, and to go alongside these, we also have some improvements in testing. Earlier this spring, we enhanced code coverage, adding a command line tool to access coverage data, and giving you the ability to select individual targets to collect coverage for. This means your coverage reports can now be actively focused on the areas you are coding and testing. In addition to these, we're adding two new testing workflows this year -- actually, three. The first is that you can now automatically include or exclude new tests in your test bundles. Next, you can randomize the order that your tests are executed in to minimize accidental dependencies. And our biggest change for this year is you can now execute your tests in parallel inside of Xcode. Now last year, you could use Xcodebuild to test on many devices in parallel, sending all the same tests to each device. Now this is perfect for use with continuous integration where you want the broadest scale of testing. When you're working in Xcode, you're more often focused on a single configuration. And once you're testing, to finish as quickly as possible. This is the configuration Xcode 10 dramatically improves with parallel testing. Behind the scenes, Xcode will create copies of your Mac app or clones of your iOS simulator and then fan your tests suites out to them. This means you continue to test a single configuration, but your tests finish in a fraction of the time. And parallel testing automatically scales to the capacity of your machine, which means on an iMac Pro, it can be pretty awesome. How awesome, you might ask? Well, let's see in another demo. So we're going to go back to our solar system project one more time. And here we see the testing log for our Mac tests that we ran before. Took about 14 seconds. Let's run it with parallel testing now. I'll click and hold on the toolbar and select the test action. And we'll bring up the scheme sheet. In the options, I'll just click execute in parallel and click test, and we're going to build our tests for parallelization. And if you watch the doc in the lower right, you'll see that we launch the tests, we now launch many different processes -- this is one for each of our test suites -- and collect the results. And if we look at our testing log, it finished almost four times faster. So where parallel testing works great for unit tests, it works awesomely for UI tests. So I will select the iOS version of our application, and we'll kick off testing. So behind the scenes, we're going to go and clone the active simulator, and then set up a number of debug sessions for each one of these, and then switch over to a space with all of those simulators running. So you'll see we'll install different test suites on each of these simulators and kick off a different set of tests on each. So I can run all of my same tests faster on all these devices, which gives me the ability to add more tests and make a much better app. This is ludicrously awesome parallel testing in Xcode 10. Last year, we introduced a preview of our new build system written in Swift. Many of you tried it out with your projects and provided great feedback. And so I'm happy to say our modern build system is now on for all projects. In addition to greater reliability and stability, we also focused on overall build performance. You'll find the build system now has faster rebuilds, better task parallelization, and uses less memory. And the build system now includes new richer diagnostics to help you tune your project configuration to achieve the best build performance. Now staying on build performance for a second, I'd like to talk up another core component of our release, Swift 4.2. Over the last year, we have made steady improvements to compile times with Swift projects. We've sampled a number of open source iOS applications, and compared to our previous release, debug build performance with Xcode 10 is often twice as fast. And for release builds, code size is up to 30% smaller using the new size optimization, which is a great win for cellular downloads. Now in addition to these, Swift also adds a number of additions and runtime language improvements. Some of these are tongue-twisting APIs like synthesized hashtable conformance. A perfect place to try out these APIs is in Xcode Playgrounds because Xcode Playgrounds now include new [inaudible]-like interaction that allows you to evaluate new lines of code without restarting the Playground session. Here's a Playground of our solar system view. And the new lines of code added to move to the next planet are evaluated and return results all while the Playground continues to run. So all of these additions to the runtime language and tools continue Swift's great momentum as part of Xcode 10. And we also have another release coming up for you in the language, Swift 5. The focus of Swift 5 is greater adoption by delivering Swift as part of the OS. Apps will no longer need to include the Swift runtime when delivering on our newer OS releases, resulting in smaller downloads [applause] and faster launches. We're very excited about this, too, and we have made great progress toward this goal. And you'll see it in a release coming early next year. So Xcode 10 includes a number of great productivity improvements, alongside deep investments in performance, robustness, and stability throughout our tools. And all of this to help you do your best work now faster than ever. And that is Xcode 10. Next, I'd like to invite up John to tell you what's new in Machine Learning. John? Thank you, Matthew. Machine Learning is at the foundation of our operating systems and many of our applications. But our goal has been to provide simple and easy-to-use API to make Machine Learning accessible to everyone. And you've all done a fantastic job brining so many innovative features and intelligence to your applications. Last year we introduced Core ML with its base performance frameworks as well as Vision and Natural Language at a high level. And I'd like to start by showing you some improvements we're making with Vision and Natural Language. If we take Vision and, of course, a photo that we want to have depth, we now have APIs that support object detection and bounding boxes like this sign being held in the picture. We can do face detect, facial landmark detection. And also, barcode like this QR code can be detected in your image. Now in addition to the APIs we previously provided for depth, we now support people segmentation, so you can remove a person from a photo and separate them from the background or substitute in the background for something a little different. For Natural Language, we have a brand-new, easy-to-use, Swift-focused API. So you can take simple sentences like this one and automatically identify it as the English language. You can tokenize the sentence and convert it into its speech parts all with simple API. And as one other option, you can do named-entity recognition. Here, determining that the sentence is talking about Apple as the organization and a location in San Jose. Now you might think this is easy in languages like English, but we support many more, including French, German, Japanese, and this Simplified Chinese example. now let's look at Core ML. This is our foundation of our Machine Learning technologies. And just one year ago, we introduced Core ML here. And since then, we've got adoption of every major Machine Learning training framework and format. This is just incredible to have achieved in only one year. But we didn't want to stop there. We're introducing Core ML 2. And we focused on making the models execute faster, those models smaller, and making it far more customizable. And we know these are the features that were most recommended -- requested. To look at performance improvements, we've added a new batch API. Where previously you needed to do inference on each image, passing them between the CPU and GPU, you can now bundle those inference requests together and exploit the full performance of the CPU and GPU. Through this technique and enhancements to the Metal Performance Shaders underneath, we now have up to 30% performance improvement on large networks like Resnet. But if you're using a smaller network like the kind you're going to be using on iOS, we see up to four times improvement when running with MobileNet. Now we didn't stop there. We wanted to look at making the model smaller, so we now support quantization. So we can take a model that previously had shipped in 4.3 -- such as this example again from MobileNet -- and reduce it down to Int 8, and take its size from 17 megabytes to less than 5. This is a huge saving for the models that you bundle with your applications. Now you can do further reduction through features like table lookup quantization. And we support many other features, including support for custom models now and, a very popular feature, flexible shapes. So you no longer need to ship a model for each shape that you want to do inference on. You ship one model, and our simple API takes care of everything for you. now let's talk about Create ML, our brand-new, easy-to-use machine learning training framework. It brings together the power of Machine Learning, Swift, and Xcode, and Xcode Playgrounds. No more downloading packages from the Internet and going through long, complicated tutorials to portray a model. We support feature-level training such as image classification and natural language. And if you do want to go deeper in machine learning, we support traditional types of algorithms such as linear regression and boosted trees as well as traditional data processing. But we think people will want to use these feature type of training far more, so let's look at those examples. For Natural Language, you can now have your own custom Natural Language model that does text classification, word tagging, and of course, we support multiple languages. So you could train a model with very small datasets to do sentiment analysis such as these reviews for a movie where you just train with positive and negative in strings, and you build your own custom image -- custom text classifier. And then you could do the same for domain analysis, being able to train a model to understand whether you're talking about a hotel or a restaurant in a given sentence. Now we think, by far, image classification will be the most popular kind of training that people want to do, and so we've put a real focus on this. Traditionally, if you were training a very large model with what might only be a small dataset because as a developer that's all you have access to, your model wouldn't train well, and it over-fed, and you get poor predictions. Now Apple has extensive experience in training very large models with datasets of photos say in the -- with many millions. And we want to bring all that experience to all of you. And through a technique called transfer learning, you can train your own custom image classifier. So we'll bundle our model into our OS, so there's no need for you to ship that. You take your data, and use transfer learning with Create ML, and augment our model. That means you only need to ship the part of the model that you've augmented, bringing a huge saving to your applications. So we've worked with a number of developers who already have models in the around 100-megabyte range, just to add one intelligent feature to their application. And now, through transfer learning, they can take that model size down to three megabytes. Now this is far cooler to see if you see how it's all done inside Xcode and Xcode Playgrounds, so I'd like to invite Lizzie up to give you a demo of that now. Lizzie? Thank you, John. Let's take a look at how to create an app to classify different types of flowers. Now I've started by using a state-of-the-art image classifier model called Inception B3, but there are two problems with this approach. One, this model is quite large. It's taking up 100 megabytes in our app. And the second is even though this model has support for 1000 classifications, it can't correctly classify a rose. Now normally what I'd have to do is switch to a new development environment, download an open source machine learning library and spend hours training a new model. But now with the power of Create ML, you can do this in minutes and in Xcode. Now I'll switch to a new Playground and import Create ML UI. The next step is to define a builder that can build image classifier models. Then to enable drag-and-drop interaction with this model, we can show the builder in the live view. And see, on the side we get a prompt to drag in images to begin training. Now over on my desktop, I happen to have a bunch of different images of flowers organized into folders with the name of the particular one that they are. So we have some daisies, hibiscuses, and of course, some roses. Now what I'll do is I'll take this folder and drag it into the UI. And instantly, an image classifier model begins training on the Mac, accelerated by the GPU. And right away, I can see what the accuracy was on this training dataset. But what I'd really like to know is how it performs on new types of flowers that it hasn't seen. And I've set some of those aside here, and I can just drag them in to let the model begin evaluating on these new ones. And if I scroll, you can see what the actual label was for each type of flower and what the model predicted. Now 95% is pretty decent on this dataset. So what I'd like to do is add it into my app. And you can do so just by dragging and dropping it. I'll then add it. And if we take a look at this new model, we can see it's only 50 kilobytes. That's a huge savings. So I'll go ahead and delete -- -- I'll delete the 100-megabyte model and initialize the new image classifier. Now if I rerun the app, it's bundling this new model into the application. We can go ahead and test it to see if it can correctly predict on the images that we've trained it on, or new images of the same types of flowers. And indeed, it can correctly classify a rose. Let's try it on a hibiscus. And it can correctly predict on those, too, since we've trained it and incorporated it into our app. So as you've seen, we've been able to train our own classifier models using Create ML in a fraction of the amount of time to produce models a fraction of the size, all using Swift and Xcode. Back over to you, John. Thanks, Lizzie. Isn't that cool, a custom image classifier trained with three lines of Swift, in seconds, right there on a Mac? So we've looked at new Vision and Natural Language APIs and the enhancements we've made there; our improvements with Core ML 2 with smaller, faster models and even more customization; and Create ML, our brand-new machine learning training framework for the Mac. Now I'd like to talk about another area of intelligence that we've built into the OS, and that's shortcuts, a powerful new way for you to expose key capabilities of your applications through Siri. And you can even expose these key capabilities using voice commands. Previously, sections of the OS that had suggested features and actions for Apple's software are now accessible to you through shortcuts. We do all this prediction on device using machine learning that preserves your users' privacy. So you're probably asking how do you adopt shortcuts? Well, many of you have already adopted NSUserActivity for features such as Spotlight search and Handoff. And if you have -- and it's as simple as adding this one line of code making them eligible to prediction for the system. Yeah, one line of code. But if you want the full, rich experience of shortcuts, then you want to adopt the new Siri kit Intense API. That allows rich, inline capabilities of your application to be exposed in Siri, custom voice triggers and responses, and more importantly, more targeted predictions of when those shortcuts will be interesting to your users in the future. Now a great shortcut is one that accelerates engagement with your application, and increases engagement, too. It's one that's likely to be repeated more often. So in the TeamSnap example you want to be able to check your kid's soccer game schedule every Saturday morning. And ideally, it's one that can be engaged right there in the Siri UI and handled without the need to punch out to your app. But you do have the option if that's something that you want to do. Now when creating a shortcut, you need to do three simple things. You obviously need to define the shortcut and do it for those actions that really are interesting to the users. You need to donate when those shortcuts occur, even if that's in your application, because we need that signal to be able to predict those shortcuts in the future. And of course, you want to handle those shortcuts when they occur. Now if you've done all this, you get something pretty cool in that you can interact with your shortcut directly from home pod. So now without picking up your phone, you can just ask Siri from your home pod for your kid's soccer roster, and it will respond using the app. Now if you also want your shortcuts to be exposed through the Siri Watch Face, you can just adopt this new Relevant API. So that's shortcuts, a powerful new way to expose key capabilities of your application and increase engagement through Siri. Now I'd like to hand it over to Jeremy to talk to you about what's new in Metal. Jeremy? Thanks, John. So Metal is Apple's modern, high-performance, high-efficiency programming interface to the awesome power of the GPU at the heart of each of Apple's platforms. It accelerates both advanced 3D graphics and general purpose data parallel computations. And since we introduced Metal in 2014, we've seen it used for everything from smooth, high-performance UI to modern 3D games, advanced computational photography, and the latest in AR and VR experiences. And when we introduced our latest iPhones last fall, we were incredibly excited to reveal the next chapter in the Metal story with the A11 Bionic chip where Apple harnessed many years of deep expertise in hardware and software design to bring the Apple-designed GPU, optimized for Metal 2 with such innovative new features as tile shading and image blocks, and advancing the state of the art of GPU programming with both faster performance and lower power. Now your applications can use Metal directly for 3D graphics and GPU Compute. And Metal powers many of Apple's system frameworks for graphics, media, and data processing. Let me give you just one example. Our iOS camera framework uses Metal to calculate depth information, identify people in photos, and generate this depth-of-field effect in this gorgeous portrait image. And developers like Epic Games are using Metal's broad support across all of our platforms to bring their smash-hit game Fortnight to iPhone, iPad, and Mac. AMB's metal-accelerated radion [assumed spelling] pro-ender plugins, are now driving high performance, 3D contact creation and professional editing in Maxon Cinema4D and Autodesk Maya. And apps like Gravity Sketch are using Metal to power the next generations of artists in immersive professional VR editing. Metal's machine learning acceleration empowers iOS apps like BeCasso to transform your photos into beautiful paintings. And drives automatic, intelligent image editing in Pixelmator Pro for macOS. And those are just a few examples as the developer adoption of metal has been truly astounding, with more than 400,000 apps now using the Metal API. And all systems running an iOS 12 and macOS Mojave support Metal, which includes all iOS devices and all Macs released in at least the last five years, which means there are now well over 1 billion Metal systems for your applications and games. So with Metal's deep and broad support across all of Apple's desktop and mobile platforms, we are now deprecating the legacy OpenGL and OpenCL GPU framework, starting in macOS Mojave, iOS 12, and tvOS 12. Now apps using these legacy APIs will still continue to work in these releases, but deprecation is a first step as we wind down legacy technologies. So if you've not already done so, you should begin transitioning your apps to Metal. And we'll communicate more details about this transition in the near future. Now as you bring your apps to metal, we are here to help. The Metal API is dramatically easier to use and much more approachable than these other GPU programming APIs. It contains a familiar, yet powerful, C++ GPU shading language. And we provide a full suite of advanced debugging and performance profiling tools for using Metal, all built right into Xcode. We have GPU performance counters with advanced profiling to identify your most expensive lines of shader code and a visual API debugger for navigating your Metal function calls, a Metal System Trace to put your Metal commands in the context of everything else happening on the system. And we're really excited to announce two new powerful tools this year, a new Metal Dependency Viewer where you can investigate your complex, multipass rendering and command encoders, and an all-new, interactive GPU source code shader debugger where you can actually explore your Metal code right down to the pixel level. Now you have to see these new tools in action, so I'd like to invite Seth to give you a demonstration. Seth? Thank you, John. Xcode's GP debugger is the tool for developing your Metal applications. In the Debug Navigator on the left, you can see all the Metal API codes and [inaudible] codes used in your frame. And on the right, you can see the results of the selected [inaudible]. The main editor shows you the -- all the buffers, textures, and other resources that were used for that [inaudible]. Well, new in Xcode 10, we're introducing the Dependency Viewer, a powerful way to understand how complex render passes combine to form your scene. This gives you a blueprint of your frame in explaining and understanding how the complex render graphs at one application such as Unity's breathtaking "Book of the Dead" demo shown here. I can zoom out to see more detail. Earlier render passes are shown at the top, with the later render passes shown at the bottom. The lines indicate the dependencies between passes, with those for the selected pass highlighted in blue. As you can see, with more than 100 render passes, there's clearly a lot going on in this scene. Now as good as this scene looks, there's always room for more flair. So I did an additional render pass, the lens flare. But as you can see, something didn't [inaudible] quite right -- far, far to green. Well, let's zoom in, select a pixel, and launch the new Shader Debugger, a powerful interactive tool to let you visually debug shaders [inaudible]. In the main editor, I can see my source code. And in the sidebar to its right, I can see variables touched by each line of code. Additionally, I can expand any of these to see more details in line. These two views visualize the area around the selected pixel, corresponding to the highlighted region in the frame attachment. The view on the left visualizes the variable value. And the one on the right, the pixel -- the execution mask. This indicates which pixels executed this line of code. This is an incredibly powerful way to debug the massively [inaudible] execution of shaders on the GPU. Now you can see here that the shape of the execution mask matches that of the visual aberration, telling me that the issue exists on this line of code. Well, now that I know where the issue is, I can see what I've done wrong, using the vector length of the lens flare rather than the color of the lens flare. That will be easy to fix. I can now hit the update shaders button to quickly apply the fix, recompiling the shader and deploying it to the GPU. And here we can see that my lens flare is fixed, and the scene looks cool. So that's the new Dependency Viewer and GP Shader Debugger in Xcode 10, giving you powerful new tools to build your Metal applications. Jeremy? All right, [applause] thank you, Seth. So in addition to these amazing new tools, we're continuing to advance Metal with a fantastic set of new features in iOS 12 and macOS Mojave. Now I'm going to highlight just three of them today -- GPU-driven command encoding, machine learning training acceleration, and ray tracing. So first, GPU-driven command encoding. Now historically, your app would encode its GPU commands using the CPU and then subsequently execute those commands on the GPU. And while Metal enabled this encoding to be very fast, it could still become bottlenecked by the synchronization between the CPU and the GPU. Well, now in iOS 12 and macOS Mojave, you can actually encode those commands right on the GPU itself, freeing up precious CPU time for other use by your games and apps. And because you issue these commands right on the GPU using a compute shader, you can actually officially construct massive numbers of commands in parallel as well, unlocking completely new levels of rendering performance and sophistication. Next, I'd like to share the latest advances in Metal's support for machine learning. In iOS 12 and macOS Mojave, we have augmented our existing library of Metal performance shaders with an enormous array of all-new compute kernels, optimized to support machine learning training right on the local GPU on your iOS and Mac devices. And the performance improvements we are seeing from these new Metal performance shaders on training are truly stunning, with an order of magnitude faster training times. We're also really excited to announce we've been working with Google to bring Metal acceleration to TensorFlow later this year, and the early performance results are showing an astonishing improvement of 20 times the previous implementation. Yeah, it's awesome. And last, ray tracing. Now this is a time-honored technique to achieve incredibly realistic scenes, often used for high-end rendering and 3D product design. However, it traditionally had to be done offline because it was so computationally expensive. Now let me describe why very quickly. First, you would need to mathematically model the rays from a light source as they bounce off of objects through the scene, toward the screen, and into your eye. And to achieve higher and higher resolutions, you would need to add more, and more, and more rays until you could reach the desired resolution. And this simple 1k-by-1k image would take nearly 6 million rays to generate. Now each of those rays also must be processed with at least two sets of expensive mathematical calculations. First, you had to determine if a given ray intersects a particular triangle in your scene. And second, you must apply a material-specific shader necessary to generate the pixel. Now originally, both of these operations would have been performed by the CPU. But while the GPU can easily handle the pixel shading, the ray-triangle intersection itself could remain an expensive CPU bottleneck, and it would be incredibly difficult to move this to the GPU efficiently. But the new Metal Ray-Triangle Intersector solves this problem for you. And with this new API, you get a dramatic increase in performance of up to 10x in a very simple-to-use package, all pre-optimized for use with our iOS and macOS GPUs. And it really is that simple, just a few lines of code. And the ray tracing, like many GPU compute operations, is exactly the kind of operation that can efficiently scale with the available GPU horsepower. So we can actually get even more performance by using Metal 2 support for external GPUs. Now you really have to see this in action. And I'd like to invite Rav to give a quick demonstration. Rav? Thank you, Jeremy. All right, let's bring up this ray trace rendering of the Amazon Lumberyard Bistro scene using the CPU to perform the intersection calculations. And this implementation is optimized to run on all 10 cores in our iMac Pro. We've also added a little benchmark mode that times how long it takes to do 80 iterations of our ray-tracing algorithm. And for context, that requires performing over 6 billion intersection tests. And as you can see, we need about 12 seconds to do that on the CPU. So let's compare that to using the new ray -- the new Metal Ray-Triangle Intersector on the built-in GPU in the iMac Pro. And you can immediately see that it's much faster, and we only need about 1.3 seconds to do the same amount of work. It's so good, I'm going to do it again. Here we go. And it's done. So getting an almost 10-times performance increase is fantastic. But of course, we didn't just stop there. As Jeremy noted, ray tracing is well-suited for parallelization across multiple GPUs, so I can enable an external GPU that I previously attached to this iMac Pro and get the render time cut in half. So you'll note the green line that we've added to help visualize how we're splitting this workload across the two GPUs, with each GPU rendering half the frame in this case. So this is a great improvement, but as Jeremy says, you can never have too many GPUs. So let's add another two for a total of four GPUs now rendering the scene. So that's over 40 teraflops of compute capability with our iMac Pro, and we're rendering the scene 30 times faster than the CPU. We think that's pretty amazing, yep. And since ray tracing is so great for rendering shadows, I'm just going to turn off a couple lights here to get them to pop. And you can really appreciate how much faster the image converges on the GPUs. So the new Metal Ray-Triangle Intersector and external GPU support on macOS we believe is going to enable some great new workflows on apps that are taking advantage of ray tracing techniques. Thank you. Back to you, Jeremy [applause]. All right, that is really stunning. Thanks, Rav. So that's Metal 2 in iOS 12 and macOS Mojave, an easy-to-use, unified 3D graphics and GPU compute API with broad support across all of Apple's products, including the A11 Bionic and the Apple-designed GPU. GPU developer tools integrated right into Xcode and all-new features to support the latest advancements in machine learning training and ray tracing. There's never been a better time to move your app to Metal, and we can't wait to see what you'll create next. Thank you. And now, I would like to hand it over to Mike Rockwell to talk about what's the latest news in AR? Thanks. Thanks, Jeremy. So last year has been an amazing year for AR at Apple. With the debut of ARKit at last WWDC, iOS became the world's largest AR platform by a lot. There are hundreds of millions of AR-enabled iOS devices, and that number is growing rapidly. As Craig showed you this morning, with iOS 12, we're taking things further by making AR ubiquitous across the operating system. We can now experience AR content via the new QuickLook Viewer in Messages, News, Safari, and more. To do that, we had to work on and create a file format that we optimized for AR. And we worked with Pixar and Adobe to create a new mobile AR format called USDZ. It's based on the universal scene description format that's used across the industry for professional content creation. It's optimized for mobile devices, and it supports Rich 3D assets and animation. It's incredibly easy to use USDZ. On the web, it just takes a couple of lines of HTML, and it's also natively supported in SceneKit using Model I/O, so you can easily use it in your applications. We've also been working closely with industry leaders in content creation tools to provide native support for USDZ. And as you heard this morning, Abhay said this morning that he had a sneak peek for you about what they're doing at Adobe. So I'd like to invite him to the stage to give that to you right now. Abhay? Thanks, Mike. It's great to be back onstage. So as you heard in this morning's keynote, Adobe's Creative Cloud and ARKit will be able to reimagine and blend the digital and the physical worlds. Now this will require a complete reimagination of new design interaction models. So earlier today, we announced a new system for creating AR experiences called Project Aero that infused ARKit with the power of familiar Creative Cloud applications like Photoshop and Dimension. So in fact, for the first time, with Creative Cloud and iOS, you will now have a what-you-see-is-what-you-get editing in AR. So as you think about this -- and we looked at it -- ARKit is absolutely the leading platform for AR. And so we're really excited to partner closely with Apple as we go jointly explore and push the boundaries of immersive design. But to fully realize the potential of AR, you really have to work across the entire ecosystem. And so today, we are also announcing that Adobe will natively support USDZ format, along with Apple and Pixar [applause]. Now AR is a unique medium in that it allows interactive content to go extend well beyond the screen, where physical spaces around us literally become a creative canvas. So let's take a look. That's pretty cool. So at its core, Project Aero is part of Adobe's vision and mission to truly democratize creation of immersive content. As you hopefully saw in that video, creators and developers will be able to collaborate seamlessly to deliver a wide range of AR experiences using these tools. Stay tuned for more updates on Project Aero at our upcoming conference, AdobeMax. Personally, I couldn't be more excited about our partnership with Apple, as we go together jointly explore the limits of this emerging and powerful new storytelling medium. Thank you. Back to you, Mike. Thanks, Abhay. Isn't that awesome? Amazing stuff. Of course, the foundation of AR at Apple is ARKit. With robust device position localization, accurate lighting and size estimation, ARKit has made it easy to create AR applications. The iPhone X has provided groundbreaking face tracking that used to require custom hardware. After the initial release, we quickly followed up with ARKit 1.5, adding 2D image triggers, a high-resolution background camera, and the ability to suspend and resume tracking so that you don't have to restart an AR session if you get a phone call. Well, I'm incredibly excited to tell you about our next big jump forward, ARKit 2. ARKit 2 delivers a big set of advances, including improved face tracking, with a new ability to track your gaze and tongue. These highly-requested features allow you to take facial animation to a new level of realism. Turns out that the first thing kids do when they play with animojis is stick their tongue out. And I think a lot of you do, too. That's why we had to put that in there. To more accurately integrate objects into a scene, we've added environment texturing. ARKit creates textures based on what the camera sees in the real world -- notice that the globe is reflecting the real picture on the table below. But what about what the camera can't see? While using machine learning, we trained a neural network on thousands of typical environments. And this enables ARKit to hallucinate the rest of the scene. This means that you'll get plausible reflections of things like overhead lighting -- you can see that in the globe -- even though it's never seen the lighting in the environment at all. We've extended the 2D image detection to provide support for those -- tracking those images in three dimensions. So you can now have 3D objects that stick to images in the real world when they're moved around -- and not only in 2D, but also in 3D. ARKit can now detect 3D objects. You can scan objects via an API, or a simple developer tool we provide, and then later, these maps can be used to recognize those objects and their locations and trigger a contextually-relevant AR experience. An incredibly important feature of ARKit 2 is support for persistent experiences. You can see here in the video that we've mapped an environment and then placed a 3D object. This map can be saved and then later used to recognize the space and relocalize to that same coordinate system -- and not only on that device. You can share these maps to other devices to allow them to have the exact same experience. This makes it possible to create apps that provide persistent experiences you can go back to again and again. You could, for example, have an augmented reality pinboard in your home with pictures and artwork. And you can share these maps without having to go to the cloud. These can be done peer-to-peer locally on your devices. One other thing that we've done is we've allowed you to have the ability to share these maps in real time. And this lets you create multiplayer AR games. So to experiment with this, we created a new game called SwiftShot. And I'll show you the video that -- of it that we did. So SwiftShot is a blast to play, and we actually have it here at the show. If you haven't had a chance to go by, we have an AR game area. We wanted to share it with you, so we've actually made the full source code available for you to download under an open license. You can play with it and modify it as you like. We can't wait to see the creative things you'll do with SwiftShot. So that is ARKit 2, improved face tracking, environment texturing, image detection and tracking, 3D object detection, and persistent experiences as well as multi-user experiences. That combined with USDZ across the operating system makes iOS 12 by far the most powerful platform for AR. And we're really excited to be giving that to you today, and can't wait to see what you'll do with it. So with that, I'll hand it back to Sebastian. Thank you. Thank you, Mike. Wow, I think we've seen a ton of exciting new technologies today, and I hope you're really, really, really excited about this. We make it easy to leverage machine learning, build great new experiences with ARKit, high-performance graphics with Metal, a huge step forward on the Mac with Dark Mode. I know you all love this. And it's all backed by great advances in our development tools that are really critical to make the most of these super-powerful technologies. And we've also covered how we, together, can focus on what's most important to our users. All these great technologies and tools are available today as a developer preview from the WWDC attendee portal. Who here has started downloading them? Few people? Okay, you've got to hurry up. Distribution is limited. Please make sure to download it right away. And also, make the most of your week. There are more than 100 sessions here at the conference that go deep in all of these topics. Really, really great sessions. We also recommend that you make good use of all of the labs that we have, because you can get help from the many Apple engineers that are here onsite to answer all of your questions. So with that, I hope you have a great conference, and I'm looking forward to seeing you around this week. Thank you.  Hi, everyone. My name's Kyle. I'm a software engineer at Apple and today we'd like to take a deep dive into iOS memory. Now, just as a quick note, even though this is focused on iOS, a lot of what we're covering will apply to other platforms as well. So the first thing we'd want to talk about is, why reduce memory? And when we want to reduce memory, really, we're talking about reducing our memory footprint. So we'll talk about that. We have some tools for how to profile a memory footprint. We have some special notes on images, optimizing when in the background. And then, we'll wrap it all up with a nice demo. So why reduce memory? The easy answer is users have a better experience. Not only will your app launch faster. The system will perform better. Your app will stay in memory longer. Other apps will stay in memory longer. Pretty much everything's better. Now, if you look to your left and look to your right, you're actually helping those developers out as well by reducing your memory. Now, we're talking about reducing memory, but really, it's the memory footprint. Not all memory is created equal. What do I mean by that? Well, we need to talk about pages. Not that type of pages. We're talking about pages of memory. Now, a memory page is given to you by the system, and it can hold multiple objects on the heap. And some objects can actually span multiple pages. They're typically 16K in size, and they can come in clean or dirty. The memory use of your app is actually the number of pages multiplied by the page size. So as an example of clean and dirty pages, let's say I allocate an array of 20,000 integers. The system may give me six pages. Now, these pages are clean when I allocate them. However, when I start writing to the data buffers, for example, if I write to the first place in this array, that page has become dirty. Similarly, if I write to the last page, that, or the last place in the buffer, the last page becomes dirty as well. Note that the four pages in between are still clean because the app has not written to them yet. Another interesting thing to talk about is memory-mapped files. Now, this is files that are on disk but loaded in the memory. Now, if you use read-only files, these are always going to be clean pages. The kernel actually manages when they come in and off of disk into RAM. So a good example of this would be a JPEG. If I have a JPEG that's, say, 50 kilobytes of size, when it's memory mapped in, that actually is mapped into four pages of memory, give or take. Now, the fourth page is actually not completely full, so it can be used for other things. Memory's a little bit tricky like that. But those three pages before will always be purgeable by the system. And when we talk about a typical app, their footprint and profile has a dirty, a compressed, and a clean segment of memory. Let's break these down. So clean memory is data that can be paged out. Now, these are the memory-mapped files we just talked about. Could be images, data Blobs, training models. They can also be frameworks. So every framework has a DATA CONST section. Now, this is typically clean, but if you do any runtime shenanigans like swizzling, that can actually make it dirty. Dirty memory is any memory that has been written to by your app. Now, these can be objects, anything that has been malloced -- strings, arrays, et cetera. It can be decoded image buffers, which we'll talk about in a bit. And it can also be frameworks. Frameworks have a data section and a data dirty section as well. Now, those are always going to count towards dirty memory. And if you might have noticed, I brought up frameworks twice. Yes, frameworks that you link actually use memory and dirty memory. Now, this is just a necessary part of linking frameworks, but if you maintain your own framework, singletons and global initializers are a great way to reduce the amount of dirty memory they use because a singleton's always going to be in memory after it's been created, and these initializers are also run whenever the framework is linked or the class is loaded. Now, compressed memory is pretty cool. iOS doesn't have a traditional disk swap system. Instead, it uses a memory compressor. This was introduced in iOS 7. Now, a memory compressor or the memory compressor will take unaccessed pages and squeeze them down, which can actually create more space. But on access, the compressor will then decompress them so the memory can be read. Let's look at an example. Say I have a dictionary that I'm using for caching. Now, it uses up three pages of memory right now, but if I haven't accessed this in a while and it needs to, the system needs some space, it can actually squeeze it down into one page. Now, this is now compressed, but I'm actually saving space or I've got two extra pages. So if, some point in the future, I access it, it will grow back. So let's talk about memory warnings for a second. The app is not always the cause of a memory warning. So if you're on a low-memory device and you get a phone call, that could trigger a memory warning, and you're out. So don't necessarily assume that a memory warning is your cause. So this compressor complicates freeing memory because, depending on what it's compressed, you can actually use more memory than before. So instead, we recommend policy changes, such as maybe not caching anything for a little bit or kind of throttling some of the background work when a memory warning occurs. Now, some of us may have this in our apps. We get a memory warning, and we decide to remove all objects from our cache. Going back to that example of the compressed dictionary, what's going to happen? Well, since I'm now accessing that dictionary, I'm now using up more pages than I was before. This is not what we want to do in a memory-constrained environment. And because I'm removing all the objects, I'm doing a lot of work just to get it back down to one page, which is what it was when it was compressed. So we really got to be careful about memory warnings in general. Now, this brings up an important point about caching. When we cache, we are really trying to save the CPU from doing repeated work, but if we cache too much, we're going to use up all of our memory, and that can have problems with the system. So try and remember that there's a memory compressor and cache, you know, get that balance just right on what to cache and what to kind of recompute. One other note is that if you use an NSCache instead of a dictionary, that's a threat-safe way to store cached objects. And because of the way NSCache allocates its memory, it's actually purgeable, so it works even better in a memory-constrained environment. Going back to our typical app with those three sections, when we talk about the app's footprint, we're really talking about the dirty and compressed segments. Clean memory doesn't really count. Now, every app has a footprint limit. Now, this limit's fairly high for an app, but keep in mind that, depending on the device, your limit will change. So you won't be able to use as much memory on a 1-gigabyte device as you would on a 4-gigabyte device. Now, there's also extensions. Extensions have a much lower footprint, so you really need to be even more mindful about that when you are using an extension. When you exceed the footprint, you will get a exception. Now, these exceptions are the EXC RESOURCE EXCEPTION. So what I'd like to do now is invite up James to talk about how we can profile our footprint. Thanks, James. Thank you. Thanks, Kyle. All right. I'm James. I'm a software engineer here at Apple. And I'd like to introduce some of the more advanced tools we have for profiling and investigating your application's footprint. You're all probably already familiar with the Xcode memory gauge. It shows up right here in the debug navigator, and it's a great way for quickly seeing the memory footprint of your app. In Xcode 10, it now shows you the value that the system grades you against, so don't be too concerned if this looks different from Xcode 9. So I was running my app in Xcode, and I saw that it was consuming more memory. What tool should I reach for next? Well, Instruments, obviously. This provides a number of ways to investigate your app's footprint. You're probably already familiar with Allocations and Leaks. Allocations profiles the heap allocations made by your app, and Leaks will check for memory leaks in a process over time. But you might not be so familiar with the VM Tracker and the Virtual Memory Trace. If you remember back to when Kyle was talking about the primary classes of memory in iOS, he was, he talked about dirty and compressed memory. Well, the VM Tracker provides a great way to profile this. It has separate tracks for dirty and swapped, which, in iOS, is compressed memory, and tells you some information about the resident size. I find this really useful for investigating the dirty memory size of my app. Finally, in Instruments is the VM Memory Trace. This provides a deep view into the virtual memory system's performance with regards to your app. I find the By Operation tab really useful here. It gives you a virtual memory system profile and will show you things like page cache hits and page zero fills for the VM. Kyle mentioned earlier that if you approach the memory limit of the device, you'll receive an EXC resource exception. Well, if you're running your app now in Xcode 10, Xcode will catch this exception and pause your app for you. This means you can start the memory debugger and begin your investigation right from there. I think this is really, really useful. The memory debugger for Xcode was shipped in Xcode 8, and it helps you track down object dependencies, cycles, and leaks. And in Xcode 10, it's been updated with this great new layout. It's so good for viewing really large Memgraphs. Under the hood, Xcode uses the Memgraph file format to store information about the memory use of your app. What you may not have known is that you can use Memgraphs with a number of our command-line tools. First, you need to export a Memgraph from Xcode. This is really simple. You just click the Export Memgraph button in the File menu and save it out. Then, you can pass that Memgraph to the command-line tool instead of the target [inaudible] and you're good to go. So I was running my app in Xcode 10, and I received a memory resource exception. This isn't cool. I should probably take a Memgraph and investigate this further. But what do I do next? Well, obviously to the terminal. The first tool I often reach for is vmmap. It gives you a high-level breakdown of memory consumption in your app by printing the VM regions that are allocated to the process. The summary flag is a great way to get started. It prints details of the size in memory of the region, the amount of the region that's dirty, and the amount of memory that's swapped or compressed in iOS. And remember, it's this dirty and swap that's really important here. One important point of note is the swap size gives you the precompressed size of your data, not what it compressed down to. If you really need to dig deeper and you want more information, you can just run vmmap against the Memgraph , and you'll get contents of all of the regions. So we'll start by printing you the nonwritable region, so, like, your program's text or executable code, and then the writable regions, so the data sections, for instance. This is where your process heap will be. One really cool aside to all of this is that all these tools work really well with standard command-line utilities. So for example, I was profiling my app in VM Tracker the other day, and I saw the, an increase in the amount of dirty memory. So I took a Memgraph, and I want to find out, are any frameworks or libraries I'm linking to contributing to this dirty data? So here I've run vmmap against the Memgraph I took. And I've used the pages flag. This means that vmmap will print out the number of pages instead of just raw bytes. I then piped that into grep, where I'm searching for a dylib, so I need dynamic library here. And then, finally, I pipe that into a super simple awk script to sum up the dirty column and then print it out as the number of dirty pages at the end. I think this is super cool, and I use it all the time. It allows you to compose really powerful debugging workflows for you and your teams. Another command-line utility that macOS developers might be familiar with already is leaks. It tracks objects in the heap that aren't rooted anywhere at runtime. So remember, if you see an object in leaks, it's dirty memory that you can never free. Let's look at a leak in the memory debugger. Here I've got 3 objects, all holding strong references to each other, creating a classic retain cycle. So let's look at the same leak in the leaks tool. This year, leaks has been updated to not only show the leaked objects, but also the retain cycles that they belong to. And if malloc stack logging was enabled on the process, we'll even give you a backtrace for the root node. One question I often ask myself is, where's all my memory going? I've looked in vmmap, and I see the heap is really large, but what do I do about it next? Well, the heap tool provides all sorts of information about object allocations in the process heap. It helps you track down really large allocations or just lots of the same kind of object. So here I've got a Memgraph that I took when Xcode caught a memory resource exception, and I want to investigate the heap. So I've passed it to heap, which is giving me information about the class name for each of those objects, the number of them, and some information about their average size and the total size for that class of object. So here I kind of see, like, not, lots and lots of small objects, but I don't think that's the problem. I, that, I don't think that's really the problem here. By default, heap will sort by count. But instead, what I want to see is the largest objects, not the most numerous, so passing the sortBySize flag to heap will cause it to sort by size. Here I see a few of these enormous NSConcreteData objects. I should probably attach this output and the Memgraph to a bug report, but that's not going far enough, really. I should figure out where these came from. First, I need to get the address for one of these NSConcreteData objects. The addresses flag in heap. When you pass the addresses flag to heap with the name of a class, it'll give you an address for each instance on the heap. So now I have these addresses, I can find out where one of these came from. This is where malloc stack logging comes in handy. When enabled, the system will record a backtrace for each allocation. These logs get captured up when we record a Memgraph, and they're used to annotate existing output for some of our tools. You can enable it really easily in the scheme editor in the diagnostics tab. I'd recommend using the live allocations option if you're going to use it with a Memgraph. So my malloc's, my Memgraph was captured in malloc stack logging. Now, to find the backtrace for the allocation. This is where malloc history comes in helpful. You just pass malloc history, the Memgraph, and an address for an instance in memory, and, if there was a backtrace captured for it, it'll give it to you. So here I've taken the address for one of those really big NSConcreteDatas. I've passed it to malloc history, and I've got a backtrace. And, interestingly, it looks like my NoirFilter's apply method here is creating that huge NS data. I should probably attach this and the Memgraph to a bug report and get someone else to look at it. These are just a few of the ways you can deeply investigate the behavior of your app. So when faced with a memory problem, which tool do you pick? Well, there are 3 ways to think about this. Do you want to see object creation? Do you want to see what references an object or address in memory? Or do you just want to see how large an instance is? If malloc stack logging was enabled when you record, when you, when your process was started, malloc history can help you find the backtrace for that object. If you just want to see what references an object in memory, you can use leaks and a bunch of options that it has in the [inaudible] page to help you there. And finally, if you just want to see how large a region or an instance is, vmmap and heap are the go-to tools. As a jumping off point, I'd recommend just running vmmap with the summary flag against a Memgraph taken of your process and then follow the thread down there. Now, I'd like to hand back to Kyle, who's going to talk about what can be some of the largest objects in iOS apps, and that's images. Kyle? Thanks, James. So images. The most important thing about images to remember is that the memory use is related to the dimensions of the image, not its file size. As an example, I have this really beautiful picture that I want to use as a wallpaper for an iPad app. It measures 2048 by 1536, and the file on disk is 590 kilobytes. But how much memory does it use really? 10 megabytes. 10 megs, that's huge! And the reason why is because multiplying the number of pixels wide by high, 2048 by 1536, by 4 bytes per pixel gets you about 10 megabytes. So why is it so much larger? Well, we have to talk about how images work on iOS. There's a load, a decode, and a render phase. So the load phase takes this 590-kilobyte JPEG file, which is compressed, loads it into memory. The decode converts that JPEG file into a format that the GPU can read. Now, this needs to be uncompressed, which makes it 10 megabytes. Once it's been decoded, it can be rendered at will. For more information on images and how to kind of optimize them, I'd recommend checking out the Images and Graphics Best Practice session that was earlier this week. Now, 4 bytes per pixel we got by the SRGB format. This is typically the most common format that images in graphics are. It's 8 bits per pixel, so you have 1 byte for red, 1 byte for green, and 1 byte for blue, and an alpha component. However, we can go larger. iOS hardware can render wide format. Now, wide format, to get that expressive colors, requires 2 bytes per pixel, so we double the size of our image. Cameras on the iPhone 7, 8, X, and the, some of the iPad Pros are great for capturing this high-fidelity content. You can also use it for super accurate colors for, like, sports logos and such. But these are only really useful on the wide format displays, so we don't want to use this when we don't need to. On the flip side, we can actually go smaller. Now, there's a luminance and alpha 8 format. This format stores a grayscale and an alpha value only. This is typically used in shaders, so like Metal apps and stuff. Not very common in our usage. We can actually get even smaller. We can go down to what we call the alpha 8 format. Now, alpha 8 just has 1 channel, 1 byte per pixel. Very small. It's 75% smaller than SRGB. Now, this is great for masks or text that's monochrome because we're using 75% less memory. So if we look at the breakdown, we can go from 1 byte per pixel with alpha 8 all the way up to 8 bytes per pixel with wide format. There's a huge range. So what we really need to do is know how to pick the right format. So how do we pick the right format? The short answer is don't pick the format. Let the format pick you. If you migrate away from using the UIGraphics BeginImageContext WithOptions API, which has been in iOS since it began, and instead switch to the UIGraphics ImageRenderer format, you can save a lot of memory because the UIGraphics BeginImageContext WithOptions is always a 4-byte-per-pixel format. It's always SRGB. So you don't get the wide format if you want it, and you don't get the 1-byte-per-pixel A8 format if you need it. Instead, if you use the UIGraphics ImageRenderer API, which came in iOS 10, as of iOS 12, it will automatically pick the best graphics format for you. Here's an example. Say I'm drawing a circle for a mask. Now, using the old API with the highlighted code here is my drawing code, I'm getting a 4-byte-per-pixel format just to draw a black circle. If I instead switch to the new API, I'm using the exact same drawing code. Just using the new API, I'm now getting a 1-byte-per-pixel image. This means that it's 75% less memory use. That's a great savings and the same fidelity. As an additional bonus, if I want to use this mask over again, I can change the tint color on an image view, and that will just change it with a multiply, meaning that I don't have to allocate any more memory. So I can use this not just as a black circle, but as a blue circle, red circle, green circle with no additional memory cost. It's really cool. One other thing that we typically do with images is downsample them. So when we want to make like a thumbnail or something, we want to scale it down. What we don't want to do is use a UIImage for the downscaling. If we actually use UIImage to draw, it's a little bit less performant due to internal coordinate space transforms. And, as we saw earlier, it would decompress the entire image in the memory. Instead, there's this ImageIO framework. ImageIO can actually downsample the image, and it will use a streaming API such that you only pay the dirty memory cost of the resulting image. So this will save you a memory spike. As an example, here's some code where I get a file on disk. This could also be a file I downloaded. And I'm using the UIImage to draw into a smaller rect. This is still going to have that big spike. Now, instead, if I switch to ImageIO, I still have to load the file from disk. I set up some parameters because it's a lower-level API to say how big I want this image to be, and then I just ask it to create it with CGImageSource CreateThumbnail AtIndex. Now, that CG image I can wrap in a UIImage, and I'm good to go. I've got a much smaller image, and it's about 50% faster than that previous code. Now, another thing we want to talk about is how to optimize when in the background. Say I have an image in an app, full screen. It's beautiful. I'm loving it. But then, I need to go to my Home screen to take care of a notification or go to a different app. That image is still in memory. So as a good rule of thumb, we recommend unloading large resources you cannot see. There are 2 ways to do this. The first is the app life cycle. So if you background your app or foreground it, the app life cycle events are great to, are a great way to know. Now, this applies to mostly the on-screen views because those don't conform to the UIViewController appearance life cycle. UIViewController methods are great for, like, tab controllers or navigation controllers because you're going to have multiple view controllers, but only 1 of them is on screen at once. So if you leverage like the viewWillAppear and viewDidDisappear code or callbacks, you can keep your memory footprint smaller. Now, as an example, if I register for the notifications for the application entering the background, I can unload my large assets -- in this case, images. When the app comes back to the foreground, I get a notification for that. If I reload my images there, I'm saving memory when in the background, and I'm keeping the same fidelity when the user comes back. It's completely transparent to them, but more memory is available to the system. Similarly, if I'm in a nav controller or a tab controller, my view controllers can unload their images when they disappear. And before they come back with the viewWillAppear method, I can reload them. So again, the user doesn't notice anything's different. Our apps are just using less memory, which is great. And now, I'd like to invite up Kris to kind of bring this all together in a nice demo. Kris? Okay. I'm going to switch to the demo machine. There we go. So I've been working on this app. What it does is it starts with these really high-resolution images from our solar system that I got from NASA, and it lets you apply different filters to them. And here we can see a quick example, applying a filter to our Sun. I'm really pleased with how it's going so far, so I sent it off to James to get his opinion on it, and he sent me back an email with 2 attachments. One was a Memgraph file, and the other one was this image. Now, James is a pretty reserved and understated guy, so when he's got 2 red exclamation points and a scream emoji, I know he's pretty upset. So I went to James and I said, "You know, I don't understand what the big deal is. I clearly have at least half a gigabyte before I'm even in the red, you know. I have all this available memory. Shouldn't I be using it?" And James, who's a much better developer than I am, pointed out a few things that's, a few things that are wrong with my logic. First of all, this gauge is measuring a device with 2 gigabytes of memory. Not all our devices have that much memory. If this code was running on a device with a, only 1 gigabyte of memory, there's a good chance our app would already be terminated by the operating system. Second, the operating system doesn't just, doesn't use just how much memory your app is using when designing when to terminate your app, but also what else is going on in the operating system. So just because we're not to the red yet doesn't mean we're not in danger of being terminated. And third, this represents a terrible experience for the user. In fact, if you look at the usage comparison chart, you can see other processes has zero kilobytes of memory. That's because they've all been jettisoned by the operating system to make room for our app. You should all take a good look at me and give me the stink eye because now when the user has to go watch your app, you have to load from scratch. So James makes some pretty good points, so I think, in general, we want to get this needle as far to the left as possible instead of as far to the right. So let's see what we can do. Let me go ahead and take a look at the Memgraph file. And I have a couple go-to tricks that I use when working with a Memgraph file or go-to strategies. And the first -- actually, let me bring this up a little bit -- is to look for leaks. So if I go down to the Filter toolbar and click on the leaks filter, that'll show me just any leaks that are in my Memgraph file. Turns out this Memgraph file has no leaks. Well, that's both kind of good news and bad news. It's great that there are no leaks, but now I have to figure out what's actually going on here. The other thing that Memgraph is really good for is showing me how many instances of an object are in memory and if there's more than I expect. But if I look at this Memgraph, I can see if I actually just focus specifically on the objects from my code, there's only 5 in memory, and there's actually only 1 of each of these. If there were, you know, multiple root view controllers, or multiple noir filters, or multiple filters in memory, more than I expect, that's something else I could investigate. Well, there's no more instances here than I expect, but maybe one of these is really big. It's not very likely, but I might as well check. So I'm going to go to the memory inspector. I'm going to look at these. Each of them, it lists the size for each of the objects. So I can see my app delegate is 32 bytes. The data view controller is 1500. As I go through each of these, none of these are clearly responsible for the, you know, 1 plus gigabytes of memory my app is using. So that's it for my bag of tricks in dealing with Memgraph in Xcode. Where do I go now? Well, I just watched this great WWDC session about using command-line tools in Memgraph files. So let me see if I can find anything by trying that. And thinking back, the first thing James suggested was using vmmap with the summary flag. So let me give that a try, and let me pass in my Memgraph file. And let's take a look at this output. So now, what should I be looking for in here? Now, in general, I'm looking for really big numbers. I'm trying to figure out what's using all this memory, and the bigger numbers mean more memory use. Now, there's a number of columns here, and, you know, some of them are more important than others. First of all, virtual size, I mean, virtual means not real. I can almost practically ignore this column. It's memory that the app has requested but isn't necessarily using. Dirty sounds like something I definitely don't want in my app. I'd much rather my app be clean than dirty, so that's probably something I want smaller. And then, swapped, which, because this is iOS, is compressed, remembering back to both Kyle and James pointed out that it's the dirty size plus the compressed size that the operating system uses to determine how much memory my app is really using. So those are the two columns I really want to concentrate on, so let's look for some big numbers there. I can see immediately CG image jumps out. It has a very big dirty size and a very big swapped size. That's a giant red flag, but let's keep looking. I can see this is IOSurface has a pretty big dirty size but no swapped size. MALLOC LARGE has a big dirty size but a really small or smaller swapped size. And there's nothing else in here that's really that big. So I think, based on what I see here, I really want to concentrate on the CG image VM regions. So I'm going to go ahead and copy that. So what's the next step? Well, we want to know more about some virtual memory, so vmmap seems like the place to go again. This time, instead of the summary flag, I'm just going to pass my Memgraph file. But I really only care about the CG image memory. I don't care about all the other virtual memory regions that vmmap will tell me about. So I'm going to go ahead and use grep to just show me the lines that talk about CG image. So let's see what that looks like. So now, I have three lines. I see there's two virtual memory regions. There, and I see their start address and their end address. And then, I can see these are the same columns as above. This is virtual, resident, dirty, and compressed. And this last line here is actually the summary line again. So that's the same data that was above. Looking at my two regions, I can see I have a really small region and a really big region. That big region is clearly what I want to know more about. So how can I find out more about that particular VM region? Well, I went looking through the documentation from vmmap, and I noticed it has this verbose flag, which, as the name implies, outputs a lot more information. And I wonder what that can tell me. So let's go ahead and pass verbose and the Memgraph file. And again, I only care about CG image regions, so I want to use grep to filter to just those. Oh, now I see there's actually a lot more regions. What's going on here? Well, it turns out that vmmap, by default, if it finds contiguous regions, it collapses them together. And in fact, if you look starting on the second line here, the end address of this region is the same as the starting address of this one. And that pattern continues all the way down. So vmmap, by default, collapses those into a single region. But looking at the details here, I can see there are actually some differences. And in particular, some of these use a lot more dirty memory and some have a lot more compressed memory, so this gives me an idea of maybe something I want to focus on. But I'm actually going to use a different strategy here. I know that the operating system, not necessarily, but as a general rule, the later the VM region was created, the later in my app's life cycle it happened. And since this Memgraph was taken during this huge spike in memory use, chances are these later regions are more closely tied to whatever caused that spike. So instead of trying to find the one with the biggest dirty and compressed size, I'm going to go ahead and just start at the end here. I'm going to grab the beginning address of that final region. Now, where do I go from here? Well, one of the tools that James mentioned was heap, but that's about objects on the heap, and I'm dealing with a virtual memory region, so that doesn't help. Then, there's leaks, but leaks, I don't have a leak here. I already know from looking at the Memgraph there's no leaks, so that doesn't seem like the tool I want to use. But I went looking through the help information for leaks, and it turns out leaks can do lots of things and including telling me who has references to either an object on the heap or virtual memory region. So let's go ahead and see what that tells us. So I'm going to use leaks, and then I'm going to pass this traceTree flag. And what that does is it gives me a tree view of everything that has a reference to the address I'm passing in. In this case, I'm passing in the starting address of my virtual memory region. And then, finally, we give it the Memgraph file. So what does this look like? So what we see here is this tree of all these references. If we scroll up to the top, which is way up here, I can actually see here's my VM region, here's my CG image region, and then I can see there's a tree view here of all the things that have references, and what references them, and what references them, and so on and so forth. And in fact, if we go back to Xcode, and we actually filter on the same address, and I go ahead and look at this object, this tree is the exact same tree I see from leaks. And if I wanted to, I could go through and expand every single one of these nodes and look at the details for each of them, but that's going to take a while, and it's kind of tedious. The nice thing about the leaks output is not only can I kind of quickly scan through it, if I want to, I can search or filter it, or I can put it into a bug report or an email, which I can't really do with the graphical view that's in Xcode. So what am I looking for here in this output? Well, ideally, I would find something, a class that I'm responsible for, a class from my application. I happen to have looked through this already, and I know there's none of my classes in here, so what's the next best thing? Well, a class that I know I'm creating, like a framework class, that's either being created on my behalf or that I'm directly creating. So I know that, you know, my app has UI views. It has UI images. And I'm using these core image classes to do the filtering. And so if we go ahead and we look through here, and I'm using a very sophisticated debugging tool called my eyeballs. And we go ahead and we look for -- let me see if I can find what I want. It's a very big terminal output. Makes it a little more confusing. Well, so, for example, you know, here's a font reference, and I know, you know, my application uses fonts, but chances are the fonts aren't responsible for a lot of my memory use, so that's not going to help. If we go down further, I can actually see there's a number of these CI classes, and those are the core image filters, or that's core image, you know, classes it's creating to do the filtering work in my application. So that may be something I want to investigate further as well. I happen to have already done that and haven't found anything useful. So I can't really get anywhere further looking at the leaks output, which is unfortunate. So what should I go to next? Fortunately, James had memory-backed trace recording, allocation-backed trace recording turned on when he captured this Memgraph, which means I can use the other tool he talked about to look at the creation backtrace of my object. So I'm going to use malloc history. And this time, I pass it, the Memgraph file, first. And then, I'm going to pass it from the help documentation, this fullStacks flag. And what that does is it prints out each frame on its own line, makes it a lot more human readable. And then, I'm going to pass it the starting memory address of my VM region. Let's see what this looks like. Well, this actually is not that big of a backtrace, and I can see actually my code appears on, here on several lines. Lines 6 through 9 actually come straight from my application code, and I can see here on line 6 that my NoirFilter apply function is what is responsible for creating this particular VM region. So that's pretty good smoking gun as to where I want to look in my app for who's creating all this memory. And in fact, if we go back to the Memgraph file, I can actually see that's the same backtrace that appears in Xcode here. You can actually see right here is also the NoirFilter apply method. We don't get the nice highlighting you normally see in the backtrace view here because we're not debugging a live process. We're loading a Memgraph file. But you can see it's the exact same output as we get from malloc history. And in fact, to just kind of a confirm things even further, if I go ahead and I look at my full list of CG image VM regions and I collect, I grab the second one from the bottom, the next one up, and let's look at the backtrace for that one. And it turns out it's the same backtrace. So the same code path is responsible for that region as well. And in fact, looking at several of those regions, it actually uses the same backtrace. So now, I have a really good idea of what in my application is responsible for creating these VM regions that are using up a whole bunch of the memory in my application. So what can we do about it? Well, let's go back to Xcode, and I can go ahead and close the Memgraph file. And the first thing I want to do is let's take a look at the code here. If I look at my filter, I can see right here is the apply function, and I can actually see right away something jumps out at me, which is I'm using the UIGraphicsBegin ImageContext WithOptions and the UIGraphicsEnd ImageContext, which I remember Kyle said you shouldn't be using. There's a better API to use in those circumstances. So that's something I definitely want to come back to, but the first thing I need is I need some kind of baseline. I need to get an idea of how much memory my app is using so I can make sure my changes are making a difference. So I'm going to go ahead and run the application, and I'm going to go to the debug navigator and look at the memory report. So now, I can see the memory my app is using as I run it. Now, I really like this image of Saturn's north pole. It's this weird hexagon shape, which is both kind of cool and a little freaky. So let's take a look at that, and let's apply the filter and see what we get. So 1 gig, 3 gigs, 4 gigs, 6 gigs, 7 gigs. This is bad. And actually, this actually brings up a good point, which is this would not fly on a device at all. So when you're running in the simulator, you have to remember that it's useful for debugging and testing changes, but you need to validate all that stuff on devices as well. But the other thing that's nice is the simulator is never going to run out of memory. So if I have a case where my app is getting shut down on a device, maybe try it in the simulator. I could see what's, you know, I can wait for a really big allocation, not get shut down, and then investigate it from there. And one thing I would like to point out is actually we do show you the high-water mark over here. And in this case, I'm up to 7.7 gigabytes. It's terrible. So let's see what we can do about that. I'm going to go back to my apply function. And now, you know, I do want to come back to this beginImageContext WithOptions thing, but thinking back to what Kyle said, when you're dealing with images, what's the most important thing in terms of memory use? It's the image size, so let's take a look at what that looks like. I'm going to go ahead and apply the filter again. And then, once I'm stopped in the debugger, I want to go ahead and see the size of this image. And I'm actually just going to take a sip of water before I hit return here. Actually, I'm not going to have any water. This is 15,000 by 13,000. Now, I checked in the documentation. On UIImage that size, that's points, not pixels. If this is a 2X device or a 3X device, you have to multiply that by a big number. You know, Kyle was upset because an image was taking 10 megabytes. Nobody tell him about this. And in fact, just to confirm things, I want to try this. I'm going to do a, the 15,000 times 13,000, and the iPhone X is a 3X device, so it's 3 times the width times 3 times the height times 4 bytes per pixel. That number looks kind of familiar. So I'm pretty sure I know exactly what's using up my 7-and-a-half gigabytes of memory, and it's not necessarily my beginImageContext thing. It's the size of this image. And there's no reason the image needs to be this big. What I want to do is scale it down so it's the same dimensions as my view. And that way, it'll take up far less memory. So if I go back to the image loading code that's up here -- actually, before I do that, I want to go ahead and disable this break point -- so let's take a look at what this does. Well, it's pretty straightforward. It's getting the URL from a bundle, it's loading some data from that URL and loading it into UIImage, then that, then, which gets passed to the filter. So what I want to do is, before I send it to the filter, I want to scale down the image. However, I remember what Kyle said. I don't want to do the scaling on UIImage because it still ends up just loading that whole image into memory anyway, which is what I'm trying to avoid. So I'm going to go ahead and let's collapse this function. And I'm going to replace it with the code Kyle suggested. Okay, so let's take a look at what this code is doing. So here again, we're getting the image from the bundle, but now, this time -- just a little lighter -- I'm calling CGImageSource CreateWithURL to get a reference to the image and then passing that to CGImageSource CreateThumbnail AtIndex. So now, I can scale the image to the size I want without having to load the whole thing into memory. Let's give this a shot and see if it makes a difference. I'm going to go ahead and rebuild and wait for it to launch on the application. And then, once it's there -- oh, there's a warning. I have an extra this. Let's see. Okay, building. Building, building, building. Okay, launching. Good. All right. Now, let's go ahead and take a look at the memory report. Let's go back to Saturn's north pole, which is something I've always wanted to say. And let's apply our image and see what it goes to now. So now, we're at 75, 93 megabytes. Our high-water mark in this case is 93 megabytes. Significant improvement. [applause] Much better than the almost guaranteed to get shut down 7-and-a-half gigabytes. But now, I remember there's actually, I want to go back, and let's go ahead and stop. And I still want to go back to my filter method and change this UIBeginImageContext code and do what Kyle suggested here. So I'm going to go ahead and delete this code and add in my new filter. So now, in this case, I'm going ahead and creating a UIGraphics ImageRenderer. And I'm using my CI filter within the renderer to do the filter, apply the filter. Let's go ahead and run this -- hopefully, it'll build -- and see if this happens to make any difference in terms of my memory usage. So let's go back to the debug navigator and to the memory report. And once again, we get to go back to Saturn, and let's go ahead and apply our filter. Now, let's see what our high-water mark ends up this time. Ninety-eight. So that's actually the same, but it turns out that's, if you think about it, that's what I expect. My image is still going to be using, in this circumstance, 4 bytes per pixel, so I'm not actually getting any memory savings by using this new method. However, if there was an opportunity for memory savings -- for example, you know, if the operating system could determine that it could use fewer bytes per pixel or if it determined that it needed to use more, it would do the right thing, and I don't need to worry about it. So even though I don't see a big improvement, I know my code is still better for having made these changes. So there's still more I could do here, right. I want to make sure we unload the image when the app goes into the background, and I want to make sure we're not showing any images in views that aren't on screen. There's a lot more I can do here, but I'm really pleased with these results, and I want to send them back to James. So I'm going to go ahead and grab a screenshot and add a little note to it for James just to show him how pleased I am with all of this. And I think we're going to go ahead and send him the starry-eyed emoji. And hopefully James will be happy with these results. So now, I'm going to hand it back to Kyle, who's going to wrap things up for us. Thank you. Thanks, Kris. Thanks, Kris. That was awesome. With just a little bit of work, we were able to greatly reduce our memory use by orders of magnitude. So in summary, memory is finite and shared. The more we use, the less the system has for others to use it. We really need to be good citizens, and be mindful of our memory use, and only use what we need. When we're debugging, that memory report in Xcode is crucial. We can just turn it on when our app is running because then, as we monitor it, the more we can notice the regressions as we're debugging. We want to make sure that iOS picks our image formats for us. We can save 75% memory from SRGB to alpha 8 just by picking the or by using the new UIImage GraphicsRenderer APIs. It's really great for masks and text. Also, we want to use ImageIO when we downsample our images. It provides us or prevents a memory spike, and it's faster than trying to draw a UIImage into a smaller context. Finally, we want to unload large images and resources that are not on the screen. There's no sense in using that memory because the user can't see them. And even after all of that, we're still not done. As we just saw, using Memgraphs can help us further understand what's going on and reduce our memory footprint. That combined with malloc history gives us great insight into where our memory's going and what it's being used by. So what I'd recommend is everyone go out, turn on malloc history, profile your tool, and start digging in. So for more information, you can go to our slide presentation. And also, we'll be down in the technology labs shortly after this for a little bit if you have additional questions for us. Thanks, and have a great remainder of WWDC.  Thanks, Mike. Hey everyone. How many of you're interested in AR? All right. Cool. Now how many of you have an AR app in the App Store already? Okay. A few less. I get it. AR can be pretty intimidating. Well, today we're going to look at a few techniques that we use which will hopefully help you in two ways. First, they're going to hopefully help make AR more approachable and second, we're going to show you how you can save time and money by making sure you build the right things before you write a single line of code. So let's get started. If you've seen previous talks by my team here at WWDC, you may already be familiar with our revolutionary app Toast Modern, the best app for finding and rating the toast around you. I know it's amazing. Well, we've been building up our database of toast, and now we've been tasked with adding AR to this app. I'm sure you're all aware that the artisanal toast scene in the Bay Area it's pretty big right now. Well, today it's going to get a whole lot bigger. Introducing ToastAR, the world's first AR toast recognition and rating app. I know. Thank you. Thank you. Through the use of computer vision and AR, ToastAR lets you check the ratings on a piece of toast and get this, where it's from. Revolutionary I know. I mean, you're all developers for Apple platforms so you expect the best APIs and the best hardware. Why not also expect only the best toast too. So how did we prototype this beautiful, revolutionary app? Well, you may think we used a programming language like Swift or a framework like ARKit or maybe even a sophisticated application such as Unity. These are all valid ways to prototype, but they're not the only solution. Sometimes you can use low-tech physical prototyping techniques to work through your design problems before you write a single line of code. So today I'm going to show you how you can use just the camera app and the world around you to prototype for AR just like we did for ToastAR. So before we get started, there's two terms you should be familiar with. First world space. When we talk about world space, we're talking about design elements in the world around you and your device. And when we talk about screen space, we're talking about these elements on the screen of your device. So let's take a look at the original design we got for ToastAR and work through our process. In this design, we have a label in world space which shows us where the toast is from and its current rating. Now, you may want to jump in and start coding this immediately. It looks pretty simple, but instead we're going to save time and money by making sure we are building only the right things before we write a single line of code. AR is about the world around you. So testing out your designs is difficult through a single still or even video. Physical prototyping, however, allows us to do this in an approachable, low-tech and highly-iterative manner. So how did we get started here? Well, we printed out our labels and stuck them to some toothpicks. I know. I know. It may seem odd, but this low-tech process is going to hopefully help us in three ways. First, we're going to be testing our designs in context. Remember, AR is about the world around you. Second, we're going to hopefully figure out any issues with our design before we write a single line of code. And third, through this process, hopefully we'll get new ideas that will help make ToastAR even better. But who even has a printer these days, right? You can even just draw something and cut it out. So now that we have our labels attached to our toothpicks, well, we stick them in a piece of toast. Then, just using the camera app, we can test this design out in world space. I know. Revolutionary here, right? When we tried this, we learned one thing really quickly. These labels, at an angle, are pretty difficult to read. So the label is hard to read at an angle. Maybe we should have it always face the camera. So let's try this out. So with some help from a co-worker and some extremely sophisticated hardware, okay, it's a clear ruler but beautiful. We tested this out. And we found that yes, the label is easier to read when it's always facing the camera. Well, this behavior is commonly referred to as billboarding. And it seems like it'd be pretty easy to implement in SceneKit using an SCNBillboardConstraint. But before we start coding this, let's continue to test this design and make sure we're only building the right things. So how does the label work when there's a lot more text or when it's at a distance? Well, we can try this out pretty easily too. Again, with the camera app, we can test this out and we find that yes, it's a little difficult to read when these labels are at a distance. Maybe we should bring these labels on the screen. So how do we prototype this? Well, we're talking about screen space, and we want to do this quickly. So we're going to go ahead and use Keynote. First, we're going to record a video of our toast. We'll record a slow pan. And notice that we're going to pause on the toast to give our upcoming animation some time. Next, we're going to bring this into Keynote. So you'll create a custom document size which matches the device you're eventually going to put this on and then simply import your video and bring it into Keynote. Third, we're going to animate our labeling, import our design, add a move action, give it an appropriate duration, and move it to the desired destination. Now here's the important part. We're going to check the build order and make sure our video and our animation begin at the same time but we're giving the animation a delay so it's on screen when the toast is in the field of view. Finally, we're going to put this on a device and test it out in context. So we'll put it on a device using Keynote for iOS. And we'll test this out in context. Great. Looks like we could fit all the text we need on this label. And putting this on screen space will hopefully make it easier to expand the scope of this label at a future date. Now let's see how this scales, though. What happens when there's multiple pieces of toast in the field of view? So using the same techniques, we'll try this out too. Now the problem here is when there's multiple pieces of toast in the field of view, I have no idea which piece of toast I'm currently interacting with. Maybe I should try putting an indicator in world space so that I know which piece of toast I'm currently interacting with. Well, we can try this out too, quickly. So, again, with some help from our co-worker and our trusty ruler, we can try this out too. Print out our design, attach it to the ruler, and try this out. It's the camera app world space. This seems to help us. We know which piece of toast we're currently interacting with. So let's put it all together. Using the techniques we just reviewed, we can put together a rough demo of our prototype. And when we know we're building the right things, we now know we have some high-fidelity prototypes to base our code off of. In just a few hours, we worked through problems that could have taken us days to code through. So what did we learn? Well, we learned these labels in world space are difficult to read at an angle and at a distance, so we brought them on screen. But we also learned that we needed indicators in world space to let the user know what they're currently interacting with. Physical prototyping allowed us to test these AR experiences out quickly. We used the camera app to test our world space interactions. And we used Keynote to test out our screen space interactions. So how did prototyping help us? Well, we tested our ideas, and now we're going to be saving time and money by making sure we build the right things before we write a single line of code. We also got some new ideas along the process. These ideas helped make ToastAR even better. And take it from me, it's pretty amazing. Hopefully when you start working on your next project, you now have a few new tools when you tackle your next AR app. Thank you so much.  Ladies and gentlemen, please welcome to the 2018 Apple Design Awards. Please, welcome your host, Senior Director of Evangelism, John Geleynse. Well, hello. It is great to be back here with the Design Awards on the main stage. And this evening we've got a great evening for you. So, thank you for coming out and joining us in this afternoon and celebration of the winners that we're going to announce, soon. We're here to honor developers who have done excellent work and with technology adoption, with innovation, making their apps unique. And in my role at Apple and in my team's role at Apple we've worked with so many of you over the years, helping you to create great apps. Apps that are engaging and apps that are compelling. Apps that are high performance and apps that are, honestly, really great. And so, it's an honor for me to be here and to be able to lead this award ceremony and honor these developers, tonight. The work that is done by you, as the developer committee, speaks to the heart of Apple. And that heart is, as we all know, creativity. In all of its forms, we believe that creativity has always been humanity's greatest super power. And when it combines with technology great things come to life. Incredible things come to life. And that's why we feel deeply that you, as developers, are amazingly creative. Like a paintbrush to an artist, chords to a composer, words to an author, code is an even more advanced creative tool. Because it can really do and create so much. As developers, you're developing on the canvas of the entire world causing great new groundbreaking apps and ideas to take shape. And each year new and transformative apps are introduced bases on those ideas. And each time this happens you take creativity to new levels. For example, take a look at the last three years of Apple Design Award winners. There are incredible games here. Powerful design tools. New workflow solution, new video workflow solutions. Innovative photography tools. Apps that empower health and fitness. Music creation and DJing tools. And apps to better understand human anatomy. As well as everyday tools that help you to get things done and make a, truly make a difference in this world. The developers of these apps and so many more over the years have amazed us with their excellent work. Now, we all understand the incredible hard work that goes into the design and development of great apps like these and others. We're all too familiar with the long nights, long hours, technical hurdles, design challenges, and honestly, often, a really hard to find bugs. But that's why we're here tonight. To honor and celebrate that hard work. Today's winners set a new benchmark in creativity and they demonstrate the mark of brilliance that makes something truly great. Okay. We've got something to do here, tonight, which is to give out a bunch of awards. And we're going to do that in a very large room. So, if you hear your name called, if you hear your app's name called, you need to make it up here fast. Because we want to move this along and it's a big room. There are two sets of stairs here on either side of me. So, just head towards me, you'll find a set of stairs, you'll make it onto the stage, and we'll move on from there. So, let's get started. Identifying the audience for your app and developing something that's perfectly suited for them and their needs is really important. The developer of our first winner today built an app that's perfectly tuned for children and the way that they learn. Join me in congratulating Bandimal by Yatatoy. All right. So, Bandimal is a fun and intuitive music composer app. It lets kids explore music and be the maestro of their own animal band. And the key to this app is its focus on creativity. There are no rules, no notes, no chords, no scores. Let's take a look. Andrea. Thanks, John. Let's dive right in. I'll tap on the plus here to start a new composition. And the music gets started right away, which we should hear, soon. There we go. But we're going to need to add some notes. The app has this adorable cast of animal characters. Each one makes a different sound. I'm going to swipe through them till I find one that I like, which I guess, I'm going to go with this buffalo, here. I'll tap down here with my finger to add some notes. And you can see each note comes with its own special dance move. Now, I'm going to speed up the tempo by turning this dial. And, maybe, after I do that I decide I want to give my buffalo a nice rest every other beat. There we go. Now, I need a nice lead singer to complement my buffalo bass, so I'm going to tap here, swipe through my options. There we go. This chicken looks like she'll be a star. Tap down here. And I'm going to speed up her tempo to match my buffalo bass. And, maybe, I'll do a call and response type thing so they can interact with each other. Now, I need one more animal to spice things up. So, I'm going to tap here. I happen to know there's a particularly funky jellyfish here. So, let's give this one a go. Now, let's listen. Sounds good. So, as kids create these compositions they're building their sense of rhythm and musicality, very playfully and naturally, without need for any prior musical knowledge. And the beauty of the app is any combination of notes is going to sound good, because it's all playing in pentatonic scale. So, there's always a feeling of success. Let's check out this percussion section, up here. So, I'm going to add some high hat there. Layer in some snare drum and some more percussion, here. And lastly, my bass drum. All of these sounds, percussion and animals, were recorded in the studio using real instruments to get the music and natural sounds. I can even add a sound effect by tapping on these mountains. That's going to give my chicken an echo. So, let's check out this reverberating chicken. So, you can see there's all sorts of fun to be had with sound in this app. And it's a great way for kids to get creative with music and feel their ear and their intuition for the rhythms and sounds that they like to hear together. The Yatatoy team of three spent two years developing this app using Swift, Sprite Kit, and Core Audio. They were working remotely between Helsinki and Vienna. And they were doing this in the afterhours from their day jobs. It's all culminated in an app that's really magical. So, congratulations. Thanks, Andrea. That's Bandimal. All right. Next up in an app developed by a team of two biologists; a physicist, and a chemist. Both are long time Indie Mac developers and one of them is, actually, a four-time Apple Design Award winner. These guys teamed up to develop our next app, which is Agenda by Momenta B.V. out of the Netherlands. So, Agenda takes a clever new approach to note taking on the Mac and now, on iOS, as well. By combining notes, calendars, and tasks in unique ways and with really powerful relationships. And as well, it's got a great clean design. And we'd like to show it to you, now. So, I'm going to hand it over to Chris. Agenda is a simple and fresh way to keep yourself organized. It combines a To-Do list and a note taking app with a delightful calendar integration. Let me show you what I mean. We start out with On the Agenda. This is a task manager like experience. I can quickly see my highest priority items and what I need to do with all my prior notes attached. I can mark things off as I complete them. So, now that we're done with Keynote, let's get that off. And State of the Union. Actually, in the midst of a house renovation, so I need to remember to call the contractor. It's super convenient to see all my prior notes attached before I make that phone call. What makes this app really great is its integration with Calendar. So, on the right-hand side here, you can see all my Calendar events. There's a HealthKit lab tomorrow and I need to remember to note down a couple of questions. So, we'll do that together. So, I bring in this note. You can see it brings in all the details from Calendar. So, the event details, the location, the attendees. So, I can focus on creating the note. So, as I create this note, you can see the app supports markdown. It also supports tagging, which is really nice. So, I'm going to tag this as a HealthKit related note. As I'm making this note, I realize a colleague and I had a meeting a couple weeks back, where we brainstormed some HealthKit related questions. So, I'm going to swing over to Calendar and find that meeting. This meeting with John. Agenda provides this link in the Calendar in my, so as I click on this it clicks me right back into this meeting with John. And here are the questions that I wanted. So, you can imagine after using this for a couple weeks, you end up with this really great integration with Calendar and Agenda. And it keeps you super organized. Lastly, because I know I'm going to be doing a lot of HealthKit talking this week, I'm going to search for all my HealthKit related events. And I can save this as an overview. And what this does, it creates this nice little pocket here, where everything HealthKit related is in one spot. Super convenient. The entire app is created in Swift and it leverages iCloud, so it syncs across all of my devices. Agenda is not simply a place to store text. Rather, a tool that gently reinforces great note taking habits. Agenda is, also, available on iOS and makes a great addition to your productivity arsenal. Thank you. Congratulations. Thanks, Chris. Great apps always leave a strong impression, and then reinforce it again and again, every time that we use them. With amazing visuals, excellent onboarding, a relaxing soundtrack, and great gameplay, join me in welcoming Alto's Odyssey as your next winner. This is an app that we fell in love with, just like we did with its predecessor. Snowman and Team Alto have done an incredible job of taking what people love from Alto's Adventure and improving on it in every way for Odyssey. And I'd like to ask my colleagues, Adrienne and Jordan, to show it to us, now. Thanks, John. Alto's Odyssey is an endless sandboarding game, where you ride through sunswept dunes, perform midair stunts, all while completing fun challenges and unlocking new characters. So, my colleague Adrienne will be joining us for the demo, today. So, let's get started. The first thing that we loved about Alto's Odyssey was that it provides the user with contextual clues and elegantly pauses the game, so the player learns by playing. So, here she's getting taught how to perform a wall ride, which is a new feature in the game. There are no carousels that she's swiping through. It's just her and the game. We, also, love that it maintains that same simple control scheme for Alto's Adventure but ramps up the difficulty with new challenges. Control-wise, all Adrienne is doing is tapping to jump, and then tapping and holding to perform some epic flips. Now, Alto's Odyssey is a real feast for the senses. Visually, there's so much to look at in this game. There is elements in the foreground, in the background like hot air balloons, sand coming off the back of the board. But each level has such a striking color palette to it that we fell in love with. That same attention to detail, also, carries over into the audio, as well. So, actually, let's take a listen to that, right now. Ah. Again, we love this. And you can, actually, listen to it, too, because it's available on iTunes and Apple Music. We, also, fell in love with how it uses the haptic engine on our devices. So, Adrienne is getting a slight vibration on the iPhone when she's, when she's landing a jump, performing a wall ride, or even, bouncing off of hot air balloons in the game. It, also, maintains a super smooth 60 frames per second on every device, including previous generation devices. In large part, thanks to Metal 2 and Unity. And it even maintains that same simple control scheme across iPhone, iPad, and Apple TV. So, Alto's Odyssey is localized into an incredible 20 languages, has over 2,000 4.5-star ratings since launching just over four months, ago. And is available only on the iPhone, iPad, and Apple TV. So, again, congratulations to the team. And back to you, John. Great going, guys. So, great. Now, talking about localization, localizing your app so that it's available to people around the world in their language of choice is really important. And it's something that we talk about a lot, here at Apple. And we encourage you to do. But making an app that helps people converse with others whose language they don't speak is a big deal. Our next winner is an app that's been downloaded more than 80 million times. Join me in congratulating iTranslate Converse, by iTranslate. So, founded in 2009, iTranslate is an Austrian company based in the city of Graz, famous for being the birthplace of Arnold Schwarzenegger. But it's now, also, famous for being the birthplace of this app. What sets this app apart is its support for 38 languages and its simple UI that enables everyday conversations between people. Let's take a look at how this works. Paulo, Carol. Thank you, John. iTranslate Converse is available only on iOS and was completely developed using Swift. Thanks to its straightforward design and intuitive user interface the app is super easy to use. All you need to do is swipe up and then, pick two out of the 38 languages available. Ah, German. What would Arnold say to you, today? [German language] Ich komme wieder. I'll be back. But for the purpose of this demo, we're going to pick Chinese Mandarin. Now, as you can see, the whole screen of the app turns into a tappable area. But the developer went one step further by implementing 3D touch and haptic feedback. So, you can use the app without even looking at the screen. Now, let's imaging I have just landed in Beijing and I'm meeting my colleague, Carol. And I'm super hungry but I don't speak a single word of Chinese. So, Carol, do you know where to have spaghetti? [ Chinese Language ] Why do you want to eat spaghetti in Beijing? Because, I'm, oops, sorry. Because, I am Italian. [ Chinese Language ] Actually, the spaghetti was invented in China. [ Chinese Language ] Let's go and try the authentic Chinese food. Want to eat roast duck? That sounds delicious. [ Chinese Language ] So, iTranslate Converse is, also, available on Apple Watch, where users can access the app via complication. On Apple Watch Series 3 you can use the app even without your iPhone. Now, let me finish by saying something to you in a more familiar language. [ Foreign Language ] Remember to install this app before going abroad. [ Foreign Language ] Thank you and have fun at the Developers Conference. Back to you, John. Good night. Well done. That's awesome. That's an app that can break down barriers and help us build meaningful relationships with people we, with whom we don't speak their language. Now, when we judge apps for the Apple Design Awards we're looking for apps that are incredibly well designed, that are innovative, and they're optimized for our devices. Next up is as wonderful puzzle-adventure platformer for iPhone, iPad, and Apple TV that's all of these things and more. Please, join me in congratulating INSIDE by Playdead. Developed by Playdead in Copenhagen, INSIDE is a beautiful and frightening adventure game with almost 10,000 4.5-star reviews on the App Store. Unbelievable. And to show it to you, now, I'd like to ask Mark and Adrienne. Thanks, John. INSIDE is an eerie adventure filled with intrigue and danger. You play as an unnamed boy as he struggled through a dystopian world. By solving puzzles, you uncover an alarming environment of lifeless bodies and their violent controllers. But buried under this dark premise is a brilliantly designed game. A worthy successor to Playdead's first game, Limbo, INSIDE also uses a monochromatic palette, which places your focus solely on the details of the environment and the animation. Check it out as he actually pulls and struggles to get these boards off the wall. So, cool. INSIDE's chilling soundtrack reinforces its sinister atmosphere. Playdead's audio designers, actually, used a contact microphone on a human skull to create many of the sound effects that you hear in the game. Let's listen in. [ Scraping Sound ] [ Bang Sound ] [ Running Footsteps ] [ Bang Sound ] [ Running Footsteps ] [ Bang Sound ] [ Running Footsteps ] With no onboarding at the beginning and no loading screen in the middle, INSIDE is both intuitive and immersive. Using Unity, Metal, and a custom temporal reprojection filter, Playdead's crafted this distinctly surreal environment and console quality graphics. They actually fully support HDR on both the iPhone 10 and the iPad. So, that makes dark scenes like this seem really crisp and clear. Now, light plays an important role on INSIDE, and roving spotlights are, often, your enemies. But, in this case, Adrienne has to, actually, turn the lights on in order to power that elevator at the end of the hallway. The shadows cast by the spotlights provide some respite, but you're never safe for long. iCloud, actually, allows you to save your progress across devices, so you can pick back up where you left off, whether you're playing on the iPad, iPhone, or on Apple TV. Now, I don't want to spoil too much of the game for you, but there's one more scene I want to show. And fair warning; it's a little intense. Let's check it out. [ Heavy Breathing And Running Footsteps ] [ Splashing Sounds ] [ Running Footsteps ] [ Eerie Sounds ] [ Running Footsteps ] [ Gunshots ] [ Dogs Barking In Distance ] [ Dogs Barking Closer ] [ Splash ] INSIDE is a beautifully designed and brilliant adventure game and a worthy winner of an Apple Design Award. Congratulations. Well done. Well done. Such a great game. Now, you know when you have a favorite movie or a favorite book you just want to watch or read, again and again? Next up is an app that's like that. But you get to play the main character and experience the world through her eyes. Our next winner is Florence by Mountains. So, Florence is a beautiful cinematic story in which you get to experience the impact of decisions and emotions of adult life and first love. Let's take a look. Lauren. Thanks, John. I really enjoy playing Florence, because it is just so compelling and approachable. And it's really personal, because you hold the story of Florence in your hand and use really familiar controls to play. Through beautiful illustrations, soundtracks, and haptics, you experience life as Florence. In this scene it's all about adult life. And this game helps you realize that we're all just trying to get through our day, to get back to the things that we really care about. Another part of adult life, as well; getting calls from Mom. And this part of the game was just too real for me. It's like, ''Hi, Mom. No, I don't need any help finding a boyfriend. And yes, work is going just fine.'' So, with the help of TC, who's doing a great job demoing today, we're going to take a look at another scene. Florence has a custom composed soundtrack and it's incredible. Music plays a huge part throughout the entire experience. And this is when you hear music and you decide to tap the notes to follow it. And you know how music can make you feel, how it can just lift you up. Well, it also can lead to a really great story. Through beautiful fades, transitions, and soundtrack, this plays just like a great movie. And the camera holds on moments that mean just a little bit more. And this well-designed subtlety, well, it makes you want to be more present in your own life and in your own relationships. So, how does it turn out with this other character? Well, you're just going to have to play the game to find out. There's a reason why people have a first love story. Because, it's powerful, emotional, and it has an impact. And if you haven't experienced it yet, well, this game is a wonderful preview to the real, honest, heart racing, and heart wrenching roller coaster that is first love. So, go for it. Play the game of first love. It's an incredible unforgettable ride. Congratulations, Florence. Thanks, Lauren. State of the art apps support the latest devices, the latest OS's, an integrate with our latest technologies. Our next winner is a state of the art and beautifully crafted smart calculator app called Calzy from India. So, this is Raja. Calzy was created in an effort to rethink the conventional calculator. It's an incredibly customizable really well-designed app. And it uses a ton of iOS technologies. It's available for iPhone, iPad, Apple Watch, Messages, and even as a Today widget. I'd like to ask my colleague, Tyler, to show it to you, now. Thanks, John. Calzy has a fantastic formula for innovation, combining great design with the clever use of iOS technologies. Let's have a look. There's Calzy, sweet as pi. As you can see, right there, it's got a simple yet elegant layout. And have a look at this, great minds think alike. So, Calzy does everything you'd expect a calculator to do. This single screen app is packed with features revealed by using 3D Touch and haptic feedback for these sliding menus. Perfect for singlehanded use. And if that single hand happens to be your left hand the keyboard is fully adjustable. You can have different keyboard layouts for the main app, as well as the widget and the iMessage's app for maximal flexibility. The standard layout, as you see here. Calzy, also, of course, supports the scientific layout, no rotation necessary. Now, I'm not really a math person, so these are all just kind of Greek to me. But speaking of science, I was recently reading an article. At least, John smiled at that. I was recently reading an article about a very ambitious effort to quantify the weight of all the biomass on earth. And so, I went to the study and I grabbed some of the numbers and I put them here in Calzy's Bookmarks function, which is a really handy way to store information. Here, we can see this is the sum of the biomass of all the taxa on the earth. And it's rendered in gigatons of carbon, of course. Anyway, the Bookmark stores both the expression and the value. So, if we find a new form of life I can always update the expression. And then, the values themselves can, actually, be stored up here in the memory area at the top of the screen, waiting patiently for me to drag them into another calculation. So, you're probably wondering, or at least I know in am, what's the prevalence of bacteria versus people on the earth. So, we go back in here to the Bookmark. I know you were wondering it. And you take a look, here, the weight of all bacteria on earth is 70 gigatons. I'm going to put that in there. And then, we're going to divide that by the weight of all the people on the earth, which is a paltry .06 gigatons. Now, if you notice that you've made a mistake, all you have to do is go into the expression, 3D Touch on the operator, put the correct one in. And this way, you can see that the weight of tall bacteria is about 1,200 times greater than the weight of all people. That's a lot of bacteria. So, wash your hands before dinner tonight, folks. There's a lot more great things Calzy can do that we don't have a whole lot of time for. But it's got support for things like Handoff and iCloud Sync, even Face ID, Touch ID protection for the bookmarks, Drag and Drop. Whole bunch of great things. Go ahead and check it out in the App Store. But before we go, a quick note about the inspirational story of the developer behind it. Gentleman from a small village in India working as a visual effects artist who decided he wanted to harness his creativity in a different way. So, he taught himself Swift, as you do. And within just a few months, was creating great iPhone apps, including Calzy. Congratulations, Raja. All right. All right. Great games are memorable. They're unique. They're artistic. They're filled with personality. And they have great performance. Our next winner has all those characteristics. Join me in congratulating Frost by Kunabi Brothers. Let's go right here. This is, actually, Kunabi Brothers' second time on stage, because another game of theirs, called Blek, won an Apple Design Award in 2014. They've now returned with a beautiful and calming freeform puzzle game that offers a deeply immersive space where players can interact with and experience the natural phenomenon of swarms. You've got to see this. Mike Stern. Thanks, John. So, swarming behavior has inspired scientists and artists through the ages. And now, it's inspired a really gorgeous and inventive game. Right from the level selection screen we can see this number one is comprised of tiny little light creatures swarming about. And as we switch between levels we can see them scatter and reform giving us a preview of what's to come. Now, when we get into the game we see this stream of blue creatures enter the gameboard. And our objective, here, is to help these little guys find their way back to their home planets. Now, Frost was designed for Touch and it really shows. Interactions are so simple and intuitive and natural. And just behold how graceful and beautiful the swarm is, just with a little bit of interaction. It's really pretty. Now, it seems here that the objective it to try to take some of the blue creatures to one planet and some to the other. So, to start over I simply tap anywhere on the screen. And using multitouch I can draw two paths to each planet. Now, when those yellow rings fill, that's when I know how many creatures each planet has to hold. Little bit more. And once it fills all the way, this really beautiful animation plays to let me know that the level is complete. And on iPhone, they use haptics to punctuate that animation and other special moments. Now, every level in Frost is a work of art. But I want to skip ahead here to a more advanced level and go from number 4 to number 24. Now, as we progress through the game we're introduced to new creative challenges along the way. But also, new species with different types of swarming behaviors. So, on this level we have yellow creatures and blue and red. And it seems, here, that my objective is to help the yellow creatures get to the yellow planet. So, I just make a beeline from one place to another, but they're not responding. Blue ones don't really do anything, either. Maybe, if I bring the red ones over here it'll do something. Oh, we can see those blue creatures really scattered. Now, what I want to do is just pause here, for a second, and just kind of enjoy this moment that Frost creates for us. The mood that it sets is so ethereal, it's so soothing and so tranquil. Watching these creatures move around is really mesmerizing and kind of hypnotic. A lot of players will, actually, just kind of stop and enjoy this experience and experiment and observe how the creatures respond to touch, and really soak it in. But, not me. I like winning. So, I'm going to try to get the red creatures to move around, pick up the blue creatures who might give a lift to the yellow creatures. And help them find their way home. It's a crazy enough plan that I think it just might work. Now, Frost offers hours and hours of gameplay. And the sheer beauty of watching thousands of individually animating creatures move around your iPhone or iPad is really breathtaking. Now, in order to power this real time simulation, they needed Metal on iOS. It's the only consumer mobile device which has the computational power needed to render these complex programmatic animations. But the thing that I love most about Frost is how anyone can play this game, regardless of their age or where they live or what language they speak. It can be a delightful, calming, and engaging experience. This is a really beautiful game. Congratulations. Thanks, Mike. You had most of us mesmerized, there. All right. Earlier, I spoke of code and creativity, which when combined with technology can produce these transformative results. Our next winner is an app that is disrupting and uniquely changing things, how things are done. Congratulations to Triton Sponge. So, the developer here, Gauss, is reinventing surgical care using machine learning and computer vision on iOS. And you've got to see this to believe it. So, let's take a look. Here's Chris and Divia from our Health Team to show you how it works. We love when teams tackle huge problems in unique ways. Especially, in healthcare. Today, many clinicians have no idea how to estimate how much blood patients are losing in procedures. As a result, a lot of clinical decisions are made based on guesstimation, which can lead to all sorts of complications. Gauss Surgical is solving this problem using an iPad, machine learning, and computer vision. Before we show you a demo, I'd like to introduce you to Chris and the tools we'll be using. We have an iPad Pro with an IR sensor, and some surgical sponges that are used in operating rooms to soak up blood. Now, for the purposes of this demo, we're going to mimic some fake blood by using some red food coloring. But if you start to feel queasy, at all, please avert your eyes for just a moment. Gauss' app is super simple to use. All you have to do is hold up a sponge in front of the camera. From there, Gauss uses Core ML to detect the sponge using the camera and IR sensor. They've then developed some custom machine learning algorithms to analyze the red spots of blood and, actually, calculate blood loss. It's amazing. Gauss' app is FDA approved and HIPAA compliant. But now, let's see if it's hippo compliant. As you can imagine. As you can imagine, it's incredibly important in an operating room to make sure you don't accidentally double count a sponge. So, if Chris hold up the hippo sponge one more time, even if in a different orientation, Gauss' app is able to detect that it's a duplicate and, actually, alert Chris to the duplication. That is amazing. Gauss' app is being used in hospitals all across the country in labor and delivery use cases. And we couldn't be more excited about their work. Congrats to the entire Gauss Team. Awesome. Well done, guys. That's pretty cool. And our final award for this evening goes to a brilliantly executed platformer. It's got a ton of personality, has great performance, a fun soundtrack, and it's super addictive. And it took four years to make. Join me in celebrating our final winner, Oddmar by Mobge. Oddmar was developed by a team, a small team of four, two of whom are here, at a university in Turkey, which goes to show that great games don't always come from the places that you would expect. We've had a blast with this game and we think you will, too. We'd like to show it to you, now. Begum. Thank you, John. Oddmar is a gorgeous action-adventure platformer that is beautiful, charming, and humorous. Let us introduce you to Oddmar and our not so successful Viking hero on the journey to prove himself worthy of Valhalla. Adrienne. The onboarding is smooth and the controls are easy to use gesture controls. Simply, swipe left and right with your left hand to move, and up and down with your right hand to jump. Oddmar with its 24 handcrafted artistic levels in a Norse mythology storyline is full of secrets and challenges. As you travel through the adventures of Oddmar, from magical forests to treacherous mines, beautifully animated comic style cuts scenes between the levels will help you find your way to Valhalla. The game really shines with its visual elements, with beautiful scenic multiplane landscapes, lighting, and animation that almost brings it to life. Oddmar, our Viking hero, is so brilliantly animated that it puts a smile on your face, whether he's jumping on mushrooms, running through the rivers, or as he's simply waiting for you to play. New game mechanics and controls are introduced for the player at every level. And as the players progress through the levels, unique dangers and boss fights challenges the players in new and fun ways. The game really does a fantastic job of bringing excitement with every new character, control, and level, while introducing these at a pace that allows the players to master it. The soundtrack influenced by the traditional Swedish folk music really gives the game its Nordic Viking setting at every level. It was recorded at a studio in a small village deep in the forests of southern Sweden. So, every aspect of the game, from level design to story to music was made with great effort, dedication to detail, and craft. As visually stunning as it is with the level of detail and artwork, Oddmar calls for a larger screenplay and we think you will love it on iPad Pro. It supports the use of game controllers, allows you to play and sync on multiple devices by saving on iCloud. Oddmar benefits from using Metal to create this visually stunning and fluid gameplay experience. It uses only [inaudible] resources to reduce the initial time to load by letting players download the levels in the background and has fun, colorful image stickers to share with your friends. Oddmar is localized into 12 languages and is only available on iPhone and iPad. Well done to the Oddmar Team. Forward, Valhalla! Back to you, John. Well done. So, that's Oddmar. So, these are our ten winners. Five incredible apps, five incredible games. Now, each winner receives the coveted Apple Design Award trophy, which they've all been handed and are walking away with, right now. This trophy is a solid piece of aluminum that lights up when touched. So, the Apple logo on top lights up just like the top of your laptop. It's very cool and it's a must-have. It's definitely something to collect and something to be super proud of. Winners today, also, receive a prize package that includes almost one of everything that we make in its best configuration. So, a 5K iMac Pro, a 15-inch MacBook Pro, a 256 GB iPhone 10, a 512 GB iPad Pro with Apple Pencil, a 4K Apple TV, an Apple Watch Series 3, and a set of AirPods. Not a bad haul. Not a bad haul. So, congratulations to all of our winners, tonight. And for the great work that they've done in the past year or years, in some cases. These ten apps set new standards for innovation, design, brilliance, execution, performance, creativity, craftsmanship, optimization, technology integration, and so much more. I want to thank you for joining us, today. It's been a long day, but we appreciate your attendance. This is a special moment for all of our winners. But it's, also, a special moment for you, their peers, and for our entire development community, many of whom are watching right now on the live stream. So, thanks for joining us, tonight. We're excited to see what you'll create, what you will create next year inspired by these winners. Check out their apps. See what they've done. Learn from them. They've done an incredible job. As well as being inspired by all of the new things that we announced, today. Thanks for coming. Good night.  Hi. I'm Conor Hughes from the iOS Accessibility team and I'm here to talk about making your drags and drops in iOS accessible. Let's get started. Let's briefly go over what we're going to cover. We'll start with a very brief refresher of the basics of Drag and Drop. Then, we'll go into come concepts behind the Accessibility Drag and Drop API. And then, we'll go into some examples, first, exposing an ancestor's drag from an accessibility element, and then exposing multiple drops from an Accessibility element. To recap Drag and Drop at a high level, Drag and Drop is a technology introduced with iOS 11 and provides powerful ways for someone using your application to transfer data both within the application and between applications. Drag and Drop introduced the concept of interactions, which are hosted by views in your application. To start a drag from a view, you add a UIDrag interaction object to it. To accept a drop, you add a UIDrop interaction. If you're not familiar with these classes you should watch the Introducing Drag and Drop session from WWDC2017 for more information, and then come back to this video. Now, let's start to talk about accessibility. Of course, you want to expose the power of your Drag and Drop interactions to people using your app through an assistive technology like Voiceover. There are a few accessibility problems the accessibility Drag and Drop API's were designed to solve. First, your accessibility element might not host interactions directly. For instance, your accessibility element might be a view whose subviews host interactions. Or your element might descend from a view that hosts interactions that are logically associated with it. The solution is that the Accessibility Drag and Drop API's allow you to specify logical drags and drops to the system in a way that makes them usable with assistive technologies. Let's get into those API's now. The UIAccessibilityDragging informal protocol defines two properties that you use to express drags and drops to Accessibility. It deals with the concept of Drag Sources and Drop Points. Drag Sources describe where to start drags that are associated with an element and Drop Points describe places associated with an element where items can be dropped. To use these properties, you can assign to them or override the getter to provide data on demand. These drags and drops are exposed to users of assistive technologies in much the same way as Accessibility custom actions are. So, that's the API, there. Often, drags and drops are exposed automatically to Accessibility by the system. And if they're found automatically they're assigned a default name but note that only interactions in an element subtree are automatically exposed. The UIAccessibilityDragging protocol allows you to expose exactly the interactions that make sense, wherever they may actually live in your View hierarchy. Allows you to specify a great name for each one. So, you should really consider implementing it to provide the best experience for someone using your application through an assistive technology. Now, let's look at an example of how you can expose drags with the UIAccessibilityDragging informal protocol. Let's say we're building a bar graph and we've chosen to draw the bars by configuring CALayer objects. And we're going to try and enable dragging of the bars themselves, to drag the bar data. So, here is an example of how that might look. And again, each of these bars is going to be a CALayer object. How would we go about implementing drag for this? It might look something like this. Note that because layers aren't views they can't host interactions. So, the bar graph itself has the interaction installed. And in this itemsForBeginning session callback, depending on the bar that is hit tested, we return different data in the drag item. Note that even though I only have one drag interaction I am using it to support multiple logical drags. This is a totally supported approach and it is an example of how powerful, in general, the base Drag and Drop API's are. Now, how do you make this accessible? A good approach to making a graph like this accessible is to expose each bar as an Accessibility element, which I've done here. Note that what I'm doing here is providing data about the logical structure of my app's content to the Accessibility Runtime. That enables assistive technologies to expose the content in a different way. We'll need to do the same thing to make the drags accessible, and to do so we'll need to learn some more specifics about the UIAccessibilityDragging informal protocol. First, let's learn about the class UIAccessibility LocationDescriptor. This class is data that describes where an interaction is by specifying a point in a view with a particular name. So, for an example in our bar graph, a location descriptor for dragging that bar there, might look like this. Its name might be Drag Bar Data. The view would be the bar graph view itself. And the point would be a point that's located within the bar. So, what do we do with UIAccessibility LocationDescriptor options? Well, we assign them to the two properties mentioned earlier. The two properties in the protocol are accessibilityDrag SourceDescriptors and accessibilityDrop PointDescriptors. DragSourceDescriptors is used to expose drag sources that are logically associated with an element and DropPointDescriptors exposes places to drop that are logically associated with this element. These are the properties you use to describe to Accessibility what can be dragged from and dropped into a given element. One important point to be aware of is that the descriptors must reference the View that actually has the interaction. So, for instance, you shouldn't reference a point in your window that happens to be within the View that hosts the interaction. You should specify that view itself. So, pulling it together; to expose a single drag from some element doesn't take that much code, at all. You make a Descriptor that specifies that drag by specifying a point in the View that hosts the interaction and giving it a name. And then, you set the accessibilityDrag SourceDescriptors property element to an array that includes that descriptor. Let's go back to our bar graph code that set the Accessibility elements of the graph view. What do we need to do to expose dragging each element? We compute a point within the bar represented by that element in the View's coordinate space, make an appropriate descriptor with that point in that View. And set the accessibilityDrag SourceDescriptors property of the element. And that's it. Now, let's look at a slightly different example. Let's try to expose multiple drops from a single element. This might be useful for a situation like a contact card, where the card is small enough that we're going to expose it as one element to Accessibility. But there are multiple wells into which we can drop content into the card. So, for instance, it might have a portrait and a space for you to drop arbitrary attachments for that contact. Now, that we know how to use the UIAccessibilityDragging informal protocol, exposing these is straightforward. We can override the accessibilityDrop PointDescriptors getter of the View, compute the midpoints of the two wells, and then return two location descriptors that name each drop point. So, in summary, because you want everyone to be able to use all the cool features you build into your app, you want to make sure that the drags and drops associated with Accessibility elements in your app are exposed to assistive technologies. To do so, use the accessibilityDrag SourceDescriptors property to specify what drag should be exposed, and the accessibilityDrop PointDescriptors property to specify what drop should be exposed. Finally, these properties are just arrays of UIAccessibility LocationDescriptor objects, which describe to Accessibility where these drags and drops are in your application and how to activate them. For more information, you can check out this video online. And thanks for watching. Hi, everybody. My name is Sean James and I'm a GPU Software Engineer. Today, I'm going to talk about Ray Tracing. So, you may have seen our Ray Tracing demo in the State of the Union and want to learn more. Or you may be interested in using Ray Tracing in your own apps. Today, I'll talk about how you can use Ray Tracing in your applications and how you can accelerate it on the GPU using METAL. Specifically, using METAL Performance Shaders. METAL Performance Shaders is a collection of GPU Compute Primitives optimized for all of our iOS and macOS devices. MPS includes support for image processing, linear algebra, and machine learning. We've talk extensively about these topics in previous sessions. This year, we've also added support for training. And there's an entire session about this topic tomorrow. For today, I'll talk about the new support we've added this year for Ray Tracing. So first, what is Ray Tracing? Ray Tracing applications are based on following the paths that Rays take as they interact with the scene. Rays can model light, sound, or other forms of energy. So, Ray Tracing has applications in rendering, audio, and physics simulation. But rays can, also, represent more abstract concepts, like whether or not one point is visible from another. So, Ray Tracing also has applications in collision detection, artificial intelligence, and pathfinding. For today, though, I'll focus on rendering as an example of how you can use Ray Tracing in your applications. So, you may be familiar with the rasterization pipeline. The rasterizer works by projecting one triangle at a time onto the screen and shading the corresponding pixels. This can be implemented very quickly in GPU hardware, which makes this the method of choice for games and other real time applications. But the rasterization model makes it difficult to simulate certain physical behaviors of light. One example is reflections. With the rasterizer, reflections are, typically, implemented using approximations, such as cube maps and screen space reflections. But with the Ray Tracer we can compute accurate reflections, directly. Another example is shadows. With the rasterizer, shadows are typically implemented using shadow maps. But these can be tricky to implement without suffering from aliasing and resolution issues. Furthermore, soft shadow mapping techniques tend to produce uniformly soft shadows. With the Ray Tracer we can directly compute whether or not a point if in shadow. So, we can produce clean shadows, including realistic transitions from hard to soft shadows as the distance between objects increases. One final example is global elimination. This simulates light bouncing of off surfaces in the scene. Global elimination can be very difficult to model with the rasterizer. But it's actually modeled quite naturally with the Ray Tracer. In fact, many games and real time applications that include a global elimination component, actually, precompute it using a Ray Tracer and store the result into textures. Which are then mapped back onto the geometry at run time. Of course, there are many other effects that we can simulate with the Ray Tracer, such as ambient occlusion, refraction, and area light sources. As well as camera effects, such as depth of field and motion blur. This makes Ray Tracing the method of choice for photorealistic offline rendering applications. The tradeoff is that Ray Tracing is significantly more computationally expensive than rasterization because we're doing so much more work to simulate these effects. So, let's first take a closer look at how rendering works with the Ray Tracer, and then we'll see how we can accelerate is with METAL. So, we use an algorithm called Path Tracing. In the real world, photons are emitted from light sources and they bounce around until they enter the camera or your eye. But most of those photons actually never make it to the camera. So, this would be very inefficient to simulate. Fortunately, due to properties of light we can, actually, work backwards, starting from the camera. We start by casting primary rays from the camera into the scene. We then compute shading at the intersection points. Shading consists of figuring out how much light is arriving at the shading point and what fraction of that light is reflected back towards the camera. There are, actually, two sources of light, which we'll handle separately. The first source is direct light. This is light that arrives directly at the shading point from the light source. We can easily compute how much light is arriving directly and what fraction of that light is reflected back towards the camera. All we need to do is check that the shading point is not, actually, in shadow, before adding the lighting to the image. To do this, we can cast additional shadow rays from the shading point towards the light source. If the shadow ray doesn't make it all the way to the light source, then the original shading point was in shadow. So, we shouldn't add the lighting to the image. The other source of light is indirect light. This is light that bounces off of other surfaces in the scene before reaching the shading point. To collect indirect light, we can cast secondary rays in random directions from the shading point. We then, repeat the shading process at the secondary intersection points. We'll first compute how much light is arriving directly at the second intersection point and what fraction of that light is reflected back towards the previous intersection point. And, ultimately, back into the camera. We'll also need to cast another shadow ray from the secondary intersection point. And we can repeat this process however many times we'd like to simulate light bouncing around the scene. Now, in order to get these nice soft shadow and bounced lighting effects we, actually, need to cast many shadow and secondary rays per point along the path. This means that the number of rays, actually, grows exponentially with the number of bounces. To avoid this exponential growth we'll, instead, choose just one shadow ray and one secondary ray direction at each bounce. This will result in a noisy image but we can average the results together over multiple frames. Each frame will, also, generate its own set of primary rays, so this, also, gives us the opportunity to implement camera effects such as depth of field and motion blur. So, let's translate this all into a diagram. First, we'll generate primary rays. Then, we'll find the intersections with the scene. Then, we'll compute shading at the intersection points. And remember, that this is an iterative process, which will generate additional shadow and secondary rays, which will be intersected with the scene, again. And finally, we'll write the shaded color into our image. So, this is describing a rendering application. But a significant fraction of the time is, actually, spent on ray triangle intersection testing. This means that the performance of the intersector has a huge impact on the overall rendering performance, even though, it has nothing to do with the actual lighting and shading. This core intersection problem is, also, common to all Ray Tracing applications. So, we decided to solve this core intersection problem for you, so that you can start with a high performance intersector and just focus on the details of you app. So, this year, we're introducing the MPSRayIntersector API. This is an API which accelerates ray triangle intersection testing on the GPU on all of our macOS and iOS devices. We wanted to make it easy to integrate his into existing apps, so we simply take in rays through a METAL buffer. MPS will find the closest intersection along each ray and return the results in another METAL buffer. All you need to do is provide a METAL command buffer at the point in your application where you'd like to perform intersection testing. And we'll encode all of the intersection work into the command buffer for you. So, let's take a closer look at the problem that we're trying to solve. Oh. Okay. So, 3D models are, usually, represented as arrays of triangles. What we need to do is search through those triangles and figure out which ones intersect each ray. And furthermore, we need to figure out which intersection point is closest to the ray's origin. And the simplest way to do this would be to just loop through all the triangles and check for intersection with the ray. But for anything but the smallest scenes, this would be far too slow. So instead, we built a data structure that we call an Acceleration Structure. The acceleration structure works by recursively dividing the scene into groups of triangles that are located nearby in space. When you want to intersect a ray with a scene, we intersect the ray with the bounding boxes in the tree. If a ray misses a bounding box, then we can skip the entire subtree. In the end, we're left with just a fraction of the triangles that we actually need to check for intersection with the ray. So, this is the main way that we speed up ray triangle intersection. Of course, this is a simplified example. In a real scene the acceleration structure can be quite a bit more complicated. We can see from this visualization that the acceleration structure is adapting to the complexity of the geometry. This means that we spend most of our time searching for intersections only in areas of high geometric complexity, which is what we want. Now, I'm describing this to give you an intuition for what the acceleration structure is and how it works. But you, actually, don't need to worry about this, too much, because MPS will take care of all of this for you. Remember that we'll model our scene using triangles. And those triangles will themselves be represented by vertices in a vertex buffer. All you need to do is call MPS to build an acceleration structure from your vertex buffer. When you're ready to search for intersections, you simply provide this acceleration structure back to the intersector. So, let's see how we can use this to build a real application. We'll break this app into three stages. First, we'll generate primary rays, find the intersections with the scene, and compute shading. This will be equivalent to what we could have done with the rasterizer, but we'll take it further in the next steps. So next, we'll add shadows. MPS has special support for shadow rays, which can make this application even faster. And finally, we'll simulate light bouncing around the scene using secondary rays. This would be very difficult to do with the rasterizer, but we'll see that it's, actually, a straightforward extension with Ray Tracing. So, let's start with primary rays. There are five things that we need to do. First, we'll create a ray triangle intersector. Then, we'll create an acceleration structure from our vertex buffer. Next, we'll generate primary rays and write them into our ray buffer. We'll then, find the intersections between those rays and the scene, using the intersector. And finally, we'll use the intersection results to compute shading. So, let's start with the intersector. The MPSRayIntersector class coordinates all of the ray triangle intersection testing. All we need to do to create one is provide the METAL device that we want to use for intersection testing. Next, we'll create the acceleration structure. This is represented by the MPSTriangleAccelerationStructure class. Again, all we need to do to create one is provide the same METAL device we used to create the intersector. We then, attach our vertexBuffer and specify the triangleCount. And finally, we build the acceleration structure. We only need to do this once. And then, we can reuse the acceleration structure as many times as we'd like. So next, we'll generate primary rays and write them into our ray buffer. To do this, we'll launch a two-dimensional compute kernel with one thread per pixel. Each thread will write this ray struct into the ray buffer. We can think about our output image as floating on a plane in front of the camera. Primary rays are emitted from the camera, so we'll simply set the origin to the camera position. To compute the direction, we'll find the direction from the camera position through the corresponding pixel on the image plan. Now that we have our primary rays, we'll use the intersector to find the intersections of the scene. The encodeIntersection call will tie together everything we've created, so far. First, remember that we'll encode into a METAL command buffer. We, actually, have a couple of options for what type of intersection search we'd like to do. In this case, we'll just use nearest, which will find the closest intersection along each ray. We'll then, provide the ray buffer, which contains the primary rays we just created, as well as an intersection buffer, which will contain the intersection results. We, also, need to provide the rayCount, which in this case, is just the image width times the image height. And finally, we'll provide our accelerationStructure. MPS will find the closest intersection along each ray and return the results in the intersection buffer. So, all that's left is to use the intersection data to compute shading. To do this, we'll launch another compute kernel. We can apply lighting and textures similar to the way that we would in a fragment shader. Most of the standard texture and math functions that are available in a fragment shader are, also, available in a compute kernel. But shading, typically, depends on both the intersection point and vertex attributes, such as colors and normals. In a fragment shader the GPU would interpolate these for us. But we'll need to interpolate them ourselves based on the intersection data. So, let's first look at how we can compute the intersection point. Remember that a ray is defined by its origin and direction. This is the intersection struct returned to us by the intersector. The distance field will tell us how far we would need to go in the ray direction to get from the ray origin to the intersection point. And this distance will be negative if the ray doesn't intersect anything. The primitiveIndex tells us which triangle we hit. And the last field is what we'll use to interpolate vertex attributes. This field contains the first barycentric coordinates called U and V. And these correspond to the location of the intersection point relative to the vertices of the triangle. There are, actually, three barycentric coordinates, but they add up to one. So, we can compute the third coordinate W by subtracting the first two from one. If we have a vertex attribute defined at each vertex of our triangle, then the interpolated vertex attribute is just the sum of the attributes at each vertex weighted by the barycentric coordinates. For example, if we have a color defined at each vertex, then the interpolated color is just the weighted sum of the colors at each vertex. So, at this point we've created a ray intersector and an acceleration structure. We then, generated primary rays and found the intersections with the scene. We computed shading at the intersection points, and then wrote the shaded color into our image. So, let's take a look at the image. We can see the geometry represented by the acceleration structure, as well as the interpolated vertex colors and lighting we just computed. Now that we have an image on screen we're ready to add some additional effects. So, we'll start by adding shadows to our image. To do this, we need to check if the light can actually reach the shading point before adding it to the image. To do this we can cast additional shadow rays from the intersection points towards the light source. If a shadow ray doesn't make it all the way to the light source, then the original shaded point wasn't shadow. So, we shouldn't add its color to the image. We'll modify our shading kernel to write out additional shadow rays into another METAL buffer. We'll then, find the intersections with the scene, again. And then, we'll launch one final kernel which will conditionally write the shaded color into the image based on whether or not the shadow rays intersect at anything. So, let's start with the changes to the shading kernel. Now, shadow rays are a little different than primary rays. First, we need to provide a maximum intersection distance so that our shadow rays don't overshoot the light source. We also don't need to know which triangle we hit or what the barycentric coordinates were. So, there are some optimizations we can do. And finally, remember that we can't write the shaded color into the image until we know whether or not the original shading point was in shadow. So, we need a way to pass the color from the shading kernel through the intersector, all the way into the final kernel, which will update the image. To do this, we can customize our ray struct. So, first we have several options for what data we actually provide to the intersector. In this case, we'll use a data type which includes minimum and maximum distance fields. MPS will ignore any intersections outside of this range, which will prevent our shadow rays from overshooting the light source. Second, if you have application-specific data associated with your rays you can append that data at the end of the ray struct and provide a rayStride. MPS will skip past this data when reading from your ray buffer. In this case, we'll add the shade of color to the end of the ray, so that we can propagate it from the shading kernel through to the final kernel. We configure these options on the ray intersector. First, we'll set the rayDataType to match our struct type. Then, we'll set a rayStride to skip past the color at the end of the struct. Next, we'll run the shadow rays through the intersector. This was our original call to the intersector. Remember that shadow rays are only checking for visibility between the original shading point and the light source. So, there are two optimizations we can do. First, just like we can customize the rayDataType, we can also customize the intersection data type or what data is returned to us by the intersector. In this case, we only need to know if the distance is positive or negative, indicating a hit or a miss. So, we can set the intersection data type to just distance. This will save some memory bandwidth reading from and writing to the intersection buffer. Second, because we don't, actually, need to know which triangle we hit, we can end the intersection search as soon as we hit any triangle. This is, typically, much faster than searching for the closest intersection. MPS has a special mode for this, which we can turn on by passing the any intersectionType instead of nearest. Finally, we can launch our last kernel, which will add the color to the image. Each thread will read in one shadow ray and the corresponding intersection data. If the intersection distance was positive, then the original intersection point was in shadow. So, there's nothing more to do. Otherwise, the intersection point wasn't in shadow. So, we should read in the ray color and write it into the output image. And that's all we need to do to add shadows to our image. We can see that each shaded point is now checking whether or not the light source is visible before adding the lighting to the image. Because we're using a ray tracer we can, also, randomly sample the surface of the light source, which gives us these nice soft shadows. So last, we'll look at secondary rays. Remember that secondary rays simulate light bouncing around the scene. All we need to do to add secondary rays is move all of our kernels into a loop. In each iteration we'll choose a new random direction to continue they ray's path. So, modify the shading kernel to produce the rays for the next iteration. Once we finish updating our image we can simply loop back to the first intersection test. And we can repeat this loop however many times we want rays to bounce. So, let's look at the changes to the shading kernel. In each iteration we'll move the ray origin to the intersection point. We'll then, choose a random direction to continue the path. And finally, we'll multiply the ray color by the interpolated vertex color. This will cause the light to take on the color of whatever surface it bounces off of. In a more advanced application, this would be a much more complicated calculation. But by choosing the random ray directions, carefully, we can actually get the rest of the math to cancel out. And this works even though we're working backwards from the camera as long as we're careful to tint the direct lighting by the ray color at each intersection point. So, that's all we need to do for secondary rays. So, we can see that light has started to bounce off of the walls and onto the sides of the boxes and ceiling. So, that's it for our example app. We started by getting an image on screen with primary rays and shading. Then, we added shadows. And finally, we simulated light bouncing around the scene with secondary rays. So, let's switch to the demo and take a look at this running live. So, here's the application we just wrote up and running on the 12.9-inch iPad Pro. We've, actually, extended this application to support more advanced lighting, shading, textures, and more. So, let's switch to a more complicated scene, which used many of these features. Here's the Amazon Lumberyard Bistro scene you saw running in the State of the Union on four GPUs. The scene has almost one million triangles. But, even with these advanced lighting and shading techniques we're still able to achieve almost 20 million rays per second on an iPad Pro. And that's a combined measurement including primary, shadow, and secondary rays. So, we've created what we think is an easy to use API that you can use to start implementing these types of applications today. So, that's if for our demo, for now. Thank you. So, don't worry if you didn't catch all of that because this application will be available for download as a sample. The sample demonstrates everything I've talked about today and more. We recommend that you get started by downloading the sample and adding your own geometry, lighting, shading, and so on. There's, also, a lot more to the API that I didn't have time to talk about, today. So, we, also, recommend that you take a look at the documentation and headers. And with that, I'll hand it off to my colleague, Wayne, who will talk about how we can extend this to multiple GPUs. Thank you. Thanks, Sean. Hi, everyone. So, say you're working on your Mac. It has an internal GPU. But you've, also, plugged in a couple or really high performance eGPUs. And what we'd like to be able to do is use all of these GPUs together to make Ray Tracing go as fast as we possibly can. So, how are we going to do this? Well, there's three things we need to think about. First of all, how are we going to divide up the work between GPUs? Secondly, at some point the GPUs are going to need a way to exchange data. So, how are we going to deal with that? And finally, we need a way to keep everything synchronized. Now, for this, I'll show you how to use the new METAL Events feature that we're introducing here, this week. So, let's get started. So, to divide up the work we're going to use something called Split Frame Rendering. The idea here is to partition our frame into regions. And then, we'll assign each of these regions to a different GPU, so that they can be rendered in parallel. Now, each GPU will run the full rendering pipeline that Sean described earlier. So, that's everything from initial ray generation right through to shadow rays and shading. Now, once all GPUs have finished we'll pick whichever one is connected to the display and we'll copy across all of our completed regions for composition. Now, composition could just be stitching the regions together into one frame buffer. Or you might want to combine them with the results of a previous render to refine the image and remove noise. Now, before we can render anything, we need to make sure that each GPU has a complete copy of the scene. Assets, such as your vertex buffers and your textures need to be replicated on all GPUs. And so, do the triangle acceleration structures that Sean introduced earlier. Now, for the acceleration structures, we really wanted to avoid you having to build them from scratch for each GPU. So, we added an API that enables you to take an existing acceleration structure and make a copy for each GPU that you want to use. Now, this copy is nonrecursive. So, any buffers that you have attached to your acceleration structure, for example, your vertex and your index buffers, you'll need to copy those separately. And then, attach them to the acceleration structure that we just created. So, now that the data is replicated on all GPUs we're ready to start rendering. Now, the interesting thing for our multi-GPU perspective is that this part of the pipeline is virtually unchanged from what Sean described earlier. The only difference we need to make for multi-GPU is to restrict regeneration to whichever part of the screen a particular GPU is working on. Everything else is the same. So, for that reason, let's move straight on to what's, probably, the trickiest stage for multi-GPU, and that's its final composition phase here. Now, for best performance on macOS, each GPU will render into its own private buffer. And once rendering has finished we need to copy that buffer over to whichever GPU we're using for composition. Now, we can't copy between the buffers, directly, because METAL resources can only be used on the device that they were created on. So, you can't create a buffer on one GPU, and then try and attach it to a Blit encoder on a different GPU. That's just not going to work. So, this means that our copies will need to go through system memory. Now, to make this as efficient as we can, we use the buffer arrangement that you see here. We're going to create two METAL buffers; one on each device that wrap a common CPU allocation. And as the buffers wrap the same underlying memory anything that is written into the METAL buffer on device A is, also, visible to the METAL buffer on device B. Now, as I mentioned earlier, for performance reasons on macOS all of the actual rendering work is done using private buffers. And then, we Blit our completed regions through system memory when it's time to copy them to a different GPU. So, here's a quick look at how to set this up. First, we create our buffer on device A, using METAL shared storage mode. And this allocates system memory, internally. And we can get appointed to it using the .contents method. We, then create a buffer on device B using the NoCopy API to wrap the memory that we just allocated to buffer A. Now, something to be aware of for this API is that the buffer needs to be a multiple of page size. So, you'll need to pad the length when you create the original buffer. So, now that we're able to share memory between devices we need to think about synchronization. Now, to help with this we have an example timeline here to help us visualize two GPUs running in parallel. The dark boxes represent command buffers and the green boxes represent work that we've encoded into those command buffers. For example, using a compute command encoder. So, the GPU at the top there, is going to do some rendering. And when it's finished, it'll Blit its completed region into the shared buffers that we were just talking about. Now, while that's going on GPU B is, also, doing some rendering. Now, this is the GPU that we're going to use for composition. So, at some point it's going to need the buffer that was produced by GPU A. Now, we can see here that we have a problem. There's no synchronization in this area, here. So, there's nothing to prevent GPU B from trying to read the buffer before GPU A has finished writing to it. Now, to deal with this we can use METAL Events. With METAL Events, we'll insert a wait into the command buffer. So, while the GPU is executing, it'll reach the wait, and then it's just going to stop. And what it's waiting for is a signal from the other GPU. Once that signal is received we know that GPU A has finished writing to the buffer and it's now safe for GPU B to access it. So, this is an elegant way to fix our synchronization problem. But, clearly, having a GPU, potentially, a very powerful GPU just sitting there waiting is not good. So, we need to make this wait as short as possible and, ideally, we want the GPU working instead of waiting. So, what I'm talking about, here, is load balancing. So currently, we just split the screen equally between GPUs. And there's a couple of problems with this. Firstly, it doesn't take into account that you might be using GPUs of different performance. If one GPU is much faster than the other, then it stands to reason that, yeah, it's going to finish first. And the other problem is that some parts of the screen are more complicated to render than others. They take longer. They might have more complex geometry or more complex materials. So, to fix this, we need to adjust our region sizes adaptively. Now, the aim here is for each GPU to take, approximately, the same amount of time to render its part of the scene. Now, the way we do this is we start with the fixed partitions that you see here, and we render a frame. And we time how long each GPU is working for. And then, we use that to decide how big a region to give each GPU the next time around. And we do this the whole time your application is running. So, it constantly adapts to the performance of the GPUs that you have connected. And wherever you are and wherever you're looking in your scene. So, to measure how hard a GPU is working, we use command buffer completion handlers. Now, completion handler is a block of CPU code that you can have run after the GPU has finished executing your command buffer. Now, on iOS command buffers have a couple of useful properties that you can read to find out how long it took to run on the GPU. But these aren't available on macOS. So, we need to come up with an approximation. And the way we do this is we store the host time when each command buffer completion handler was called. And if you do this for every command buffer, you can then use the differences between these times to figure out how long the GPU was running for. So, for example, to estimate how long the three command buffers shown here took execute, we've measured the time between the completion handler was called for command buffer 3 and command buffer 0. So, that's the theory. And now, let's see it in action. All right. So, this is the Amazon Lumberyard Bistro scene that Sean was showing you earlier. And this time, it's running on a MacBook Pro. And you can see in the top left of the screen here, we have a rays per second metric. As you can get an idea for how it's running. So, this includes the primary rays, secondary rays, and the shadow rays. They're all included in this metric. So, you can see, we're getting about 30 million rays per second. And it'd be nice if we were going a little faster. So, I'm going to enable one of the eGPUs that I have connected, here. So, you can see from the text there, we're now running on an RX 580, as well as the internal GPU, here. And performance has doubled to about 60 million rays per second. And you can also see the green lines here that we're using to help visualize how the load is split between GPUs. So, one GPU is rendering everything above the line. And one GPU is rendering everything below. So, with the eGPU we're now going about twice as fast as we were before. I was kind of hoping for a bit more, there. And so, the problem is the eGPU is sitting there waiting. And that's because we're using these fixed partitions. So, if we switch on the adaptive load balancing, here, you see the RX 580 just grabs a big chunk of the work, now. And we're going much faster than we were, previously. So, the scene here has approximately one million triangles. And we're now going to switch to an outside view. It's the same Amazon Lumberyard scene but we're outside, now. And this scene has approximately 3 million triangles. And I have another GPU sitting here. So, we'll turn that one on, too. And, this time, it's a Vega 64. So, you see the Vega grabbing a big chunk of the work, there. And what's interesting about this configuration is we have three very different GPUs working together. They're different architectures and they're very different performance, too. But they're all working together to help produce this great image. So, today we introduced the MPSRayIntersector, a new API that you can use to accelerate ray triangle intersections on the GPU. As you saw from my demos, it's available on all of our iOS and macOS platforms. And it scales really well as you add multiple GPUs on macOS. Now, we're really excited to see how you use Ray Tracing in your applications. We used Path Tracing in our example, today. But there's also hybrid rendering. You might want to incorporate Ray Tracing to do really nice looking shadows or ambient occlusion or reflections. And there's also non-rendering in cases. For example, autosimulation, physics, AI collisionless action. There's a ton of stuff you can do with this. So, to help you get started you can find sample code on developer.apple.com. So, be sure to check that out. Also, the header files are full of documentation and information on some additional features that we weren't able to cover today. And finally, we have our lab session tomorrow from 12. Sean and I will be on hand to talk more about the API and to help you get started with Ray Tracing in your application. So, we hope you can join us for that. So, thank you, for coming to our talk. And enjoy the rest of WWDC.  Good morning, it's a pleasure to be here with you. My name is Emmanuel and I'm an engineer on the Core Image team. This morning we'll be looking at creating photo and video effects using depth. Let's get started. With iOS 11 we started delivering portrait depth data alongside your portrait still images. During last year's WWDC sessions we showed how you can leverage that depth data to achieve amazing effects, such as force perspective, simulated depth of field effects, as well as various foreground and background separation effects. This year we're extremely excited to announce that we're coming out with a new feature, a portrait matte. So during the first half of this session we'll be focusing on portrait still images and how you can apply really great effects on them. During the next half my colleague Ron over from video engineering will be focusing on using a TrueDepth camera to achieve real-time video effects. All right, let's take a look at the portrait segmentation API. So I mentioned a portrait matte, so what is a portrait matte? A portrait matte is a segmentation from foreground to background and what this means precisely is that you have a mask which is 1.0 in the foreground and 0.0 in the background and you get soft and continuous values in between. The portrait matte is of extremely high quality and is able to preserve fine details, such as curl and hair on the outline of your subject. This is amazing. The matte can be used to achieve great many effects, here is just one of them where we essentially do a foreground and background separation by darkening the background. But we're really putting this tool into your hands so that you can create amazing apps and new effects and delight your users. All right, so the portrait effects matte is coming to you with iOS 12. It is available for both the front and the rear facing camera. It is available to you with portrait still images and at the moment only when there are people in the scene. Note that the portrait matte is linearly encoded, so it's not gamma encoded and what you get is a grayscale buffer. And also there's no guarantee that you will be getting a portrait matte alongside your portrait still images, you always do get the depth data but you need to make sure to test for its existence. All right, let's take a look at the API and how we can actually load that data in. So ImageIO provides a low-level API that allows you to load portrait effects matte. So by calling CGImageSourceCopy there's a new key it can pass kCGImageAuxiliaryData TypePortraitEffectsMatte. And this call returns a dictionary containing three main pieces of information. The image data itself as a CFDataRef, metadata pertaining to the buffer itself as a CFDictionary, as well as metadata pertaining to the capture itself. AVFoundation also provides a higher-level API that sits on top of ImageIO that you can use. So taking the output from CGImageSourceCopy you can feed it to the AVPortrait effects matte class. And what you get out of it is very simple it's a CV pixel buffer along with pixel format type so you can use that CV pixel buffer for your further processing needs, it's really that simple. AVFoundation also supports portrait matte delivery at capture time. So starting with your typical AVFoundation setup with your AVCaptureInput, device, as well as capture session. The first thing you want to do is make sure that your environment supports the delivery of the portrait effects matte. To do this you'll be checking for is that data delivery supported, as well as is portrait effects matte delivery supported. The reason we have the two there both depth and portrait effects matte is that they come together. You can either opt in to get only the depth data, but whenever you want the portrait effects matte you also need to activate the depth data delivery. And to activate or to opt in for that delivery make sure to modify your AVCapturePhotoSettings and set the isPortraitEffects MatteDelivery enabled, as well as isDepthDataDeliveryEnabled to true. Then the capture time your ddiFinishProcessingPhoto callback will give you the portrait effects matte. It's really that simple. All right Core Image also provides you with a way to load and save your portrait effects matte. A new queue was introduced auxiliaryPortrait EffectsMatte which you just pass your image with contents of URL and you get a CI image back which contains the portrait effect. Core Image also allows you to save your portrait effects mattes directly into your files. To do this there's a new context option called portraitEffectsMatteImage, you pass in your CI image containing the portrait effects matte, and then you can write your file to disk using for example writeHEIFRepresntationOfImage. All right so one thing that's important to note here is that the three images, so your RGB, the depth buffer, and the portrait matte buffer live at a different resolution. So for example, the portrait matte for the rear facing camera is half-size and the depth data is even smaller. So let's look at the images side-by-side in the case of the rear facing camera. What this means that in your applications you need to make sure to either down-sample your RGB image to the size of your portrait depth or portrait matte or [inaudible] and sample them to the size of your RGB image. So that's all I wanted to talk about today for the portrait segmentation API and we have a great demo for you to see this live in action. So during this demo I'll be making use of a Jupiter Notebook which is a browser-based, real-time interpreter for Python. And we'll be making use of Python bindings for Core Image which we'll be introducing later today in a separate session. So let's start and load an image that contains portrait depth and portrait matte in. So this is the image we're going to be working with. The first thing I want to show you is what the depth data looks like for that image, so let's look at the two side-by-side. So we have the portrait depth on the left and the portrait matte on the right-hand side and you can just see how fine the details are. And we'll do a zoom crop in just a minute so that you can better appreciate just how high quality it is. Then next thing we do is we resize the images. As you see the RGB and the depth data vary greatly in size. So we resize our images and let's have a look at them side-by-side. So we have our RGB and our depth data on the right-hand side. During the first part of this demo I'll focus on depth data, then we'll see how things get much, much simpler when you use a portrait effects matte. So the effect I'm going to be working on today is depth thresholding and essentially what I'll be doing is computing a histogram of the gray level values in my portrait depth. And I'll be applying a threshold or a clipping point in that histogram so that everything becomes zero or one depending if it's sitting below or above that threshold. Then we'll be closing holes in the image by using morphological closing operations and then blurring the mask so that we get a nice feathered look. Let's have a look at this in action. So remember all of this is executed live in the browser using Core Image as a back end. All right so the first thing I want to show you is how changing percentile point changes my mask here. So the higher it is the less aggressive I am on clipping the foreground. So let's pick a value that's reasonable, maybe something like this here. And what you can see here is that there are regions or islands we call them that are connected to my foreground and there's a bit of the subject here I'd like to take out. So what I'll do is I'll add a bit of morphological closing. Look at how this appears magically. If I go too far obviously I lose my entire subjects, I don't want to do that. So let's pick something like this. What I can do then is change the feathering by applying the mask on top of it all. Let's take a look at how the RGB is threshold in back. This is not the effect we're coming up it's just to give a sense of how the mask is applied, so let's keep going. So I've chosen a few parameters for this thresholding here and this is what I'll be using for my foreground. Next, I'm applying an effect only on my foreground, so in this particular case I'm using the Core Image photo effect Noir which turns everything grayscale and has a has a bit of contrast. I'm doing an exposure adjustment, as well as desaturating my image slightly and augmenting the contrast even further. Let's take a look at the output. This is going to be the foreground that I'll be using and what I want to do here is leverage the depth data mask that I have to composite this foreground onto a background. Let's just generate a background which is just a darker version of the original image. We can then composite the two together using the Core Image filter blendWithMask and we have the result right there, it's that simple. All right. Thank you. Okay so as you saw that required a bit of fiddling with the parameters for clipping, for smoothing, and then so on and so forth. The portrait effects matte enables you to do this without actually doing much process, this is really exciting so let's have a look at this. So here we're starting with another image that also has portrait depth and portrait matte information embedded into it, so let's look at these. As I mentioned earlier, this is an extremely high-quality foreground mask. So let's have a look at a crop on this hair. Look at how fine the detail is, this is beautiful. On the right-hand side here is the depth that it shows you just how coarse the depth data is compared to effect matte. So we'll do another foreground separation effect here similarly to what we just did but we used a portrait matte instead of using the portrait depth. So let's look at our foreground similar as before but in this case, we'll desaturate the foreground slightly, add a bit of contrast, as well as some vibrance to the image. Now let's generate a background, in this case we'll do a disk blur which is another Core Image filter, as well as bring down the exposure so things get pretty dark. But we still get a bit of background remaining, it's quite faint but it's still there. And again, I use a CI blend with mask which is going to do the compositing for us with the mask and we have left and right. Isn't this beautiful? Thank you. All right let's look at another great demo which we call Big Head. So because the portrait matte is so fine we can actually do things like change the [inaudible] size of our subject with respect to the background using it. So let's do just that and we'll do it live. So here's our input image on the left here and the portrait matte on the right-hand side. And what I'll be doing here as I'll be playing with the size of the subject in my frame notice how the subject is getting smaller and bigger. And you can actually you know give more weight to the subject in your frame, but you can also do pretty cool things like now that I have this let's say that I pick my favorite size here. You can do things like pseudo simulated depth of field here by giving more contrast to the foreground and using a [inaudible] in the background to give more pop to your subject. All right let's take a look at just another image to see just how easy that is because they're using the exact same pipeline under the hood which is very simple just using the portrait matte and using it to blur the foreground and background. Again our input image here, we can change the size of it, make it bigger, then apply a bit of contrast to give more focus to our subject. It's that easy, really exciting. All right let's take a look at another demo which we call Marching. I'm not even going to try to explain what it does, let's just take a look at the filter in action. There you go, fun stuff and just because we can do it I can expose how many of these I want to stitch together. So going from just a few to you really, really pushing that way, way too far. Really exciting stuff. All right. So that's it for this demo, I hope you enjoyed this. So if you'd like to know more about using Python bindings for Core Image I encourage you to come to this afternoon's session on Core Image Performance Prototyping in Python. That's all for me today, let me introduce you to my colleague Ron from video engineering who will be talking about real-time video effects with TrueDepth. Thank you everybody. Thank you Emmanuel. Great photo effects but what about video. My name is Ron Sokolovsky and I am from video engineering. In this part we are going to leverage the TrueDepth's camera to create similar effects with real-time video, like for example this background replacement app. In order to create such effects we are going to deep dive into the stream coming from the TrueDepth camera, the characteristics, best practices, and challenges. We are also going to show you how to work with point clouds, a completely different way to process and render rich depth information. And that background replacement app we're calling it Backdrop and we'll show you how to make it step-by-step. But first things first, the stream for the TrueDepth's camera is made of frames, each frame is a depth map, a 2-D image in which each pixel contains the depth information or the distance to the scene in that direction. We've chosen a specific coloring scheme, closed pixels are colored in red while fire red pixels are colored in blue. In between them there is a colorful spectrum so you can see the texture of the depth map. There are also black pixels, those are holes in a depth map. For those pixels we have no information what is the depth. We are releasing today a new tool, a sample app for you to explore this stream and we call it TrueDepth Streamer. You can slide between the video stream and the TrueDepth stream. Now because the TrueDepth camera has active illumination even in complete darkness while the video is pitch black it is business as usual for the TrueDepth camera. So now you see me and now you don't. So how do you add the stream from the TrueDepth camera into your application? Well I'm glad you asked. The first thing you need to do is to discover the built-in TrueDepth camera and then you initialize the device capture input. And you add the depth data output into your session. At this point you're good to go, you can start the session and you will have the TrueDepth stream with your session. This stream can come in two forms of data, disparity or depth. Now disparity is the inverse of depth and vice versa, so which one should you choose? Well disparity usually yields better results, especially for machine learning applications but the depth data has more meaning in terms of real-world measurements. Know that if you work with depth that the depth error goes with the depth squared. That means that an object at 1 meter would have four times the depth accuracy as an object at 2 meters. We have two streams, video and depth, and they don't necessarily share the same resolution. The native resolution of the TrueDepth's camera is VGA or 640x480 and that's what you'll get if you choose a video preset of an aspect racial of 4:3. If however you choose an aspect ratio of 16x9 you'll get a depth map of 640x360. In both cases the depth map will cover the entire field of view of the RGB image. Now we are talking about video applications, so we are crunching a lot of numbers very, very fast and that could create system pressure over time. So you can test your application and gauge the system pressure level which goes from nominal to fair, serious, critical, and then shutdown. And the responsibility is in your hands because the system will let you go all the way to shutdown but when it does it's bye-bye every capture device. Another thing you can do is to adopt a degradation scheme, if the pressure level gets serious you can reduce the frame rate to 15 frames per second or you can choose a more elaborate scheme with gentle degradation going from 30, 24, 20 and 15 frames per second anytime the pressure level increases. So we have holes in the depth map what can we do about it? Well, in fact you could get the stream already filtered for you. There is a parameter called isFilteringEnabled and it's' defaulted to true, which means you get a filtered depth map smooth, spatially and temporally and the holes are filled from the RGB image. This is especially useful for photography and segmentation applications because you know every time you query a pixel you get the depth's value. In TrueDepth Streamer you can switch to the filter stream and see that it is smoother and the holes are filled. So this is great, but it is not applicable to 100% of the use cases. If you're working with point clouds or any type of real-world measurements you're better off staying with the raw data which holds the highest fidelity. If you do you will have holes, you will have pixels marked as zero, it does not mean that they are the distance of zero meters from the camera it just means we have no information about them. Therefore, you should watch out for operations like averaging and downsampling because you don't want to mix those real values with those zeros. But why do we even get holes? Well the TrueDepth camera detects objects up to a distance of about 5 meters, but not all materials are made the same. Some materials have low reflectivity. they absorb most of the lights. For example this extreme scenario is a very low reflective fabric watch what happens when we switch to the depth map and I walk away from it. Even though there are objects in the scene with larger distance we see holes forming on this fabric because it's absorbing most of the light. If we switch to the filtered stream and repeat the same motion those holes are filled. But it's not only about the amount of light reflected back it's also about the direction in which it is reflected to. Some materials are specular or shiny and they are very picky and choosy in which direction they send back the light. An extreme scenario would be this display, you can watch the video stream to see the reflection. And when we switch to the depth map holes are forming depending on the angle between the device and the screen. And if we switch to the filtered stream those holes are filled but with less fidelity. Another challenging scenario is outdoor, typically in an outdoor scene the background is very far away so we don't expect to get any depth on the background. Also, the sun acts as an aggressor to the active illumination. To demonstrate that I went outside on a very sunny afternoon, positioned myself against the sun. And when we switch to the depth map you can see there's no depths on the background and we get some holes around the frames, specifically in this case the hair. But still most of the depth on the foreground is intact and very useful. One final point I want to cover for getting holes is the fact that from the perspective of the TrueDepth camera some of the light projected hits an object on the way back so we get shadows from the parallax between the projector and the camera. You can see an example on the right side of this mug, but something is different here. This is not a depth map so why do we even get holes? Well in TrueDepth Streamer you can switch from 2-D mode into 3-D mode, which gives us a point cloud view. With point clouds we can dynamically change the perspective of the scene creating even more holes when we do so. And now I can ask you is this mug half empty or half full and the answer is we have no idea. By virtually changing the point of view of the camera we don't add new information and that is because the TrueDepth camera can do many things, but bending light into the mug not one of them. Let's see this live. So I'm starting with a video view and I want you to look in this corner. Watch what happens when I touch the screen, I will touch my forehead, you can see an indication of the depth. And if I move the phone you can see the [inaudible] changing and the reason I can do so is because we have the stream from the TrueDepth camera running as well and you can see that it is overlaid on the video. So we have this livestream 30 frames per second and we can switch to the filtered stream and then all the holes are filled. If I switch to the point cloud view I can dynamically change the point of view to the scene. So even though I'm looking directly to the device it looks as if somebody's watching me from up above. Now the reason we call this a point cloud is if I zoom in you can actually see the points in 3-D space. But being here with you in WWDC I feel like I have to pinch myself just to get things back in perspective. Thank you so that brings us to point clouds. How do we create them? We're starting from a depth map, a 2-D image in which the depth Z is a function of the pixel coordinates U and V. And we want to transform it to a new coordinate system in 3-D space, X, Y, Z. Now we already have Z right, that's the depth from the depth map but we want to get X and Y. For that we need to get the help from the Intrinsics Matrix which holds information for the focal lengths and principle point. If for example I want to get X I need to start with the pixel coordinate U, subtract the principle point, multiply by the depth, and divide by the focal lengths. And naturally I have to do the same thing for the other dimension as well. Now this Intrinsics Matrix is accessible through the camera calibration data. In fact, this operation is done in every frame of the TrueDepth stream. The reason for that is that the video stream and the depth stream are coming from two separate cameras. But because the TrueDepth camera gives us a depth map we can transform it into a point cloud and re-project it to the perspective of the RGB image so the depth stream is already registered on the video stream for you and you get RGBD data. Now, thank you. Yeah, it's pretty cool. Now these types of operations are best done in metal graphic shaders. And you can download the code for TrueDepth Streamer and you want to focus on two areas. In the vertex shader we control the location of the points, we'll start with the depth map and transform it to real-world coordinates or X, Y, Z. Then we can multiply it with a view matrix to change the point of view to the scene. In the fragment shader we get the output of the vertex, but we have to see if it's a real value or a hole in the depth map. If it's a hole and it's marked as zero we don't know its depth so we cannot transform it to X, Y, Z and we would need to discard this point. If it is a real value we can sample the RGB texture and add color to the fragment or point in this case. So I understand this part was a bit technical and a lot of you come from different backgrounds. Have no fear we have just the app for you, an app to replace your background, let's see it live. Let's see it live. So I can put myself in Yosemite, I can swipe down put myself in something more abstract. I can even go all the way to Antelope Canyon, Arizona, it took me 15 hours to get there last time, I could have just swiped down, saved a lot of money on gas. In fact, this application can even put you in space where nobody can hear you stream. So how do we create that? Anytime we're dealing with a video application there's other things that are going on a per frame basis, in this case we have to detect a face, create a brand-new mask from the depth map, smooth and upscale it to the RGB resolution. And then we take this foreground mask and upscale it again to the low-light background image. And then we can blend or [inaudible] them, but there's something we can do to reduce some of the complexity. If anytime we load a background image we resize it to the RGB resolution just once not per-frame, then we don't need that second upscale and the blending is done at low resolution which makes a big difference. So let's deep dive into those depths. The first thing we need is to find the center of the face. And in iOS there are actually quite a few ways you can get face metadata. You can use a Core Image detector or the Vision Framework, but in this case since we just need the pixel at the center of the face we can use AV meta data object type face. But it gives us the center in the coding system of the RGB image and we need to map it to the depth map which might not be in the same resolution. Once we have the value of the depth of the face we can use it plus a margin of characteristic 25 centimeters to threshold the depth map and create a binary mask, foreground is one, background is zero. In fact, we can stop here, we can use this binary mask and create the effect. The transition from background to foreground will be very sharp, but we'll get some fidgeting around the edges. So we want to filter it a bit. The first stage will be to apply some smoothing from the background to the foreground, in this case Gaussian Blurring. The radius of Gaussian Blurring will determine the slope of the transition and you can play with the value to get different effects. Another processing stage we add is gamma adjustments, it allows us to further fine-tune this transition from background to foreground. If we use a gamma value which is higher than one we'll get a narrower foreground mask. On the other hand, if we use a gamma value that is smaller than one we'll get a wider foreground mask and maybe some aura. So you can create different effects by combining those two parameters. If you use a large blur radius and a large gamma value you create this transparent transition that makes you seem as if you're a hologram in space or similarly it could be underwater and you can play with the values to create different effects. If I keep the radius high and reduce the gamma value to a very low number I create this halo around my head. So you can play with this to create your own effects. How do we implement this? In Core Image it is very straightforward, we can concatenate three filters in a row. We start with a Gaussian Blur, we add the gamma adjustment, and we upscale to the RGB resolution. But there are a couple of small points I want to emphasize as best practices. Anytime you work with a convolutional based operation such as Gaussian Blurring the best practice will be to start by clamping to extent. By repeating the border pixels outwards we can make sure all the borders of the image are handled correctly by the filter. Moreover, after the filtering and just before the upscaling the best practice will be to crop back to the original extent because that's the part of the image we really care about. At this point we have an alpha matte of the foreground and you can use it to create different kinds of effects for the background and the foreground just like Emmanuel showed in the first half. In Backdrop, we blend the RGB stream with a loaded background image in a single line of Core Image code using the alpha matte we created from the TrueDepth camera to create this background replacement effect. So the TrueDepth camera gives us a resolution of depth map of 640x480 coming at you 30 frames a second, already registered to the video stream. You can use it to create point clouds and dynamically change the perspective of the scene or use the filter depth to create different kinds of video effects. You can go to the webpage and download the Jupiter Notebook TrueDepth Streamer and Backdrop and we hope it inspires you as a starting point to many new cool video effects to create in your applications. Come say hi at the AVCapture lab, thank you so much for your time. Have a great day.  All right [applause]. Thank you [applause]. Good afternoon everyone and thank you for coming to our session today on Core Image. My name is David Hayward, and I'm really excited to be talking about the great new performance and prototyping features our team has been adding to Core Image over the last year. We have a lot to talk about, so let's get right into the agenda. So, the first thing we're going to be talking about today are some great new APIs we've added to Core Image to help improve the performance of your applications. After that, we're going to segue into another topic, which is how you can use Core Image to help prototype new algorithm development. And lastly, we're going to be talking about how you can use Core Image with various machine learning applications. All right. So, let's get into this and start talking about performance APIs. There's two main areas where we've worked on performance this year. First of all, we've added some new controls for inserting intermediate buffers -- we'll talk about that in some detail. And the second thing is we'll be talking about some new CI kernel language features that you can take advantage of. So, let's start by talking about intermediate buffers. As you are aware, if you've used Core Image before, Core Image allows you to easily chain together sequences of filters. Every filter in Core Image is made up of one or more kernels. And one of the great features that Core Image uses to improve performance is the ability to concatenate kernels in order to minimize the number of intermediate buffers. In many cases, to get the best performance you want to have the minimum number of buffers. However, there are some scenarios where you don't want to concatenate as much as possible. For example, your application might have an expensive filter early on in the filter chain. And the user of your application at a given moment in time might be adjusting a filter that follows it in the graph. And this is a classic situation where it's a good idea to have an intermediate buffer at a location like this in between. The idea is that by having an intermediate buffer here, the cost of the expensive filter does not have to be paid for again when you adjust a secondary filter. So, how do you do this in your application? We have a new API, very aptly named, inserting intermediate. So, let's talk about how this affects our results. What we do is instead of concatenating as much as possible, we will respect that location of the intermediate and concatenate as much as possible around it. Some notes on this. One thing to keep in mind, is that by default, Core Image cashes all intermediate buffers so that the assumption is that a subsequent render can be made as fast as possible. There are, sometimes, however, when you might want to turn off caching of intermediates. So, for example, if you application is going to be doing a batch export of 100 images, there is little benefit of caching the first one, because you'll be rendering a completely different image afterwards. So, you can do that today in your application by using the context option cache intermediates and setting that value to false. However, if you are also using this new API that we spoke about, you can still turn on caching of intermediates, even if this context option is turned off. So, this allows you to really make sure that we cache something and don't cache anything else. The next subject I'd like to talk about is some new features we've added to the kernel language that allows us to apply image processing. So, one thing to keep in mind is that we have two different ways of writing kernels in Core Image. The traditional way is to use the CI kernel language. And in this case, you have a string inside your source file; either your Swift code or your objective C code. And at run time you make a call, to say, kernel with source. And later on, when you create an image based on that kernel, you can then render that to any type of Core Image context, whether that context is backed by Metal or open GL. When it comes time to render, however, that source needs to be translated. It needs to be translated either to Metal or GLSL, and that step has a cost. Eventually then, that code is compiled to the GPU instruction set and then executed. Starting last year in iOS 11, we added a new way of writing CI kernels, which has some significant advantages. And that's CI kernels based on the Metal shading language. In this case, you have your source in your project and this is -- this source is complied at build time rather than at runtime. As before, you substantiate a kernel based on this code by using the kernel with Metal function name and binary data. The advantage here is that this data can be applied without paying the cost of an additional compile. The caveat, however, is it works on Metal backed CI context. But it gives a big performance advantage. So, starting in this release we're going to be marking the CI kernel language as deprecated, because while we will continue to support this language, we feel that the new way of writing Metal kernels offers a lot of advantages to you, the developer. For one thing, you get the performance advantage I outlined earlier, but it also gives you the advantage of getting build time syntax coloring on your code and great debugging tools when you're working with your Metal source. So, great. So, with that in mind I want to talk about a few other things that we've added to our kernel language. For one thing, we have added half float support. There are a lot of cases when your CI kernel can be perfectly happy with the precision that half float gives you. If you're working with RGB color values, half float precision is more than adequate. The advantage of using half floats in your kernel is it allows operations to run faster, especially on A11 devices like the iPhone 10. Another advantage of using half floats in your kernels is it allows for smaller registers, which increases the amount of utilization of the GPU, which also helps performance. Another great feature we've added to the kernel language this year is adding support for group reads. This gives your shader the ability to do four single-channel reads from an input image with only one instruction, so this really can help. And as a complement to that, we also have the ability to write groups of pixels. This gives you the ability to write four pixels of an image with just one call inside your shader. So, all three of these features can be used in your shaders to give great performance improvements. So, let me talk a little bit about an example of how that works. So, imagine today you have a simple 3 by 3 convolution kernel that is working only on one channel of an image. This is actually a fairly common operation, for example, if you want to sharpen the luminance of an image. So, in a kernel like this, typically, you're -- each time your kernel is evoked, it is responsible for producing one output pixel. But, because this is a 3 by 3 convolution, your kernel needs to read 9 pixels in order to achieve that effect. So, we have 9 pixels read for every one pixel written. However, we can improve this by making use of the new group write functionality. With the new group write functionality, your kernel can write a 2 by 2 group of pixels in one evocation. Now, of course this 2 by 2 group is a little bit bigger, so instead of a 3 by 3, we need to have a 4 by 4 set of pixels read in order to be able to write those four pixels. But, if you do the math, you'll see that that means we have 16 pixels read for 4 pixels written. So, already we're seeing an advantage here. The other feature we have is the ability to do gathers. In this example, we're reading a 4 by 4 or 16 pixels. And with this feature, we can do these 16 pixels red with just four instructions. So again, if you look at the math on this, this means we're doing just 4 group reads for every 4 pixels written. And this can really help the performance. Let me walk you through the process of this on actual kernel code. So, here's an example of a simple convolution like the one I described. Here, what we're doing is making 9 samples from the input image and we're only using the red channel of it. And then once we have those 9 values, we're going to average those 9 values and write them out in the traditional way by returning a single vec4 pixel value. Now, first step to make this faster is to convert this to Metal. This is actually quite simple. So, we start with code that looks like this, which is our traditional CI kernel language. And with effectively, some searching and replacing in your code, you could update this to the new Metal-based CI kernel language. There's a couple things that are important to notice here. We have added a destination parameter to the kernel, and this is important if you're checking for the destination coordinate inside your shader, which a convolution-like kernel like this does. And then we're using the new, more modern syntax to sample from the input by just saying sample -- s.sample and s.transform. And the last thing we've done when we've updated this code is to change the traditional vec4 and vec2 parameter types to float 4 and float 2. But as you can see, the overall architecture of the code, the flow of the kernel is the same. All right. Step 2 is to use half-floats. Again, this is an example where we can get away with just using the precision of half-floats because we're just working with color values, and so again, we're going to make again some very simple changes to our code. Basically, places in our code where we were using floating point precision, we're going to use half-float precision as well. This means the sampler parameter and the destination parameter have an underscore H suffix on them and any case in their code where we're using float 4 now becomes half 4. So again, this is very simple and easy to do. Another thing to be aware of is if you've got constancy in your code, you want to make sure to add the H on the end of them, like the dividing by 9.0. So again these -- this is another simple thing. The last thing we're going to do to get the best performance out of this example is to leverage group reads and group writes. So, let me walk you through the code to do this. So, again, we want to write a 2 by 2 group of pixels, and from that we need to read from a 4 by 4 group of pixels. So, the first thing we're going to do is specify that we want a group destination. If you look at the function declaration, it now has a group destination, H datatype. Then, we're going to get the destination coordinate like we had before, and that will point to the center of a pixel. However, that coordinate actually represents the coordinate of a group of 2 by 2 pixels. The next thing we're going to do in order to fill in this 2 by 2 group of pixels is do a bunch of reads from the image. So, the first gather read is going to read from a 2 by 2 group of pixels -- in this case, the lower left-hand corner of our 16. And it's going to return the value of the red channel in a half-four array. The four parameters will be stored in this order, which is X, Y, Z, W going in a counter-clockwise direction. This is the same direction that is used in Metal, if you're familiar with the gather operations in Metal. So, again in that one instruction we've done four reads and we're going to repeat this process for the other groups of four. So, we're going to get group 2, group 3, and group 4. Now that we've done all 16 reads, we need to figure out what values go in what locations. So, the first thing we're going to do is get the appropriate channels of this 3 by 3 sub group and average them together. And then we're going to store those channels into the result 1 variable. And we're going to repeat this process for the other four result pixels that we want to write -- R1, R2, R3, and R4. And the last thing we're going to do is called "Destination Write" to write the 4 pixels all in one operation. So note, this is a little different from a traditional CI kernel where you would've returned a value from your kernel and said you're going to be calling "Destination Write" instead. All right. So, the great result of all this is that with very little effort, we can now get two times the performance in this exact shader. This is a very simple shader. You can actually get similar results in many other types of shaders, especially ones that are doing convolutions. So, this is a great way of adding performance to your kernels. So I'm going to seg -- I like to tell people to go to this great new documentation that we have for our kernel language, both the traditional CI kernel language and the CI kernel language that's based on Metal. I highly encourage you to go and read this documentation. But now that we've talked about improving the performance of how your kernels can run, I'd like to bring up Emanuel on stage, who will help talk to you about how you can make your development process of new algorithms even faster as well. Thank you, David. Good afternoon everyone. It's great to be here. My name is Emmanuel, I'm an engineer on a Core Image team. So, during the next half of this session, we'll shift our focus away from the Core Image Engine and explore novel ways to prototype using Core Image. We'll also see how we can leverage Core Image in your machine learning applications. So let's get started. Since I want to talk about prototyping, let's take a look at the lifecycle of an image processing filter. So, let's say that we are trying to come up with a foreground to background segmentation. And here, what this means precisely is that we'd like to get a mask which is 1.0 in the foreground; 0.0 in the background and has continuous values in between. The difficulty in implementing such a filter heavily depends on the nature of data you have available. So, for example, if you have an additional depth buffer, alongside your RGB image, things can become easier. And if you're interested to combine RGB images with depth information, I highly encourage you to look at the session on creating photo and video effects using that. Today, I don't want to focus on these other sources of information, but I want to focus on prototyping in general, so -- so let's say that -- so, we have this filter well-drafted, and we know the effect we're trying to come up with, so in this particular case, a foreground and background mask. The very next natural step is to try implementing it, and you pick your favorite prototype in the stack and you start hacking away and combining different filters together and showing them in such a way that you achieve the filter effect that you're looking after. So, let's say you did just that, and here we have an example of such a foreground to background mask. Now, if you're in an iOS or Mac OS environment, the very next natural step is to deploy that algorithm. So, you have a variety of [inaudible] that you can use such as Core Image, Metal-with-Metal performance shaders, as well as VImage if you want to stay on the CPU. That initial port from prototype to production can be quite time consuming, and the very first render might not exactly look like what you're expecting. And there is a great variety of sources that can contribute to these pixel differences, one of them being simply the fact that the way filters are implemented across frameworks can be quite different. If you take an example here on the left-hand side, we have a [inaudible] blur that applies this nice feathering from foreground to background. And that's an example of a filter that can leverage a grid variety of performance optimizations under the hood to make it much faster. All these optimizations can introduce numerical errors which will propagate in your filter stack, thereby potentially creating dramatic changes in your filter output. Another problem that typically arises when you're putting your code, is that when you're prototyping environment, a lot of the memory management is taken care of for you. So, you don't often run into issues of memory pressure and memory consumption until it's pretty late in the game. Another topic, of course, that's important to consider is performance. Oftentimes, the prototypes are already using CPU code, and we over - sometimes-- we often over-estimate the amount of performance we can get from pointing our CP Code to GP Code, thinking that everything is going to get real-time. So, what if we could catch these concerns way, way earlier on in our prototyping and workflow? Well, we believe we have a solution for you. And it's called PyCoreImage. Python bindings for Core Image. So, this is combining the high-performance rendering of Core Image with the flexibility of the Python programming language together. And by using Core Image, you also inherit its support for both iOS and Mac OS along with more than 200 built-in filters. Let's take a look at what's under the hood of PyCoreImage. So, PyCoreImage is made of three main pieces. It uses Core Image for its rendering back end. It uses Python for the programming interface. And it had -- it also has a thin layer of an [inaudible] code to allow interoperability with your existing code bases. We believe PyCoreImage can now allow you to reduce the friction between your prototyping and product-ready code. If you'd like to stay in a Swift-centric environment, a lot of that can be done as well using Swift playgrounds, and we encourage you to look at a session on creating your own Swift playgrounds subscription. All right. Let's take a look at the main components of PyCoreImage. So, PyCoreImage leverages Python bindings for objective C, PyObjC and interestingly, we've been shipping PyObjC since Mac OS 10.5 Leopard. It was initially implemented as a bidirectional bridge between Python and Objective C and [inaudible] in the context of Coco app development. But since then it's been extended to support most Apple frameworks. The calling syntax for PyObjC is very simple, you take your existing Objective C code and you place columns with underscore. There's a few more intricacies and I encourage you to look at the API if you'd like more information. But let's take our CIVector class as an example here. So here we have some Objective C code where we create an instance of a CIVector by calling CIVector, Vector with X, Y, Z, W. Let's take a look a the PyObjC code. It's very similar. We import the CIVector from the Quartz umbrella package and we can call a vector with X, Y, Z, W and the CIVector class directly. One thing you may note here is that the code does not exactly look Python-like. And so, we're going to address that in just a few minutes. Now, let's take a look at the [inaudible] gram for PyCoreImage. So the rendering back in is then using Core Image, and Core Image is very close to the hardware, so it's able to redirect your filtered calls to the most appropriate rendering back end, to give you as much performance as possible. PyObjC lives on top of Core Image, and it can communicate with it through the Python bindings for Core Image by the Quartz umbrella package. And the Quartz is a package that also contains a variety of other image processing frameworks such as Core Graphics, and all the classes that's using Core Image, such as CIVector, CIImages, and CI Context. PyCoreImage lives on top of PyObjC, and it provides-- essentially leverages PyObjC to be able to communicate with Core Image and makes a lot of simplifications under the hood for you, so that you don't have as much setup code when you're working with Core Image. And we'll take a look at this in just a moment. A lot of it is done through the class CIMG that you can also use to interpret with NumPy via vendor call. And you can also wrap your NumPy buffers by using the class constructor directly. All right. So, let's take an example how you can apply a filter using PyCoreImage, and you'll see just how simple and powerful the framework is. So, the very first thing you want to do is import your CIMG class from your PyCoreImage package, which we can then use to load the image from file. Note that at this point we don't have a pixel buffer. Core Image creates recipes for images and in [inaudible] the recipe is just giving instruction to load the image from file. You can create a more complicated graph by applying a filter, by just calling the CI filter name on it and passing the input primaries in this case, a radius. And we can that we are assembling a more complicated graph. And if we zoom on it, we can see that we have out blur processor right at the middle. If you want to get your pixel buffer representation, what you can do is call render on your CIMG instance. And what you get out is a proper unit by buffer. So, to make that possible, we need to make a few simplifications on how Core Image is called or do a bit of the setup code for you. So, for those of you who are already familiar with Core Image, this will not come as a surprise, but for those of you who are not familiar, please stay with me until the end. You'll see the few simplifications we made, and that should become very clear. So, Core Image is a high-performance GPU image processing framework that supports both iOS and Mac OS as well as a variety of rendering back ends. Most pixel formats are supported. That, of course means bitmap data as well as raw files from a large variety of vendors. Most file formats are supported, so, like I said, bitmap data and raw from a large variety of vendors, most pixel formats are separated. So, for example, you can load your image in an unsigned 8-bit through your computation and half float and during your final render in full 32-bit float. Core Image can extract image metadata for you, for example, capture time; exist tags, as well as embedded metadata such as portrait map and portrait depth information. Core Image handles color management very well. This is a difficult topic on its own that a lot of frameworks don't handle. Core Image supports many battery conditions, infinite images, and has more than 200 built-in filters that you can use, so you don't need to invent the wheel. All right, so I don't think I need to convince you that that's a lot of information and if you're trying to use Core Image in your prototype and your workflow, the learning curve can be quite steep. So what we did is we kept the best of that list, made a few simplifications, which, remember, these simplifications can all be overridden at one time. And since we'll be giving you a weighted code, you can actually hardcode these changes in if this was your prototyping stack. The first thing we did is that we still have the high-performance feature of Core Image. We still render to a Metal backend. Most all formats are still supported in and out and we can still extract capture time of the data as well as portrait depth and matte information. Last but not least, you have more than 200 built-in filters that you can use. The first change we made is that by default, all your renders will be done using full 32-bit float. Second change, everything will be done using SRGB color spaces. Third, all the boundary conditions will be handled with clamped and cropping. What that means is, if you're applying convolution or creation, for example, your image will be repeated infinitely. A filter will be applied, and the resulting image will be cropped back to your input size. This is a setting that can also be overridden at one time. Finally, infinite images become finite so that we can get their pixel buffer representation, and that's what really PyCoreImage is under the hood. So, before looking at a great demo of all of this in practice, I just want to go through quickly a cheat sheet for PyCoreImage. So, let's have a look at the API. So, as you saw earlier, we import the CIMG class from the pycoreimage package. We can use this to load images fromfile [inaudible]. Here's a Swift equivalent for those of you who are wondering. You can use CIImage for contents of file. You can use fromfile to load your portrait matte information directly as well as your portrait depth by just using the optional arguments usedepth and usematte. You can interpret with NumPy by wrapping your NumPy buffers in the CIImage constructor or calling render directly under CIImage instances to go the other way around. If you're in Swift, there's a bit more set of coding to do. You need to first create a CIrender destination. Make sure to allocate your buffer previous. Make sure to give the right buffer properties, initiate and create an incidence of the CI Contex and Qtest render. So, all of that is handled for you under the hood. Core Image also supports residual images, such as creating images from a color or creating an image from a generator. Let's have a look at how to apply filters now. So, applying a filter has never been easier. You take a CIImage instance, call the filter name directly on it and pass the list of input primaries. Every CIImage instance is augmented with more than 200 lambda expressions, which directly map to the Core Image filters. If you're in Swift, this is the syntax you've seen before I'm sure, applying filter, passing the filter name as well as the list of input arguments as a dictionary of key value pairs. To apply kernels, you can go applykernel in your CIMG instance, passing of the source string containing your kernel code and the list of input parameters to that kernel, and we'll have a look at this in just a second. Then you just specify the extent in which you're applying that kernel, as well as a region of interest from which you're sampling in the buffer where you're sampling from. PyCoreImage provides a selection of useful APIs that you can use, such as a composite operations. Here is a source-over as well as geometrical operations such as translation, scaling, rotation, and cropping. All right. I just want to spend a bit more time on the GPU kernels because that's an extremely powerful feature especially for pro typing. So what we have here is a string containing the code to a GPU fragmentor. And what we have there is essentially a way for you to prototype in real time what that effect is. This is an example of five tap Laplacian and we're going to be using this for sharpening. So, we make five samples in neighborhood of each pixel. Combine them in a way to compute a local derivative, which is going to be our detail, and we're adding in on -- back on top of the center pixel. I don't want to focus too much on the filter itself, but how to call it. So, we call it black kernel on our CIMG instance. Bass source code, that's just sitting above using a triple [inaudible] python string. [Inaudible] the extent in which we're going to be applying the kernel. And define the region of interest is along the expression that we're going to be sampling from. If you're not familiar with the concepts of domain of destination as well as regions of interest, I encourage you to look at online documentation for Core Image as well as previous WWDC sessions. But here is the convolutional kernel, we are reading one pixel away from the boundary, so we need to instruct Core Image if we're going to be doing so, so that it can handle boundary conditions properly. All right. So, that was a lot of information and looking at APIs is going to always be dry, so let's take a look at a demo and let's put all of this into practice. All right. So, during that demo I'll be using Jupiter Notebook, which is a browser-based real-time Python interpreter. So, all the results you're going to be seeing are rendered in real-time using Core Image in the backend. So, none of this has been pre-computed; this is all done live. So, the very first thing I want to do here is import the utility classes we're going to be using, the most important one being the CIMG class here for my PyCoreImage package. Then, we just have a bit of setup code so that we can actually visualize images in that notebook. Let's get started. First thing I want to show you is how to load images in. So, using from file here, we see that the type of my object is a PyCoreImage CIMG. And we can see that it's backed by an actually proper Core Image object in the back. We can do a render on our image and have a look, I would -- its actual pixel representation using Matte [inaudible] lib here. This is our input image, and now I want to apply a filter on it, so let's take a look at the 200 plus filters that are supported in Core Image. Let's say that I'm interested in applying GaussianBlur here, and I'd like to know which parameters are supported by that filter, so I'm going to call inputs on my CIMG class, and I see that it supports input image -- I shouldn't be surprised -- as well as an input radius. So, I'm going to do just that here. Take my input image. Apply GaussianBlur filter on it with a radius of 100 pixels, and then show the two images side-by-side. So, pretty easy, right? Okay. Let's keep going. Like I mentioned earlier, you can generate procedural images using Core Image. So, we'll take a look at Core Image generators. And the first thing we do here is we call from generator -- specify the name of our generator. In this case, CIQR code and pass them in the message that we're trying to encode. Here it is in real time, so I can do changes to that message and see how that affects the QR code that's being generated. Core Image also has support for labeling your images, so you can use a CI text image generator to do that. So, here's the example here. WWDC and using the SFLO font. All right, let's keep going. As I mentioned, we support interoperability with a -- to and from NumPy, so this is the first thing we're going to do here. We're going to start with an image and apply some interesting and non-trivial affect to it. In this case, a vortex distortion. Next thing we'll do is, we'll render that buffer getting NumPy area out of it. You can see its type here as well as its shape, its depth, and a few statistics on it. It's minimum, median value as well as its maximum value. We can also go the other way around and go from NumPy to Core Image. To do this, let's start with a NumPy array-- that's non-trivial. In this case, a random -- a random buffer where 75% of the values have been [inaudible] to black. First thing I do here is wrap my NumPy array into my CIMG constructor, and we can see that we have again, our CIMG class instance and the backing CIImage. Now that I have my CIImage, I can apply a variety of filters on it. So, the first thing I do is I'll apply this blur. I'll use a wire filter; a light tunnel. Change of contrast in my image. The exposure adjustment as well as the gamma value. Let's take a look at these filters side-by-side. So at this blur, light tunnel, exposure adjust; gamma adjust, here's our final effect. Pretty fun and really easy to work with. So we -- let's put it all together. So, I'm going to start with a new image here, and what I'll be showing you in this demo is, how we can do bend processing. For those of you who are filming here with slicing images in Python, this is exactly what we're going to be doing. We'll define bands or slices -- horizontal slices in our image, and we'll only be applying filters on these. Let's take a look at a code first. This is our add band function here. And we can see that the very bottom of it, we render our image in two composites, which is an actual NumPy buffer. But the right-hand side is a CIImage. By using slicing like this, we forced Core Image to only do a render in that band, not in the entire image, thereby being much more performant. So, let's do this and create five different bands into our image and show the final composite. Pretty amazing. And we've got other labels on top as well, which correspond to the filters being applied. It's really that simple to work with PyCoreImage. All right. And I mentioned performance earlier, so let's take a quick look at this. First thing I want to show you is that whenever you call render on your CIImage instances, the NumPy is baked and cached under the hood. For example, here we create an image where we scaled down as well as applied GaussianBlur, so the first call took 56 milliseconds; the second one only 2 milliseconds. And let's take a look at large convolutions as well. Core Image is extremely fast and is able to handle large convolutions as if it was nothing. Here we're using CIBlur, CIGaussianBlur with a radius -- with a sigma of 200 -- a value of 200 for a sigma, which is huge. Just to give you a sense here, as I was look -- showing you the image, I'm actually executing the equivalent using scikit-image. And we had a 16 seconds running time. But this time the same thing using CoreImage; 130 milliseconds. Yeah, it's that fast [applause] -- 200X, yeah. Thank you. All right, let's keep going. So, one of the most powerful features of PyCoreImage is its ability to create custom GP kernels inline and execute them on the fly and modify them on the fly. So, let's take a look at that. All right. So, the first thing I want to show is how to use color kernels. So, color kernels are kernels that only take a pixel in and spit a pixel out and don't make any other samples around that pixel. So, here's our input image and here's our kernel. So what we actually get in is a color and we turn a color out. So, let's take a look at this effect here. I'm going to be swapping my red and blue channels with my blue and red channels and we'll be inverting them. Not a terribly exciting effect, but what I want to show you is that I can do things like, start typing away, and say, maybe I want to scale my red channel by my blue channel and I want to just play with the amount of scaling I'm playing here, so we can go from .25 to pretty high values if we want to and generate interesting effects here. It's extremely powerful, and this all [inaudible] time so you can really fine-tune your filters this way and make sure you achieve the effect that you're looking for. Let's take a look at a more complicated kernel here. So, we'll look at a general kernel, which is a bit like the [inaudible] I showed you earlier, which is a kernel that makes additional taps in the neighborhood of each pixel. So start with an image from file, which is the same image we saw earlier, and we have our kernel code here. Without going into the detail, this is a bilateral filter, which is an edge over blurring filter. So let's just get the code in and use apply kernel with some parameters that will allow us to get this very nice effect. And what we did here, essentially, is clipped the non-redundant high frequencies in the image. And if we take a look -- let's take a look at this a bit more closely. Look at a crop here. We can see how the strong edges are still there, but the fine frequencies that are not redundant were washed away. And bilateral filter can be used for many, many different purposes. In this particular case, we'll use it to do sharpening. And to achieve sharpening with this filter, we can simply take the image on the left and subtract the image on the right, giving us a map of high frequencies or details of the image. Let's do just that. So, here what I'm doing is I'm rendering my image, it's an NumPy buffer. Rendering my bilinear, my filtered image and we're subtracting them together using the operator overloading that's provided with NumPy. Let's take a look at the detail layer. So, if you have detail on your left-hand side for the entire image and a crop for the center of the image. Now, what we can do with this is we can add it on top of the original image. We're going to be doing just that here. We're going to be adding it twice. By doing this, we achieve formed sharpening. It's really that simple. If I wanted, I could go back to my filter kernel string and start hacking away and making changes there in real time. The other thing I wanted to show you is how to load metadata from your images. So, here I have an image that has portrait effect matte loaded in, as well as portrait depth data. Here are the images side-by-side. The image on the left is the RGB image. In the center is the depth data. On the right-hand side is a high-quality portrait effects map, which we introduced in another session today. We can also look at the exist tags directly by looking at the underlying CIImage from CIMG instances and calling properties. Here, we get information pertaining to the actual capture itself. Like I said, we introduce the portrait effects matte at another session, Session 503, so I highly encourage you to look at it. So without going into the details here, I'm going to be choosing this filter. If you are interested to know how we did this, I highly encourage you to take a look at this session. Pretty fun stuff [applause]. Thank you. All right. Let's go back to the presentation. I want to switch gear a little bit here and talk about bringing CoreImage and CoreML together. If you would like to get more information about working with Portrait Matte and Portrait depth information, I encourage you to look at session on creating photo and video effects [inaudible]. All right. Let's look at bringing Core Image and CoreML together. This year, we're really excited to announce that we're coming up with a new filter, CI CoreML model filter. It's an extremely simple, yet very powerful filter that takes two input. The very first input is the image itself with a filter and input CoreML model, and you get an output, which has been run through the underlying neural network. It's really that's simple; extremely powerful. Just to show you how simple the code is, let's take a look at Swift. So, we have an input image on the left-hand side, all we need to do is call applying filter. But as a new filter that we've entered -- are introducing this year and give your [inaudible] in the model. It's really that simple. And if you'd like to look at other ways to leverage machine learning in your image processing applications, I encourage you to look at the other sessions on A Guide to Turi Create as well as Vision with CoreML. All right. On a related topic, one of the common operations we carry in our training datasets in machine learning is data augmentation. And data augmentation can dramatically increase the robustness of your neural networks. In this particular case, let's say we're doing object classification and we're trying to determine whether that image is a bridge or has water in it. So, augmentation is on your original trend dataset will increase yet the number of images you have in that dataset without needing to gather new images. You essentially get them for free. So, there's many operations you can carry. One of them is just changing its appearance. For example, the tense, the temperature and the white point of your image. Changing the spectral properties of your image by adding noise. Or changing the geometry of your image by applying transforms. Well, it turns out all of these are trivial to achieve using Core Image. Let's take a look at a few filters and how you can use them for your data augmentation purposes. So, we have our input image on the left-hand side here. And we can change the temperature and tint using CI Temperature and Tint. We can adjust the brightness, contrast, as well as saturation in your images using CI color controls. Change the frequency spectrum of your image using CI dither as well as CI GaussianBlur. And change the geometry of your image using affine transforms. Let's take a look at all of this in practice. All right. So, we're back to my Jupiter notebook here. Same setup as before. First thing I want to show you is how to [inaudible] augmentations using Core Image. So, we're loading an image in and we're going to define our augmentation function here. And what we'll be doing essentially is sampling from a random space for each of the filter I've defined here. So, we'll be applying GaussianBlur, scaling rotation, a few adjustments -- exposure adjustments -- fibrines as well as dithering for noise. All right? Let's cache that function in and let's have a look at a few realizations of that augmentation. So my slider here controls the [inaudible] that I'm using in the back end. All right, pretty cool. I'm not sure how efficient that is, so here I'm going to be processing 200 of these augmentations in real time and we'll take a look at here -- how they are being -- actually saved to disc in real time. So let's just do that, just to give you a sense of how fast that is. That's really powerful. All right. This next thing I want to show you is how to use CoreML using Core Image. And first thing you do is to load your Core ML model in, which we did here. We have a glass model, which we're going to be using to generate interesting effect. So, let's start with the procedural image. We've seen this one before. And then to make it a bit more interesting, we'll add some texture to it. So, we'll be adding multi-band noise on it as well as some feathering and some [inaudible]. All right, so this is the input image we're going to be feeding to our neural network alongside the other -- the CoreML model that we have pre-trained. All right? So, let's run this. And -- There you go. WWDC 2018, just for you. All right. On that note, I want to thank you for coming to this session today. I hope you enjoyed this as much as we enjoyed preparing these slides for you. I highly encourage you to come and talk to us tomorrow at Core Image Technical Lab at 3:00 pm and thank you very much.  Good morning, and welcome to What's New in Managing Apple Devices. I'm Todd Fernandez, and I'm very pleased to be here with all of you here in the hall this morning, as well as those of you watching this video now or in the future. I'd like to cover all the things that have changed in the past year, since we last did this at WWDC 2017. But, before we dive into all of those details, I want to take a moment to take stock of how far we've come. This year, we are very proud to celebrate 40 years of Apple in education. And, it's fascinating to see how much has changed. From the audacious goal of an Apple 2 in every school in 1978, to an iPad or MacBook in every student's hands in 2018. But, it's even more important to consider how much has remained the same over those tumultuous 40 years. Apple had a unique insight into how technology could inspire people and unleash their creative genius. And, we believed technology could help teachers deliver unique and personalized experiences to all of their students. We have never stopped believing in this goal, and never stopped working hard to achieve it. Over the years, we have created a number of tools to make it easier for schools to put Apple devices into the hands of each of their students. And, I want to highlight one of those now. Classroom is now two years old, and teachers really appreciate the power it puts at their fingertips to accelerate teaching and learning without technology getting in the way. But, we want to provide our tools on whichever OS our customers choose. So, we were excited to announce at our March education event, that Classroom was coming to the Mac. And, it looks like this. I think Classroom looks fantastic on the Mac. It has the same great feature set already available on iPad, plus some surprises. But, instead of describing those to you, I'd like to invite Curt and Raheel up to actually show them to you. Guys? Thanks, Todd. Team's been working hard to bring the great Classroom experience to the Mac, and we're thrilled to share it with you today. To get started, I can just go to Launchpad, and click on Classroom. As you'd expect in a Mac app, all the actions in Classroom for Mac are available in the toolbar, from menus, and with keyboard shortcuts. For example, I can hit Command-T to bring up my teacher info. Oh, that photo's a bit much. Let's go with something a little more laid back. That's better. I can hit Shift-Command-N to bring up a new Class sheet. We have all the great icons and colors that teachers use in Classroom for iPad. I have enough Classes for this demo, so I just hit Escape to dismiss that. In the Classes view, I can drag and drop to rearrange my Classes. And, I can just double-click to start a Class session. In a Class session, we have all the actions that teachers expect from Classroom for iPad. They're available in the toolbar, from the Actions menu, and of course, with keyboard shortcuts. And, because this is a Mac app, and I'm demoing on a Mac, I can use QuickTime to show you what this looks like from a student's perspective. So, now on your left, we have Classroom for Mac, and on the right, we have Raheel's student iPad. Let's navigate the students to a website. I hit Command-G to bring up the Navigate sheet. Now, I could use my mouse to pick the site to go to here. I could also use the arrow keys. Let's open this great National Geographic kids' site. So, all my student iPads are taken to this site. And, I can even click this link to go to the same site in Safari on my Mac. Now, while I'm browsing the site, I might find something else I want to share with my students, like this cool Monarch butterfly page. I can click the Share toolbar item, choose to Share via AirDrop, and in the list my class, and any manual groups I've created will show up. So, I can share this page. Another thing that I love about having Classroom on the Mac, is that it gives me a great way to keep my students on task. I can see what apps they're in by looking at these icons next to their avatars. And, through these Smart Groups that show what students are in each app. It looks like Raheel's getting distracted. Brooklyn and Ella are still in Safari, but it looks like Raheel's browsing the App Store. So, I can use another cool feature of Classroom. If I double-click on Raheel's avatar, I can see his iPad screen on my Mac. And, while I'm looking at a student's screen, I can perform actions on just that student's iPad. So, in the Actions menu, I can choose Open App, and then Safari, and click Open, to bring Raheel back on task. Students can share links and files from their iPads to the teacher, too. For example, I can share an image from Photos, like this great bear picture I found. All I have to do is tap the Share button, and then tap Doctor C in AirDrop to share. When a student shares an item with me, the Inbox button in my toolbar will light up to let me know that there's something new there. And, I can click to see a list of shared items. And, on the Mac, I can tear off this Inbox, and place it next to my Classroom window. And, this is great, because I can get at the items that my students are sharing with me, while still keeping an eye on the class as a whole. And, I can double-click to open any shared items. Well, that's a great bear photo, Raheel. I know, right? It's unbearably great. Indeed. Besides Smart Groups, I can also create groups manually in Classroom. I'll hit Command-N to bring up the New Group sheet, enter a group name, and select the students to include. Manual groups are a great way to get different groups of students started on different tasks. When I'm done with my class session, I can hit End Class. That ends my session. I'm presented with this great Summary view. I can see all the apps that my students used during class, and for each app, a timeline of when they were in that app. I also see all the shared items, like the great bear photo. And, for each student, I get a timeline of all the apps they were in, and when they were in those apps. So, that's Classroom for Mac. We think with drag and drop, keyboard navigation, toolbar items, the menus and the tear off inbox, the teachers are going to love the power and convenience of having Classroom on their Macs. And, of course, any macOS Mojave app would not be complete without support for Dark Mode. So, that's Classroom for Mac. It's available in public beta now, and from the App Store this fall. Thank you. Todd? Thank you very much, guys. Doesn't Classroom really look great in Dark Mode? I think teachers are really going to appreciate having it here, on their Macs. Now, while education has been part of Apple's DNA from the very beginning of our company, over time we've broadened our audience to include the enterprise. And, in 2018 and beyond, we want to empower people in both schools and businesses around the world to manage all of their Apple devices, from iPad to iPhone to MacBook to Apple TV, as well as all of their apps, whether they're in the App Store, or custom enterprise apps. With the same technologies and tools running on whichever OS they choose. And, that's why today I'm going to organize the content, first covering all the developments organized by the common features available on all our OS's. And then, continue through capabilities specific to one or more OS. And, just to give you a little bit of a legend ahead of time, you'll see new badges up in the upper right-hand corner on slides where all of that content is new in our fall releases. There will also be specific version badges on some slides for things that have already shipped in one or more releases, and if there's a slide that has some with version badges, and some bullets without, everything without a version badge is new in a fall release. So, with that, let's get started. The first thing is to get your devices enrolled for remote management. And, schools use Apple School Manager to do that. Take advantage of the device enrollment program to enroll all of their devices with the correct MDM server. And, I'd like to bring you up to date on all the changes in Apple School Manager over the past year. First, I want to call out, is now every student-managed Apple ID comes with 200 gigabytes of iCloud storage. Definitely. Creating more great content, presentations and documents. We've also made it much easier for schools to create and distribute passwords for those student accounts. We've dramatically modernized and streamlined the experience of purchasing apps and books in bulk, as well as managing those licenses over time. Being able to transfer them from one content manager to another, both within a location, and to another location, as your needs change. And then, finally, a big customer request was to enable you to set a default MDM server for a particular device type, making it very easy to manage all of your Macs with one MDM server, and all of your iOS devices with another. But, we didn't want to just make all this great experience available to schools, we also wanted to bring it to the enterprise. And so, I'm pleased to let you know that we have now created Apple Business Manager as well. It offers the same great features to manage accounts, purchase apps and books, and manage device enrollment, with one important caveat on the accounts. Apple Business Manager allows you to create accounts for all of your administrators to manage these other features, but it doesn't not support creating managed Apple ID's for all of your employees. It does offer the same, great, integrated apps and books purchasing experience, including the license management features, and all of the new features for managing device enrollment. And, it should very familiar to anyone who's ever seen Apple School Manager. To allow you to create accounts for your administrators, purchase and manage your apps and books licenses, and manage your MDM servers, including default MDM type. So, until this week, Apple Business Manager has been in a private beta. But, I'm excited to let you know if you haven't seen the announcement already, we actually launched the United States yesterday, and our global launch will be in two weeks, on June 20th. We're very excited about bringing all of this to, this integrated experience to all of our enterprise customers. So, where and when will all that happen? Well, today, Apple School Manager is available in 34 countries, and that's our global launch in two weeks. But, we didn't stop there. I'm very excited to announce that in fact, this summer we're also going to expand into 31 more countries around the world, bringing us to 65 with support for Apple School Manager and now Apple Business Manager. We're also adding book support in Canada and Germany, which currently only support apps. So, I know the map looks great. You can kind of see where the expansion was, but I thought it would be much easier to actually see a list of countries, so you can see if your country will now have support for Apple School Manager and Apple Business Manager. And, one thing I noticed, looking at this list of countries-- this is a World Cup year-- that seven of these countries actually have a team in the World Cup, unlike, sadly, my country, United States, but that means I'm in the market for a new team. So, go Iceland. I wanted to tell you also about another expansion of one of our deployment program features. You can add credit to your account via purchase order to allow you then to purchase apps and books later. Purchasing from either Apple or a reseller. And, we have just launched last week in 10 new countries in Europe. Again, here's the list. I believe that doubles our access to this program as well. The next slide is an evergreen topic. Every year we add new Setup Assistant panes in one or more of our OS's, and this year is no exception. And, we continue to want to enable organizations to configure the experience they provide to their users as they set up their devices. So, in the spring's release, we added a new privacy pane on all three OS's. We also added a new iCloud storage pane for macOS. And, two new panes for tvOS. And, in iOS 12, there'll be new panes configuring some of the new features, some of which you heard about earlier this week. We want to continue to give you that option to get your users right to the desktop or home screen as quickly as possible. So, next, I'd like to cover two updates for how MDM servers should handle both enrolling devices, as well as ongoing communication with each of those enrolled devices, starting with Apple Push Notification service. So, if your MDM solution is still using the Legacy Binary Provider API, we definitely want you to adopt the new, modern, HTTP/2 API, which is far more capable and efficient. You can read all about it in the Communicating with APNs section of the Networking documentation. And, since I brought up documentation, I wanted to take a moment here to calm the waters about the MDM and Profile documentation. The documentation team is going through a publishing tools transition, and the disclaimer that you might have noticed looking at the documentation this week is not an indication of any change in commitment to providing up-to-date documentation for all these technologies. In fact, the only reason you saw that disclaimer is because we did update both guides on Monday for both the MDM protocol, and the Configuration Profile reference to cover the changes I'm going to talk about today. So, next topic is security. There'll be number of these throughout today's session. This is something that Bob talked about last year, that we were going to begin requiring transport security this year. And, in fact, we're going to do that in both iOS and macOS this year. Your SCEP server should make sure to advertise its capabilities, so that we know what the highest level of security you support, and don't have to fall back to a lower security encryption algorithm. We stopped supporting DES last year. Definitely supports one of the modern and much more secure algorithms. This year, I also wanted to give you a note on how you can verify that your server is ready for this transition as we roll out the new versions of iOS and macOS this year. You can use NSCurl against each of the URL's that your server supports, and verify that there are no issues that the diagnostics find. Now, let's turn to the new management controls, commands and settings. And, I'd like to start, again, with everything that's supported on two or more of our OS's. You may have heard about some of the new password features that we're introducing in this fall's iOS and macOS releases. And, of course, we want to enable you to manage them, via profile. So, the great new automatic strong passwords and AutoFill features within Safari and within apps, we have a new password AutoFill restriction that also covers the existing Safari AutoFill feature and restriction. We've also added a new password sharing restriction that covers all versions of the password sharing feature, including the previous WiFi password sharing. And, this restriction prevents you from sharing your password with others. The last bullet on the slide, password proximity requests. This restriction actually is supported on tvOS as well, because this feature, or this restriction, prevents your device asking others for their password. And, if you didn't attend, you can check out the video of the password and AutoFill session which occurred earlier this week. We added a new restriction to prevent users from modifying the Bluetooth restriction last fall. And, in the spring, we added a new MDM command to actually be able to set the value of that setting. And, I'm even more pleased to let you know that it's not in Seed 1, but I saw the change go in yesterday, that this command will now work even if you have that restriction in place. And, we think this will be great for schools deploying Classroom, and in other situations where you want to make sure that Bluetooth is enabled, or disabled, as the case may be. Big customer request, we have enabled OAuth authentication for exchange accounts configured via profile. That's in iOS 12, and in macOS Mojave. And, a really big one, managed software updates that we brought to both OS's this spring. Thank you. One person's excited about that. And when I talked-- just because I'm excited about it, talk a little bit more in detail. It consists of two different restrictions, one to enable the feature in the first place, to put the device so that it will delay when the user will see a new update once we release it. And, an optional parameter that you can configure the delay period from 1 day to up to 90 days. If you don't specify that setting, it defaults to 30 days. The scheduleOSUpdate command has been supported on both platforms for a long time, and on macOS, it's always allowed you to specify which update you actually wanted to install on that Mac. But, in iOS 12-- I'm sorry, this spring, in iOS, we added the ability to specify a version number for just the iOS version that you have tested with all the software important for your organization's devices. Wq also added a new Apple software lookup service, so that your MDM server can look up the eligible versions for a particular device at a particular time. And, we have documented that API to look that up, so that your MDM solution can populate the UI presented to the admin. It's in the MDM protocol guide. Alright. And, that brings us to the end of our common section. And now, I'd like to talk about some iOS-specific changes. Again, starting with security. Now, some of you may have heard about this change that we started to make in iOS 11.3, and now it's back in 11.4.1, and iOS 12. And, of course, we wanted to make this manageable as well, beyond the switch that's in the UI in iOS 12. So, there's a new restriction that allows you to control this feature, and whether USB accessories can still connect devices if they're locked. And, of course, Configurator kind of relies on devices being able to connect via USB, so we have implemented a special behavior for those devices. When Configurator prepares a device to supervise it, but not enroll it in MDM, it will automatically install a profile that installs this restriction, and allows those devices to continue connecting to the Mac running Configurator. Alright. Another topic which is not new, we've talked about it for a number of years, last year Prodop [assumed spelling] told you that we are going to start honoring the certain set of restrictions which were created before supervision existed, but really should only be honored on supervised devices. I want to make clear that these restrictions are not going away. They're still going to be usable, but they will only be honored on supervised devices. But, after hearing your feedback, we decided to delay one more year, and we'll make this change next year to help make a smooth transition. And, we've also come up with an upgrade and migration policy that we think will further smooth this transition. Essentially, if a device which is not supervised has one or more of these restrictions in place, they will continue to be honored even after upgrading to the iOS version that includes this change until they're wiped. So, we'll remember, and we'll continue to allow you to use them until that device is wiped, allowing you to time your refresh more conveniently. Of course, any new device configured, or if you wipe the device and restore from a backup, they will get the new behavior, where each of these restrictions is only honored if the device is supervised. But, of course, if you're wiping a device, that's a great opportunity to go ahead and supervise it before you configure it again. Just to refresh your memory, this is the list with one minor change that we took advantage of the fact that we're giving you one more year. That in fact, the three Siri restrictions should also only be honored on supervised devices, so this is the list, and we really mean it this time. We're going to do it next year. Be prepared. Alright. Managed Open In, it's a great feature, most-- more used in enterprises, and we've made a number of improvements, both in iOS 11.3, and iOS 12, to make sure that the boundary we've established between managed apps and unmanaged apps and sharing files and data between them is-- behaves the way everyone would expect. This included making the context API respect the boundary in iOS 11.3. Which of course was exactly what many customers wanted, but we know that that did have some challenges for some organizations that were deploying in a specific way. And, I'd just like to make clear, that if you are using Managed Open In, and want a managed app to manage Contacts, you need to deploy that managed Contacts source as a managed source. Alright. Now, let's turn to some of the new settings we've been adding in this year's software releases. A bunch of things we added in iOS 11.3. I already mentioned allow USB accessories while device is locked restriction, completing our set of restrictions to allow you to get classroom behavior on a supervised device, even if it's a teacher-created class. We'll talk a little bit more about the remote pairing later on in the tvOS section. And, that last one I really wanted to mention, because this is, again, another long-standing customer request, that both schools and businesses love the Home Screen layout payload, but they were also using WebClips. And now, in iOS 11.3, you can use WebClips in Home Screen layout payload. Thank you very much. Moving on to the changes in iOS 12. Added a couple of new notification types to the Notifications payload. And, another big customer request, we had a lot of schools in particular that wanted to prevent students from changing the date and time, and there's a new restriction that essentially turns on set date and time automatically on supervised devices. Now, this feature will, of course, only work if we can reach the time server, or a cell tower, or location services is enabled. But, with that caveat, we think this is going to meet the need. We've also made a lot of improvements to how S/MIME is managed for Mail and Exchange accounts configured via profile. Giving users more flexibility about when to sign and encrypt, as well as an important changes to allow them to update the certificates that they're using for either feature. Even when their account has been configured via profile. We also took the opportunity to rename a number of keys to clarify the purpose. Of course, the existing keys are still honored for now. But, please check the documentation and update your implementations. There's also a number of new settings in the VPN payload for configuring IKEv2 connections, managing your DNS server settings. And, one important option that we added to the Erase Device command. Allows you to skip proximity set up on your way back through Setup Assistant, further configuring the device enrollment experience for an end user. This is really important for deployments for guests that are using your devices, and if you're using device enrollment to provide a fresh experience for each new guest using that particular iPad or other device. Thank you, Eric. And, finally, due to macOS server deprecation, we have removed support for the macOS server account payload in iOS 12. If you're still using some of those services as you're transitioning to a new solution, you can replace those account configurations with normal account payloads. Next, I'd like to give you a few tips on troubleshooting issues with delivering and executing MDM commands on enrolled iOS devices. There are a number of logging profiles which you can obtain at the link at the bottom of the screen. And, all of the URL's are going to be available at the More Information link, which will be at the end of the session, so you don't have to feverishly copy those down. Depending on what type of a problem you're investigation, you can install either both the MDM and/or the Apple Push Notification service profiles. Once you've reproduced the problem, you can get those logs using Console or Apple Configurator 2. And then, look through the logs by process, depending on what kind of a problem you're investigating. Whether you're looking at communication, or connection issues. Installing profiles or apps, or working with Shared iPad. Now, next I'd like to turn to cover some topics that are a particular interest to app developers, who would like to sell their apps to schools and businesses. Or, as I like to call it in honor of the biggest fan of initial iPhone, say it with me, the developers, developers, developers section. I'll first cover some topics specific to education apps, before continuing with some topics for enterprise apps. At our education event in Chicago in March, we announced a brand-new app for teachers called Schoolwork. Schoolwork allows teachers to easily share content with their students, leveraging the power of your apps. And then, they can view student progress across all of their work within those great apps. Helping them to tailor instruction to the needs of each of their students. And, also allowing them to collaborate and provide instant feedback on what their students are learning quickly, and where they might need a bit more support. Now, all of this is based upon a new framework called ClassKit. And, that's where you come in. Apps which adopt ClassKit integrate with Schoolwork in order to help teachers discover assignable activities within your app, to take students directly to the right activity for what they're supposed to be working on. And, most importantly to securely and privately share that progress data as they work through those tasks with their teachers. Now, they had a session yesterday, and I encourage you to watch their video. Alright. While I have, hopefully, your attention, I'll, you know-- MDM developers are developers too, so this is really more for them. But, I wanted to make sure I kept their attention. They didn't start tuning me out. So, the Roster API is how MDM servers can get class information from Apple School Manager. And, we have had this class name field for a while. And, we want to encourage you to use that as a display name in your MDM console, as well as what you pass along to Classroom via the education payload, the configure managed classes. This is because Schoolwork is using that field, and we'd like to achieve a consistent experience for teachers using both apps. The reason you might not be already doing this, is because of class name's history. Before January, it was there in the API, but we actually didn't return a value. In January, we began returning a value that was derived using logic in Apple School Manager. But, this month, we're going to start allowing administrators to configure that name based on how the school names their classes, and what the teacher will expect right within Apple School Manager. So, again, if you're not currently using that to configure the education payload for Classroom, please start, so we can achieve that great experience for teachers. Want to, again, remind you about shared iPad. If you want your app to be used in Shared iPad, need to make sure that it doesn't depend on any data being available locally on a new device. When a student moves, and signs into a new Shared iPad. Persist all of the app data to the cloud, whether that's our cloud, or your cloud. And, while we'd, of course, prefer that you test your app on a real Shared iPad, you can simulate this by deleting all the local data, and then making sure that your app still works well. Also like to encourage you to adopt managed app configuration. There are thousands of developers who have, and have created a number of shared schemas that can make it-- your app much more friendly to education and enterprise, by enabling them to create a customized experience for their employees or students, to customize the look of the app, or to prepare some custom data to warm up the app so they get the right experience at first launch. Here's the URL for the site to learn all about it. Again, that will be available on the More Information page later on. We also have a number of great enterprise partners, who've provided SDK's to make the power of their services available to your apps. From IBM's Watson to allow you to do machine-learning models, and we have great pages on our developer.apple.com website. They're really hard to figure out, because they're slash and then the company name. But, I encourage you to check those out, and see what you can do within your apps. And, finally if your app depends on network performance, for some time now, we've enabled you to configure that with profiles for enterprise apps and MDM solutions. But, this year we've added a number of new quality of service keys to allow you to fine-tune it even further. Encourage you to check out the networking session, if your app is sensitive to network performance, to find out how you can achieve the best performance on our platforms. And, that brings us to the end of our iOS-specific section. So, following the pattern established by this week's keynote, let's turn next to tvOS. Now, with tvOS, we, over the past number of years and releases have been playing a little bit of catch up and adding some of the great features for device management that we had previously had for iOS and macOS. And, I'm very pleased to let you know that we continued to do that this year. This spring, adding the ability to configure content restrictions, just like you can on iOS and macOS. And, enabling you to lock down which app, or which devices, can use the Remote app to manage a particular Apple TV. This is great in the classroom, so that the teacher can just have the Remote app on her phone, and you cannot need a physical remote in the classroom. No student would ever get up to no good with one of those. And, perhaps, even more important, some great key features of the device management experience. Being able to install App Store apps on Apple TV. Thank you. And, being able to update to the latest version of tvOS via MDM command, just like another Apple device. Thank you. And, we think that combining these new features with some of the features that we've added over the past few releases, you can do some amazing things with Apple TV, on its own, and in combination with other Apple devices. Provide new experiences to your guests. So, to illustrate the possibilities, I'd like to welcome you to the entirely fictional Hotel Cupertino. Such a lovely place. Where each enrolled Apple TV is programmed to receive commands from the hotel's MDM server. For example, including updating to the latest version of tvOS. And, installing a custom enterprise app to allow your guests to manage their experience, including ordering of room service, when they get a hankering for pink champagne on ice, or informing them about options for exploring the area. You can also provide your guests with great entertainment options, when they're actually spending time in your room, by installing App Store apps. And, this is the time when I would love to share a clip of my favorite show, Game of Thrones, with you, but we don't have time for that right now. So, assuming your guests actually ever leave the room, and take some photos, they can of course, use AirPlay to share those photos, and display them on the big screen in the room you've provided to them. So, this is just one example of what can now be done with Apple TV by managing it remotely. But, of course, there are many other types of businesses that could take advantage of these capabilities to provide amazing experiences to their guests, including integration with the Apple devices they're already bringing with them. We look forward to seeing the fruits of your creativity, even if it doesn't involve dragons. So, that brings us to the end of our tvOS section. And, last but not least, macOS, just like in the keynote. So, let's start right at the beginning, installing macOS. And, I'd like to make everyone aware of the great new command added to the macOS installer this spring, called startosinstall. If you're installing from and to your computer startup disk, you can use this command. And, it supports all the latest Mac hardware. It also includes some great features that allow you to install packages on top of the freshly installed macOS. And, an option to start fresh, and erase that partition before you install the new version of macOS. Of course, once you've installed the OS, you want to get enrolled for remote management. And, we received some feedback from some of our enterprise partners in particular, that they really prefer the iOS experience, and felt it was more user friendly. So, we're going to take that feedback, and simplify, and make the macOS MDM enrollment experience match iOS. It's not in Seed 1, but you will see it soon in macOS Mojave. I told you there were more security topics. Here's another one. We want to strongly encourage our MDM partners to take advantage of this new capability to make enterprise app manifest delivery more secure. The transition is easy because we're continuing to support the existing installApplication MDM command to install enterprise apps, but we really want you to switch to one of the new methods using the new installEnterpriseApplication command as soon as possible. There're two options. You can either specify the manifest right within the command, inline. Or, you can specify certs that we'll use later to pin our request to fetch the manifest. Please read up about this, and make the switch as soon as you can. Now, this is not actually a change so much, because none of these six payloads ever worked to install in a system profile. But-- because they only make sense in a user context. But, next year, we're going to start treating installing any of these payloads in a system profile as a hard failure. So, you have some time to prepare for that, and make sure you're not already doing that. Now, let's talk about some of the new things we've added in this year's macOS releases. Big customer request, you can now mark generated private keys as not exportable, so the user can't get access to them on their Mac. We added content caching last fall in macOS High Sierra. And now, you can configure it via profile. And, we've added a number of new controls for managing how smart cards are used on your organization's Macs. Thank you. Smart cards. Last fall, we introduced a new concept for enrollments on Macs called user-approved MDM. And, this is to protect features that should really only be available on an organization-owned Mac, and be tied to affirmative consent from a user or an admin, and not configured via rogue script. The first example was the kernel extension permissions. And, we introduced this in 10.13.2, although user-approved MDM enrollments was not actually required until 10.13.4. There will be more security features that will fall into this group. So, this is something that's going to be with us, and in fact, next thing I want to talk about also requires user-approved MDM. And, that's the additional support we made for testing apps for high stakes testing apps on the Mac, so they can achieve the controlled environment that they require. That support requires both an entitlement, as well as a Mac which has a user-approved enrollment. And, just like for iOS and tvOS, I'd like to give you some tips on troubleshooting issues with communicating with enrolled devices. Similarly, you can install the right logging profile for a managed client, or again, Apple Push Notification service. Get the logs using Console, and it's a lot simpler-- you can just filter for the Managed Client process on the Mac. But, I also wanted to highlight the profile's command line tool. It's a Mac, we're got Terminal. You can, of course, use it to install and remove profiles, but it also has some great features for verifying your deployment. The first we added this spring, allows you to verify whether the enrollment is user-approved, with the profile status command. And, new in macOS Mojave, there's a validate command that allows you to confirm, or tells you any differences between the device enrollment profile in the cloud, and what's actually configured on the device at that moment. So that you can more easily detect when your change that you know you've already made in the cloud hasn't been reflected on the Mac yet, or the device has become unconfigured. So, that brings us to the end of our content for today. And, I'd like to quickly sum up some of the takeaways for the different groups, and different audiences for this talk. Administrators, we hope you love having access in more places for Apple School Manager, and now Apple Business Manager to manage all of your organization's accounts, devices, and apps and books. Take advantage of all those new device management capabilities that we've talked about today. And, prepare for the security changes that will impact your deployments. For MDM developers, of course, we need you to support all these new features, because the administrators won't be able to take advantage of them until you do, in the solution that they've already paid you for. Please get on adoption of those security features to help us make sure that we are keeping the communication between enrolled devices and your product as securely as possible. And, finally, app developers. Take advantage of all these great technologies that we've made available to you on our OS's from ClassKit and Shared iPad for schools, to managed app configuration for all kinds of apps, and then the enterprise features for enterprise apps. We have a number of labs, one later today after lunch. And then, tomorrow morning. And, if you'd like to learn more about the new password and AutoFill features, they also have a lab tomorrow afternoon. And, with that, I'll thank you for your attention, and hope you enjoy the rest of the show. Thank you very much.  Good morning and welcome to the session, Live Screen Broadcast with ReplayKit. My name is Alexander and I'm very excited to be here and talk to you about ReplayKit concepts and new features that we bring to ReplayKit this year. We've got lots to discuss so let's get started. ReplayKit is a framework that allows you capture application screen audio and microphone content in real time or record to a video file that the users can later edit or share. And ReplayKit also supports live broadcast. For application generating content like games, ReplayKit provides tools that allow them to stream audio and visual content using [inaudible] and broadcast services. And for the broadcast applications ReplayKit provides ability to receive that content captured from other applications or entire iOS and encode and stream to the servers directly from the same device. ReplayKit delivers its content in high quality with lowest latency and very low performance overhead and better usage. It also safeguards the user's privacy by asking for approval before any recording or broadcast begins, and also showing a prominent indicator whenever content is being captured. In this session today I'm going to talk about the live broadcast functionality of ReplayKit and we're going to start with overview and talk about [inaudible] broadcast and iOS system broadcast, and then we are going to talk about the new API called System Broadcast Picker. Now, this is an API that allows you to initiate the broadcast directly from your broadcast application. After that we're going to discuss how you can actually implement the broadcast extensions and how that would work for you with the new API that [inaudible]. Also, we're going to talk about the protecting of the content of your applications from being captured. So let's start with overview of the live broadcast. As I said, ReplayKit allows you to broadcast your app screen, audio and audio content to search a third-party broadcast service directly from your iOS or [inaudible], and on iOS you can also provide voice or video commentary using microphone or camera. And all this content is absolutely secure and only accessible to the servers that you're using. So when you're streaming your game play to Mobcrush or YouTube, share your screen in WebEx video call. Use TeamViewer to work as customer support, or maybe stream your drawing app on a Facebook. All of that is powered with ReplayKit technology. Originally, with the ReplayKit live broadcast the user would be in the app and would use the app to start and stop the broadcast. An app would do that by directly communicating to a ReplayKit API and ReplayKit itself would involve the broadcast extensions and begin providing it with visuals and audio content from that application, and the extension would encode the media and stream to the servers and this is what we call In-App Broadcast. And what was new with ReplayKit 2 is that instead we have entire system in a pack being broadcasted. And how this works is the user would initiate that broadcast from within the Control Center. So this is where they start and stop the broadcast. And that begins a system wide ReplayKit 2 broadcast session and, thus, audio and video samples, again, go to the extension and upload it to the servers. And conceptually this is the difference between the two. And just to talk about the In-App Broadcast in the beginning. In the In-App Broadcast this is your app or your game that is providing the content that is captured live as you play in a game and your game is calling the ReplayKit API to start and stop the broadcast. And it also presents as broadcast activity controller so that the user would pick the service. And in line with that, the broadcast application is providing you the broadcast extension which ReplayKit is working on behalf of that game to sign users into the service and upload the broadcast content live to their servers. That was our original way of using ReplayKit Live Broadcast and we covered this before. And in case your application does need this fine [inaudible] control for starting and stopping the broadcast, for more information please find the session called Go Live with ReplayKit from a couple of years ago. But really, what today's session is about is what we call iOS system broadcast. Now, this is, again, the broadcast of all your onscreen activity and all your sounds, and instead of being initiated from within your application and starting and stopping when the app is paused, for example, this session is started and stopped within Control Center, and this is systemwide and it runs continuously as the user moves from the home screen to the app or as they move from one app to another. And all of this is built into iOS 11 and above and this is what was introduced last year as ReplayKit 2. So here's some game and you're playing a game and you want to start the live stream and initiate the broadcast. So you pull down the Control Center and say you touch into that screen recording button and that will bring up the picture for you to record your game play, or if you tap on one of the [inaudible] of broadcast providers, it will stream out to their service. So once you have done that, you're back to your game and now you're streaming to that service. The broadcast extension receives the media samples captured from the screen and uploads the video stream to its back end and the viewers they have in their web browsers, or however they want to use that stream, are watching your broadcast on their own devices around the world. This is how you start the iOS system broadcast. And now, what's interesting about this kind of broadcast again, is that, as I said, it continues regardless of you moving from one app to another. So I'm broadcasting from my home screen here and then my home screen is what is being broadcasted. And then if I'm launching a different app, that app is going to be broadcasted now, and that's really the core thing now. So the broadcast itself is a systemwide experience and you can go back to the Control Center and stop it or you can tap on the status bar at the top and it will bring up the controls to stop the broadcast. And another thing is that if next I ran as a landscape, there's also a way to support that and you can move between landscape and portrait as you go from one app to another. So this was an iOS system broadcast we enabled last year and the feedback and adoption we've seen since then was amazing. And our live broadcast API today is used in many apps in categories like communication, streaming, house, help desk, education and social. But at the same time we had a lot of feedback that people are having hard time finding this broadcast UI in the Control Center so we wanted to make it easy for you to edit right into your app. And this year our goal was to allow you integrate iOS system broadcast into your application, and we are very excited to announce System Broadcast Picker. So let me go back to my diagram. So what we added now is sort of a hybrid approach to last two [inaudible] styles. And in this hybrid approach we edit up the ability for your application to also be initiator of iOS system broadcast. So your app now can do the same thing as what Control Center is doing; start the broadcast and then allow it to continue as you move from one app to another and then move to the home screen, and so on. So as the user starts a broadcast from your application, he can later go to the Control Center and stop it from there or vice versa; he could start from the Control Center and then go to your app and tap the button that stops. So again, the System Broadcast Picker allows users to start the iOS system broadcast without leaving your application, and this is just a simple button in the view that we provide and this is a new feature we are enabling in iOS 12. Here is example of that. Here I have the sample app, Fox 2 and I edit app with button at the top. So if I hit this Broadcast Picker button, that's going to bring up the same UI as you would have seen if you would go to the Control Center. So I pick out the broadcast provider and start my broadcast. Now I'm back in the app and now I'm just broadcasting to that provider and people around the world can watch my live stream. So how is this done? Okay. There is a new class called RPSystemBroadcastPickerView and it's just a subclass of UI View and you can add it to your application using the interface builder as a view with a custom class, or you can do it programmatically and all you need to do is just run it in instance of RPSystemBroadcastPickerView and add it to your view hierarchy. So this is a more simple way to use the broadcast picker, but I know that many of you in this room broadcast service developers. And finally, with this new API your users can start the broadcast directly from your application and there's no more need to include all those tutorials, how you can enable the screen recording and settings and how to find your service and that [inaudible] in the Control Center. But, however, you would probably like the Picker to show the broadcast extension for just your service, and ReplayKit provides API that lets you do exactly that. And it is just a property of that view and what you need to do is just get the bundle identifier from your broadcast extension and assign to the property of the view called preferredExtension. Here's a code example of how you could do this. You would probably like to set the property right after correcting the view, and the only difference from that example would be that you need to replace the string called com.your-app.broadcast. extension. It's a bundle idea of your extension. So here's a diagram, and I just want to emphasize the point about the picker view that you create in your app. So that view is just a shortcut that brings up a system UI of the broadcast picker, the same UI that you can access using the Control Center. And the picker itself belongs to the system and your application does not own any part of the broadcast state and also cannot start or stop the system broadcast programmatically. So this is what you need to do to add the iOS system broadcast to your app. There's a new API and it is fairly simple and you could really be done adopting it before leaving this session. And next we are going to walk through the process for the broadcast extension developers and discuss how you can actually implement the broadcast extension and how things would work for you if your app provides both the extension and the broadcast picker. So let me come back to that diagram. We are sort of moving to the broadcast extension site now and we want to show you the path assuming the system broadcast. Let's start with talking about the broadcast application and the broadcast extension and what each of them needs to do during the system broadcast session. So you install the broadcast extension along with the application that contains it. So if you install Facebook, you also install Facebook broadcast extension. And broadcast extension and the application are separate binaries and each runs in its own process and those processes are invoked directly by the user and independently from each other. And typically you would use application to allow user sign in or sign up for your service. And now for the apps that will adopt the broadcast picker API, you can also let user to type in some message before posting this particular broadcast to his Facebook. And the broadcast extension, that's all the actual work when everything required to start the broadcast is already available. So again, the extension does the work of taking samples edited by ReplayKit, the raw samples of the audio from the app, video of the screen and perhaps audio from the microphone and encoding starts and uploading to your service using whatever technology you prefer to use. To help you get started developing the new broadcast services we provide the Xcode template and just add the extension to your project using the template and you'll be ready to begin. So now let's look how you can code all of this. So when you create an extension in your project using Xcode template, you'll get a SampleHandler class like this and this is where you add your code to handle different events during the broadcast. And also, you would implement this function that handles incoming audio and video samples. This is how the life cycle of the broadcast usually looks like. So we have states called setup, initialized, started, processing and stopped. Over on the left there's this green square which corresponds to the state when your broadcast extension is not running yet and at this point your app could, for example, get the login credentials from the user and use a shared keychain to process credentials to the extension. Or again, here you could let the user to type in that message for the Facebook part and pass it to the extension like one way or another. So once the user initiates a broadcast, using the Control Center or your application that adopted the broadcast picker, ReplayKit launches an extension process and creates an instance of your SampleHandler class. And at that point you can override the default initializer of SampleHandler and do something essential for your broadcast and not directly related to handling the media samples. So, for example, you could get login credentials from this keychain and establish a broadcast session with your servers. So once the process is launched and the instance of your SampleHandler already exists, the extension is notified that ReplayKit will begin providing it with audio and video samples using the function called broadcastStarted, and in this function you'd probably like to create your media engine or do anything else that you need to start receiving the samples in real time and encoding them and uploading to your servers. So once a broadcast is running now ReplayKit will provide raw audio and video samples and your extension needs to encode it and upload it to your video service using whatever technology you prefer to use. ReplayKit provides your extension three types of the samples; samples with video captured from the screen, samples with audio tapped from the application and samples with audio captured from the microphone, and your extension will encode all of this and upload to the service. And all of those are going to the extension and are handled by the function called processSampleBuffer. This function receives a single CMSampleBuffer, as you see, and the type of the buffer. And what it needs to do is to encode and upload the media samples, and in this code example we show you how you can encode the video using the Video Toolbox. So we recommend using the Video Toolbox as it provides hardware accelerated encoding capabilities, and that is especially important for our use case because extensions have much lower memory limits compared to their applications. So once you're encoding and streaming the video to the broadcast servers the viewers around the world can or could watch your broadcast live, but they need some way to find your broadcast. And say if a person likes playing and watching Angry Birds, there should be someplace on your website or in your app where he can find all the live broadcasts with that game. And in order to implement this you need to add to your video stream some information about the game that is being broadcasted now, and for that ReplayKit provides API function called broadcastAnnotatedWith ApplicationInfo, and as soon as a player starts his game Angry Birds sends this example, ReplayKit notifies your extension that that has happened and provides a dictionary with details about the app that was started. And as an example, you can use the key defined by ReplayKit called RPApplicationInfoBundle IdentifierKey and get the bundle ID of that game and pass it as a metadata to your stream. When the user stops the broadcast, one way or another, ReplayKit is using function called broadcastFinished to communicate to your extension that broadcast session is over and there will be no more samples delivered to the SampleHandler. And in this function you would probably like to finish uploading your buffered video and [inaudible] your media engine or anything else that you need to release. So this is how the broadcast life cycle usually goes. And there's one more point that I would like to mention here. As we discussed, everything required to set up the broadcast should happen in the application and you would get the login credentials using the app or you could get the name for the broadcast in the application, and we have a state for this, this green square on the left side. However, this could be the case that when the broadcast has started, your extension is missing something to proceed as a broadcast. For example, the login process could have failed or something else is needed, and I want to show you how you can handle this case. ReplayKit provides an API that allows extension to communicate to the user that broadcast cannot be started and provide a particular action required to fix the problem. And the way it works, the extension calls some API function and ReplayKit will terminate the session and present the user the alert that will include the failure reason provided by the extension, and in this example this error message is from Mobcrush and it says that user is not logged in. So next is the user tabs at bottom go to the application, ReplayKit is launching your application and what you need to do there is handle this case in some way, present the UI by user will sign in or sign up, and the next time the broadcast will just work. So we have this code sample for the broadcastStarted before, and here I want to show you how do you handle this case when user is not logged in. So there's a function called finishBroadcastWithError, and what you need to do is provide the user info dictionary with a failure reason and ReplayKit is going to use that string and allow that is shown after the session is stopped. Okay. This is how you develop the broadcast extension, and as you see, the process is pretty straightforward. The application handles the broadcast setup and then communicates that info to the extension. And after that the session starts and you receive and encode the media samples. And also, you can get information about application on the screen so the viewers could find your broadcast. And so just one more point I would like to discuss today. This is protecting content of your applications. So your app may display some content that you don't want to be captured in screen recording or live broadcasts. That applies to both audio and visual content. There's an API that you can use to tell if your content is being captured and that API is provided UIKit framework and I'm talking about the property of UIScreen called Captured. So you can register to receive notifications whenever the value of this property is changed and stop your audio playback or hide the visual content when recording starts. There's also a special case that you need to take care of, which is Airplay Screen Mirroring, and I would like to show you how you could do this using this code example for the notification handler. And basically, in the case of a screen mirroring there's always more than one screen and this array called Screens is going to have more than one element, and even though that isCaptured is going to return yes during the screen mirroring session, we have this extra check that will allow your playback to continue. So this is the Live Screen Broadcast and how it really works. And to summarize, ReplayKit provides high level API that allows your broadcast content of just one app or all your onscreen activity. In iOS 12 we provide you an API or a way to initiate the system broadcast directly from your broadcast application. And what you actually need to code is a broadcast extension that does all the actual work of encoding the video. And also, we just discussed the way to protect the content of your apps. You can find more information on the website of our talk and we also will be having the ReplayKit labs later today at 3:00 p.m. in the Technology Lab 5, so if you have any questions or comments, please come. We'd love to hear from you. So thank you so much for being here with us today. We are really excited of bringing you the new approach for live stream broadcast in iOS and looking forward to see test applications adopting it. So please go ahead, create your first broadcast extension using the Xcode template, or open your existing project and add the broadcast picker to your app and enable live stream broadcast from iOS to your platform today. Thank you.  Hello and welcome. I'm James Vest, and I'm here to teach you today how learning VoiceOver can help make your app more usable for everyone. And here's how I'm going to do that. I'll tell you what VoiceOver is and why you should support VoiceOver and also how to use VoiceOver if you have never done it before and also how VoiceOver benefits your app. So what is VoiceOver? I get some common answers like well VoiceOver is a screen reader. That's true, but VoiceOver doesn't just read the screen, or VoiceOver is for the visually impaired. It's true, but it's not just for the visually impaired. So if VoiceOver is not just for the visually impaired, and it's not just a screen reader, what is it? VoiceOver is an alternative way of using you app, and it uses gestures, a keyboard, or a Braille display to give a variety of users equal access to your experience, and it's built in to every Apple device. Now think about that last point. It's kind of awesome. I mean that means that every person who uses your app has access to VoiceOver. But not every VoiceOver user has access to every app. And that's why you're here. That's why you are such an important role in this story because you must update your app for us to take advantage of VoiceOver. Now when I say updating your app or supporting VoiceOver, I'm not talking about creating or maintaining a alternative or simplified layout or tailored to a subsection of users. It's about allowing all users to access the experience that you created. So why support VoiceOver? Well, supporting VoiceOver makes your app more inclusive. VoiceOver users are more likely to choose you app over other apps that don't support VoiceOver. And this is a great community, so they are more likely to in turn recommend your app to other VoiceOver users. But sadly, for many developers, those reasons are not enough, and I get it. There's only so much time. There's only so many resources, and you want to make the best decisions to create the best experience for the most of your users. So that's why it's so important today that you realize that learning VoiceOver can help make your app more usable for everyone, not just VoiceOver users, for all users. So I bet you're asking yourself, right? Can a screen reader do that? Yeah, absolutely. I mean look at some of the latest apps coming out, those larger texts. Those cleaner layouts. They're not just benefiting low vision users or users with cognitive needs. They benefit us all, and the same is true with VoiceOver. So here's how VoiceOver could help you with your app. Well, testing with VoiceOver can give you a new perspective. Now it's easy to take for granted your app because it's your app. You know exactly what it does, what it's supposed to do, how it's supposed to do it. But we take a lot of those things for granted, and when I say testing with VoiceOver gives you a new perspective, it's getting a chance to experience your app in a new way. That's because VoiceOver goes beyond the way your app location looks. I mean an app might look good, but that doesn't make it usable. So side-scrolling states, hover states, they can all give the allusion of straightforward, clean, easy-to-use interface. But testing with VoiceOver goes beyond that, and so it allows you to discover new challenges and insights, and finally, I believe VoiceOver gets really at the heart of what we do. As iOS developers, we pride ourselves on creating easy-to-use, easy-to-understand apps. Well those values are especially apparent when using VoiceOver. So my question to you today is your app so easy to use you could use it with your eyes closed? And that's not a question just for testers. That's a question for designers, for developers. Anyone responsible for creating the best experience possible for our users. And the only way to answer that question is to start testing with VoiceOver, and that's what I'm here for today. I'm going to give you some ways of being able to turn on VoiceOver and start using it. Figure out where you're at, where you could improve, and then I'll give you some things else to watch out for that might not only improve your VoiceOver experience but possibly the experience that all users get. And here's where we're going to start. We need to know how to use VoiceOver. Now in the next slide, I'm going to show you how to enable VoiceOver in settings. What I don't cover in the next slide is how to navigate with VoiceOver, activate with VoiceOver, heck even turn VoiceOver off using VoiceOver. So if you're following along out there on your iOS device, you may want to wait. But I promise you, everyone in this room is going to have a chance to turn on VoiceOver before my presentation is done. So how do we turn on VoiceOver? Well, it starts in settings, and we go to general. In general, I can open up the accessibility menu. It's a great list of all the features that we include on our iOS devices so that we can have equal access to our users. And VoiceOver is right at the top of that list, but I discourage you from opening it there. I discourage you from going in and enabling VoiceOver because, again, we might not know enough about VoiceOver to turn it back off. So when I teach people how to use VoiceOver for the first time, I encourage you to go to the bottom of this list to the accessibility shortcut. It's here inside of a list of options. If I just check VoiceOver, that means I can easily turn VoiceOver on when I want to test it and also turn VoiceOver back off when I'm not ready to test it or my phone goes to sleep or I get a phone. So just checking VoiceOver in this box I can turn it on by triple clicking the home button. Click, click, click. And if you have a brand new device that doesn't have a home button, well then you'd use the side button, click, click, click. When VoiceOver is enabled, we see the VoiceOver cursor. It's a visual reinforcement of what VoiceOver is trying to describe, and that's the first objective when testing VoiceOver, is to make sure that these descriptions are accurate and complete. And the only way you're going to test every single element inside of your app is to navigate with VoiceOver, and here's how to do it. Navigating can be as easy as just reaching out and touching the screen. Dragging your finger across the screen, VoiceOver will just read whatever is under your finger. But that's actually not really how a lot of people review their apps. I start at the top. I work through everything in read order to the very end. VoiceOver users accomplish using flick navigation, and that's taking my finger and putting it on the screen and then flicking to the right, flicking to the right. Now if I went slow enough, it would just read under my finger, so you have to really give it a quick flick until your finger flies off the screen. And conversely, I could go back to the left, and I could go to the previous item or the next item. And it's important that you also want to keep in mind the starting position of your VoiceOver cursor, because if will start in the middle of your page. Well, that would assume as a user that there isn't anything before it, and so you might be leaving your users without all the story. So, we have two ways to navigate, and we have no way to control how a user navigates. So you want to test both these ways to make sure they work as expected in your app. Now while you're navigating your app, you may need to pause that output. So, if you ever need to have so stop speaking, just take two fingers and tap the screen. And if you make that peace sign again and tap the screen, VoiceOver will just pick up right where you left off. Now, if you accidentally enabled VoiceOver earlier in settings when I said not to, don't worry. You're halfway to turning it back off. But there's one piece that's still missing, and that's activating. You know, we take for granted that if I see a button on the screen I just tap it and it opens, but with VoiceOver when I tap an element, it describes it to me, and it gives me the choice to decide. Do I want to open this element or do I want to move to another? So, when I reach out and I hear a description of what I want to open, then I follow that up with a single finger, double tap, tap, tap. And the selected item will open. So to illustrate how navigating and activating with VoiceOver, well, I'd like to welcome to my stage my colleague, Ryan Dour, to the stage. Thanks James. So you guys have all heard about how VoiceOver is used, but in order to really see this thing in action, we got to turn it on. So I'm going to go ahead here, and on this phone, I have an iPhone X, I'm going to triple click the side button. There we go. Flash Thursday, June 7. Excellent. Heading not found. [ Background Noise ] Facetime. There we go, okay. Fantastic. So we've started off here on the home screen, and the first thing I'd like to show you is that we've got a VoiceOver cursor on the Facetime icon. It's a black cursor. That's going to follow me everywhere I go while I'm using VoiceOver, and there's really several different ways that we can move the VoiceOver cursor around. The first is that I can reach out and touch just about anything on the screen, and the VoiceOver cursor is going to follow along, and it's also going to speak that item. So if I just reach out and touch-- Reminders. One task due today. Excellent. I can also just drag my finger around the screen to move that VoiceOver cursor, and it's going to follow along, so I'll try that. No mail [inaudible] maps, weather. We also have flick navigation. If I flick left. Maps. I move back an item. If I flick right. Weather. I move right an item. And this reads left to right, top to bottom. Notes. See how it wrapped to the next line there? So from here I can actually go ahead and if I want flick my way to an app and launch it. Remind, news, stocks, TV, stocks, weather. Or I can find it this way, and I can double tap. Cupertino. Cupertino. No weather info-- San Jose, [inaudible]. There we go, that's the right location. Seventy-three degrees. Except I don't want to hear the entire thing, so I paused it with a two-finger tap, again, using that peace sign, just performed that two-finger tap, and it paused the speech. If I do it again. Thursday, clear. High 76. This feature zooms. Low 52. And, again, I can pause it. That way I can send it back to James. Thanks James. Thanks Ryan. Now I just want to stop here and point out that using these steps alone you can actually start testing your app with VoiceOver. You can turn it on. You can navigate your app, and you can see how well it works but just with VoiceOver on. So now the other side of testing VoiceOver is about getting familiar with the expected result, and that means getting outside your app and starting to experience areas that already have a good VoiceOver experience. So here's how we do that. Now if I needed to go home, well, I could press the home button. But on a new device, like iPhone X, we need to go home by taking our finger from the very bottom edge of your device and starting to slide it up. The first vibration I feel will go home, and if I keep dragging up, the second vibration I feel will open the app switcher. I can also repeat this gesture from the very top edge of my display and open things like the Navigation Center and the Control Center. And when you're not testing your app, you can three-finger flick. It's basically the same as a one-finger flick to allow me to scroll or move between my home screens. But in this case, I'm using three fingers. So using three fingers back and forth, up and down, will allow me to get to those areas. So whether or not you're exploring other apps or going into Control Center or Notification Center, these areas are full of complex UI that's waiting to teach you the expected results when we focus the VoiceOver cursor on them. Now another gesture you may need is called the two-finger scrub, and that's where I take that same two-finger tap from before, but instead of letting go, I scrub back and forth very quickly, almost like in a fast Z pattern, and that will allow me to get out of a window without having to navigate back to cancel buttons or back buttons. Or in the case of UI, where they don't even have close buttons, to quickly scrub back and forth and dismiss the window. So while we're busy navigating and exploring and learning about this great expected behavior, we also kind of need to know what we're listening for. So when you open a newspaper, you receive a lot more visual information than just words on a page. Here's that same newspaper. If we fail to infer all of that additional information, like headings or images, so we need to explore and listen to headings and images, tabs and buttons, and make sure we're honoring that experience. Now to demonstrate these initial gestures, here's Ryan Dour. Cool. Thanks James. So we're still here in our weather app, and I would actually like to go back to the home screen. This is an iPhone X, so in order to do that, I need to use our new home gesture, and that is going to be again, touching my finger at the bottom in the home [inaudible], and then sliding my finger up until I feel that first vibration to go home. VoiceOver will also tell me, if I haven't let go of the screen yet, that this is exactly what I'm going to do if I let go. Let's go ahead and check that out. Lift for home. And I'm going to do that. Weather. Okay, so we're back on the home screen. Many of us have many pages of apps, so in order to switch between those with VoiceOver, we use a three-finger swipe. Actually it's a three-finger flick, but, yeah, it's funny, we actually interchange that a lot here at Apple. We say three-finger swipe or three-finger flick, but they're interchangeable. I'm going to go ahead and three-finger swipe left. Page three of three. There we go. Utilities folder. Four apps. I'm going to go ahead and double tap on this utilities folder so that we can explore the inside. Utilities, heading. Let's explore. I'm going to go ahead and flick right. Voice memo, compass, measure, calculate, calculator. Hear that bonk? I've reached the end. If I flick left-- Measure, compass, voice, utilities, heading. Well, there's a lot of apps and a great heading here. However, there's no apparent way to close this folder. Without voiceover on, you would just tap outside of the folder's area in order to be able to close it, but with VoiceOver, we use the two-finger scrub. Now, you can use it just as James described it, like a Z gesture, but I'm a DJ, and I kind of think of it more like a scrub, like you're scratching a record. So let's go ahead and take those two fingers and-- Utilities folder. Boom that folder's closed. Four apps. Cool. Thanks, James. Thanks, Ryan. So when faced with a lot of information, a sided user may choose not to go through every single object in read order to the very end. No, instead we choose to pay attention to maybe a select group of elements, like these headings on this newspaper. VoiceOver's users can also navigate this way using the Rotor. And the Rotor is a VoiceOver feature that dynamically presents options that the user can adjust or navigate by. A VoiceOver user can set the Rotor by making two fingers on the screen and turning it around a center point, almost like you're turning an invisible knob on screen. And as we adjust the rotor, it'll speak all the options that are supported within the view that we're focused in. So in this case, we could turn that rotor and switch it to headings, and once it's set to headings, I'd be able to go to the next heading by taking a single finger and flicking down. Or, conversely, I could flick up with one finger, and that would go to the previous heading. And here's why the Rotor matters to you as a developer. Because a Rotor only works if we as developers remember to include headings in our apps. So when your app doesn't support one of these rotor options, well then we're kind of taking a tool out of that user's toolbox to have a great experience in their app. So I invite you to go back into settings, general, accessibility. Inside of the VoiceOver menu is a lot of great settings that VoiceOver users customize, and one of them is the Rotor settings. So review what we can support and make sure that we are honoring those in our apps. So here's Ryan Dour to demonstrate how the Rotor works. Cook, thanks James. So again we're back here on the home screen. We're on our second page of apps, and I'm going to open the App Store. So I'm going to flick once, right. App Store. There it is. Image going to double tap again to activate. Apps, heading. So I'm on the apps tab of the App Store. There are an awful lot of fantastic apps here. However, I don't want to have to explore each and every one of them to get to the various categories. So I can actually touch one of them. WWDC18, celebrating the Apple design award winners. And I'm pausing this speech again with that two-finger tap. Now, to do the Rotor gesture, all I have to do is take two fingers and turn either clockwise or counterclockwise. Characters, words, headings. There's the one I want. As soon as I let go, I am on that rotor. Now, if I flick down-- New apps we love, best new updates, stream yourself selfie with light skin tone, heading. Now, if I flick up-- Best new updates, new apps we love, heading. I move back up that list, and of course if I want, I can go ahead and dive into a category, but before I do, I want to tell you guys about one more gesture, one that is, at least for me, extremely important. So James started this whole thing off with asking you a very important question, which is, is your app easy enough to use with your eyes closed? And I'm going to guess that for a lot of you, you might say, geez, I have no idea. I'm going to find out, but for me that is my experience. I don't get to see your app. I cants see your app. So for me I really need that question to actually be the most serious one you ask yourself. Now, keeping your eyes closed is a challenge. It's very difficult to keep it up, and also you might want to be looking at your computer, taking notes, and doing other things. So in order to do this, the most effective way I know possible, I encourage you to use the screen curtain. The screen curtain blacks out your entire screen and puts you in my shoes. You have a chance to use your app the way I use it entirely, and there's no cheating here. That is the only interface you're going to get. Let's turn it on. So three-finger triple tap. Again, three fingers, three times. Screen curtain on. There we go. At this point, we're on the same plane, and if I, again, flick down-- Best new updates, heading. And I flick right, let's start exploring. See all, button. Of course now I know that's a button, and I can activate that button. So I'm going to go ahead and do a double tap. See apps, back button. I'm in the apps areas, I'm going to start flicking through to the right, let's explore. Best new update, [inaudible] get, but, world of dinosaurs, $1.99, in-app purchases, button. That's a lot of information that you're getting that's usually very visual. Procreate pocket, entertainment, button. And of course, if I want to go back, that two-finger scrub-- Apps, heading. And we're back. Screen curtain off. So that is screen curtain, and I can't encourage you enough to use it because when you make your app experience something that you can take VoiceOver through, both enjoy and make sure that it's completely amazing, then so can I. I can enjoy your app in full if you take this through as the final step before you call it done. Thank you. Thanks Ryan. So now it's time to talk about how VoiceOver can benefit your app. Because you're ready. Today you can leave this area and go and test your app, and I would encourage you to test it like you normally would just with VoiceOver on. But I want you to pay attention to these additional insights that could make your app better for all users. So one, if something doesn't sound right, it may not read right either. It's like when a writer reads something aloud that they wrote. It's just another way to check that your message is coming through, and it doesn't just look right, it actually sounds right as well. Two, if navigating with VoiceOver takes effort, it probably takes a lot of effort visually as well. So maybe it's time to add section headings or break up your content to allow users of your app to benefit from a cleaner, more straightforward visual and spoken experience. Three, if your app sounds out of order, your content may be too. VoiceOver navigates your app in read order. So if you're navigating your app wondering where's the login? Well, likely is a lot of your audience asking the same question. There it is. So these issues can't hide from VoiceOver and neither should you. So let's turn it on. Are you guys ready? Come on, do you remember how? All right, let's review then, all right. So inside of settings, we're going to go to general. If you have an iOS device, feel free to take it out. Inside of general is accessibility. Accessibility, we're going to ignore the very top of the screen, and we're going to go all the way to the bottom. If we can enable today the accessibility shortcut, not only are we going to have a chance to turn on VoiceOver, potentially for the very first time, but it's also a quick way to go back and turn it on again, back to show your colleagues. Turn it on again when you go back to your office or maybe when you're trying to make a case for trying to give more resource time to focus on VoiceOver. So, if we've gone into settings, general, accessibility, and gone into the accessibility shortcut and checked VoiceOver, now we'll be able to turn on VoiceOver quickly with the home button. Click, click, click. Three clicks of the home button or on iPhone X, three clicks of the side button, click, click, click. Ready to activate VoiceOver. Let's do that now. Yeah. And how does it feel, right? For a lot of us, it's brand new, and so we could have mixed feelings or uncertain feelings. This could be challenging, but I've never done anything worthwhile in my life that wasn't challenging. All right. So our next challenge will be turn VoiceOver back off. [laughter] Let's see if we can do that now. So with the home button, give it a triple click, click, click, click. Or with the side button on an iPhone X, click, click, click. So, yes, I'm going to call you out individually now, and, but no, in review. VoiceOver is an alternative way of using your app. And you can navigate and activate elements based on just how they're labeled. And you can explore other apps to learn expected behavior. And then rediscover your app by hearing it to help identify other issues. So here's some steps I want you to do as soon as you leave today. Just like Ryan suggested, I want you to close your eyes and test your app. So really experience this. Start identify issues that prevent users from getting the most out of your app. And then open your eyes and confirm that experience, because if you've ever wondered what a VoiceOver experience should be, just remember, the goal is just to match the experience everyone else gets. So compare the two and improve both based on what you learn. And then make VoiceOver a part of your regular testing. Talk to your team. Make VoiceOver a part of your practice, not just so that you can continue to improve your experience with every release, but also to avoid losing the lessons that we gained here today. And lastly, go out there and teach others what you know because that's why I'm here today, because teaching is the best way I know how to master any subject. Now helping you become a master, I also want to share these resources. Apple.com/accessibility is a great resource to get to know VoiceOver and other accessibility features and also have access to resources such as the guides, not only to reinforce what we learned today but build on it. And the same is true for developer.apple.com/ accessibility/ios. And for more information, feel free to rewatch this video. Anytime we do something for the first time, it's hard to master, so watch it again. Practice with me until it feels comfortable and you're ready to share. And also, take advantage of these other opportunities. For instance, tomorrow starting at 7:00 in the morning until 7:30, you can sign up for accessibility design by appointment lab, and that will allow you to talk to an engineer like me, who can also help you go through your app, identify issues, and then you can take those issues to the accessibility lab and technology lab nine tomorrow at 11 a.m. so you can meet with other engineers and say, how do I start fixing some of these issues? How do I start improving that experience? And then, also go to deliver an exceptional accessibility experience. That's the book and presentation to the presentation that you watch today. We showed you how to start using VoiceOvers to identify issues, and tomorrow we'll take the stage again, and we'll start showing you how to resolve some of those issues. Thank you everyone.  Hi everyone. Welcome to the Introduction to Dark Mode session. My name is Raymond Sepulveda, and I'm a macOS designer on the Human Interface Design team. For the first half of today's session, I'm going to be going over some of the design principles that went into the design of Dark Mode, as well as some of the design considerations that you should take into account when updating your own applications to take advantage of Dark Mode. Then, for the second half of the session, my engineering colleagues Rachel Goldeen and Taylor Kelly from the Cocoa Frameworks team are going to be going over how to adopt and support Dark Mode using an example application that they built together. The three of us are super excited to share all of these details with all of you, so let's jump right in. As we announced in yesterday's keynote, macOS 10.14 is going to be getting a new interface appearance, and we call it Dark Mode. The new design is very attractive and engaging, while at the same time being very calm and understated. These are great qualities for an interface, and they make it great for creative professionals who are dealing with heavily detailed images and colorful assets such as images and video assets. But, in actuality, it's actually great for just about any type of user who is looking to concentrate or focus on any given task at hand. So, whether you're trying to concentrate on writing the next great novel or you're trying to read one without disturbing your bedside partner, Dark Mode is really great for either type of situation. The interface shares a lot of family resemblances between the light appearance and the dark appearance, and that is because we wanted to encourage users to switch between both appearances easily and find the one that really best suits their needs. And so it was very important that things like control metrics, window layout, and the distribution of translucent materials be the same as much as possible between both appearances. As such it's also equally important that all of your applications take full advantage of both appearances so that they update correctly between both appearances no matter what the system is running under. You really don't want to be that one light appearance that's stuck in dark appearance. All in all, we're super happy with how the design turned out, and we are really anxious to see what you guys are going to turn your apps into as soon as you get hands on the developer beta and start adopting the Dark Mode. Aside from Dark Mode, another new interface feature that we added into macOS 10.14 are accent colors. Previous releases of macOS offered two accent colors, blue and graphite, but for macOS 10.14, we've expanded the color palette to include eight fun colors to really help you personalize your Mac. So, if you're like me and you love the color red, you're going to immediately switch over to the color red [laughs]. If orange or yellow are more of your preference, you're going to switch over to those, and we make those available as well. I should note that the accent colors are not just a sub-feature of Dark Mode, but they're also available in Light Mode. So if you prefer Light Mode but want green accent color, you can do that as well. So, now that we've covered a little bit of the basics of what Dark Mode is and what accent colors are also, let's dive a little bit deeper into some of the design principles that went into Dark Mode. At Apple, when we begin a new design project, we like to establish a set of design principles to help guide us throughout the entire design process. And, with Dark Mode, there were three in particular that we referred to quite often throughout the entire process. The first one is the most straightforward, so let's just jump right into that one, and that is that dark interfaces are cool, period, right? They are almost effortlessly cool in a way that light interfaces sometimes kind of struggle to be. But why is that, really? Part of it is because we all have these notions of what a dark interface is and what a dark interface looks like. And a lot of that is fed into us by what we've seen in pop cultures in things such as sci-fi movies and action movies. So much so that if I were to go around the audience and ask everyone to give me one-word association of what a dark interface is to them, I'd get responses such as cool, of course; slick; professional; futuristic; and even beautiful. So going into the design for Dark Mode, we really wanted to acknowledge all of these aesthetic associations that we all have with dark interfaces, while at the same time embracing a lot of the real benefits that a dark design offers you, such as a higher contrast for typography. We realize that Dark Mode is really, really cool, and that a lot of you are going to want to adopt Dark Mode on your apps permanently. But, in actuality, only a small subset of apps really should be dark all the time, and those are things that are media-centric or content-creation apps. Those are the ones that are best suited to staying dark all the time, so if your app doesn't fall into one of those categories, it's better to let your app follow what the system appearance is, rather than to make it dark permanently. The second principle was that dark interfaces are not just inverted. And, really, what's at the core here is that when you look at a dark interface, it can sometimes be easy to think of just taking the light interface and inverting it to end up with your final result. But, in actuality, that can be very creatively limiting, and so it's better to take a look at your elements and your application on a case-by-case basis and try to determine what is the visual cue that they're trying to communicate and then figure out whether an opposite visual cue is necessary. So, if you take something, for example, like, a standard Light push button in the light appearance, the buttons look like this in their active state. And, then, when you click on them, the buttons darken. And this is generally true of almost all of the controls in the light appearance in that their click state is a darker version of what their active state was. In Dark Mode, however, the general active state looks like this, and when you click on them, they brighten, and that, again, is generally true of all of the controls. They will all generally brighten up. Another example are group boxes. In light appearance, the group boxes have a dark fill behind them and are shaded such along the edges to give them a recessed quality such that they look like they're carved into the window background that they're inside of. In Dark Mode, however, the group boxes are treated almost as if they are self-illuminated. But, at the end of the day, the general purpose of the group box is the same between the two modes in that what they're really trying to communicate to the user is that the contents contained therein are all related somehow. Why I said that it was important to go on a case-by-case basis and try and determine what the visual cue that your elements are trying to communicate-- It's important, is because, in some cases, the visual cue that's being communicated is already effective enough in the Light Mode and should be left as is in Dark Mode. An example of this are the window shadows. As we all know, macOS is a layered window environment, and the window shadows really help to communicate the sense of depth, the Z-depth of the windows. So, if we were to have inverted the window shadows, for example, and started using a white shadow, for example, the entire UI would have become flat. You would have lost that sense of depth. Another thing to note is that the window shadows in macOS are constructed of two layers. We've got the diffused shadow and the rim shadow. So, if we look at this little portion of the screen and look at how the shadows were constructed and treated in Dark Mode, the first thing to note is that, in taking into consideration that users are going to be able to go back and forth between light and dark appearance quite often and that some apps are going to stay dark all the time, it was important for us to keep the diffused shadow exactly the same. So, we used the exact same shadow blur parameters for the shadows in Dark Mode. Next, we took a look at the rim shadows and saw what changes we needed to do for those, and the changes that we did for Dark Mode where we adjusted the opacity to make them a little bit higher so that the contrast edge was more defined and that we made them a little bit crisper. To further the focus and the edge detail on the windows, we added an inner rim stroke to the windows to really help define the windows and pop them off. The end result is that the visual cue still looks very similar to the light appearance, but the end result has been finely tuned for dark appearance so that it works much better. The second principle was that Dark Mode is content focused, and this was the one that we referred back to the most often throughout the entire process, actually. When you look at a light interface screen, it can sometimes be a little difficult to tell what the content is, where to focus your attention to, especially when there are lots of windows on the screen. And part of the reason for that is because a lot of the content that we all create is just bright in general. However, in a dark UI, the content stands out because of the background being pretty dark. All of the window chrome in the background generally just melts away, and the content becomes the primary focus of your attention. Generally, we found that there were three methods for how to treat your content regions inside of your windows to optimize the content focus. The first method was to just generally go completely dark. An app like Finder is a great example of where this is possible because of the way that the content is displayed. Finder's content are displayed as colorful file icons and so the dark background really helps the icons stand out as much as possible. The second method was to leave the content as is. Pages is a great example of what you see is what you get app, where the contents should be rendered and presented exactly as the user authored and created it. Dark Mode is a great UI for what you see is what you get apps, because the content really becomes the primary focus, and the UI really becomes the secondary focus and almost melts away. The third method was to make the content region appearance a user option. Mail is a good example of an app where the appearance of the content might be ambiguous as to whether the user would prefer it to be light or dark, and so the best thing to do is to make an application setting so that the user can determine whether they want it to be light or dark on their own, while their application's chrome area still respects the overall system appearance. One thing that all three of these windows had in common was that the chrome remained dark and became secondary to the content area. And Dark Mode kind of achieves this using a feature that we call desktop tinting, which causes the window's background colors to camouflage themselves with the desktop picture. We found that completely desaturated gray colors have a tendency to have a color temperature that was sometimes in conflict with what your desktop picture was, and this effect was magnified when you have a translucent sidebar area, for example, against a solid opaque area of your window, and so desktop tinting helps mitigate that. Once applied, the desktop tinting gives the whole window a harmonious color scheme, which helps it to blend in with the surrounding areas and really reduces that color temperature disparity. So, how do we do the desktop tinting effect? The first thing that we do is, actually, we take a sample of the desktop portion that is behind the window, and then we derive a average color for that area, and then we blend that average color with a gray, which then becomes the window background color. This background color is dynamic, so as the user moves the window around the screen, the color will update itself so that the window always stays in sync with the wallpaper portion that was behind it. So, if we look at the effect in action across a couple of different wallpapers, you can see that the window is adjusting itself on the right side to match the desktop picture that is behind it. Not only does the window background color adjust itself to take advantage of the desktop color, but the controls also get a little bit of that going through them also due to the control's opacity. For users that work in color-sensitive applications or have very color-sensitive content or just generally would prefer not to have the desktop tinting apply to their UI, we offer the Graphite Mode so that they can disable the color tinting altogether. If we take a look at the color swatches for the window background colors, along the top there, we can see the window background color in the Light Mode, which is the standard light gray that we're all accustomed to. And, in the middle there, we've got the under page background color, which is a mid-tone gray, and then we've got the content background, which is the white background color that we're all familiar with behind all of our content areas. In the middle row, you can see how the desktop tinting affects these colors, and, then, along the bottom, you can see the effects of the graphite accent color disabling the desktop tinting effect. So, now that we've gone over some of the design principles, let's go into a little bit of the design considerations that you should take into account when updating your apps. First, let's talk about colors. For macOS 10.14, we added a lot of colors. We also removed a lot of colors, updated some, and, as you can see behind me, these are some of our system colors. Along the bottom, we've got the dark variance of the colors, and, as you can see, the dark versions are generally a little bit brighter and more saturated. Rachel and Taylor are going to a little bit deeper into the color usage during their portion of today's session, but, for now, I just generally want to try and cover three colors in particular, which are the blues. Going from left to right, we've got controlAccentColor, systemBlueColor, and linkColor. Linkcolor, as the name implies, is meant to be applied to links, text links in particular. So, things that are hyperlinks, taking of the user to a website, or a link within your application that navigates the user to another area of your application-- those should be treated with linkColor. ControlAccentColor and systemBlueColor are really meant to be used on controls that have visible borders or button bezel backgrounds or for tinted glyphs. ControlAccentColor, as the name implies, is going to respond to whatever the system accent color is set to, in this case, red. So, if you have a custom control that is trying to mimic the appearance of a standard system control, or you just have a control that you would like to follow suit with whatever the system appearance is set to, you should use controlAccentColor. If you have a control where it's important that the control stays blue because of informational purposes or for branding purposes, then systemBlueColor is a better choice for you to use. Next, let's take a look at designing with vibrancy. When we talk about vibrancy, what we're generally referring to is the package of a translucent material working in conjunction with the content that is displayed above it. So, in this case, the vibrancy is the vibrant Finder sidebar. Generally, we have non-colored artwork that is meant to be treated vibrantly. An example of that is the glyphs and the text in the Finder sidebar here. The end result of the vibrancy is is that the colors become higher saturation, and the contrast is increased for things like the text. Colored artwork, such as the finder tags in this example, should be treated as non-vibrant. Same thing with colored iconography. The vibrancy will destroy the original colors that you have in your image, and so you want to treat those as non-vibrant. So, taking a look at how you can construct vibrant content, it might seem almost obvious to just use opacities, in this case, opacities of white. But while it's true that some color contribution from the background is occurring through the materials here-- and that's happening because of the opacity-- the end result is actually not as highly saturated as we would desire. And, as you can see along the bottom with the Quaternary Label Color, the legibility is a little bit compromised. Instead, what you want to do is construct these using opaque grayscale colors. One thing to note here is that the text color for the RGB label colors here are full black. What's going to happen once the Blend Mode is applied is that the text that is black is going to become completely transparent. So, if you have areas of your content that need to be treated as a knockout, use full opaque black, and if you have areas that need to be gradiate towards an opacity, use a gray that is ramping itself towards a full black. This is true in Dark Mode, and the opposite is true in Light Mode, in light, vibrant areas, I should say, where white full, opaque white is full transparent and gradiating towards that is going to give you a transparency. So, once we apply the vibrancy, we can see that the gray colors become a lot more saturated, and the legibility of the Quaternary Color Label is a lot more improved. Putting them side by side, we can see just how much of a difference there is between the opacity construction, versus the grayscale construction. In some cases, over certain colors, the opacity construction will fly, but it's not exactly the best end result, aesthetically or legibility wise. Next, let's take a look at artistically inverting glyphs, and, by artistically inverting, what I'm referring to is the process of creatively looking at your glyphs and determining what areas need to be adjusted to take advantage of the Dark Mode, for example. So, here we've got a set of our standard glyphs that we have in the light appearance. One thing to note is that the light appearances glyphs generally use a lot of edge strokes with hollow centers. And the reason for that is that we take advantage of the white backgrounds that these are generally placed on top of in order to denote form and shape. But, if we were to take these exact same vector forms and place them over a dark background, all of that sense of form and volume is kind of lost, and so we had to artistically invert them to bring back some of that sense of form and volume. Looking at how some of these are constructed, we've got a couple of simple examples, like this finder tag. On the left side, it's denoting a white tag with a dark eyelet cutout. On the right side, however, in Dark Mode, that sense of it being a white sheet of paper is lost. So, once we invert it, we get back that white sheet of paper with an eyelet knockout hole. Looking at how this is constructed, generally, what you would do is expand the clipping paths, I should say, and then select your eyelet shape and then reverse the knockout on that. And then it's just a game of selecting any redundant vector paths and removing those. Taking a look at a slightly more complex shape, for example, the house. We've got a white house on the left side, and on the right side, the house isn't white anymore. So, inverting it returns us back to a white painted house. Generally, the construction for this is about the same as the other process, but one important aspect of this glyph, in particular, was that we have to add a new shape into the vector form. In this case, we're mimicking the roof line, creating a shadow underneath the roof that we're going to knock out. And this is important, because if we were to have left the construction of the house without that shadow line, the roof would have no definition, and it would be really hard to see that at small size. Moving on to a more complicated glyph, like this mail envelope, for example. On the left side, we can see a sense of volume and shape. We've got two layers, generally. We've got the back flap of the envelope, and we've got the front flap of the envelope with a letter contained in between. On the right side, a lot of that sense of volume is just missing because of the lack of fill colors. And so, once we invert that, we return back to having a sense of volume. Again, generally, the process of construction for this one is about the same as some of the other ones, but it's a little bit more detailed and time consuming just due to the number of paths that there are. But, generally, what you want to do is break it apart into two pieces, again, so that you have the front and the rear portion, and then you have your letter portion in between. One important detail that this glyph exemplifies is the dual opacities that we use in some of our glyphs. In this case, the letter is set to 80% opacity in order to make it secondary to the envelope, itself, being opened, which is what we're trying to communicate. And the opposite is true in Light Mode also, where we do use 80% opacities, in some cases, also. So, quickly wrapping up what we've covered so far. Dark interfaces are cool, so cool that you're probably going to want to make your app potentially dark all the time, but unless your app is a creative or media-centric app, you should reconsider that, and, instead make your app follow what the system appearance is set to, instead. Don't limit your app design to just inverting your existing light appearance, and, instead, take a look at what elements are in your app on a case-by-case basis, and try and determine what are the visual cues that are trying to be communicated by the controls, and then determine whether an opposite visual cue is necessary or not. Then take a look at what type of content your app produces or displays, and then use that as a guidance to determine what type of method to use for how to treat your content area-- whether to go fully dark, whether to make it stay as is, or whether to make it a user option via the app settings. Then, taking a look at colors, generally, you want to make sure that you're using the three colors that we went over, appropriately, if you have controls that are text controls, try using text link if it's applicable. If you have controls that you want to follow the system accent color appearance, set those to control accent color, and if you have a control that has to stay blue all the time, adopt system blue color. And then Taylor and Rachel are going to go into a lot more detail about color uses, so definitely pay attention to their section. When constructing your vibrancy areas, make sure that you're constructing them in a way that takes full advantage of the vibrant effect blending, so make sure that you're using grayscale colors that are opaque. And if you have areas that need to be knockouts or have opacities, ramp towards black in Dark Mode or ramp towards white in Light Mode. And then, lastly, make sure that the glyphs that you're using are artistically inverted in order to showcase the intent that the glyphs are trying to communicate to the user as effectively as possible. Finally, make sure to go to the Human Interface Guidelines to see more design guidance. And then as well as checking the design resources section, where, by the end of the month, we're going to be making available updated Photoshop, and sketch templates, and kits for you to use. And with that, I'd like to hand it off to Rachel to go over some details on how to adopt and support Dark Mode. Thanks Thank you, Raymond. Hello everybody. Let's take a moment just to thank Raymond and the rest of the design team for designing Dark Mode. Isn't it awesome? Now Taylor and I will take you through the engineering basics of adopting Dark Mode in macOS Mojave. A while back, Taylor and I were chatting, and we discovered that we were both huge fans of chameleons. And while we don't yet own our own pet chameleons, we thought we'd be prepared by writing an app called Chameleon Wrangler. We did this back in High Sierra, and so we thought, "Hey, let's update it for Mojave." The first thing we did was put our system in Dark Mode and launch our app from the dock. And it didn't really look too dark. In fact, it's exactly the same between light and dark. And you'll note throughout this portion of the talk that on the left, when we want to compare, we'll have the light on the left and the dark on the right, so you can compare them side by side. Can't actually run the system this way. It's just for show. So, what we needed to do was link against the 10.14 SDK and Xcode 10. We built and ran our app, and voila. It's a lot darker. Comparing with light, you can see a lot of things are different and darker, automatically, but there's still some things we'd like to fix up. So, we'll go into detail about some of the colors. There's a lot of colors. Let's take a look at this header area that's light green. We implemented this using a hardcoded color, where we specified our custom RGBA values. The problem with this approach is that it doesn't change between the different appearances. You're stuck with the same color no matter what the system preference setting is. Instead of doing it this way, we're going to take advantage of colors in asset catalogs. You've been able to do this since 10.13-- have colors in asset catalogs, but now you can specify colors for different appearances. So, we've added a dark version of our header color. Asset catalog colors have several advantages. You can give them meaningful names that describe where the color is used or how it is used, and those are often called semantic names. You can have multiple definitions of the color, as I showed you, with the light and dark versions and also for different color spaces-- sRGB versus P3. And it gives you a central location for defining all of your colors. That makes it easy to go in and tweak your colors without changing any code if needed. So, here's our color now in a asset catalog, and it's called headerColor, because it's the header color. And let's see how it looks now, using headerColor. Looks a lot better and dark with that dark green. But something becomes obvious, and that's the text. Chloe's name and age are hard to read now against the darker green. Things looked okay in light, but they don't look good in dark. Well, there, we were using black and dark gray, and those are static colors, also. Not exactly the same as hardcoded, because we're using available colors, but they don't change. So, instead of using asset catalogs this time, we're going to take advantage of dynamic system colors. NSColor has many dynamic colors available for you to use with definitions across the different appearances. And a special thing about NSColor is that the colors are resolved at draw time, which means that the actual RGBA values that are used depend on the circumstances at the time that the color is drawn. I'm going to go through some examples of dynamic system colors now. Starting with the label colors that Raymond talked about earlier, and I'm showing the dynamic colors on the top half and some corresponding static colors just using black in varying levels of opacity on the bottom half. And, as you can see, in the light appearance, everything looks fine. But over in dark, labelColor looks great still, but the black colors disappear against the dark background. Here's an example in the system in Mail of where labelColor and secondaryLabelColor are used. The more important text is given the labelColor, and the less important text is secondaryLabelColor. To find out which colors are dynamic, in the Interface Builder popup menu for colors, it'll show you the list, and they've done a really nice job of making those be the dynamic colors. So, now switching, once we know that, black color and dark gray to labelColor and secondaryLabelColor, this fixes our text, and it looks great in both light and dark. A little bit more about dynamic system colors. As Raymond showed you, there are a bunch of colorful system colors-- systemRed, systemYellow, systemGreen are just three examples. And they subtly change for the different appearances so that they look great in both places, as opposed to the static, plain red, yellow, green colors. Another special thing about using systemRed, etc., is that they will match other uses of systemRed throughout the OS as well as matching on iOS. So, it's great to stick to these kinds of colors when you have an informational content in your color. It needs to be red or yellow for a warning, for example. Raymond also talked about linkColor, systemBlue, and controlAccentColor, so I wanted to talk about that again. LinkColor and systemBlue are very similar but slightly different and use linkColor for actual links to webpages or for going someplace in your app, whereas systemBlue is used for standard actions in your application, or you might even use controlAccentColor if you want to match the system preference, in this case, purple. You may search through your application code and find all the places you're using color and fix it all up and then still end up with a situation like this, where, for some reason, even though your text switched color, there's something behind it that's still drawing wrong. And if you're stuck and can't figure out what to do, I suggest taking advantage of Xcode's View Debugger. I'm just going to cover this briefly here, and the advanced Dark Mode session will go into more detail about debugging tips and tricks. So, I can see that light view is actually my scroll view, and I had set in the interface builder to a custom gray color. That's why I couldn't find it searching through my code. So, I switched it to the default background color, and now everything looks great. And while we're here on this text view, I'd like to go through some of the colors in play here that are dynamic system colors that also may be useful to you. First, there's textColor, which is switching between black and white; textBackgroundColors switches from white to black; selectedTextColor goes from black to white; and then selectedTextBackgroundColor, which is blue. But you can see it's two different versions of blue, and this is special, because it follows the system preference for the highlight color. And you can see, when the highlight color changes, the selectedTextBackgroundColor changes with it. Now, I'd like to move along and talk about accessibility. In the Accessibility display preferences, there's the Increase contrast checkbox. If we turn on Increase contrast mode, you'll see that colors and artwork throughout the system change to make the UI easier to see for people to see with low vision. Well, now in asset catalogs, we can add colors specially for the high contrast appearances as well. So, here I've added to my header color a version for high contrast light and high contrast dark. And this is how Chameleon Wrangler now looks in the dark high contrast appearance. You'll notice that not only is my header color using that special color that I defined, the rest of the system colors are also specially defined for the high contrast appearances. And this is true for light high contrast, as well. Now I'd like to move along and talk about images. We'll take a look at the toolbar buttons here. This chameleon color picker button looks pretty good in light, but in dark, that dark outline disappears against the dark background of the button. So, we'd like to have a special version of it for the dark appearance, and, once again, we turn to our asset catalog friend and add a version for the dark appearance. And now it looks much better against the dark button with the light outline. That's great for full color images. How about these other two buttons? Well, they already look pretty good. We didn't have to do anything, so why? Why are these right? Well, that's because we had made these images template images. To tell you a little bit about template images, in case you don't already know about them, it's important to understand that the transparency of each pixel is maintained, but the color is ignored. You can make the starting image any color you like, that's ignored. It's only the transparency. They can be bitmaps or PDFs. And here's an example from the OS of the speaker volume image. The original image is black, and then it has this partially transparent section for the larger soundwaves. And if we put this image into an NSImageView, it's automatically given the Secondary Label Color, and you can see in light, it's a medium gray, and in dark, it's a lighter gray. And then the partially transparent areas have less contrast against the background. Another advantage of using template images is that they automatically pick up on the button state. When a button is disabled, the images are dimmer, and that's handled for you if you use template images. Okay. So, Chameleon Wrangler is now looking pretty good. We got our colors sorted out. We got some of our images sorted out, but there's more to the application than what you see here, so I'd like to invite Taylor to the stage to tell you about the rest of it. Thank you very much. Thank you Rachel. Now that Rachel's covered a lot of the fundamentals of having our colors and images react appropriately for Dark Mode, let's first take a look at applying some of those to some more detailed areas of our application. First up is Moodometer, where we can keep track of Chloe's mood shifts throughout the day. You can see here that the popover's background has automatically adjusted for the Dark Mode. However, the button content is way too dark, still, on top of the background. Taking a look at our Asa Catalog, we can see that we've prebuilt these images with the color defined in the asset themself. So, we can use the technique that Rachel showed us, where we add a dark variant for those images with a lighter color. However, it's kind of unfortunate that we're only varying the fill color of these images but keeping the same shape. So, there's probably a better way, and, in Mojave, there is, using this contentTintColor API. With this, you can create a template image and provide a custom tint color to be used instead of the default appearance sensitive one. That will be used as the base fill color for any of the places that the image appears. So, we can set the content tint color on an NSImageView to tint its image or on an NSButton to tint both its boot image and title. So, let's take a look at adopting this. The first step here is to convert these to simple template images and remember to set the render mode to be the template effect. Taking a look at Chameleon Wrangler now, we can see that these images are now getting the default templating effect, which looks great in Dark Mode, but the next step is going to be applying that color. So, we could just pull the color out of the asset, add a dark variant, and set the contentTintColor on these buttons to be that named color. And this button will automatically pick the right variant based on whether it's appearing in the light appearance or dark appearance. However, Rachel also showed us how there are a number of colorful system colors that already have these light and dark variants, which we could use instead. For instance, we could use systemRed, systemYellow, and systemGreen, which, if we return back to Chameleon Wrangler, we can see looks really great and has those buttons pop on top of that background. What's really cool about this combination is that not only do we get a very specific green color for both Light and Dark Mode, but with the content tint API, the system effects applied on top of that vary based on appearance as well. As Raymond discussed, in the light appearance, there's a darkening effect on top of that base tint color. Meanwhile, in the dark appearance, there's a brightening effect. All of these effects are further customized for the increased contrast setting without any changes from our application. So, it's really great that we didn't have to specify all 12 of these colors. And instead, we specified a single one and, thanks to dynamic system colors and dynamic system effects, we got all of this completely automatically. So, our Moodometer is looking great. Next, we can step over to another type of app where we keep track of very important information about Chloe, such as how stressed she's feeling or more importantly, what's her favorite type of berry? We can see here that this already looks great, because we are using stock Cocoa controls, which automatically adjust for Dark Mode. We can see how it reacts to the accent color feature that Raymond talked about by opening up system preferences, changing the color to a nice chameleon striped yellow, and returning back to Chameleon Wrangler. We can, again, see that the built-in controls automatically adjust to that yellow accent color, including the focus ring and highlight color. However, we can notice that our custom tab bar has remained blue, even though we've changed the accent color. Because this blue doesn't really have any significance for our application, we probably want it to be following that accent color. Like Rachel talked about, we have a few different colors that will follow the users' accent and highlight color selection. New in Mojave is a control accent color, which you can use to directly mimic the accented areas on system controls, such as the sides of popup buttons, default push buttons, or the selected segment of a segmented control. There's also selected content background color, which is what you see in the background of selected table rows, and you can use to indicate custom selection. And selected text background color is exactly what it sounds like. So, going back to our app, we can see that we're using system blue color for this custom tab selection, and, instead, we can just swap that over to using controlAccentColor. The next problem here with the tab bar is a little more subtle. If we bring our cursor over and press down on it, we can see that we're getting a darkening effect instead of the brightening effect that we should in Dark Mode. This is because we are taking that base color and just applying a constant 30% darkening effect to it, assuming that would look fine in the light appearance. Instead, we want to describe something that basically this color is being pressed, and that semantic description can carry whatever effect it needs to in the light versus dark appearance. So, with the new system effect API in Mojave on NSColor, we can describe that exact semantic. We can take our base color of controlAccentColor, say that we want to get the pressed variant, and it will react appropriately in both the light and dark appearance. There's one more area of our application where you can see the accent color, and that's in this photo browser where we draw a selection ring around the selected photo. Here, we were using the selectedContentBackgroundColor, and that, again, automatically adjusts based on the user's selected accent. Let's open up one of these photos and take a look at a nice, big, beautiful photo of Chloe. The first thing that stands out is the fact that the background has not adjusted for Dark Mode. It's still using its light translucency effect similar to a hardcoded color. This effect is provided by NSVisualEfffectView, which provides the translucent blur materials you see across the system. For instance, there's the sidebars which blur onto the desktop and other windows behind it, as well as things like the title bar, which blur the content that's scrolled underneath. NSVisualEffectView also enables the vibrant foreground blending, that Raymond talked about, by taking a grayscale color plus a Blend Mode and making it really pop on top of these backgrounds. The advanced Dark Mode talk is going to go into a lot more detail about how you can use those blend modes effectively. Turns out that a lot of places automatically get these blur materials for free. For instance, every titlebar of an NSWindow will automatically have a titlebar material showing onto the content behind it. Source list table views and sidebar style split-view items will automatically get a sidebar material showing onto the content behind the window. For custom UIs it's pretty easy to build these custom effects ourselves, as well. But the most important quality of them are the material that you choose to have them represent. We have a few nonsemantic materials, which, like hardcoded colors, just describe the exact visual quality of the material and not the intent of what you're trying to represent. These will not be adjusted for Dark Mode and will stay with that exact quality. However, we have a number of semantic materials, as well, which allow you to describe what type of UI your custom UI is intending to represent. For instance, for a custom menu like UI, you would want to use the menu material. And these not only adjust for Dark Mode, but guarantee that UI to always match those materials even when the system changes in the future. We've had a few of these in the past, but in Mojave, we have several more so that you can make sure to really tailor your custom UI to match the material it needs to. Jumping back to our photo here, we can see that we were using the mediumLight material, because we were trying to represent Quick Look style popover windows. And instead, we can just simply swap that over to the popover material and get the exact effect we want to in Dark Mode. Now, this isn't the only area where we get a unique material showing in the background of our UI. Like Raymond talked about, there's this desktop tinting effect that's shown here in the background of our photo gallery. We're getting a little bit of that blue from that desktop behind it, creating this really visually harmonious result. It's a lot easier to see on top of a more vibrant background, such as these beautiful flower pictures. There's three different variants that have semantics with these background materials. There's the window backgrounds that we see on the backgrounds of generic windows; Under Page Background, which appears behind document areas such as in Preview or Keynote. And perhaps most commonly is the Content Background material, which is white in light appearance and, of course, dark in the dark appearance. In fact, all of these are now dark in the dark appearance but also have that wallpaper tinting effect. The great news is that it's really not hard to get these effects. For the most part, it comes completely automatically by using system views. For instance, the default backgrounds of every window has that window background material built right in. In addition, [inaudible] views such as NSCollectionView or NSTableView also automatically come with the content background material built right in. We've seen a number of places across this system that were actually able to improve their Dark Mode support by deleting an override of these default colors that these views have. If you want to customize beyond the default color that these views have, you can explicitly set a variety of system colors as their background color to get the exact material effect you want. One really important note here, though, is that if you were to read the RGB values of these colors, they would not include the desktop tint. This is because, as Raymond mentioned, this tint is extremely dynamic. As you move your window around on screen, that's getting updated asynchronously to the applications rendering itself, creating a really nice experience as the user is moving around windows. So, jumping back to Chameleon Wrangler, we automatically got this content background because we just use a stock NSCollectionView. In the vital section, we got that window background material showing through because we had nothing opaque covering it up. However, in our notes section, where we have this awesome custom stationary, we have a light grey background that isn't appropriate for Dark Mode and hasn't been adjusted with that under page semantic. We can open up Interface Builder and see where we're setting up this background drawing view and see that, indeed, we are filing it with a custom gray color. And pretty quickly in IB, we can change that to using the Under Page Background color that represents that semantic and immediately see that we'll get the right result, making it look just at home with the rest of the document apps in Dark Mode. The next thing that kind of stands out here, though, is the dark foreground on top of light background of our stationary and document area. And because this is a WYSIWYG editor where we, as users of Chameleon Wrangler have complete control over the colors, images, and fonts seen within it, we expect this to stay the same as a light appearing document. However, there are a few subtle details that don't seem quite right. For instance, the text highlight color is still getting the dark variant of the text highlights, as well as the autocorrect bubbles have the dark appearance, which really clashes with the light appearance of our document. We, basically, want to have everything in this area revert back to the light appearance, and we can do that using NSAppearance. It turns out that NSAppearance is what's been providing the magic of having light versus dark variance automatically switch in Dark Mode this whole time. You can conceptualize NSAppearance as a little bundle containing all of the colors, materials, images, and control artwork that you see across the system. Now, when your application does something like request to draw with labelColor, what that does is it looks up in the current appearance for what the exact RGB values should be. The same is true for control artwork or the different material effects. Now, what's new in Mojave is the dark appearance, which our designers have meticulously crafted with redefining everything across the system. When the system is running in Dark Mode, what happens is that the dark appearance gets applied to your application instead of the light appearance. So, when labelColor goes to draw itself, it references the dark appearance as white value instead of the light appearance as darker value. The same is true for all of those materials. And these aren't the only materials in the system. Right, these aren't the only appearances in the system. There are also the high contrast appearances, which get used when the accessibility high contrast setting is turned on, these will get applied to your application instead. And there's actually even more. There are the vibrant appearances that, again, completely redefine all of these colors that appear in sidebars and titlebars. And the advanced Dark Mode talk is going to go into more detail about how you can effectively use the assets that come out of those vibrant appearances to really make them stand out. Now, by default, the appearance of your application will follow the system. However, you can explicitly override it by setting the appearance property to whatever you want. So, here we have Chameleon Wrangler in Light and Dark Mode, and we can see it's following the system, but as soon as we set its appearance to always be darkAqua, we can see it's always getting that dark appearance. The inverse is true by setting the explicitly light appearance, and, of course, setting it back to nil will have it return back to the default behavior of following the system. Now, we know that after a lot of you see your apps running in Dark Mode, you're going to be really tempted to have your app always be dark, because it's just going to look so good. But like Raymond mentioned, it's really important to keep in mind "When it is appropriate to have an always dark app?" This should be generally reserved for very media focused apps like QuickTime player or professional color-sensitive workflow apps, where it's important to eliminate all of the light behind the content. And, in general, it should be left up to a choice for the user. So, in general, this is the system preference. Some of you maybe already have a dark theme for your application controlled via app preference, so your first step would be to tie that back to the system preference so the people using the system can have a consistent experience. And in some ambiguous cases like Raymond mentioned, there may be some additional need for an in-app preference, such as in the mail case, but for the most part, the system preference should be exactly what users go for. If you do decide you need to have an always dark application, it's really important to not to use the Vibrant Dark appearance on the entire window, because that's not appropriate for the opaque window areas of the application and are really only intended for sidebars. Instead, you want to use the Dark Aqua appearance if you want to have that always dark look. In addition to be able to be set on the window as a whole, you can also set it on an individual window or view scoping, and that's inherited by all of the views contained within it. For instance, here, we're setting the sidebar to always have the darkAqua appearance if we want to experiment with something like that. And all of the text and images within that also get that dark styling, but we can also use this to fix the problem we had before with our text area. We can see here that we actually want this entire document area to remain having a light appearance, and so by setting the lightAqua appearance, we can make sure that the text highlighting color gets the correct appearance as well as the autocorrect bubble inheriting that light appearance as well. However, in addition to be able to be used for some really cool effects, it can also lead to some hard-to-debug situations. For instance, here, we're using the vibrantLight appearance explicitly on our table view, and that looks perfectly fine in the light appearance, where it's actually already inheriting the vibrantLight appearance. In Dark Mode, this quickly becomes a very visible problem. We can see that the text and images inside that table view are now inheriting that vibrantLight appearance and look bad on top of that dark background. So, for the most part, we just want to remove these explicit overrides in places where we really do not intend to have an always light appearance. This can also happen in Interface Builder, where, at the very bottom of the Attributes Inspector, we've accidentally overridden the appearance for our outline view to always be the light Aqua appearance. Here, the fix is simply to change it back to the inherited style so that it can make sure to follow Dark Mode when that's turned on, so do be sure to check for all of those. At this point, Chameleon Wrangler's looking really great in Dark Mode. But kind of an understated thing here is that, at this point, we've written 0 lines of code specific to Dark Mode. The types of changes necessary for being a great Dark Mode citizen is that you need to make sure to express the ideas semantically rather than literally. So, for instance, these same sets of changes allowed our application to not only look great in Dark Mode but also in the light appearance. Adding support for things like the accent color as well as making our label colors more consistent with the rest of the system. Further, these same sets of changes needed to be great in Dark Mode are also the same sets of changes needed to have great support for the high contrast accessibility setting. So, to reiterate what those steps were, the first and foremost was linking on the 10.14 SDK using Xcode 10. For a handful of apps that already use system controls and system colors, this is the only step that's going to be needed, and it's already going to look great in Dark Mode. For the rest of us, the next step is to audit our app for the use of static colors, non-template images, and nonsemantic materials, and replace those as appropriate. For instance, this might be using a system color, a semantic material, or, in some cases, defining a custom asset catalog color or image. And, again, it's important to keep an eye out for those accidental appearance overrides. The advanced Dark Mode talk is going to go into a lot more detail here, including how to correctly use NSAppearance, how to do custom drawing and effects within NSVisualEffectView, how to make sure all of the myriad of ways you might be drawing custom views work correctly in Dark Mode. Some tips and tricks for using View Debugger and Interface Builder to really polish your app. And because we know that a lot of you and your apps' fans are really going to be wanting to have Dark Mode as soon as possible, we want to make sure you can do that without sacrificing how far you can back deploy. So, in general, a lot of these features are available for a year or more, but the advanced talk is going to go into details about how to take that even further back. We have a few Cocoa Labs, but I also really want to point out the Dark Mode Lab, where, in addition to engineering support, Raymond and other members of the design team will be there to help answer any design questions you might have, as well. So, that's a really special treat that I encourage you to take a look at. Thanks for staying until 6:00 today, and have a great rest of the WWDC.  Good morning. Welcome to Session 204, Automatic Strong Passwords and Security Code Autofill. My name is Chelsea. You may have seen Automatic Strong Passwords and Security Code Autofill in the State of the Union, yesterday. I'm really excited to tell you more about these features, today. These new features and other features in iOS 12 will help users log into and create accounts with ease in your app. If you have a login screen or an account creation screen in your app, this session is for you. A major pain point for users is dealing with passwords. They can be a pain point, even for people that use the best practice, which is to use a password manager to create and fill passwords for them. For everyone else it can be tempting to do something easy but insecure, like reuse a password they're already using on another service or use an easy to guess password that they can memorize. We know that the most private secure thing for your users is to use strong unique passwords for each service that they use. We've all heard of breaches in services that result in users' passwords being exposed. Users then need to go and change their password on every service where they were using it. The features that we'll discuss today will help users choose strong unique passwords for your apps. This way, you both help users that are trying to consciously use best password practices, as well as your users that would rather not think about passwords. Many of your users use the iCloud Keychain. The features that we're going to talk about today are built on top of the Keychain. It has best in class security and it protects users' passwords behind biometric authentication. Apple does not have access to the credentials stored in Keychain, so users' privacy is preserved, as well. The iCloud Password Keychain Manager can help users log into and create accounts in your app. We've added some new features on iOS 12 to help make account creation and login even easier. I'm really excited to show you these features with a demo. So, I'm going to go to the Shiny app. This is my favorite demo app for creating an account and logging back into it. So, I'm going to go ahead and create-- and tap create account. I'll focus the email field and you'll see that I've been given a suggested username. This is a new feature on iOS 12. These suggested usernames are based on credentials that the user already has stored in the Keychain. Since I always use Chelsea@example.com, I'm going to select that as my username. Without another tap, the password field is focused for me and a strong unique password has been provided. I'm also told that I can go and look up this password in Settings at any time. So, if I need to go and type it onto another device where iCloud Keychain is available, it's available to me. So, I'm going to go ahead and use this strong password and sign up. So, with just a couple of taps I'm logged back into the Shiny, or I'm signed up for the Shine app. Now, let's fast forward through all of the other account setup. I've turned on second factor authentication. And let's look at what logging back into Shiny looks like. So, I'm going to go ahead and tap the email field. As you can see, the credential I just created is suggested right on the QuickType bar. Going to select that. And then, after Face ID, my password and username are filled and I can log in. Now, I've set up second factor authentication. Normally, I'd need to try to memorize this code or go back to Messages. But as you can see, the code that I just received for second factor authentication is right on the QuickType bar. So, with one tap I can fill the code. Yeah. It's really awesome. So, I can fill that code, submit, and again, with a minimal number of taps I'm logged back into Shiny. Back to the slides. So, as you saw, creating an account with Automatic Strong Passwords is quick and easy. I didn't think about the password, since one was provided for me and inserted into the password field. I also didn't think about the password when logging back into Shiny. Passwords have truly become an implementation detail to logging into my app with Automatic Strong Passwords and Password Autofill. Here's today's agenda. First, we're going to do a quick recap of Password Autofill, since many of the features that we'll talk about today are built upon that feature. Next, we'll talk about how to ensure that Automatic Strong Passwords works in your app. Then, we'll talk about Security Code Autofill. Next, we'll talk about federated authentication with a third-party service. And finally, we'll discuss some new password management features that the iCloud Password Keychain Manager provides. In iOS 11, we introduced Password Autofill for apps. Some of the new features that we're going to discuss today are built upon the same adoption you may have done for that feature. Password Autofill helps users log into your app by surfacing credentials right on the QuickType bar. Here's a quick recap of how to ensure that it works in your app. Passwords in the iCloud Keychain Password Manager are stored based on domains on the Web. Like Apple.com. Thus, it's important to have an association between your app and the domain on the Web. This way, we're able to confidently surface credentials on the QuickType bar. You already have this association if you've adopted Universal Links or Handoff. The process of adding this association is pretty simple. You'll have a small change to your apps Entitlements file and you'll serve a file from your domain on the Web. If you'd like to see an in-depth look at how to set this up, see Introducing Password Autofill for Apps from WWDC 2017. It's important to always tag your fields with text content type, so that Autofill can be offered at the right place. Tag your username fields with the username content type. Tag fields where users are going to be filling, or passwords for existing accounts with a password content type. If you've chosen not to use Secure Text Entry for your password fields, it's particularly important that you tag your fields with the password content type. This way, we know we're in a password context. Now, that we've discussed how to get Password Autofill working in your app, let's talk about some improvements that it has. Since, iOS 11.3, WKWebView supports Password Autofill. This helps your users if your login screen is implemented using Web technologies. New to iOS 12, password managers from the app store can provide information to Autofill. This means that any work you do to support the iCloud Keychain Password Manager filling credentials in your app, also, helps users of these other password managers, as well. If you're a developer of a password manager, see Implementing Autofill Credential Provider Extensions. On iOS 12, we now offer to save credentials when a user logs into your app with a new account. This way, users can then use these credentials in your app and website on all of their devices. Let's talk about how to ensure users are prompted to save and update passwords in your app. Here's how saving works. First, Autofill infers that we're in a login scenario. Then, Autofill checks the eligibility of your app based on if there's an association between your app and the domain on the Web. Without this association saving passwords will not be offered. Next, it finds the username and password fields so that it knows which data to save in the new credential. Then, it detects that a sign in action has occurred. And finally, Autofill decided whether to prompt to save or update the password based on if this is as brand new credential that's not yet in the Keychain. Or if the user is updating an existing credential. Now, this may work with no changes to your app on iOS12. But let's talk about some steps you can take to ensure that it works. First, make sure to tag your fields with the username and password content types, like, with filling passwords. Make sure to remove your username and password fields from the view hierarchy when a sign in occurs. This way, Autofill can detect that a sign in is occurring. You could do this by dismissing the view controller that your sign in fields are in. Make sure to only clear the username and password fields after they've been removed from the view hierarchy. This way, we can read out the data and save it into credential. Make sure the Autofill saves the credential to the correct domain. You can do this by saving a password in your app, and then going to Settings to check where the credential is saved. If you notice that Autofill is not saving to the correct domain, you can override where it's saving using the Web credentials associated domain service. Finally, you may have previously been using SecAddSharedWebCredential to manually save credentials when a user logs into your app. However, now that Autofill automatically prompts users to save passwords you may no longer need this. You will still want to use this if your app has a web view for its login screen because saving is not supported there. So, here are some of the key steps that we've discussed so far to make sure that filling and saving passwords in your app works. Make sure to associate your app with the domain on the Web. Tag your fields with the username and password content type. And for saving passwords, ensure that login is detected by making sure that the saving prompt appears when you sign into your app. By ensuring that Password Autofill works in your app, you help your users log in with ease. For new users, one of the first encounters they'll have with your app is creating an account. In my demo you saw how Automatic Strong Passwords made this process super easy. I'd like to invite Reza to the stage to discuss how to ensure Automatic Strong Passwords works in your app. Thank you. Thank you, Chelsea. Hi, everyone. This is Reza and I'm really excited to tell you all about Automatic Strong Passwords. Account creation is a point of frustration for many users. They might even leave your app and never come back, or as many of them do, decide to use a weak password or keep reusing the same one. This significantly reduces their security. Although, it might be an alternative to account creation, some of you might have concerns about allowing social media to be used to sign into your apps. Automatic Strong Passwords makes account creation just as easy. It brings convenience and security to the signup process. Users no longer need to think about or worry about passwords. Autofill even suggests usernames to ease the signup process. With Automatic Strong Passwords, account creation is only a few taps away. So, users are more likely to sign up and user your services. Now, I'd like to talk about how Automatic Strong Passwords works. Similar to what Chelsea explained in a login scenario, when your app presents a view controller Autofill infers its type. In this case, it's a signup view controller. It will then check the eligibility of your-- it will then check the eligibility of your app based on the associated domains to figure out if it can save passwords. If that's the case, Autofill will then detect relevant signup form elements; the username and the password. Once the username field become first responder Autofill suggests usernames. This is a new feature we're adding in iOS 12. The user proceeds with the suggested username. And eventually, the password field becomes first responder. Autofill automatically inserts a strong password into the password field. At this point, the user only needs to proceed with the suggested password and sign up. Autofill takes care of saving the password. In many cases, this happens automatically without any adoption requirements in your apps. However, in order to ensure your app's compatibility with Automatic Strong Passwords there are a certain number of steps you should take. Many of these steps are identical to those Chelsea explained to make your apps compatible with saving. Make sure to tag your username field with UI textContentType username. New in iOS 12, make sure to tag your new password and confirm new password fields with UI textContentType newPassword. If you're using a UI table view to represent your signup form make sure to use unique instances of UITextField for the username and the password field. This is important, because once Autofill detects username and password fields, it expects to be able to reliably read their values later on. Some of you might have changed password forms in your apps. Automatic Strong Passwords is compatible with change password forms if Autofill is able to detect username and password fields on the same screen. Note that the username field can be Read Only. Best practices that we discussed for signup forms, also, apply to change password forms. Now, let's take a moment and talk about the format of these generated passwords. The generated passwords are 20 characters long. They contain uppercase, digits, hyphen, and lowercase characters. This gives you a strong password with more than 71 bits of entropy. We designed this to be a strong, yet compatible with most services. It is, of course, possible for your apps to define their own custom password rules. As I mentioned before, in most cases, you don't need to do this because the default format of Automatic Strong Passwords should be compatible. However, if your app's backend requires a separate set of rules that are not compatible with the default format of Automatic Strong Passwords, you could define your own rules. To do so, use the new password rules language in iOS 12. Following the format of the password rules language, create a rulesDescriptor. Using the rulesDescriptor create an instance of UITextInputPasswordRules and assign it to the password rules property of UITextField. Once you do this, Automatic Strong Passwords will generate passwords based on these rules. We have also created a new web-based Password Rules Validation Tool. Use this tool to ensure the rules that you specify are correct and produce the type of passwords that you expect. I'll talk more about this, shortly. Now, that we talked about the steps it would take to make your apps compatible with Automatic Strong Passwords, I'd like to show you a demo. I'm the developer of the Shiny app and I want to make sure that Shiny is compatible with Automatic Strong Passwords in iOS 12. So, the first thing to do is to run Shiny using the iOS 12 SDK and try it out. So, in Xcode I'll click on Run. And here is Shiny. I'll tap on Create Account. Tap on Email. And I don't see any suggested username. When I tap on the Password field I don't see any Automatic Strong Passwords suggestion, either. So, let's go back to Xcode and investigate this, further. The first thing to make sure of is associated domains for your apps. In this case, I've already ensure that Shiny has an associated domain. In fact, earlier today I was able to get Autofill suggestions when I wanted to logging into Shiny. Next, you should take a look. You should take a closer look at your signup view controller. I'll click on the Email field. And here, under Text Input Traits, Content Type, I see that I'm correctly setting the content type to username. And because I want the format of this username to be of type email address, I'm also correctly setting the Keyboard Type to Email Address. So, this is good. Let's take a closer look at the Password field. I see that the Content Type is set to Unspecified. This should, actually, be set to New Password. So, I'll select New Password, here. And because this is a password field, I'm going to mark it as Secure Text Entry. All right. Let's run Shiny again and try it out. :15 Okay. Tap on Create Account. Tap on Email. Here, I see I'm getting suggested username, now. I'm going to go ahead and proceed with the suggested username. I also, see Automatic Strong Passwords. So, in most cases, this should be enough and you should be done. Let's proceed with the signup process. For the purpose of this demo, I'm requiring Shiny passwords to contain a dollar sign. This means that we need to specify our own custom password rules for Automatic Strong Passwords. And actually, the best way to do so is using the new Password Rules Validation Tool. Using the tool, I will be able to specify correct password rules, and also, ensure the rule will produce the type of password that I expect. So, let's jump right in. This is the new Password Rules Validation Tool that you can have access from developer.apple.com. For the purpose of this demo, I'm allowing uppercase, lowercase, digit, and requiring at least a dollar sign to be present in my password. At the bottom of the page I see some examples of the password generated. I also, have the option to download a number of these passwords in case I need to run some tests in my app's backend. Once I'm happy with the formatting of the passwords I have two options. If I'm making a native app for UIKit, I can go ahead and copy the rules formatter for UIKit. Or if I'm making a webpage I can copy the HTML formatted rules. For this demo, since we are making a native app for UIKit, I'm going to go ahead and copy the UIKit version. And once I've copied it, all I need to do is go back in Xcode, select the password, and paste in the rule-- in the Password Rule text field. Now, let's run Shiny, again. Okay. I'll tap on Create Account. Tap on. I'm going to go ahead and proceed with the suggested username. And at this point, the password should be compatible with the rules that I just specified. Let's proceed with the signup. And just like that, I'm in. Thank you. We just saw how easy it is to make your apps compatible with Automatic Strong Passwords. Than you, Reza. As Reza showed you, these are some steps you can take to ensure that Automatic Strong Passwords works in your app. Make sure to associate your app with the domain on the Web, just like filling and saving passwords. Make sure to tag your fields with the username and new password content types. Ensure that signup is detected by checking that when you sign up for your app the password is saved. Most of you will be done after doing these three things. However, if your service is not compatible with Apple's password generation format you may want to use some password rules on your password text field. To ensure that Automatic Strong Passwords generates a compatible password. By ensuring that Automatic Strong Passwords works well in your app, you're setting your users up for success by encouraging them to use a strong unique password in your app. Some of your apps and services may use SMS, or security code sent via SMS to your users. I'd like to invite Harris to the stage to discuss how you can service these codes right in your app. Awesome. Thank you, very much, Chelsea. Hello, everyone. My name is Harris and I am super stoked to talk to you about Security Code Autofill. But first, this section needs a little bit of audience participation. So, show of hands, how many of you have ever gotten a text message that looks something like this? All right. I think it's fair to say that most of us have to deal with these codes on a fairly regular basis, these days. All right. Question of the second, after getting one of these text messages how many of you go through a process like this? Where you think you have the code memorized, you type it in, you switch the last two digits around, and then you have to do it all over, again. Let me see those hands. All right. Final question. After going through this process, how many of you feel something like this? All right. So, let me tell you it doesn't have to be this way. Because starting in iOS 12 and macOS Mojave, we're introducing Security Code Autofill. A brand new feature which takes all the pain and frustration out of message based second factor flows. Now, it is important to note that Security Code Autofill does not alter the calculus around the security of second factor methods. What it merely does is remove the indignity out of having to type the code in yourselves in the age of computers. Nothing more, nothing less. So, starting in the new iOS and macOS, instead of exercising your memorizing skills you get to do something like this. Just one tap, right on the QuickType bar, and you're signed in. And hopefully, at the end of this process, you feel a little bit more like this. All right. So, let's talk about some technical details. As with other Autofill features, Security Code Autofill will work in your apps right out of the box, most of the time. Still, you can do just a tiny bit of work to ensure that Security Code Autofill works every single time. We've told you this before but tag your fields. In iOS 12 we're introducing a new UITextContentType with the value oneTimeCode. Tag your security code fields with this and Security Code Autofill will work every time. Additionally, it is very important that you allow usage of the system keyboard for inputting these codes. Tempting as it may be, you should refrain from building Bespoke keyboard UI's within your view hierarchy or setting custom input views in your controls. Because if you do so you are preventing iOS from surfacing the necessary UI or injecting the appropriate events to type the code in on behalf of your users. And additionally, you may be hampering the experience of folks that leverage accessibility technologies like voiceover. So, bottom line is let the system keyboard do the heavy lifting for you and Security Code Autofill will work every time. Another step that you can take to ensure that Security Code Autofill works great with your service is to craft your text message, accordingly. iOS and macOS use data detector heuristics to infer that an incoming message carries a security code. Specifically, our heuristics are looking for words like code or passcode in proximity to the actual code string. And you can see a few sample text messages in different languages in the slide behind me. Now, verifying that your specially crafted message properly triggers the iOS and macOS heuristics is super simple. You can be your own best friends and simply text yourselves. Then, when you go into the message transcript and you see the code being underlined and tapping the code offers you a Copy Code option, you know you have it right. Security Code Autofill is available in all supported locales of iOS and macOS. And if after following these best practices you find that you cannot get Security Code Autofill to trigger appropriately inside of your app or website, please let us know by filing a bug report. We'll be looking out for them. So, so far, we've talked about getting Security Code Autofill to work within your native iOS apps. Security Code Autofill is also available in iOS Safari on websites and, and this is pretty cool, if your users are signing into your service in Safari on their Mac, text message forwarding securely pushes incoming messages from their iPhone to their Mac. Now, what this means is that they can fill in a security code with a single click in Safari's completion UI. You no longer have to switch back to Messages to copy the code out or, you know, go find your iPhone all the way across your home. It's really cool. To support Security Code Autofill in Safari, we are introducing a new input autocomplete attribute value one-time-code. In fact, all of the UI Text Content Types that you heard about today have web platform equivalents that you can leverage to ensure a smooth seamless autofill experience in your websites, just as well as in your apps. You've heard this before but tag your form fields this time and Autofill will work great every time. So, this was a brief introduction to Security Code Autofill, a brand new addition to iOS 12 and macOS Mojave that takes all the pain and all the frustration out of message based second factor flows. Now, with the Autofill features that we've introduced today, it becomes super simple for you to build a relationship with your users while safeguarding their security and respecting their privacy. Still, some of you may find that you have to support federated authentication flows with third party providers, such as popular social networks. For those of you among the audience, we are introducing in iOS 12 a new API to support such flows. We call it ASWebAuthenticationSession and I would like to show you how it works. Now, here I am back in the Shiny app. And this time I will choose to log in with example.com, a super popular social network. Now, as soon as I tap the button the app under the hood calls in to the new ASWebAuthenticationSession API. Now, one of the ways in which ASWebAuthenticationSession makes logins faster is by sharing its underlying cookie storage with Safari. Now, of course, before we do any of this, iOS asks for explicit user consent. Upon consenting, the user is presented with a secure view controller and guided through the third party's federated authentication flow. In this case, I'm already signed into example.com in Safari on my iPhone, so all I have to do is tap Allow. There we go. And with just two taps I've gone through a full federated authentication flow with a third-party service. So, in the past you may have used one of the other iOS platform technologies to implement federated authentication inside of your apps. Starting with iOS 12, ASWebAuthenticationSession is the go-to way to implement such flows. As we mentioned, already, ASWebAuthenticationSession makes login flows faster by sharing its underlying cookie storage with Safari upon explicit user consent. Now, what this means is that depending on the implementation of the federated authentication flow by the third party. If your user is already signed into that service in Safari on their device, they may not even have to see a login form, at all. If they do have to see a login form ASWebAuthenticationSession supports Password Autofill and Security Code Autofill to make the experience as painless as possible. And for you developers, ASWebAuthenticationSession offers a straightforward block-based API that makes adoption super simple. So, let's take a look at it. Now, let's see how simple it is to implement federated authentication using ASWebAuthenticationSession. You're going to start off by importing the brand new AuthenticationServices framework. And then, you simply define the oauthURL as it's described to you by your third-party authentication provider. Now, following this you will create an ASWebAuthenticationSession object. You're passing the oauthURL and a callback handler that gets involved at the end of the authentication session. Now, it is crucial that you hold onto a strong reference to the authentication session object while the session is in flight. And by doing so, you also have the ability to cancel an ongoing session should you choose to do so. Finally, you call the nonblocking start method. Upon doing so, first iOS will ask for user consent to share the underlying Safari, the underlying cookie storage that Safari has with your session. And then, the user is presented with a secure view controller and guided through the authentication flow. When the user completes the authentication flow or if they cancel out of the authentication session, your completion handler gets involved and you can handle the result. And that's how simple it is to implement federated authentication in iOS 12, using ASWebAuthenticationSession. It only takes a few lines of code. Starting with iOS 12 this new API is the go-to way to implement federated authentication and it replaces SFAuthenticationSession. If you find that you have to support federated authentication inside of your apps, we strongly encourage you to adopt this new API. And with that, I'll turn things over back to Chelsea, to talk to you about a number of super cool new password management features that we built right into iOS 12 and macOS Mojave. Thank you, very much. Thanks, Harris. I'm really excited to tell you about some of the new features that the iCloud Keychain Password Manager gives you in iOS 12 and macOS Mojave. You may have seen some of these in the State of the Union, yesterday. What we've talked about, so far, is how you can help users log into and create accounts in your app. However, there are some cases when users will be interacting with accounts for your app when they're outside of the context of your app. For these cases we've added some features to the iCloud Keychain Password Manager to make managing and interacting with these accounts a breeze. If a user needs to look up a password, they can now simply ask Siri. They'll be taken right to their password after authentication. This way, they can read off their password to type into another device that doesn't have iCloud Keychain. Some users may share account information for services like utilities. In order to share this information, users can now AirDrop passwords to their contacts. On iOS 12 the password list UI is redesigned so it's more scannable and delightful to interact with. The password list UI in macOS has been redesigned, as well. And it looks great. Another new feature of the iCloud Keychain Password Manager is it now warns users if they're reusing passwords across multiple websites. When the user taps on one of these passwords they'll be given the option to change their password on the website. Automatic Strong Passwords on the web can help them update to using a strong unique password for that service. The Password Reuse Auditing feature is built right into the Safari Passwords List on macOS, as well. Last but not least, your tvOS apps can now offer to autofill from a nearby iOS device. In order, to learn more about this incredibly convenient feature see the Password Autofill section of the What's New in tvOS 12 session. In summary, Autofill is a powerful platform feature that helps users interact with accounts in your apps. As we discussed, the features that we talked about today may work with no changes to your app, however, it's important that you test your app with these new features. Apply the recommendations that we discussed today. If after applying these recommendations, Autofill is not working as you'd expect, please file a bug report. We'd love to hear from you. To use the Password Rules Validation Tool that Reza showed you during his demo, you can visit the URL listed here. Here are some related sessions you might be interested in. If you're a developer of a password manager, see Implementing Autofill Credential Provider Extensions. If you'd like to learn more about Password Autofill for apps, see Introducing Password Autofill for Apps from WWDC 2017. And if you'd like to talk to us or you have questions about any of the content we've covered today, come to the Safari, WebKit, and Password Autofill Lab after this session. We've also got a couple more, later in the week. Thank you, so much, for attending and have a great WWDC 2018.  Hello, and thanks for tuning in. My name is Anil Katti and I'm an AVFoundation Engineer at Apple. Today, I'm going to present some best practices and recommended patterns for adopting AVContentKeySession API. AVContentKeySession is an AVFoundation API that enables delivery of FairPlay Streaming keys on Apple platforms. So, here's what I'll cover in the session. I'll start with an overview of FairPlay streaming and AVContentKeySession. I'll then present some ways you can optimize playback using AVContentKeySession. I'll, also, cover some common pitfalls around FairPlay Streaming key delivery and answer a few frequently asked questions from developers, along the way. So, let's get started. FairPlay Streaming was introduced back in 2015 to help protect HLS streams delivered to our platforms. FairPlay Streaming specification specifies a set of steps an app needs to follow in order to securely deliver content decryption keys so the platform could decrypt and play back encrypted media. While delivering FairPlay Streaming keys your app works as a liaison between the platform and your key server. When it receives an on-demand key loading request from AVFoundation it, in turn, requests for an Encrypted Key Request, also known as SPC. The SPC is then sent to the key server, which responds back with an Encrypted Key Response, also known as CKC. Finally, the app provided CKC back to AVFoundation so it could start decryption and playback. Until recently, apps used the generic AVFoundation API called AVAssetResourceLoader to deliver content decryption keys. But last year we introduced the new AVFoundation Class called AVContentKeySession that was designed specifically around content decryption keys. We had two goals in mind for AVContentKeySession. One, to simplify key loading process and provide applications better control over the life cycle of content decryption keys. And two, serve as a home for new content protection features. AVContentKeySession adoption has been great, so far. And already, a majority of FairPlay Streaming keys delivered on our platforms come through AVContentKeySession. The API has helped developers optimize key delivery in their applications. I will present some ways you could get more out of the API. But before that, let's see what makes AVContentKeySession different. With AVAssetResourceLoader applications could load keys only when AVFoundation requires one and sends an on-demand key loading request. This, typically, happens when AVFoundation downloads playlists, parses them, and then finds out that the content is encrypted. Further, AVFoundation could send these key loading requests at any point during playback. For example, AVFoundation sends new key loading requests if it's switched to a radian that uses a different key in the middle of the playback. AVContentKeySession, essentially, changes that model. It decouples key loading from media loading or playback and gives applications more control over the timing of key loading. AVContentKeySession allows applications to initiate key loading on their own at any point in time. This opens up new use cases, allows applications to optimize key delivery, and improve different aspects of playback experience. Playback startup is one such thing AVContentKeySession could help improve. Instead of waiting for AVFoundation to send on demand key loading requests. After requesting playback, you could use AVContentKeySession to proactively load keys that you predict might be required in the near future based on user actions. You could do all this, even before a user picks something to play. We call this key preloading or prewarming. Further, if you have assets that use multiple keys across different radians, you could batch up all key requests before even talking to your key server. This reduces some load on the key server, and also, eliminates round trips for each keys. You could initiate key loading process by calling processContentKeyRequest method on an AVContentKeySession instance. Once you call this method AVContentKeySession will send you an AVContentKeyRequest object by invoking the delegate callback. AVContentKeySession allows you to perform FairPlay Streaming specific operations like request encrypted key requests and respond with encrypted key response. Okay. So, now let's say you have successfully preloaded a particular key. Is it possible that the key is requested again after the playback starts? Yes, that's possible. The app could receive an on-demand key loading request if the playback route changes, for instance. Keep in mind that the user could decide to AirPlay the content to an Apple TV or plug in a Lightning to Digital AV adapter. And in these cases, we need new keys for decryption. So, the app should always be prepared to answer on-demand key loading requests, even when it preloads keys. Here's another frequently asked question around this topic. How could the app preload all keys that might be required during playback? Well, in order to preload a key, you need the corresponding key identifier. The one that's specified in EXT-X-KEY tag in the HLS playlist. We recommend that you obtain all key identifiers and asset [inaudible] from your server in an out-of-band fashion. Another option is to include all key identifiers in the master playlist as session keys while authoring the content and use preloads eligible content keys property on AVAssetResourceLoader. At this point I would like to point out that we have a dedicated session about optimizing HLS performance in general, not just key delivery. You should definitely check that out. Moving on from preloading, AVContentKeySession could also help you scale your live offering. Are you wondering how? Well, you could use AVContentKeySession to disperse key loading requests from clients that are streaming your live content. Typically, live streams rotate keys periodically in order to add an extra layer of protection for the content. It is easy to imagine that these keys appear at the exact same time when clients refresh live playlists. When that happens, millions of clients request keys, all at once, resulting in huge impulse load on the key server. When you disperse this key request event across a small time window before they appear and before the EXT-X-KEY tag appears in the playlist, you're, basically, load balancing requests reaching your key server. We covered this use case in detail with sample code during last year's session on HLS. So, do check out the sessions were recording on developerapple.com or WWDC app. Apart from allowing you to manage and deliver content decryption keys AVContentKeySession also serves as a home for new FairPlay Streaming content protection features. We released one such feature last year. We call it Offline Rentals. Offline Rentals is a FairPlay Streaming feature that allows you to specify two expiration durations on persistent keys; storage and playback duration. Storage duration specifies for how long you wish the key to be valid while it is sitting in your storage. That is, before being used for playback. It is, typically, large, like 30 days. Playback duration, on the other hand, specifies how long you with the key to be valid after it was used for playback. It is, typically, shorter than storage duration. For example, 24 hours. This feature allows you to specify such expiration durations when you create persistent keys and the platform ensures that the two expiration durations are enforced, even when the device is offline. So, you might be wondering how and where can you specify these two expiration durations. FairPlay introduced a new TLLV called Offline Key TLLV for this purpose. This has to be signaled in the CKC that is used while requesting persistent key. In terms of code, you generate a persistent key by calling this method on AVPersistableContentKeyRequest Object. This returns a persistent key data blob that you should save in your app storage and use it to answer future key loading requests. Note that this key is valid until the end of storage duration, 30 days from our example. So, when you use this key to start the playback for the first time you might receive an updated persistent key through a new delegate callback. When the delegate callback is involved you should discard the original persistent key and save this updated persistent key in your app storage. You should use this to answer future key loading requests. Note that this updated persistent key is valid until the end of playback duration, 24 hours from our example. So, with that, let's shift gears and talk about error handling, now. Error handling is one of the most important topics from the perspective of improving playback experience. Successfully loading keys involves a number of steps and something is bound to fail at some point or another. So, when something fails the first thing you should do is report the failure back to AVFoundation using AVContentKeyRequest or AVAssetResourceLoadingRequest. Some errors are fatal and some are not. You should help platform make that decision. Another thing you should do is monitor error logs and access logs, investigate the root cause. And try to mitigate errors so your users can get the best experience they deserve on our platforms. If you haven't already, you should also check out one of our recorded sessions from last year to learn more about error handling, in general. Over the years we've seen developers fall into a few pitfalls around FairPlay Streaming key delivery. We have seen apps that hold on to key responses for too long. When AVFoundation sends you a key loading request you should provide a response as soon as possible. Delays in key delivery could result in playback timing out. By the way, you should be able to identify and debug timeouts by surveying your playerItems accessLogs. Another pitfall is around HDCP enforcement. It is important to keep in mind that HDCP requirement for your assets is signaled inside encrypted key response from your key server. If you wish to enforce different HDCP levels for different radians, you should specify different key identifiers for those radians. And provide appropriate key responses from your key server. Lastly, you should be careful while responding to key loading requests with persistent keys because persistent keys are tied to the device they were generated on. For example, you should not use a persistent key to respond to a key loading request from an AirPlay session. Doing so will result in playback failure. We made some API changes in iOS 11.2 to help you avoid and catch such issues sooner, rather than later. Let me walk you through some code snippets to explain. If you are using AVContentKeySession to deliver keys, now you won't even be able to RequestPersistableContentKey request object if the AVContentKeyRequest cannot accept the persistent key as a response. So, when requesting a PersistableContentKeyRequest field you should respond with an online FairPlay Streaming key from your key server. If you are using AVAssetResourceLoader, before responding with a persistent key data blob you should check if persistent key is listed as one of the accepted content types in the newly introduced allowedContentTypes property. So, I hope this provided some overview of AVContentKeySession and clarified a few questions you might have had. To summarize, we covered what makes AVContentKeySession different. We saw how this API could be used to improve playback startup and scale live streaming. We spoke a little bit about Offline Rentals, what it really is, and how to do it the right way. We, finally, highlighted best practices around handling errors while delivering FairPlay Streaming keys and common pitfalls that you should be aware of. Thank you, for listening. And please, visit developer.apple.com for more information about this session and other related sessions. Thanks. Good morning. How is everybody? Good. My name is Matthew Badin, and I'm an engineer at Apple. Welcome to the Using Accelerate and simd session. So my colleague Luke Chang and I are pretty excited today. We get to talk to you about all the great APIs that are available in Accelerate and its associated frameworks, and we're going to start by taking a high-level overview of Accelerate and some of the high-performance libraries that are contained inside it. We're then going to take a deep dive into a few of libraries, and we're going to start by taking a look at vDSP. We have two examples for you. The first one, we're going to show you how to extract signal from noise, and then we're going to show you how you can remove certain types of artifacts from an image. We're then going to take a look at simd, and we're going to show you how you can use quaternions to represent rotations in three dimensions. I'm then going to pass off the presentation to my colleague, Luke Cheng, who's going to show you some of the interesting things that you can do with vImage. So with that, let's get started. So you're probably asking yourself exactly this. What on Earth is Accelerate? So the primary purpose of Accelerate is to provide thousands of low-level math primitives, and we provide these primitives across all of Apple's platforms. So this includes not only iOS and macOS but watchOS and tvOS as well. Most of these primitives are hand-tuned to the microarchitecture of the processor. So this means we get excellent performance. And this performance translates directly into energy savings. So if you're an app developer and you use the Accelerate framework, not only will your application run faster, but you'll also use less battery life. This means your users are going to have an overall better experience. Now, because we provide so many primitives, we find it useful to organize or group them into domain-specific libraries. So, for instance, we group all the signal processing primitives under vDSP. So, for instance, this would be your FFTs or your DFTs and your DCTs, your fast Fourier transforms, your discrete cosine transforms. VImage contains the image processing primitives. If you're doing color space conversions, this is the library for you. VForce contains vector versions of the transcendental functions. So for instance sine and cosine. We also have support for dense linear algebra as well as sparse linear algebra, and we have a specialized library for neural networks called BNNS. It stands for Basic Neural Network Subroutines. Not strictly part of the Accelerate framework but very closely related, and we find these libraries very useful, include libraries like simd, which is a vector programming aid for the CPU, and Compression, which contains several different lossless data compression algorithms. So let's take a look at our first library. We're going to take a look at vDSP. VDSP is a state-of-the-art signal processing library, and it contains a wide range of signal processing primitives. These range from basic arithmetic operations on arrays such as add and subtract as well as more complicated operations like convolution and FFTs. If you're a successful app developer and maybe in the past you have avoided FFTs, I want to show you how we make that easy with Accelerate. With just a few lines of code, you can make that work. And I have an example. I'm going to show you how to extract signal from noise. So what we have here is an audio signal. This is the base signal. We haven't added any noise to it yet, and you'll notice I have two sliders at the bottom. In the lower left I have a slider that lets me add noise, and you can see that. And I also have a second slider that let's me remove this noise called Threshold. You'll also notice I have a toggle switch in the lower right. This lets me look at this under a different domain. So currently we're looking at this signal under the time domain. We're going to do some analysis and see this signal in the frequency domain. You can see all the spikes on the left. Those are the frequency components of the signal. You'll also notice the blue bar. The blue bar is the threshold slider, so you can see I can move it. So what I'm going to do now is add some more noise to the signal. Actually, let's add quite a bit of noise. So you can see the signal that I'm still interested in is represented by the spikes on the left or another way of looking at it is the spikes with the tallest height, and what we've added, the type of noise we're adding to this, is background noise. So it's kind of evenly distributed spikes everywhere, but they're low-level spikes. So now I'm going to remove this noise, and the way I'm going to do this is I'm going to move this threshold slightly higher. As I do this, what's happening is, behind the scenes, is we're identifying any frequency component less than this threshold and removing it. We're saying effectively if there is any spike with a height less than this blue bar, we're going to pretend it's noise and remove it. So if I keep sliding this higher, eventually I've removed all the noise. And if I go back to the time domain, you can see that I removed the noise. And in case you don't believe me, we can remove the threshold, and this is what this looks like with all the noise still added. So let me show you how we do that. At a high level what we're going to do is first perform an analysis on a signal. That's what the switch let me do. We're then going to identify the frequency components that represent the noise and remove them. After we have done that, we're going to reconstruct the audio signal. So let me show you some code. What we're going to use here is a discrete cosine transform or a DCT. You can see here the DCT CreateSetup. This context is going to describe the type of work we're going to do as well as give us space to perform this work. In this case, we're going to use a type 2 DCT. We're then going to pass this context to an execute function. This will actually perform the work. Once we've performed the analysis, we want to remove the noise. This right here is where the magic happens. This routine is going to identify any frequency component less than that threshold and make it zero. It's going to zero it out. After we've done that, we want to reconstruct the audio signal. Again, we're going to need to use CreateSetup to create context. In this case, we're going to use a type 3 DCT to reconstruct the signal. We're then going to pass this context DCT execute, and this will actually perform the work of reconstructing the audio signal. So what we've shown you is an example of how you can remove certain types of noise from an audio signal using vDSP. I want to show you an example of how we can remove certain types of noise from an image also using vDSP. In this case, we're going to restore an old newspaper photograph. So what we've done is we've taken this image, and we've applied a two-tone screen to it. So this could represent an old newspaper photograph. And what we're going to try to do is remove this screen. We're going to try to remove the artifacts that you see. Currently, we're in off, so we're not doing anything. What we're going to do is actually take a sample of this screen, and we're going to create a mask from that sample and then apply that to the image to try to remove it. So let me show you the first attempt. So what we're doing is we are identifying a frequency component at a certain threshold, and any frequency component higher than that threshold we're going to remove. And you see, if we set the threshold too low, not only do we remove the artifacts, but we also remove quite a bit of the image as well. If we set it too high, you can see that we didn't do much of anything. Medium seems about right. Medium seems too correctly identify the artifacts in the image without damaging too much of the image. So let me show you how we do that. At a high level what we're going to do is we're going to perform an FFT on the image and the sample. We're then going to create a mask from that sample and apply it to the image. Once we've done that, we're going to reconstruct the original image. So let me walk you through some code. We're going to do an FFT. This means it needs to be a power of 2. That's why you see the log2 call. The 1024 x 1024 is the size of the image. We're then going to pass it to fft2d zrop. This is quite a mouthful. The important part here is the op stands for out of place. So we're going to have to create some temporary space just for the result. We're going to store in this complex structure, and effectively this says we're going to store the complex number in two arrays with the real component in one array and the imaginary component in the second. We also need to specify a direction. In this case we're going to do a forward FFT. Now the artifact removal is a little bit more advanced, so I'm only going to go through this at a very high level. I recommend you download the sample application that's available online right now, and each one of these routines is also documented under vDSP, and we have excellent online documentation. A high level what we're going to do is we're going to identify the magnitude of the frequency components, in this case with the sample. We're then going to identify the components we want to remove. We're then going to create a mask from that. And once we've done that, we're going to apply that mask to the image. Effectively what we're doing is we're multiplying by 0 the components we want to remove and by 1 the components we want to keep. We're going to use zrop again to reconstruct the image. Because this is an FFT, we get to reuse the context. In this case, one key detail is we're going to store the image in two arrays because this is a complex structure. So the even pixels are going to be in the real array, and the odd pixels are going to be in the imaginary. And again we're going to specify a direction. In this case, we're going to use an inverse FFT. So now I want to shift gears for a moment. Previously we showed you two examples, and we had two working examples for vDSP, and then we worked backwards, and we showed you how we constructed these examples. We worked backwards and showed you the moving pieces. For this next library, what I want to do is start at the basic components. I want to build up to a result. So we're going to take a look at simd, and we're going to start at the basic low-level primitives, and we're going to build up to rotations. In this case, rotations of 3D objects. At a high level, simd is an abstraction for the vector processing unit. So what this lets you do is declare vector and matrix objects, and you get to perform operations on these objects, and this will map directly to the vector hardware of the processor. So let me show you a coding example. So what we're going to do here is we're going to take two arrays, and we're going to average the components. So we're going to iteratively go through each of the scaler components, add them together, and divide by 2. This is going to be really slow. Instead, you can declare these arrays as simd float4 vector types. So then we can perform basic arithmetic operations on these objects. So not only can you express the computation more naturally, but this will also run about as fast as it can. And this will work across all of Apple's platforms. Simd has a tremendous amount of functionality. In addition to vector and matrix objects and allowing you to perform arithmetic operations on these objects. It has extensions. So for instance, dot products and clamp. It also has support for the transcendental functions, so for instance sine and cosine as well as quaternions. Quaternions are very useful for representing rotations in three dimensions, and I actually want to talk a little bit more about those. So I'm going to walk you through a coding example. There's quite a bit to unpack here. So we're going to start on the right. We have a unit sphere. That's what this gray sphere is. And you'll notice this red dot. And that's actually the tip of this vector. We've declared a simd float3 vector. We've set the x and y components to 0 and the z component to 1. So it's coming out at us. And that red dot is represented by the tip of that vector. We're now going to perform a rotation on this vector using a quaternion. Technically, we're rotating the entire scene, but for purposes of illustration, we're going to say we're rotating this vector. When using a quaternion for rotations, you need to specify an axis and angle of rotation. Or to put it another way, what are you rotating about and by how much. So we're going to rotate about the x axis, and we're going to rotate pi over 3 radians up. You apply the rotation by calling the simd act function. This applies the action of the quaternion on that vector and returns a rotated vector. So let's take a look at that now. So normally you're not interested in rotating along a single axis. You usually want to rotate along multiple axes, and if you're already familiar with rotation matrices, this is going to seem very natural. Like rotation matrices, you can combine the rotations using multiplication, and also like rotation matrices, the multiplication is not commutative. So this means if you change the order of the operands, you will change the order of the rotations. So effectively what we're going to do here is rotate pi over 3 radians up, and then pi over 3 radians to the right. And we're going to combine that into a single rotation. Some of the more interesting things you can actually do with quaternions and simd is interpolation, and we support two types of interpolation. The first is Slerp. It stands for Spherical Linear Interpolation, and there are actually two variants of it. We have simd slerp, which will find the shortest arc between these two points, in this case between the blue and the green, and we have simd slerp longest, which will find the longest arc. So you'll actually see it go behind the unit sphere. The second variant is Spline. Spline is really more useful if you have more than two rotations. So, for instance, here we're going to interpolate between an array of rotations, and there's quite a bit of boilerplate code here, so I want you to focus just on the Spline call. Effectively what we're doing is just iterating through all the individual rotations and applying Spline. So the thing you have to specify with Spline is not only the two rotations that you wish to interpolate between but also the previous and the next as well. And this is what this looks like. So if you're a game developer, you're probably not interested in rotating individual vectors. You're probably interested in rotating objects. So we have that for you. We have a cube. It's represented by multiple vectors, and it's going to go through a series of eight rotations. On the left, we're going to trace those rotations using Slerp, and on the right we're going to use Spline. So let me show you what Slerp looks like. And you can see because it's a linear interpolation, every single time it changes direction you get these sharp corners. Whereas if we look at Spline -- Because it's aware of the previous and next rotation as well, you end up with these rounded corners. So let's see that again. So now I went through all those topics pretty quickly, so as a quick recap, we started by taking a look at vDSP, and we showed you two examples. The first was how to extract a signal from noise, and the second was how to remove certain types of artifacts from an image. We then took a look at simd, and I showed you how you can represent rotations in three dimensions using quaternions. I'm now going to pass off the presentation to my colleague, Luke Chang, who's going to show you some of the interesting things you can do with vImage. Thank you, Matthew. Hello everyone. My name is Luke Chang. I'm an engineer in Vector and Numerics group. Today, I'm going to talk about vImage, what vImage offers, and how easy it is to use vImage in your apps. With just a few lines of code, you can create engaging video effects in your app. Let's get right to it. VImage is our image processing library. It has several components. The first component is the conversion function. Conversion function help you move image between different image formats. Different image formats have different advantages. For example, RGV format matches the pixels on your display, so it's best for the display. On the other hand, we have YCbCr image, which is similar to how human perceive the image. Human eyes recognize brightness, which is the luminance channel. Also, the color, which is the chrominance channel. Also, camera uses YCbCr format to capture images. So converging function helps you easily move images between these formats. Next, we have geometry function. Geometry function changes the size or orientation of the image. We have vImage scale that can do enlarge or shrink the image. We use Lanczos algorithm, so we'll have high-quality output after the operation. We also have vImage rotate that can rotate image clockwise or counterclockwise. Next, convolution function. The most notable effect of convolution function is the blur effect. You see blur effect all the time, in UI, in photography. If you want to phase something into the background, you can apply the blur function, blur effect. Next, transform function. Transform function is basically a matrix multiply. It lets you operate on the data channel of each pixel. Let's say you want to increase red or increase green, you can do that with transform functions. Morphology. Morphology changes the size or shape of the objects in the image, not the image itself. We have vImage erode, vImage dilate to make the object smaller and bigger. If you're feeling adventurous, you can actually provide a custom shape of kernels to these functions, and vImage erode and dilate will make the object smaller or bigger according to the kernel you provided. Those are the five things in vImage. Now, I want to show you a demo app that we wrote based on vImage and show you what kind of effect you can get from vImage. What I have here is a lab that captures images using the back camera and put the image onto the screen. And we're doing in real-time. This is a live stream, so you can see the drinking bird doing its motion next to the roses. All right, the first effect I want to show you is the color saturation effect. You can see this effect in a lot of photo editing software. So I want to bring out the color. What I can do is move this slider to the right to make the red really red and green really green. And on the other side, I have white roses. I feel that the color of white roses is not that interesting to me. I would like to direct the focus of my audience to the composition and the contrast of this image. I can slide to the left to desaturate the image to the point that this becomes a black and white image. So now color is no more distraction in this image, and the viewer can focus on the composition and contrast. Okay. So, how do we do this? There are several steps we need to take. First, of course, we have to get the image from the camera, and then we want to use vImage to process, to apply the effect, so we have to prepare the input and output buffer for vImage. Then we actually calling vImage functions to apply these effects, and we display the output to the screen. Let me jump ahead and talk about how do we apply effects using vImage functions. The effect I show you is a color saturation effect, and here is the formula to do color saturation. Basically, we want to remove the bias from the pixel, and using multiplication to apply the saturation effects, and then we put the bias back to the pixel. VImage has exactly the function to do this operation, which is vImage matrix multiply. VImage matrix multiply takes the preBias, in this case minus 128, to remove the bias, and because the saturation is [inaudible] point and the image is an integer, we want to convert this saturation value first into fixed point format. We chose Q12 as the fixed point format, hence a divisor of 0x100. And then we have the postBias 128 times the divisor, just to put the bias back to the pixel. And the matrix itself is really, really simple. All we want to do is just doing a scaling of CbCr channel. So the matrix itself is just a scaler, want to multiply CbCr channel with this scaler. We have all the information, so let's call vImageMatrixMultiply, and with just one line of code, one function call, you can achieve the saturation effect. Now let me come back to other steps that we need to take. We need to take the image from the camera. How do we do that? We need to write a delegate method, and what camera gives us is a CV image buffer. So we get the buffer. We have to make sure this buffer is accessible to CPU. That's where vImage lives. After we apply the effects, whatever effects that may be, and we have to unlock the base address of this pixel buffer so that the camera can reuse this piece of memory. The second step, we have to prepare the vImage input and output buffer. We already have this image in CV image buffer. All we need to do is just get the information such as height and width, and then we can package this into a vImage buffer object so it can be consumed by vImage library. We do this for luminance and chrominance channel. Now we need to prepare an output buffer. Remember, we don't have a piece of memory allocated to the output image yet, so we need to do that, and vImage has a convenience function, vImage buffer in it, to do just that. Given this height, width, and bits per pixel, vImage buffer in it will allocate a memory that's large enough to hold this image and then also create a vImage buffer object so it can be consumed by the vImage library. Last step is put this process image to the screen. Like I said before, RGB is really the best format for display, so let's use the conversion function to convert YCbCr image into a RGB image. And then because the UI expect the cgImage object, we have to create one. There is a convenience function in vImage. vImageCreatesCGImage FromBuffer that helps you to create a cgImage based on the buffer you already have in vImage. One thing to note is that we're not actually copying the large data buffer in the image from one place to another. We're simply creating a cgImage object that adds a container to this image buffer. So we are only filling in the information that cgImage needs, create a cgImage objects, instead of copying data around. Once we have that, we can send the cgImage object to the image view, and it will be displayed on the screen. So it's that simple. Four steps and you can create your own effects, we show you the saturation effects. Now, there are other effects we can do with vImage. We can do a rotation, like I said before, rotate the image clockwise, counterclockwise. We can do blur or phase something into the background. And you feel, if you feel like you want to add some retro feeling to your images, you can do dithering for black and white images, and color quantization for color images. Let me show you how they look like in the app. So, again, I have the slider here to control the rotation. I can do rotate counterclockwise or rotate clockwise. Now I want to try the blurring effect. Let me click on the one here. And I can apply more blurring or slide to the left, bring the roses back to the foreground. For black and white, I can use dithering. Now this black and white image, the gray scale is represented now by the density of the dots, that's the dithering effect. And we use the accents and dithering algorithm for this, and I will show you how to do it later, and for color quantization, we have lookup table. And I can move this slider to increase the quantization level. As I move the slider to the right, fewer and fewer color is available in this image. That's sort of creative, that's how your computer screen looks like in the '90's or in the '80's. All right. So let me show you how we do that. For rotation effect, you can call vImageRotate and given the angle of rotation, it will do counterclockwise or clockwise to better align your images. For the blur effect, we use TentConvolve. The blur effect is controlled by the size of the kernel. The larger the kernel, the more blur you'll get. Dither effect is basically converting an 8-bit image into a 1-bit image. At the same time, you can specify a dithering algorithm. In this case, we used the Atkinson dithering algorithm. Color quantization, we used the quantization level to create a lookup table for the RGB channel, and we call vImageTableLookUp to apply this table lookup to RGB channels to limit the number of colors on the screen. Those are the four additional effects I wanted to show you, and I think now is a good time to move onto the next topic. LINPACK Benchmark. We talked about the functionality of Accelerate. We talk about how easy it is to use in Accelerate in your apps. We haven't talked about how fast Accelerate really is. And LINPACK Benchmark is a perfect tool to do that. What is LINPACK Benchmark? Basically, it's trying to measure how fast you can solve a linear system on your machine. There are actually three different LINPACK Benchmarks. The first one is solving a 100-by-100 linear system. The second one is solving a 1000-by-1000. The last one, which is the most interesting one, that's the one we're going to use today, is no holds barred. You can solve as large a system you want to fully utilize every last bit of the computational power on your machine. Let's see the performance of iPhone X using Accelerate. The performance is measured in gigaflops, for double precisions catch up with iPhone 5S, iPhone 6, iPhone 6S, iPhone 7, iPhone X comes in around 28.7 gigaflops. That's double precision. Let's look at single precision. Again, we run out of space, we have to shrink to try to make it come closer. IPhone X comes in at 68 gigaflops. Now you might be thinking, this is not that surprising. However, it improves over time, so the performance improves over time as well, but in fact, that's only half of the story. When there is a micro architecture change to have additional computational power into a machine, you need the matching [inaudible] to fully utilize this additional computational power. And that is where we come in. Remember, this is the same LINPACK Benchmark executable running on all five generations of iPhones. They all got the best performance using Accelerate without changing a bit. The same is also true for your apps. If you use Accelerate in your apps, you'll get the best performance automatically on all the architectures we support. Moreover, Accelerate supports across platforms. Accelerate works on macOS, iOS, tvOS, watchOS. So let's say if tomorrow Apple comes out with a new architecture or new platform, you don't have to worry about it. All you need to do, the most you'll ever need to do is just to rebuild your apps, link against Accelerate. You will automatically get the best performance on the latest release platform or architecture. Just as a summary, we talk about Accelerate supports a wide variety of functionalities. More than likely, you will find something you need in Accelerate. If you need something that's not available, please feel free to file feature requests. We constantly look at this feature request, evaluate them, and then put into builds. Actually, some of our best features come from feature requests. Accelerate is easy to use. Most of the time it's just one function call and the job is done. It's fast and energy efficient, so your app is more responsive and the battery life is longer. Accelerate is portable across platforms and architectures. You get the best performance on all the platforms we support and all the architectures we support, and the best part is, you don't have to change your code. For more information, you can go to our online documentation at developer.apple.com, and all our demo apps, simple code, and session material will be available online. We have a lab session tomorrow afternoon at 2. I look forward to seeing you guys there. If you have any questions or you want to learn more about Accelerate, I'd love to see you there. That's all for our presentation today. Thank you all for coming. Have a great day.  Hello, everybody. Welcome to Image and Graphics Best Practices. My name's Kyle. I work on UIKit. And today, I'm going to be sharing with you some techniques and some strategies for efficiently working with graphical content in your applications. We're going to take a tour of the framework stack. First, we're going to start with UIImage and UIImageView, which are UIKit's high level tools for working with graphical content in your app. Then, we're going to focus on how you can best do custom drawing inside of your applications with UIKit. And finally, we're going to touch, briefly, on integrating advanced CPU and GPU technologies into your applications. And throughout this talk we're going to be focusing primarily on our use of two scarce resources on the device; memory and CPU. And we tend to think of these things as separate quantities. They have their own tracks in the debug navigator. They have their own instruments in the instruments application. But really, they're intricately linked. And it might be apparent that as your application uses more CPU, that has a negative impact on the battery life and the responsiveness of your application. But what might be less obvious is that as your application and other applications on the system consume more memory, that also causes more CPU utilization. Which, has further detrimental effects on battery life and performance. So, we're going to focus on how to improve our use of these resources. What better context for discussing this problem than an application that works pretty extensively with photographic content, like the Photos app? You see, we're editing a photo here. And as I mentioned previously, UIImages, UIKits, high level class for dealing with image data. So, we have a UIImage representing this rich content. And we tend to divide graphical content in our applications into two categories; rich content like this photograph and iconography. UIImage is also the data type in UIKit that we use to represent things like the icon displayed in this button. And as I mentioned previously, UIImageView is the class that UIKit provides for displaying a UIImage. Now, in classical MVC style UIImage can be thought of as a model object. And UIImageView, of course, as the name implies, is a view. And these objects in their roles as model and view, have traditional responsibilities. UIImage is responsible for loading image content. And UIImageView is responsible for displaying it, for rendering it. Now, we can think of this as a simple relationship that we establish once. It's a one-way relationship. Bu the actual story is a little bit more complicated. In addition to rendering being a continuous process, rather than a one-time event, there's this hidden phase. It's really important to understand in order to measure the performance of your application. And this phase is called decoding. But in order to discuss decoding, I first need to discuss a concept called a buffer. A buffer is just a contiguous region of memory. But we tend to use the term buffer when we're discussing memory that's composed of a sequence of elements of the same size, usually, of the same internal construction. And for our purposes, one really important kind of buffer is an image buffer. This is a term we use for buffer that holds the in-memory representation of some image. Each element of this buffer describes the color and transparency of a single pixel in our image. And consequently, the size of this buffer in memory is proportional to the size of the image that it contains. One particularly important example of a buffer is called the frame buffer. And the frame buffer is what holds the actual rendered output of your application. So, as your application updates its view hierarchy UIKit will render the application's window and all of its subviews into the frame buffer. And that frame buffer provides per pixel color information that the display hardware will read in order to illuminate the pixels on the display. Now, that last part happens at a fixed interval. It can happen at 60 fps. So, every 1/60th of a second. Or on an iPad with ProMotion Display, it can happen as fast as every 1/120th of a second. And if nothing's changed in your application, the display hardware will get the same data back out of the frame buffer that it saw, previously. But as you change the content of the views in your application, for example, you assign a new UIImage to our image view, here. UIKit will re-render your application's window into the frame buffer. And the next time the display hardware pulls from the frame buffer it'll get your new content. Now, you can contrast an image buffer to another kind of buffer, a data buffer, which is just a buffer that contains a sequence of bytes. In our case, we're concerned about data buffers that contain image files. Perhaps, we've downloaded them from the network or we've loaded them from disk. A data buffer that contains an image file, typically, begins with some metadata describing the size of the image that's stored in that data buffer. And then, contains the image data itself, which is encoded in some form like JPEG compression or PNG. Which means that the bytes subsequent to that metadata don't, actually, directly describe anything about the pixels in the image. So, we can take a deeper look at this pipeline that we've set up. We have a UIImageView here and we've highlighted the region of the frame buffer that will be populated by the image view's rendering. And we've assigned a UIImage to this image view. It's got a data buffer that represents the content of an image file. Perhaps, downloaded from the network or read from disk. But we need to be able to populate the frame buffer with per pixel data. So, in order to do that UIImage will allocate an image buffer whose size is equal to the size of the image that is contained in the data buffer. And perform an operation called decoding that will convert the JPEG or PNG or other encoded image data into per pixel image information. And then, depending on the content mode of our image view. When UIKit asks the image view to render it will copy and scale the image data from the image buffer as it copies it into the frame buffer. Now, that decoding phase can be CPU intensive, particularly, for large images. So, rather than do that work every time UIKit asks the image view to render, UIImage will hang onto that image buffer, so that it only does that work once. Consequently, your application, for every image that gets decoded, could have a persistent and large memory allocation hanging out. And this allocation, as I mentioned earlier, is proportional to the size of the input image. Not necessarily, the size of the image view that's actually rendered in the frame buffer. And this can have some pretty negative consequences on performance. The large allocation that is in your application's address space could force other related content apart from content that it wants to reference. This is called fragmentation. Eventually, if your application starts accumulating a lot of memory usage the operating system will step in and start transparently compressing the content of physical memory. Now, the CPU needs to be involved in this operation so in addition to any CPU usage in your own application. You could be increasing global CPU usage that you have no control over. Eventually, your application could start consuming so much physical memory that the OS needs to start terminating processes. And it'll start with background processes of low priority. And, eventually, if your application consumes enough memory, your application itself could get terminated. And some of those background processes are doing important work on behalf of the user. So, they might get started up again as soon as they get terminated. So, even though your application might only be consuming memory for a short period of time, it can have this really long-tail effect on CPU utilization. So, we want to reduce the amount of memory that our application uses. And we can get ahead of the curve with a technique called downsampling. Now, here we see a little bit more detail about our image rendering pipeline. Including the fact that the image view we're going to display our image in is actually smaller than the image we're going to display inside of it. Normally, the core animation framework would be responsible for shrinking that image down during the rendering phase, but we can save some memory by using this downsampling technique. And what we're going to do, essentially, is capture that shrinking operation into an object called the thumbnail. And we're going to wind up with a lower total memory usage, because we're going to have a smaller decoded image buffer. So, we set up an image source, create a thumbnail, and then capture that decoded image buffer into UIImage. And assign that UIImage to our image view. And then, we can discard the original data buffer that contained our image. And we're left with a much smaller long-term memory footprint for our application. The code to do that has a few steps. So, I'm going to walk you through them. I'm not going to do extremely low-level detail. But I'll highlight the important bits. First, we're going to create a CGImageSource object. And CGImageSourceCreate can take an option dictionary. And the important option we're going to pass here, is this ShouldCache flag. And this tells the Core Graphics framework that we're just creating an object to represent the information stored in the file at this URL. Don't go ahead and decode this image immediately. Just create an object that represents. We're going to need information from this URL. Then, we're going to calculate on the horizontal and vertical axis. Based on the scale that we're going and point size we're going to render at, which is the larger dimension in pixels. Calculate that information, and then create an options dictionary for our thumbnail. There are a couple of options listed here. You can look in the documentation for exactly what these options do. But the very important one is this CacheImmediately option. By passing this option here, we're telling Core Graphics that when I ask you to create the thumbnail that's the exact moment you should create the decoded image buffer for me. So, we have exact control over when we take that CPU hit for decoding. Then, we create the thumbnail, which is a CGImage, that we get back. Wrap that in the UIImage and return it from our helper function that we've written here. So, to give you an idea of the magnitude of savings that this technique gives us, we're just displaying the full screen image here. This is a photograph. It's 3,000 by 2,000 pixels. If we do no optimization, just throw UIImageView in the storyboard and assign our image to it, this application takes 31.5 megabytes just sitting doing nothing. Now, using this downsampling technique and only producing an image buffer that's the size of the actual display, we can get the memory usage of this application down to 18.4 megabytes. And that is a huge reduction in memory usage. Thanks for the applause, but you should all get the applause for implementing this technique in your applications. You can imagine how much of a big deal this is for an app that's displaying a lot of potentially large input images in a small space on screen. For example, the Camera Roll. You might implement such a view using UICollectionView. So, here we've implemented cell for item at indexPath. And we're using our helper function that we wrote earlier to downsample the images to the size that they're going to be displayed at when the cell is actually put on the screen. So, you think this is a pretty good thing to do, right? Like rather than having these large allocations hanging around, we're reducing our memory usage. Unfortunately, that doesn't save us from another problem that's common in scrollable views like table views and collection views. It's a, probably seen this before. You scroll through an application and it starts hitching as you scroll. What's happening here is that as we're scrolling the CPU is relatively idle, or the work that it does can be done before the display hardware needs the next copy of the frame buffer. So, we see fluid motion as the frame buffer is updated and the display hardware is able to get the new frame on time. But now, we're about to display another row of images. And we're about to ask Core Graphics to decode those images before we hand the cells back to UICollectionView. And that could take a lot of CPU time. So much so, that we don't get around to re-rendering the frame buffer. But the display hardware is operating on a fixed interval. So, from the user's perspective the application has just stuttered. Now, we're done decoding these images, we're able to provide those cells back to UICollectionView. And animation continues on, as before. Just saw a visual hitch, there. Now, in addition to the obvious responsiveness consequences of this behavior, there's a more subtle detrimental effect on battery life. Because iOS is very good at managing the power demand on the batter when there is a smooth constant demand on the CPUs. And what we have here are spikes. As new rows are about to come into view on the scroll view, we're spiking the CPU usage. And then, returning back down to a low level. So, there are two techniques we can use to smooth out our CPU usage. The first one is prefetching. And if you want to know a whole lot about prefetching check out the A Tour of CollectionView Talk from this year's WWDC. But the general ideas here, is that prefetching allows CollectionView to inform our data source that it doesn't need a cell right now, but it will in the very near future. So, if you have any work to do, maybe, you can get a head start. That allows us spread out CPU usage out over time. So, we've reduced the maximum size of the CPU usage. Another technique we can use is performing work in the background. So, now that we've spread out work over time we can, also, spread it out over available CPUs. The consequences of this are that your application is more responsive and the device has a longer battery life. So, to put this in action here, we've got a implementation of the prefetch method on our data source. And it's going to call our helper function to produce a downsampled version of the image that we're about to display in this CollectionView cell. And it does this by dispatching work to one of the global asynchronous queues. Great. Our work is happening in the background. This is what we wanted to do. But there is a potential flaw here. And it's a phenomenon that we like to call thread explosion. And this is what happens when we ask the system to do more work than there are CPUs available to do it. If we're going to display a whole number of images, like 6-8 images at a time, but we're running on a device that only has 2 CPUs, we can't do all of that work at once. We can't parallelize over CPUs that don't exist. Now, to avoid deadlock when we dispatch asynchronously to a global queue, GCD is going to create new threads to capture the work we're asking it to do. And then, the CPUs are going to spend a lot of time moving between those threads to try and make incremental progress on all of the work we asked the operating system to do for us. And switching between those threads, actually, has a pretty significant overhead. We'd do a lot better if one or more of the CPUs just got a chance to get images out the door. So, we're going to borrow a technique that was presented last year in the Modernizing Grand Central Dispatch Usage talk. And we're going to synchronize some work, or I'm sorry. Not synchronize, we're going to serialize some work. So, rather than simply dispatching work to one of the global asynchronous queues, we're going to create a serial queue. And inside of our implementation of the prefetch method we're going to asynchronously dispatch to that queue. Now, it does mean that an individual image might not start making progress until later than before. But it also means that the CPU is going to spend less time switching between bits of work that it can do. Now, these images that we're displaying can come from a number of places. They might come with our application, in which case they might be stored in an image asset. Or they might be stored in a file instead of our application wrapper. Or they could come from the network. Or they could be in a document that's stored in the application documents directory. They could be stored in a cache. But for artwork that comes with your application, we strongly encourage you to use image assets. And there are a number of reasons why. Image assets are optimized for name based and trait-based lookup. It's faster to look up an image asset in the asset catalog, than it is to search for files on disk that have a certain naming scheme. The asset catalog runtime has, also, got some really good smarts in it for managing buffer sizes. And there are, also, some features unrelated to runtime performance that are exclusive to image assets. Including features like per device thinning, which mean that your application only downloads image resources that are relevant to the device that it's going to run on and vector artwork. The vector artwork was a feature that was introduced in iOS 11. And you enable it by checking the Preserve Vector Data checkbox in the editor for your image asset. And the upshot of this is that if your image gets rendered in an image view that is larger or smaller than the native size of the image it doesn't get blurry. The image is, actually, re-rasterized from the vector artwork so that it has nice crisp edges. One place that we use this in the operating system. If you turn on dynamic type to a very large size in the Accessibility settings. And then you tap and hold on an item in the tab bar a little HUD shows up as a magnified view of the item that you're currently holding your finger over. So, if you want your artwork to look good in places like this check the Preserve Vector Artwork checkbox in. I'm sorry. The Preserve Vector Data checkbox in the image asset inspector. Now, the way this works is very similar to the pipeline we saw before. Rather than a decode phase, we have a rasterize phase that's responsible for taking the vector data and turning it into bitmap data that can be copied to the frame buffer. Now, if we had to do this for all of the vector artwork in your application we would be consuming a lot more CPU. So, there's an optimization we make here. If you have an image that has Preserve Vector Data checked, but you render it at the normal size. The asset catalog compiler has, actually, already produced a pre-rasterized version of that image and stored it in the asset catalog. So, rather than doing the complicated math of rasterizing your vector artwork into a bitmap, we can just decode that image that's stored in the asset catalog and render it directly into the frame buffer. If you're planning on rendering artwork at a few fixed sizes. Maybe, you have a small version and a large version of an icon. Rather than relying on the Preserve Vector Data checkbox, create two image assets that have the two sizes that you know you're going to render your image at. That will allow the optimization to take the CPU hit of rasterizing your artwork at compile time, rather than every time the image is drawn into the frame buffer. So, we've seen how to work with UIImage and UIImageView. But that's not all of the graphical work that your application does. Sometimes, your application draws content at runtime. The example of this happening might be seen in something like this editing view in the Photos application. The UIButton that's displaying an icon and UIButton can use UIImageView directly. But UIButton doesn't support the style of this Live button, here, that you can tap to enable or disable the Live Photo. So, we're going to have to do some work here, ourselves. And one implementation of this might be to subclass UIView and implement the draw method. And this implementation here draws a yellow roundRect, draws some text, and an image on top of it. Don't recommend this approach for a couple of reasons. Let's compare this view subclass to our UIImageView. Now, as you may already be aware, every UIView is, actually, backed by a CALayer in the Core Animation runtime. And for our image view, the image view creates the, asks the image to create the decoded image buffer. And then, hands that decoded image over to CALayer to use as the content of its layer. For our custom view that overrode draw, it's similar, but slightly different. The layers responsible for creating an image buffer to hold the contents of our draw method, and then our view, excuses draw function and populates the contents of that image buffer. Which is then, copied into the frame buffer as needed by the display hardware. In order to understand how much this is costing us and why we should, perhaps, pursue alternative ways of implementing this UI. The backing store that we're using here, the image buffer that's attached to the CALayer, the size of that is proportional to the view that we're displaying. Now, one new feature and optimization that we have in iOS 12 is that the size of the elements in that backing store will, actually, grow dynamically depending on whether you're drawing any color content. And whether that color content is within or outside of the standard color range. So, if you're drawing wide color content using extended SRGB colors, the backing store will, actually, be larger than the backing store would be if you used only colors within the zero to one range. Now, in previous versions of iOS, you could set the contents format property on CALayer as a hint to Core Animation saying, ''I know I am not going to need to support wide color content in this view'', or, ''I know I am going to need to support wide color content in this view''. Now, if you do this, you're actually going to be disabling the optimization that we introduced in iOS 12. So, check your implementations of layerWillDraw. Make sure you're not going to accidentally defeat an optimization that could benefit your code when running on iOS 12. But we can do better than just hinting at whether we need a wide color capable backing store. We can, actually, reduce the total amount of backing storage that our application needs. We can do that by refactoring this larger view into smaller subviews. And reducing or eliminating places that override the draw function. This will help us eliminate duplicate copies of image data that exist in memory. And it will allow us to use optimized properties of UIView that don't require a backing store. So, as I mentioned, overriding the draw method will require creating a backing store to go with your CALayer. But some of the properties in UIView can still work, even if you don't override draw. For example, setting the background color of a UIView doesn't require creating a backing store, unless you're using a pattern color. So, I recommend not using patterned colors with a background color property on UIView. Instead, create a UIImageView. Assign your image to that image view. And use the functions on UIImageView to set your tiling parameters appropriately. When we want to clip the corners of that rounded rectangle, we want to use the CALayer cornerRadius property. Because Core Animation is able to render clipped corners without taking any extra memory allocations. If we, instead, use the more powerful maskView or maskLayer properties we'd wind up taking in extra allocation to store that mask. If you have a more complicated background that has transparent areas that can't be expressed by the cornerRadius property, again, consider using a UIImageView. Store that information in your asset catalog or render it at runtime. And provide that as an image to the image view, rather than using maskView or maskLayer. Finally, for that icon, the Live Photo icon, UIImageView is capable of colorizing monochrome artwork without taking any extra allocations. The first thing you want to do is either check the, not check the checkbox, but set the property in the image asset editor, the render mode property to always template. Or use the withRenderingMode function on UIImageView to create a UIImage whose rendering mode is always template. Then, assign that image to an image view and set the tintColor of that image view to the color you want the image to render in. UIImage, as it's rendering your image to the frame buffer, will apply that solid color during that copy operation. Rather than having to hold on to a separate copy of your image with your solid color applied to it. Another optimization built into UIKit provided view, UILabel is able to use 75% less memory when displaying monochrome text than when displaying color text or emojis. If you want to know more about how this optimization works in detail and how to apply it to your custom subclasses of UIView, check out the iOS Memory Deep Dive session. Goes into great detail about this backing store format called A8. Sometimes, you want to render artwork offscreen stored in an image buffer in memory. And the class UIKit provides to do that is UIGraphicsImageRenderer. There's another function that's older; UIGraphicsBeginImageContext. But please, don't use that. Because only Graphics Image Renderer is capable of correctly rendering wide color content. What you can do in your applications is use UIGraphicsImageRenderer to render to an offscreen place. And then, use UIImageView to display that, efficiently, on the screen. Similarly, to the optimization that we've introduced in CALayer backing stores. We've, also, made UIGraphicsImageRenderer capable of dynamically growing the size of its image buffer, depending on the actions you perform in the actions block. If you are running your code on a operating system prior to iOS 12, you can use the prefersExtendedRange property on UIGraphicsImageRendererFormat to tell UIKit whether you plan on drawing wide color content or not. But there's a medium middle ground here. If you're primarily rendering an image in to a graphic image renderer, that image may use a color space that required values outside of the range of SRGB. But doesn't, actually, require a larger element size to store that information. So, UIImage has a image renderer format property that you can use to get a UIGraphicsImageRendererFormat object preconstructed for optimal storage of re-rendering that image. Lastly, we're going to talk a little bit about how to integrate advanced CPU and GPU technologies that we provide in iOS into your applications. So, if you've got a lot of advanced processing to do to your images, perhaps, in real time, consider using Core Image. Core Image is a framework that allows you to create a recipe for processing an image and handle that on the CPU or on the GPU. If you create a UIImage from a CIImage and hand that to UIImageView, UIImageView will take care to execute that recipe on the GPU. This is efficient and it keeps the CPU free for doing other work in your application. In order to use it create your CIImage as normal, and then use the UIImage ciImage initializer. There are other advanced frameworks for processing and rendering graphical content that are available on iOS, including Metal, Vison, and Accelerate. And one of the data types that is common among these frameworks is CVPixelBuffer. And this is a data type that represents a buffer that can be in use or not in use on the CPU or on the GPU. When constructing one of these pixel buffers make sure to use the best initializer. The one that's closest to the representation you have at hand. Don't unwind any of the decoding work. It's already been done by the existing UIImage or CGImage representations. And be careful when moving data between the CPU and the GPU, so that you don't just wind up trading off work between the two. You can, actually, get them to execute in parallel. Finally, check out the Accelerate and simd session for information on how to properly format your buffers for being processed by the Accelerator framework. So, to summarize a few key points. Implement prefetch in your table views and collection views, so that you can get some work done in advance and avoid hitching. Make sure that you're not defeating any optimizations that UIKit is providing to reduce the size of the backing stores associated with your views. If you're bundling artwork with your applications store it in the asset catalog. Don't store it in files that are associated with your app. And finally, if you're rendering the same icons at different sizes don't over-rely on the Preserve Vector Data checkbox. For more information there is a couple of related sessions, including one about actually investigating your performance problems. And we'll also have labs, tomorrow and Friday. And if you have any questions, come see us in the labs. Thanks for watching.  Good afternoon and welcome everybody to our session, what's new in user notifications. I'm Kritarth Jain [inaudible] on the iOS notifications team, and we're very excited to be back at WWDC to share with you all the new and exciting features around user notifications that your applications can start using with iOS 12. Today, we will be going over a range of topics as you can see from the list here. We will start with talking about grouped notifications, a new paradigm that we've introduced to iOS notifications when presented in the user's notification list. Then we'll talk about notification content extensions, which are existing extension points with notifications, and discuss new APIs that you've added around these. Then, we'll cover notification management and talk about all the new ways in which your application users can now tweak your notification settings and what you need to do to respond to these new options. Then, we'll cover provisional authorization, which allows your applications to have a trial phase for sending notifications to users without their explicit permission but do it quietly. And lastly, we'll cover critical alerts, which allows your applications to send important notifications to the users, which bypass certain system settings if your users allows your applications to do so. So there's a range of topics to be covered today, and let's begin with looking at grouped notifications. Now up to iOS 11, all new incoming notifications for the users were inserted in a chronological order in the notification list. So these would be interspersed across multiple applications, and it would be hard for the user to find a certain notification or triage multiple notifications together. So starting in iOS 12, we've decided to improve this by introducing notification grouping, so now, as you can see here, all notifications across different applications get grouped into their unique groups. Let's take a deeper look at how grouped notifications works. Now all these notifications will be automatically grouped, so there's nothing explicit that you need to do to start using notification grouping. However, if you do want to have your own custom groups then you can use the thread identifier, which is an existing property on the UN notification content object. So some of you might already be using the thread identifier and for a local notification. You can set it on the UNMutableNotificationContent object as seen here. And for a remote notification payload, you can also include it as part of your notification payload. Now the thread identifier might be familiar to some of you already. We use it today for forwarding notifications to a notification content extension that is presented for your application, which has the exact same thread identifier, allowing the content extension view to update it based on the new incoming notification. Starting in iOS 11, we started using the thread identifier for doing grouping of notifications, if the user had turned on [inaudible] notification previous. So we're just taking this concept and expanding it for all notifications in general. So how does this grouping work? So when a new notification comes in to you, the user's device, if there is no thread identifier set on this notification, then this notifications gets grouped with the application bundle. We can see that from our sample application here, that as new notifications are incoming, they are getting bundled with the same group and the group is getting updated with the latest content. And then the user can simply expand this notification group to see all the notifications that are present in that group. On the other hand, if the notification does have a thread identifier set on it, then it gets grouped with all the other notifications from that same application with that exact same thread ID. What this also means is that the same application can then have multiple different custom groups, depending upon the unique thread identifiers that you're setting on them. A good example of this is the messages application, where here you can see there are two different threads, and as new notifications are incoming, they are going to their own respective groups. And then the user can expand a specific group to see all the notifications that are part of that group. So by using the thread identifier, messages is able to do so. Now, your application users also have the option of tweaking this notification grouping setting from your per application notification setting's page. Here, they get three options. If they choose automatic, then they get the behavior that we just described. However, the user also has the option of just grouping by application, where the system will ignore your thread identifier and group all notifications into a single group. And if the user wants the same behavior as it exists in iOS 11 today, then they can simply turn off grouping for your applications' notifications. So do keep this in mind when you're creating your own custom groups that they create enough value for users when they receive your applications' notifications. Now what are the different components of a notification group? The content that we show is for the latest notification that was received as part of that group. And then the user can simply see all the notifications by tapping on this group, and we expand all the notifications' content. And then the user can interact with all these notifications individually as well. The two buttons at the top give the users much greater control like collapsing the stack as well as clearing all these notifications together. Now, notification grouping also makes triaging of notifications much better. For example, in this case, the user can clear all these notifications together by simply swiping to the right and tapping clear all. Apart from the content of the notification group, we also show a summary text. Now, this summary text, by default, shows the count of all the notifications that are part of that group. However, you can also create a custom summary text so you can give your users much better context of what kind of information is included in that group. Now, we will cover this API and go over much larger use cases of how you can create your custom groups in the advanced session around using group notifications, which will follow this session. So let's do a quick summary of group notifications as we saw them today. Starting in iOS 12, all application notifications are going to be grouped automatically. You can start using the thread identifier if you want to create your own custom groups for your applications, but the user does have the option of changing this grouping setting for your applications' notifications. And lastly, you can use the summary text for customizing the information you want to provide the user around the notification groups that you're creating. All right, so that was group notifications. Now, let's move on to the next topic and talk about notification content extensions. Now, some of you might already be familiar with these content extensions that we included with iOS 10. Content extensions allow your applications to present a rich notification view around the user's notifications, so you can have a much more customized and interactive interface for the notification that the user is seeing. Let's do a quick recap of setting up these content extensions. Xcode gives you a standard template to add a target for the content extensions to your applications and once you set that up, we create a default class for the notification view controller, which implements the UNNotificationContentExtension protocol. Here, the did receive notification method is important because this is your entry point for setting up the view associated with the content extension, and you can use the notification object past here to get all the information around that notification to set up your custom view. The info.plist file associated with your content extension gives you more options. The important thing here is the category identifier. Now, this identifier needs to match the same category identifier you're setting on your notification requests because that's how the system knows which content extension to launch with which notification. Along with this, you can do some quick configurations of your content extension such as setting the initial content size ratio, hiding the default content, as well as overriding the title of this content extension. Now, the primary way in which your users interact with these content extensions is through notification actions, and these actions are presented right below the content of the content extension. Let's summarize how we can set up these actions as well. So doing so is fairly trivial in code. For example, here, we have two actions here for like and comment, and we create a simple UNNotificationAction for like and a text input action for commenting. And once we've created these actions, we create a new category giving it the same identifier as the content extension where we want these actions to be presented. And then, we pass it, the two new actions that we created. Once we've set up this category, then we call setNotificationCategories on the UNNotificationCenter object associated with our class, giving it the new category that we created. So by simply doing so, the next time when the user goes to your content extension we can see that these actions are now available for them to interact with your notification content. Now let's take a look at how we can handle the responses from these actions. There are two ways to do that. Firstly, you can handle this response in the AppDelegate that is associated with your application that implements the UNUserNotificationCenter Delegate protocol. Here, the function UserNotificationCenter did receive response, includes the response object which includes information about the request, the notification request from which the user took this action. However, the content extension also allows you to intercept this action response so that you can update your view and make a much more interactive and dynamic experience for users for the content extension. So for our sample here, we enter the did receive response method and checked the action identifier for the like action. And then we update our UI with the new label as well as update our application state. Finally calling the completion block we do not dismiss. If you do want to dismiss your content extension view here, then you can simply change the parameter you're passing to the completion block to dismiss or dismiss and forward, where we will forward this response to your AppDelegate function as well. All right, so now that we set this up, we can see that when the user takes the like action, the content extension content gets updated right there and then. So it's a much more interactive experience for your user and they're getting real-time feedback. However, if you look at the current state of the content extension, we see that there is some redundant information. The user has already taken the like action, so having the action there doesn't really serve a purpose anymore. Now notification actions, in general, have certain limitations. They are not very dynamic and can't be updated based on the context of your content extensions. Also, these tend to be tied to the notification categories that you have to define at the time of your application setup. So we wanted to address these issues and we have introduced a new API around notification actions, where now we're exposing these notification actions as part of the NSExtensionContext tied to your content extension. What this API allows you to do is access the currently presented notification actions to the user as well as replace these actions by setting a brand new array of notification actions for your content extension. So going back to our sample, what if after the user took the like action we wanted to replace it say with the unlike action so that they can do the reverse of the action they just took? So using this new API, let's take a look of how we can set this up. So we go back to our did receive response method and again identify the like action and update our application state. This time, we also create a new action for unlike, giving it a unique identifier as well as a title. We can also take a look at the currently presented actions so that we can extract the comment action from there without having to create it again. Then we create a new array of these new actions that we've created and simply set that on the notification actions variable. So once we've done this and the user takes the like action, then the UI will automatically update to show them the new action, and then the user can then toggle between the two actions, depending upon how you handle that state in your content extensions. Now, this API can be used in multiple other ways as well. For example, now you can set your actions at the time you're setting up your content extension view in the did receive notification method. What this means is your notification requests are no longer tied to the category to define the actions that you want to present around these notifications. You can also now present a secondary set of actions by replacing the currently presented actions. For example, if the leading action was rate, then you can provide a secondary list of the different types of ratings that you want your user to take. And you can also remove all these notification actions if you feel it does not make sense anymore for your content extension to present these actions. So that's the new API around notification actions. And we feel this will really help you enhance the experience that your users have around your content extensions with the different actions now you can present to them. Let's move on and talk about user interaction with these content extensions. Now notification actions were important up till this point because till iOS 11 we did not allow user interaction touches with your content extension view. Now we received a lot of feedback around this. And I'm happy to announce that we're taking away this restriction with iOS 12. So now your content extensions have the option of opting in to receiving user interaction [inaudible] touches, and setting this up, it's fairly trivial. All you have to do is add a new key value option to your info.plist file. And the key that we've added is the UNNotificationExtensionUser InteractionEnabled. So going back to our sample, what if we want to remove the like action from a notification action and make it a UI interaction touch that's part of the view itself? So once we've configured our info.plist file, we can go back to our content extension view and create our own custom button to handle the like gesture. We add a target for our own private method and inside that function, we update the UI as well as update our application state. So here, it's important that since you're implementing your own user interactions that you are responsible for handling all these actions, responses, and callbacks from the users yourself. So once we've set this up, now when the user goes to your content extension we see the Like button, part of the UI itself, and the user can simply interact with that button right there and then. So that's the new functionality that we've added around content extensions. And coupled with notification actions, along with user interaction touches, you now have a much richer set of tools for creating much more interactive and dynamic content extension experiences for your applications notifications users. Now let's talk about launching your application from the notification content extension. So today the user can launch your application if touches were not allowed by simply tapping the content extension view. They could also do so by tapping your application icon in the top left corner. Or you could create a foreground action, which then would in turn launch the application. But what if you wanted to do this from your own custom control? What if you wanted to launch the application programmatically? To enable this, there is a new API on the NSExtensionContext called performNotification DefaultAction, which would allow you to do this now. Now, what does the default action mean? So, as we said, it launches the application, but at the same time, it calls the UserNotificationCenter did receive response method in your application delegate. Now the UNNotificationResponse object contains the information of the notification from which the user came, so you can update your application state based on the notification. And the identifier that's passed here is the UNNotificationDefault ActionIdentifier. So going back to our sample, let's see how we can set this custom control up. Now again, we create our own UI button for the all comments and then tie it up with our own private function. And in that function, we're simply calling PerformNotification DefaultAction. So by simply doing that, you get this functionality to call this method programmatically from anywhere in your content extension code. So that was launching the application. What about dismissing the content extension view? Again, let's take a look at how the user can do that today. They can do that by tapping the Dismiss Button in the top right corner, or you can create your own custom notification action, which would in turn dismiss the content extension view. Which you can set up, as we saw before, by passing dismiss to the completion block. But again, what if we want to dismiss the view through our own custom buttons, and we want to do this programmatically? Say that when the user taps the Like button, then the view dismisses because we feel the user's done interacting with the content extension. To enable this as well, there's a new API called dismissNotificationContent Extension that's on the NSExtensionContext. We go back to how we set up our Like button, and now this time, we also call the new function that we added for dismissing the content extension view. And once we set this up, now when the user takes the like action, the view of the content extension gets dismissed. Now one thing to note here. That calling this method does not withdraw the notification that was posted to the user. If you want to do that then use the existing API for removing delivered notifications with identifiers to get that functionality. All right, now let's summarize all the new APIs that we've looked at today around the notification content extensions. We started with talking about notification actions where now you can access these notification actions as well as replace them dynamically from anywhere in your content extension code. You can now opt in to having user interaction based on touches within your content extension views. You can programmatically launch the application from anywhere in your content extension code as well as dismiss the content extension view, depending upon where you feel it serves best your user's experience around the content extensions. So that's a varied list of APIs around content extensions, and we hope this really helps you enhance your user's experience around your content extensions and then you start using these APIs. So that was notification content extensions. Now, the next topic today we're going to look at is notification management, and to tell you all about that, let me invite my colleague Teja to the stage. Thank you. Thank you Kritarth. Hi everyone. My name is Teja Kondapalli, and I'm also an engineer on the iOS Notifications' Team. And, of course, I'm here to talk to you about a couple of the new APIs that we have. The first of which is notification management. But before I dive into this API, I want to cover some of the user facing features to give you some more context and then we can deep dive into the API. As our users get more and more apps on their phones, notifications become the primary way that they interact with these apps. But often, they find themselves in a situation like this. With far too many notifications. And it becomes hard to sift through and find the important ones. So perhaps to make this easier, this user has decided that notifications from podcasts don't need to be shown on the locked screen. Right now to configure that they'd have to launch the settings app, find notifications, find the podcast app, and then they can configure their settings. We wanted to make this easier. So, in iOS 12, we're introducing a new management view where the users can configure their notification settings directly from the notification without having to launch the settings app. There's three really easy ways to get into this management view. The first is what we just saw. You simply swipe over a notification, tap manage, and the management view comes up. The second is if you can go into the rich notification, you can tap in the right corner, and you can also launch the management view. And the third is actually in the list itself. Depending on how your users are interacting with their notifications, they will occasionally see suggestions, like this one, that ask them if they want to keep receiving podcast notifications. And from here, as well, they can tap manage and bring up the management view. Let's take a closer look at the management view. And we obviously have options here where users can configure their notification settings directly from this view. But if they want to go into the settings app and configure in a more detailed manner, they have a quick link to the settings for this application, the notification settings. A And also from this view, they have some actions they can take directly, the first of which says deliver quietly, which is probably a concept that's new to all of you. Some of these management views will also have an option that says deliver prominently, so let's talk about what this means. In iOS, we have a lot of settings that users can configure, and this is really great for the power user. The can customize their settings to every detail, but for the regular user, we think that we can help them out by categorizing their notification settings into two big categories. Notifications that are delivered prominently and notifications that are delivered quietly. Notifications that are delivered prominently are what we're used to. They show up on the locked screen. They show up in notification center. They roll down as banners. They badge the AP icon and they can play a sound. Notifications that are delivered quietly only show up in notification center and they don't play a sound. And from the management view, in addition to configuring whether they want their notifications delivered prominently or quietly, users also have the option to turn off their notifications. Now, I know that you might worry that your users are going to turn off the notifications for your app, so we've added this extra confirmation sheet just in case they do tap turn off. And from here, they can also turn off their notifications. But we've also added an API to add a second option to this confirmation sheet, and podcast has taken advantage of this API, so it says configure in podcast. This is a link that will deep link within the podcast app to a custom settings view that allows the user more granular control about what kind of podcast notifications they want. And as your apps send more and more notifications and various type of notifications, we think it's really important to allow them this granular level of control over what kind of notifications are important to them. This link can also be accessed from the systems settings app from your apps' notification settings. And you can see for podcasts it's right at the bottom. It says podcast notifications settings. Let's see how we do this in code. In the class that conforms to UNUserNotificationCenter Delegate, we have a new delegate method. Open settings for notification, and as long as you implement this delegate method, those links that we talked about from the management view, or from the settings app, will automatically be populated by the system for you. So when the user taps on any of these links, this delegate method will be called. And it's really important when this delegate method is called that you immediately take your users into the view where they can configure their notification settings within your app. And if you notice, we have [inaudible] parameter to this method, and it is notification. So depending on where the link was tapped from, if it was tapped from one of the management views, it will have the value of the notification that that management view came from. If it was tapped from the settings app, the value of notification will be nil, and you can use this information to show the appropriate notification settings when this delegate method is called. So that's what we have for notification management. It's a new way for your users to configure whether they want their notifications delivered prominently or quietly, or turn them off, or even configure them at a granular level within your app. And to encourage your users to keep getting your notifications delivered, we think it's really important that you make the content in the notifications relevant. We also encourage you to use thread identifiers to group the notifications when you think it's appropriate. This will help the users organize their lists better and will make sure that they're not overwhelmed by the notifications from your app. We also think that as your apps send various types of notifications, it's really important to provide that custom settings view within that app so that users have more granular control about what kind of notifications are important to them. That's what we have for notification management. And the next big feature I want to talk to you about is provisional authorization. Right now, when a user installs your app, before they start receiving notifications, at some point they'll have to respond to a prompt that looks like this, which is asking them if they want these notifications. And the biggest problem with this is, at this point, the user doesn't know what kind of notifications this app is going to send, so they don't know if they want them or not. So, in iOS 12, we're introducing provisional authorization, and this is an automatic trial of the notifications from your app. This will help your users make a more informed decision on whether they want these notifications or not. So you can opt into this, and if you do, your users will not get that authorization prompt that we just saw. Instead, the notifications from your app will automatically start getting delivered. But these notifications will be delivered quietly, and if we recall, notifications that are delivered quietly only show up in notifications center, and they don't play a sound. Notifications that are delivered with provisional authorization will have a prompt like this on the notification itself. And this will help the users decide after having received a few notifications whether they want to keep getting these notifications or whether they want to turn them off. And this turn off confirmation sheet will also have the custom settings link if you have provided it. Let's see how you can do this in code. In the location where you regularly request authorization, in addition to whatever options you might be requesting, you can add a dot qualifying option called .provisional. And if you include this, you will automatically start participating in the trial. It's really important to note that the .provisional option is in addition to whatever other options you may be providing. That's because if the users decide to keep getting your notifications delivered, we want to know how you want them delivered, with badges or sounds or as alerts. So that's what provisional authorization is. It's an automatic trial of the notifications from your app to help your users make a more informed decision about whether they want these notifications. And again, to encourage your users to keep getting your notifications delivered, it's really important to make the content in your notifications relevant. And also, it's really important to use .provisional as a qualifier option in addition to whatever other options you're requesting. That's what we have for provisional authorization. And the last big feature that I want to talk to you about are critical alerts. Often when I'm in the middle of a meeting or attending something important, my phone looks like this. And as you can see, I have do not disturb turned on. Or at least I have the ringer switch turned off so that I don't hear any sounds when I get notifications. And usually this is really good, but I would have missed a really important notification like this one. This is a health-related notification. That's from a glucose monitor that's warning me of low blood sugar, and this is something I would want to see immediately. Scenarios like this made us realize that we need a new type of notification, and this is what we call critical alerts. Critical alerts are medical- and health-related notifications. Or home- and security-related notifications. Or public safety notifications. And the key to a critical alert is that it requires the user to take action immediately. The way that critical alerts behave is that they bypass both do not disturb and the ringer switch, and they will play a sound. And they can even play a custom sound. But what that means is that these are very disruptive, and for that reason we don't think that all apps should be able to send critical notifications. Critical alerts. So in order to start sending a critical alert, you will need to apply for entitlement, and you can do that on the developer.apple website. This is what a critical alert looks like, and you can see that it has a unique icon indicating that it's critical. And it would have also come in with a sound. Critical alerts also have their own section in notifications settings. This means that a user can choose to allow critical alerts for a particular application but choose not to allow any other type of notification. And before users start receiving critical alerts, they will have to accept a prompt that looks like this that's asking them specifically whether they want to accept critical alerts from a particular application. So, of course, in order to start sending critical alerts, you'll have to request authorization. So after you apply for entitlement and get it, in the place where you regularly request authorization, in addition to whatever other options you want to request, you can also request a .criticalAlert option. And this will give your users the prompt. And let's see how it actually set up and send a critical alert. It actually behaves very similarly to a regular notification. You can see that I just set up a notification with the title body and category identifier, but what distinguishes this as a critical alert is that it plays a sound. So I need to set a critical alert sound. And here, you can see that I'm setting the default critical alert sound that's provided by the framework. I can also set a custom sound. And I can also set a custom audio volume level. And of course, critical alerts can also be push notifications so all of this information can be set in the push payload as well. So that's what we have for critical alerts. They're a new type of notification that requires the users to take action immediately. And they're very disruptive, so you need entitlement to be able to send them. So that's all the new exciting APIs that we have for you today. I just want to quickly go over all the things that we covered. We talked about how you can use thread identifiers to group your notifications to help your users organize their notification lists better. We talked about all the great new APIs around notification content extensions, which will help you make your rich notifications much more interactive. We talked about how you can provide a custom settings view within your app to allow your users more granular control over what kind of notifications they want to receive. We also talked about provisional authorizations, which is an automatic trial of the notifications from your app, which will help you users make a more informed decision about whether they want these notifications or not. And last, we talked about critical alerts, which are a new type of notification that requires the user to take action immediately and that are disruptive. So we hope you take advantage of all of these great APIs and make the notification experience for your users even better. You can find all the information about this session on our sessions' page at developer.apple.com. We're session 710. We have another session just following this one in hall three called using grouped notification where we'll help you determine how to best group notifications for your app. We have two notifications labs, one today and one tomorrow, where you can come and ask the engineers on our team any questions that you may have. And on Friday morning, we have an interesting session called designing notifications. That's going to be hosted by the designers who helped us come up with the designs for these, and they'll be talking about notification best practices. Thank you and have a great dot dot.  Hey, everyone. Thank you. Good afternoon, and welcome to our session on Turi Create. This session builds upon yesterday's session on Create ML. That session covered many of the foundations of machine learning in Swift. So if you didn't catch that session I do recommend you add it to your watch list. The goal of Turi Create is to help you add intelligent user experiences to your apps. For example, you might want to be able to take a picture of your breakfast. And tap on the different food items to see how many calories you're consuming. Or maybe you want to control a lightbulb using your iPhone and simple gestures. Maybe you want to track an object in real time like a dog or a picture of a dog if you don't have dogs in the office. Maybe you have custom avatars in your game, and you want to provide personalized recommendations like hairstyles based on the beard that the user has selected. Or maybe you want to let your users apply artistic styles or filters to their own photos. These are very different user experiences. But they have several things in common. First and foremost, they use machine learning. These experiences all require very little data to create. All these models were made with Turi Create and deployed with Core ML. They all follow the five, the same five-step recipe that we'll go over today. And all these demo apps will also be available in our labs. So please join us today or Friday for the ML labs to get hands-on experience. Turi Create is a Python package that helps you create Core ML models. It's easy to use, so you don't need to be an ML expert or even have a background in machine learning to create these exciting user experiences. We make it easy to use by focusing on tasks first and foremost. What we do is we abstract away the complicated machine-learning algorithms so you can just focus on the user experience that you're trying to create. Turi Create is cross platform so you can use it both on Mac and Linux. And it's also open source. We have a repo on GitHub and we hope you'll visit. There's a lot of great resources to get started. And we look forward to working with you and the rest of the developer community to make Turi Create even better over time. Today we're excited to announce that the beta release of Turi Create 5.0 is now available. This has some powerful new features. Like GPU acceleration. And we'll go into these features in detail later on today. The main focus today is going to be the five-step recipe for creating Core ML models. I'm going to start by going over these steps at a high level. And then we'll dive into some demos and code. So the first step you need is to understand the task you're trying to accomplish. And how we refer to that task in machine learning terms. Second, you need to understand the type of data you need for this task. And how much of it. Third, you need to create your model. Fourth, you need to evaluate that model. That means understanding the quality of the model and wheth-- whether it's ready for production. And finally, when your model's ready to deploy, it's really easy using Core ML. Let's dig a bit deeper into each of these steps. Turi Create lets you accomplish a wide variety of common machine learning tasks. And you can work with many different types of data. For example, if you have images, you might be interested in image classification. Or object detection. You might want to provide personalized recommendations for your users. You might want to be able to detect automatically activities like walking or jumping jacks. You might want to understand user sentiment given a block of text. Or you might be interested in more traditional machine learning algorithms like classification and regression. Now we know this can get really confusing to those of you who are new to machine learning. So we've taken a stab at making this really easy for you in our documentation. We begin by helping you understand the types of tasks that are possible and then how we reference them in machine learning terms. So what we can do now is revisit those intelligent experiences that we walked through at the beginning of this presentation and assign them to these machine learning tasks. For example, if you want to recognize different types of flowers in photos we call that image classification. If you want to take pictures of your breakfast and understand the different food objects within them, we call them, that object detection. If you want to apply artistic styles to your own photos, we call that style transfer. And if you want to recognize gestures or motions or different activities using sensors of different devices, we call that activity classification. Finally, if you want to make personalized recommendations to your users, we call this task recommender systems. Now the great thing is that the same five-step recipe we just walked through also applies to your code. We begin by importing Turi Create. We proceed to load our data into a data structure called an SFrame. And we'll go into a bit more detail on the SFrame data structure shortly. We'll proceed to create our model with a simple function, .create. This, this function extracts away the complicated machine learning behind the scenes. We proceed to evaluate our model with the simple function .evaluate. And finally we can export the resulting model to Core ML's ML-model format to be easily dragged and dropped into Xcode. Now I mentioned that this same five-step template applies to all the different tasks within Turi Create. So whether you're working on object detection, image classification or activity classification, the same code template applies. For our first demo today, we're going to walk through a calorie-counting app that uses an object detection model. So we'll want to recognize different foods within an image. And we'll need to know where those, where in the image those foods are so that we could tap on them to see the different calorie counts. So let's take a look at the type of data that we'll need to create this machine learning model. Of course we need images. And if we were just building a simple image classifier model, we would just need our set of images and labels that describe the images overall. But because we're performing object detection, we need a bit more information. We need to understand not just what's in the image. But where those objects are. Now if we zoom in a bit closer to one example, we see a red box around a cup of coffee. And a green box around a croissant. We call those boxes bounding boxes and we represent those in JSON format. With a label, and then with coordinates x, y, width and height. Where x and y refer to the center of that bounding box. So it's also worth noting that with object detection, you can reference or detect multiple images, multiple objects within each image. So I mentioned that we'd be loading our data into this tabular data structure called an SFrame. And in this example, we will end up with two columns. The first column will contain your images. The second column will contain your annotations in JSON format. Let's or by now you're probably wondering what is an SFrame? So let's take a step back and, and learn more about it. SFrame is a [inaudible] tabular data structure. And what this means is you can create machine learning models on your laptop. Even if you have enormous amounts of data. SFrame allows you to perform common data manipulation tasks. Like joining two SFrames or filtering to specific rows or columns of data. SFrames let you work with all different data types. And once you have your data loaded into an SFrame, it's easy to visually explore and inspect your data. Let's zoom in a bit more on what's possible with SFrame. With our object detector example. So after we import Turi Create, in the case of our object detector, we're actually going to load two different SFrames. The first containing our annotations, and the second containing our images. We have a simple function .explore that will allow you to visually inspect the data you've imported. We can do things like access specific rows. Or columns of our data. And of course we can do common operations like joining our two SFrames into one. And saving the resulting SFrame for later use or to share with a colleague. Next we create our model. So I mentioned we have this simple function .create that does all the heavy lifting for the creation of the actual model. And what we do behind the scenes is we ensure that the model we create for you is customized to the task and that it's state of the art. Meaning it's as high quality and high accuracy as we can get. And we're able to do this whether you have large amounts of data or small amounts of data. It's very important for us that all of our tasks work whether you have even a small amount of data as small as about 40 images per item that you're trying to detect in the class of object. In the case of object detection. Let's move on to evaluation. So I mentioned we have a simple function .evaluate that will give you an idea of the quality of your model. In the case of object detectors. We have two factors to consider. First, we want to know did we get the label right? But we also have to know if we got that bounding box right around the object. So we can establish a simple metric with these two factors and go through a test data set scoring predictions against known what we call ground-truth data. And so we want to make sure that we have correct labels. And then a standard metric is at least 50% overlap in the predicted bounding box when compared with to the ground-truth bounding box. Let's look at a few examples. In this prediction we see that the model got the label right with a cup of coffee. But that bounding box is not really covering the whole cup of coffee. It's only about ten percent overlapping the ground truth. So we're going to consider that a bad prediction. Here we see a highly accurate bounding box, but we got the label wrong. That's not a banana. So let's not consider that a successful prediction either. Now this middle example's what we want to see. We have 70% overlap of our bounding box, and the correct label, coffee. So what we can do is systematically go through all of our predictions with a test data set. And get an overall accuracy score for a new model. And finally, we move to deployment. We have an export to Core ML function that saves your model to Core ML's ML model format. So you can then drag and drop that model in to Xcode. This week we've actually some exciting new features related to object detection specifically. So I encourage you to attend tomorrow's Vision with Core ML session to learn more. In that session, the speaker will actually take the object detection model that we're building today and go into more detail about deployment options. And there you have it! The five-step recipe for Turi Create. Thank you. So with that, I'm going to hand off to my colleague Zach Nation, for a demo. Thanks, Aaron. I think let's just jump straight into code. Who wants to write some code live today? We're going to go ahead and build an object detector model right now. So I'm going to start out in Finder. Here I've got a folder of images that I want to use to train a model. We can see this folder's named Data, and it's full of images of breakfast foods. I've got a croissant, some eggs, and so on. This is, this is a good data set for breakfast food. I think let's go ahead and write some code with it. I'm going to switch over to an environment called Jupyter notebook. This is an interactive Python environment where you can run snippets of Python code and immediately see the output. So this is a great way to interactively work with a model. And it's very similar in concept to Xcode Playgrounds. The first thing we're going to do is import Turi Create as TC. And that we way we can refer to it as TC throughout the rest of the script. Now, the first task we want to do is load data. And we're going to load it up into SFrame format. So first we can say images equals TC.loadimages. And we're going to give it that folder day that I just showed in Finder. And Turi Create provides utilities to interactively explore and visualize our data. So let's make sure those images loaded correctly and we got the resulting SFrame that we expected. I'm just going to call .explore, and this is going to open up a visualization window where we can see that we have two columns in our SFrame. The first is called path, and it's the relative path to that image on disk. And the second column is called image. And that's actually the contents of the image itself. And we can see our breakfast foods right here. It looks like these loaded correctly, so I'm going to proceed. Back in Jupyter notebook. Now I'm also going to load up a second SFrame called annotations. And this I'm just going to call the SFrame instructor and provide a file name to annotations.csv. This is a CSV file containing the annotations that correspond to those images. And let's take a look at that. Right in Jupyter notebook, we can see that this SFrame contains a path column, again pointing to that relative path on disk of the image. And an annotation column containing a JSON object describing the bounding box and labels associated with that image. But now we have two different data sources and we need to provide one data source to train our model. Let's join them together. In Turi Create, this is as easy as calling the join method. I'm going to say data equals images.joinannotations and now we can see we have a single SFrame with three columns. It joined on that path column. So for each image with a path, it combined the annotations for that path. And so now for each image, we have annotations available. Now we're ready to train a model. So I'm going to create a new section here called train a model. And that's just one line of code here. I'm going to say model equals TC.objectdetector.create. And this is our simple task-focused API for object detection that expects data in this format. I'm going to pass in that data SFrame that I just created. And for the purposes of today's demo, I'm going to pass another parameter called max iterations and normally you wouldn't need to pass this parameter because Turi Create will pick the correct number of iterations for you. Based on the data that you provide. In this case, I'm going to say max iterations equals one just to give an example of what training would look like. And the reason this is going to take a minute is it actually goes through and resizes all of those images in order to get them ready to run through the neural network. That is under the hood of this object detector. And then it will perform just one iteration on this Mac GPO. But this is probably not the best model we could get because I just wanted to train it in a couple of seconds. So I'm going to go ahead and switch over to like cooking show mode. And I'm going to take one out of the oven that we've had in there for an hour [laughter]. So I'm going to say TC.loadmodel and it's called breakfastmodel.model. And this is one that I've had an opportunity to train for a bit longer. So let's inspect that right here in the notebook. And we can see that it's an object detector model. It's been trained on six classes, and we trained it for 55 minutes. This is means, this means you can train a useful object-detector model in under an hour on your Mac. Next, let's test the predictions of this model and see if it's any good. So I'm going to make a new section here called inspect predictions. And we're going to go ahead and load up a test data set. And here I've already prepared one in SFrame format. So I'm just going to load it, and I called it testbreakfastdata.sframe. There are two important properties of this test SFrame. One is that it contains the same types of images that the model would have trained on. But the second important property is the model has never seen these images before. So this is a good test for whether that model can generalize to users' real data. I'm going to make predictions from that whole test set by calling model.predict and providing that test SFrame. And we'll get a batch prediction for the whole SFrame. And that'll just take a few seconds. And then we're going to inspect. I'm just going to pick a random prediction here. Let's say index two. Here we can see the JSON object that was predicted in just the same format that the training data is provided in. So here we have coordinates, height, width, x and y. And a label, banana. And we get a confidence score from the model. In this case about .87. This is a little bit hard for me as a human to interpret though. I can't really tell if this image is really supposed to be a banana or whether these coordinates are where the banana would appear in that image. Turi Create produces a function to take the predicted bounding boxes or the ground-truth bounding boxes and draw them right onto the images. So let's go and do that. I'm going to create a new column in my test SFrame called predicted image. And I'm going to assign it the output of the object detector utility called draw bounding boxes. And I'm going to pass into draw bounding boxes that test image column. So that's the image itself and then I'm also going to pass the predictions that I just got from the model. That's going to draw those predicted bounding boxes onto each image. Now let's take a look at that number two prediction again. This time in image form. So I can say testpredictedimage2.show. And it will render right here in the notebook. And this is great as a spot check because at least for one picture, we know that the model's working. But this doesn't tell us if it'll work for say the next 50,000 images that we pass in. So for that, we're going to evaluate the model quantitatively. And I'm going to start a new section here in the notebook called evaluate the model. And what we're going to do is call model.evaluate and once again, I'm going to pass in just that whole test data set. Here the evaluation function is going to run the metric that Aaron described, testing whether the bounding boxes are overlapping at least 50% and have a correct label. And it's going to give us that result across each of the six classes that we trained on. So here we can see that our overlapping bounding boxes with the correct label are happening about 80% of the time for bagel. About 67% of the time for banana. And so on. That's pretty good, so I think let's, let's see if this model's actually going to work in a real app. I'm going to go ahead and call exportcoreml to create a Core ML model from the model we just trained. And I'm going to call it breakfastmodel.mlmodel. And then as soon as that's done training, I'm going to go ahead and open it in finder. Or sorry. As soon as that's done exporting. So here in finder, I've got my breakfastmodel.mlmodel. And when I open it in Xcode, I can see that it looks just like any Core ML model. It takes an input image, and as output we get confidence and coordinates. And that's going to tell us the predicted bounding box and label for the image that we have. Now let's switch over to the iPhone app where we're going to consume this model. So here on my iPhone, I've got an app called Food Predictor. And this is going to use the model that we just trained. Here I'm going to choose from photos. And I've got a picture of this morning's breakfast. This is a pretty typical breakfast for me: coffee and a banana. Well, often I skip the banana. But suppose I ate a banana this morning. We can just tap right on the image. And because we know the bounding box, we can identify the object within that bounding box. And here we see the model tells us this is a banana, and this is a cup of coffee. So let's recap what we just saw. First, we loaded images and annotations into SFrame format and joined them together with a simple function call. We interactively explored that data using the explore method. We created a model just with a simple high-level API< passing in that data object containing both the images and the bounding boxes and labels. We then evaluated that model both qualitatively, spot-checking the output as a human would. And quantitatively, asking for a specific metric that applies to the task that we're doing. Then we exported that model to Core ML format for use in an app. Next, I'd like to switch gears and talk about some exciting new features in Turi Create 5.0. Turi Create 5.0 has a new task called style transfer. We have major performance improvements from native GPU acceleration on your Mac. And we have new deployment options including recommender models for personalization and vision feature print-powered models so that you can reduce the size of your app. Taking advantage of models that are already in the operating system. Let's talk a little bit more about that style transfer task. Imagine we've got some style images and these are really cool looking recognizable stylistic images. Here we've got sort of a light honeycomb pattern and a very colorful flower pattern. And we want apply those as filters to our own images that we take with a camera. We've got a dog photo here and what it would look like to apply those styles to that dog is something like that. And with a style transfer model, we can take the same styles and apply them to more photos. Let's say a cat and another dog. And that's the sort of effect we would get. Here's an example of an app that uses style transfer for filters on photos that a user would take. The code to create the style transfer model follows the same five-step recipe as any other high-level task in Turi Create. So you can start by importing Turi Create, loading data into the SFrame format, creating the model with a simple high-level API. Then we make predictions, in this case with a function called stylize to take an image and apply that style filter. Finally, we can export it for deployment into Core ML format, just like any other model in Turi Create. So let's take a look at another demo. This time, we're going to build a style-transfer model. So switching back over to that Jupyter notebook environment. I'm going to start once again by importing Turi Create. As TC. Then, I'm going to load up two SFrames, each containing images. One, is the style images I'm going to call tc.loadimages, and I'm going to give it a directory name, styles. And then the other is content images. And the way that this works is the style images are the styles that you want to turn into filters you can apply. And the content images can be any images that are representative of the types of photograph that you would want to apply those filters to. So in this case, it's just a variety of photographs. We're going to load a folder called content into an SFrame for that one. And then we're ready to go ahead and train a model. So I'm going to say model equals tc.styletransfer.create. And I'm going to pass in style and content and that's all we need. But that's going to take a bit too long to train for today's demo. So once again, I'm going to do it like a cooking show, and we're going to load up one that we've had in the oven already. I'm going to say model equals tc.loadmodel and I'm going to load up my already trained style transfer model. Let's take a look at some of these style images to see what we should expect this model to produce. We have a style image col-- we have an image column in our style SFrame. And let's take a look at just style number three and see what that looks like. It-- sort of like a pile of firewood. And this is pretty stylistic. I think this would be recognizable if we were to apply it as a filter to another image. Now let's take a look at some content images. I'm going to load up a test data set. And once again this is a data set that is representative of the types of images that users will have at runtime in your app. And the important thing is that the model never saw these at training time. So by evaluating with the test images, we'll know whether the model can generalize to users' data. I'm going to load up a test data set now. With tc.loadimages function once again. And we're going to call that folder test. And I'm going to pull out one image from the test data set called ample image. And I'm just going to take the first image there. And I'm going to call .show. So that we can we see what that image looks like without any filters applied. That's my cat, seven of nine. She always looks like that. And we're going to go ahead and stylize that image using the model that we just trained. So I'm going to say stylized image equals model.stylize. And in this case, the function is called stylize because the model is specific to the task of style transfer. And we're going to pass in that sample image. And I'm going to say style equals three because that's the style that we picked earlier that looks like firewood here. So let's see what that stylized image looks like. I can call .show on that, and here is my cat looking like a pile of firewood. Let's make sure this works on other styles, too. I'm going to go ahead and make a stylized image out of that sample image. And I'm going to specify a style equals seven. And then let's see what that looks like. That looks pretty good. I wonder what the style was that we just applied to that. Let's take a look at style images. Style image seven. And once again we can just call .show to see what that style image looks like and yeah, that looks like the filter that we just applied to my cat. Now that we've got a good style transfer model, we can just call model.exportcoreml exactly the same as any other model, and save it into Core ML format. Now, let's switch over to the iPhone where we have a style transfer app ready to apply the filters in this model. So here I've got my iPhone once again. And I have an app called style transfer. I'm going to choose a photo from my photo library to apply these styles to. These are my dogs. This is Ryker and we're going to see what styles we have available in this app. We can scroll through all of the styles here and what's important to note is that a single style transfer model was trained on all of these files. And one model can include any number of styles. So to have multiple filters, you don't need to greatly increase the size of your app. Let's see what those styles look like applied to Ryker. Pretty cool. So-- So to recap what we just saw, we loaded images into SFrame format. This time style and content images into two SFrames. We created a model using a high-level API for style transfer that operates directly on a set of style images and a set of content images. We then stylize images to check whether the model is performing well. We visualized those predictions in Turi Create. And finally we exported the model in Core ML format for use in our app. Switching gears a bit. I want to talk about some other features in Turi Create 5.0. We now have Mac GPU acceleration offering up to a 12x performance increase in image classification. And 9x in object detection, and that's on an iMac Pro. We have a new task available for export into Core ML format. Personalization. The task here is to recommend items for users based on user's historical preferences. This type of model is deployed using Core ML's new custom model support that's available on macOS Mojave and on iOS 12. This has been a top community feature request since we open sourced Turi Create. So I'm really excited to bring it to you today. The recommender model in Core ML looks just like any other Core ML model. But what's worth noting is there's a section at the bottom here called Dependencies. And in this section, you can see that this model uses a custom model and that model is called TC recommender. And this is just Turi Create providing support for recommenders in Core ML through that custom model API. Using that model in Core ML look very similar to any other Core ML model as well. You can just instantiate the model, create your input. So in this case, we've got that avatar creation app. And a user might have picked a brown beard and a brown handlebar moustache and brown long hair for their avatar. And we can make predictions from the model. By providing those interactions as input. And where we say k10 means we'll get the top ten predictions given those inputs. So to recap what we've learned today. Turi Create allows you to create Core ML models to power intelligent features in your apps. It uses a simple five-step recipe starting with identifying the task that you're doing. And mapping it to a machine learning task. Gathering and annotating data for use in training that model. Training the model itself using a simple, high-level API specific to the task that you're doing. Evaluating that model in Turi Create both qualitatively and quantitatively. And finally, deploying in Core ML format. That five-step recipe maps to code starting with import Turi Create. You can load data into the SFrame format. Create a model using that task-specific API. Evaluate the model with an evaluate function that's once again specific to the task that you're doing. And export for deployment, calling the export Core ML function. Turi Create supports a broad variety of machine learning tasks. Ranging from high-level tasks like image classification and text classification, all the way to low-level machine learning essentials. Like regression and classification on any type of data. And using the resulting models, you can power intelligent features in your apps like object detection or style transfer for use as a filter. For more information, please see the Developer.Apple.com session URL. And please come to our labs. We've got a lab this afternoon and Friday afternoon. And we welcome your feedback. We're happy to answer any questions you have. And we'll have all of the demos we showed today available to explore. Thank you.  Hello and good afternoon everyone. Welcome to our session on natural language processing. I'm delighted to see so many of you here today, and I'm really excited to tell you about some of the new and cool features we've been working in the NLP space for you. I'm Vivek, and I'll be jointly presenting this session with my colleague, Doug Davidson. Let's get started. Last year, we placed your app center stage and told you how you could harness the power of NLP to make your apps smarter and more intelligent. We did this by walking you through the NLP APIs available in NSLinguisticTagger. NSLinguisticTagger, as most of you are familiar with or have used at some point, is a class and foundation that provides the fundamental building blocks for NLP. Everything from language identification to organization, part of speech tagging, and so on. We achieve this in NSLinguisticTagger by seamlessly blending linguistics and machine learning behind the scenes. So you, as a developer, can just focus on using these APIs and focus on your task. All that's great. So what's new in NLP for this year? Well, we are delighted to announce that we have a brand-new framework for NLP called Natural Language. Natural Language is now going to be a one-stop shop for doing all things NLP on device across all Apple platforms. Natural Language has some really cool features, and let me talk about each of those. First, it has a completely redesigned API surface, so it supports all the functionalities that NSLinguisticTagger used to and still does but with really, really Swift APIs. But that's not it. We now have support for custom NLP models. These are models that you can create using Create ML and deploy the model either using Code ML API or through Natural Language. Everything that we support in Natural Language, all of the machine learning NLP is high performed. It is optimized for Apple hardware and also for model size. And finally, everything is completely private. All of the machine learning in NLP that is powered in Natural Language is done on device to protect user's privacy. This is the very same technology that we use at Apple to bring NLP on device for our own features. So let me talk about each of these features of Natural Language. Let's start with the Swift APIs. As I mentioned, Natural Language supports all the fundamental building blocks that NSLinguisticTagger does but with significantly better and easier to use APIs. In order to provide some of these APIs and go over them, I'm going to illustrate them with hypothetical apps. So the first app that we have here is an app that you wrote, and as part of the app, you enable a social messaging or peer-to-peer messaging feature. And an add-on feature in this app that you've created is the ability to show the right stickers. So based on the content of the message, which in this case is "It's getting late, I'm tired, we'll pick it up tomorrow morning, good night." Your app shows the appropriate sticker. You parsed this text, and you bring up the sticker. The user can attach it and send it as a response. So all of this is great. This app has been doing really well. You've been getting rave reviews. But you also get feedback that your app is not multilingual. So it so happens that users are bilingual these days. They tend to communicate in several different languages, and your app, when it gets the message in Chinese, simply doesn't know what to do with it. So how can we use natural language to overcome this problem? Well, we can do this with two simple calls to two different APIs. The first is language identification. With the new Natural Language framework, you start off by importing Natural Language. You create an instance of NLLanguageRecognizer class. You attach the string that you would like to process, and you simply call the dominant language API. Now this will return the single best hypothesis in terms of language for the string. So the output here is essentially simplified Chinese. Now in Natural Language, we also support a new API. There are instances where you would like to know the top-end hypothesis for a particular string. So you'd like to know what are the top languages along with their associated probabilities. So you can envision using this in several different applications where there's a lot of multilinguality, and you want that leeway in terms of what could be the top hypothesis. So you can do this with a new API called Language Hypotheses. You can specify the maximum number of languages that you want, and what you get back is an object with the top-end languages and their associated probabilities. Now in order to tokenize this Chinese text, you can again see the pattern is very similar. You again import Natural Language. You create an instance of the NLTokenizer, and in this particular instance, you specify the unit to be word because you want to tokenize the string into words. You attach the string, and you simply call the tokens method on the string, on the object. And what you get is an array of tokens here. Now with this array of tokens, you can look up the particular token here is goodnight, and lo and behold, you have multilingual support in your app. So your app can now support Chinese with simple calls to language identification and tokenization APIs. Now let's look at a different sort of an API. I mean language identification and tokenization are good, but we would also like to use auto speech tagging, named entity recognition, and so on. So let me illustrate how to use named entity recognition API again with the hypothetical app. So here's an app. It's a news recommendation app. So as part of this app, your user has been going and reading a lot of things about the royal wedding, so a really curious user. They want to find everything about the royal wedding. So they've perused a lot of pages in your app, and then they go on to the search bar, and they type Harry. And what they see is completely things that are not pertinent to what they've been looking for. You get Harry Potter and so on and so forth. What you'd like to see is Prince Harry, something related to the royal wedding. So now you can overcome this issue in your app by using the name entity recognition API. Again, as I mentioned, the syntax here is very familiar. Those of you who have been used to using NSLinguisticTagger, they should look familiar and feel familiar, but it's much easier to remember and to use. You import Natural Language. You now create an instance of NLTagger, and you specify the scheme type to be name type. If you want part of speech tagging, then you would specify the scheme type to be lexical class. You again specify the string that you want to process. In this particular instance, you specify the language, so you set the language to be English. So if you were not familiar or if you're not sure about what the language is, Natural Language will automatically recognize the language using the language identification API under the hood. And finally, you call the tags method on this object that you just created. You specify the unit to be word and the scheme to be name type. And what you get as an output is the person names here, Prince Harry and Meghan Markle, and the location to be Windsor. Now if the user were to go back to the search bar, based on the contextual information of what the user has been browsing, you can significantly enhance the search experience in your app. So there's a lot more information about how to use these APIs. You can go to the developer documentation and find more information. What I'd like to emphasize here is NSLinguisticTagger is still supported, but the future of NLP is in Natural Language. So we recommend that and encourage you to move to Natural Language so that you can get all the latest functionalities of NLP in this framework. Now let's shift gears and look at a situation where you have an idea for an app, or you need a functionality within your app that Natural Language does not support. What do you do? So you can certainly create something, which will be great, but what if we gave you the tools to make that much easier? To talk about custom NLP models and how to build custom NLP models using Create ML and use the subsequent models, which are essentially Code ML models in Natural Language, I'm going to hand it over to Doug Davidson. Thanks Vivek. So I'm really excited about the new Natural Language framework, but the part I'm most excited about is support for portraying and using custom models. And why is that? I'd just like you to think for a second about your apps, maybe the apps you've written or the apps that you want to write, and think about how they could improve the user experience if they just knew a little more about the text that they deal with. And then think for a second about how you analyze text. So maybe you look at some examples of text, and you learn from them, and then you understand what's going on, and then you can look at a piece of text, and at a glance, you can figure out something about what's going on with it. Well, if that's the case, then there's at least a reasonable chance that you can train a machine learning model to do that sort of analysis in your app automatically for you, giving it examples that it can train and learn from and produce a model that can do that analysis. Now, there are many, many types of machine learning models for NLP, and there are many different ways of training it. Probably many of you are already training machine learning models, but our task here has been to produce ways to make this sort of training really, really easy and to make it integrate really well with the Natural Language framework and APIs. So with that in mind, we are supporting two types of models that we think support a broad range of functionality and that work well with our paradigm in NLTagger of applying labels to pieces of text. So the first of model we're supporting is a text classifier. A text classifier takes a chunk of text, maybe it's a sentence or a paragraph or an entire document, and applies a label to it. Examples of this in our existing APIs are things like language identification, script identification. The second type of model we support is a word tagger, and a word tagger takes a sentence considered as a sequence of words, and then applies a label to each word in a sentence in context, and examples of existing APIs are things like speech tagging and named entity recognition. But those are sort of general purpose examples of these kinds of models. You can do a lot more with them if you have a special purpose model for your specific application. Let me give you some hypothetical examples. So, for text classification, suppose you're dealing with user reviews, and you want to know automatically whether a given review is a positive review or a negative review or somewhere in between. Well, this is the sort of thing you could train a text classifier to do. This is sentiment classification. Or suppose you have articles or just article summaries or maybe even just article headlines, and you want to determine automatically what topic they belong to according to your favorite topic classification scheme. This again is the sort of thing that you can train a text classifier to do for you. Or going a little further, suppose you're writing an automated travel agent, and when you get a request from a client, the first thing you want to know probably is what are they asking about? Is it hotels or restaurants or flights or whatever else that you handle. This is the sort of thing that you could train a text classifier to answer for you. Going on to word tagging. So we provide word taggers that do part of speech tagging for a number of different languages, but suppose you happen to need to do part of speech tagging for some language that we don't happen to support quite yet. Well, with custom model support, you could train a word tagger to do that for you. Or named entity recognition. So we provide built-in named entity recognition that recognizes names of people and places and organizations, but suppose you had some other kind of name that you were particularly interested in that we don't happen to support right now. So, like for example, product names. So you could train your own custom named entity recognizer as a word tagger that would recognize names or other terms of whatever sort you are particularly interested in. Even further, for your automated travel agent, once you know what the user is asking about, probably the next thing you want to know is what are the relevant terms in their request. For example, if it's a flight request. Where do they want to go from and to? So a word tagger can identify various kinds of terms in a sentence. Or another application, if you need to take a sentence and divide it up into phrases, noun phrases, verb phrases, propositional phrases. With the appropriate sort of labeling, you could train a word tagger to do this, and many, many other kinds of tasks can be phrased in terms of labeling, applying labels to portions of text, either words in sequence or chunks of text in the text classifier. So these are supervised machine learning models, so there are two phases always involved. The first phase is training, and the second phase is inference. So training is what you do in part of your development process. You take labeled training data, and you feed it into Create ML and produce a model. Inference is then what happens in your app when you incorporate that model into your application at run time when it encounters some piece of data from the user, and then it analyzes that data and predicts the appropriate labels for it. So let's see how these phases work. So let's start with training, and training always starts with data. You take your training data, and then you feed it in to in this case Create ML in a playground let us say or a script [inaudible] as you may have seen in the Create ML session. Create ML calls the Natural Language framework under the hood to do the training, and what comes out is a core ML model that's optimized for use on device. So let's look at what this data might look like. So Create ML supports a number of different data formats. Right here we're showing our data in JSON because JSON makes things perfectly clear, and this is a piece of training data for a text classifier that's a sediment classifier. So each training example, like this one, consists of two parts, a chunk of text, and the appropriate label for it. And so this for example is a positive sentence, so the label is positive, but you can pick whatever label set you want. Now, then when you start using Create ML, the Create ML provides a very, very simple way to train models in just a few lines of code. First line, we just load our training data from our JSON file. So we give it a URL to the JSON file, create a Create ML data table from it. Then in one line of code, create and train a text classifier from this data. All you have to tell it is what the names of the fields are, text and label, and then once you have it, one line of code writes that model out to disk. Now for training a Word Tagger, it's very similar. The data is just a little more complicated because each example is not a single piece of text. It's a sequence of tokens, and the labels are, again, a sequence of labels, the same number of labels, one label for each token. So this, for example, is training data for a Word Tagger that does name identity recognition, and each word, each token, has a label, either none, it's not a name, or org, it's an organization name or prod, it's product name, or a number of different other labels for whatever kinds of names you're recognizing. So each token has a label, and each sample consists of one sequence of tokens and their corresponding labels. And then the Create ML to train this is almost identical. You load the training data into a data table from the JSON. Then you create and train a Word Tagger, in this case instead of a text classifier, and then you write it out to disk. Now, there are a number of other options and APIs available in Create ML. I encourage you to, if you haven't already, take a look at the Create ML session, which happened yesterday, and Create ML documentation for more information on that. Now, once you have your model, we then go to the inference part. So you take your model, you drag it into your Xcode project. Xcode compiles it and includes it in your applications resources, and then what do you do at run time? Well, it's a Core ML model. You could use it like any other Core ML model, but the interesting thing is that these models are able to work well with the Natural Language APIs just like our built-in models that provide the existing functionality for NLP. So what will happen is data comes in. You pass it to Natural Language, which will use that model and do everything necessary to get all of the labels out and then pass back either a label, single label for a classifier or a sequence of labels for a tagger. And so how do you do this in Natural Language API? First thing you have to do is just locate that model in your application's resources, and then you create an instance of a class in Natural Language called ML Model from it. And then, well, the simplest thing you can do with it, at least for a classifier, is just pass it in a chunk of text and get a label out. But the more interesting thing is that you can use these models with NLTagger in exactly the same way that you use our built-in models for an existing functionality. So let me show you how that works. In addition to the existing tag schemes that we have for things like named identity recognition part of speech tagging, you can create your own custom tag scheme, give it a name, and then you can create a tagger that includes any number of different tag schemes. Your custom tag scheme or any of our built-in tag schemes or all of them, and then all you have to do is to tell the tagger to use your custom model for your custom tag scheme. And then you just use it normally. You attach a string to the tagger, and you can go through and look at the tags for whatever unit of text is appropriate for your particular model, and the tagger will automatically call the model as necessary to get the tags and return the tags to you and will do all the other things that NLTagger does automatically, like language identification, tokenization and so on and so forth. So I want to show this to you in a simple hypothetical example. And this hypothetical example is an app that users will use to store bookmarks to articles they may have run across and then might be intending to read later on. But the problem with this application as it currently stands is that the list of bookmarks is just one long list with no organization to it. Wouldn't it be nice if we could automatically classify these articles and put them into some organization according to topic? Well, we can train a classifier to do that for us. And the other thing is that the articles, when we look at them, it's a long stream of text. Maybe we'd like to highlight some interesting things in those articles like for example names. Well, we have provided built-in name identity recognition for names of people, places, and organizations, but maybe we also want to highlight names of products, so we could train a custom word tagger to identify those names for us. So let me go over to the demo machine. And so here's our application as it stands before we apply any natural language processing. As you can see, even just one long list of articles on the side and a big chunk of text for our article on the right. Well let's fix that. So, let's go into -- so the first part of training a model is data, and fortunately, I have some very hard-working colleagues at Apple who have collected for me some training data to train two models. The first model is a text classifier that will classify articles according to topic. So this is some of what the training data looks like. Each training example is a chunk of text and the appropriate label by topic, entertainment, politics, sports, and so on and so forth. And I also have some training data to train a word tagger that will recognize product names in sentences. So this training data is pretty simple. Each example consists of a sentence considered as a sequence of tokens and then a sequence of labels, and each label is either none, it's not a product name, or prod, it is a product name. So, let's try to train with these. So first thing I want to do is bring up a playground that I have using Create ML, and this playground will just load. In this case, this is my product word tagger. It'll load the training data, create a word tagger from it, and write it out to disk. So let me just fire that off, and let it start running. It's loaded the data, and so under the hood, we automatically handle all of the tokenization, the feature extraction. We do the training. This is a fairly small model, so it doesn't take all that long to train, and I have it set to automatically write my model out to my desktop, and there it is. All right. So that's one model. Now I have another playground here that's set up to train my text classifier. As you can see, it looks very similar. Load the training data, create a text classifier from it, and write it out to disk. So I start that off. And again, automatically natural language is loading all the data, tokenizing it, extracting features from it. This one is a bit larger model. It takes a couple minutes to train. So let's just let that go, and in the meantime, take a look at some of the code we have to use these at run time. So I've written two very small classes to do what I need to do at run time. The first one uses the text classifier by finding that model in my apps resources and creating an NL model for it. And then when I run across an article, I just ask the model for a predicted label for that article, and that's really all there is to it. Slightly more code for use of my word tagger. So as you saw before, I have a custom tag scheme for my product name recognition, and the only tag I'm really interested in is the product tag. So I create a custom tag for that. Again, I have to find the model in my bundles resources, create an NL model for it, and then create an NLTagger, and this NLTagger I'm specifying two schemes. The first is the built-in name type scheme to do name identity recognition, and the second one is my custom product tag scheme, and they'll both function in exactly the same way. And then I just have to tell that tagger to use my custom model for my custom scheme. Now if I supported multiple languages, I might have more than one model in here for this scheme. And then what I'm going to do is highlight text in this article that is located, determined to be a name of one sort or another. So I'm going to get a mutable attributed string, and I'm going to add some attributes to it. So I'll take this string of that mutable attributed string, attach that to my tagger, and then I'm going to do a couple of enumerations over tags. The first one uses the built-in name-type scheme for name identity recognition of people, places, and organizations, and if I find something that's tagged as a person or place or organization, then I'm going to add an attribute to the attributed string that will give it some color. And then we can do exactly the same thing with our custom model. We're going to enumerate using our custom product tag scheme, and in that case, if we find something that's labeled with our custom product tag, then I can add color to it in exactly the same way. So you can use custom models with Natural Language API just in the same way that you use built-in models. Now, let's go back to our playground, and we see that the model training has finished, and in fact there are now two models showing up on my desktop. So all I need to do is drag those into my application. Let's take this one and drag it right in. Okay. And let's take this one and drag it in, and Xcode will automatically compile these and include them in my application. So all I have to do is build and run it. And let's hide that. Here's my new application, and you'll notice that my list of articles is all neatly sorted automatically by topic, and if I go in and take a look at one of these articles, you'll notice that names are highlighted in it, and you can see, using our built-in name identity recognition, we highlight names of people, places, and organizations, but if you look a little further, you can see that it has used our custom product tagger to highlight the names of products like iPad, MacBook, iPad mini, and so forth. So this shows how easy it is to train your own custom models and to use them with the natural language APIs. So now I'm going to turn things back over to Vivek to talk about some important considerations for training models. Thank you, Doug, for telling us how to use these custom NLP models. We are really excited to sort of have a very tight integration of natural language with Create ML and the Core ML [inaudible], and we hope that you do some really unbelievable things with this new API. So I'd like to shift attention now again and talk about performance. So as I mentioned before, Natural Language is available across all Apple platforms, and it also offers you what we call as standardized text processing. So let's take a moment again to understand what we mean by this. Now if you were to look at a conventional machine learning pipeline that didn't use Create ML, where would you start? You would start with some amount of training data. You would take this training data. You would tokenize this data. You'd probably extract some features. This is really important for languages like Chinese and Japanese where tokenization is very important. You would throw that into your favorite machine learning toolkit, and you'd get a machine learning model out of it. Now in order to use that machine learning model on an Apple device, you'd have to convert that into a Core ML model. So what would you do? You would use a Core ML converter to do this. This is sort of the training procedure in order to get from data to a model and deploy it on an Apple device. Now, at inference time, what you do is you drop the model in your app, but that's not it. You also have to make sure that you write the code for tokenization and feature extraction that is consistent with what happened at training time. It's a lot of effort because you have to think about maximizing the fidelity of your model. It's absolutely important that the tokenization featured extraction is identical at both training and inference time. But now with the use of Natural Language, you can completely obviate this. So if you look at the sequence at training time, you have training data. You just pass it to Create ML through the APIs that we've discussed so far. Create ML calls Natural Language under the hood, which does the tokenization feature extraction, chooses the machine learning library, does all the work, and returns a model which is a Core ML model. Now at inference time, what you do is you still drop this model in your app, but you don't have to worry about tokenization feature extraction or anything else. In fact, you don't have to write a single line of code because Natural Language does all of that for you. You just focus on your app and your task and simply drag and drop the model in. The other aspect of Natural Language as I mentioned before is it's optimized for Apple hardware and for model sizes. So let's look at this through a couple of examples. So Doug talked about named entity recognition and chunking, and here are two different benchmarks. So these are models that we built using an open source tool kit called CRF Suite, and through Natural Language. The models were built from identical training data and tested on identical test data. The same sort of features were used. The accuracy obtained by both these models is the same. But you look at the model sizes that Natural Language is able to generate. It's simply just about 1.4 megabytes of data size, model size for named entity recognition and 1.8 megabytes for chunking. That saves you an enormous amount of space within your app to do other things. In terms of machine learning algorithms, we support two different options. We can specify this for text classification. So for text classification, we have two different choices. One is maxEnt, which is an abbreviation for Maximum Entropy. In NLP, we call maxEnt is essentially a multinomial logistic regression model. We just call it Maximum Entropy in NLP feed. The other one is CRF, which is an abbreviation for Conditional Random Feed. The choice of these two algorithms really depends upon your task. So we encourage you to try both these options, build the models. Now in terms of word tagging, that is one default option, which is a conditional random feed. When you instantiate an ML word tagger, specify data to it, the default model that you get is a conditional random feed. Now as I mentioned, the choice of these algorithms really depends on your task, but what I'd like to emphasize is sort of draw [inaudible] between your conventional development process. So when you have an idea for an app, you go through a development cycle, right. So you can think of machine learning to be a very similar sort of a work flow. Where do you start, you start with data, and then you have data, you have to ask a couple of questions. You have to validate your training data. You have to make sure that there are no spurious examples in your data, and it's not tainted. Once you do that, you can inspect the number of training instances per class. Let's say that your training a sentiment classification model, and you have a thousand examples for positive sentiment, you have five examples for negative sentiment. You can't train a robust model that can determine or distinguish between those two classes. You have to make sure that the training samples for each of those classes are reasonably balanced. So once you do that with data, the next step is training. As I mentioned before, our recommendation is that you run the different options that are available and figure out what is good, but how do you define what is good? You have to evaluate the model in order to figure out what suits your application. So the next step here in the work flow is evaluation. Evaluation in convention [inaudible] for machine learning is that when you procure your training data, you split your data into training set, into a validation set, and into a test set, and you typically tune the parameters of the algorithm using the validation set, and you test it on the test set. So we encourage you to do the same thing, apply the same sort of guidelines that have stood machine learning in good stead for a long time. The other thing that we also encourage you to do is test on out-of-domain data. What do I mean by this? So when you have an idea for an app, you think of a certain type of data that is going to be ingested by your machine learning model. Now let's say you're building an app for hotel reviews, and you want to classify hotel reviews into different sorts of ratings. And the user throws a data that is completely out of domain. Perhaps it's something to do with a restaurant review or a movie review, is your model robust enough to handle it. That's a question that you ought to ask yourself. And the final step is well in a conventional development workflow you write patches, you fix bugs, and you update your app. How do you do that with machine learning? Well, the way to do that or fix issues with machine learning is to find out where your models do not perform well, and you have to supplement it with the right sort of data. By adding data and retraining your model, you can essentially get a new model out. So it's, as I mentioned, it's very similar to sort of the development workflow, and they are very [inaudible]. So you can think of it as part of your fabric if you're employing machine learning models as part of your app, you can just combine it with the word process itself. The last thing I'd like to emphasize here is privacy. So everything that you saw in this session, all of the machine learning and Natural Language processing happens completely on device. So we at Apple take privacy really seriously, and this is a remarkable opportunity to use machine learning completely on device to protect user's privacy. So in that vein, Natural Language is another step towards privacy preserving machine learning but in this case apply to NLP. So in summary, we talked about a new framework called Natural Language framework. It's tightly integrated with the Apple machine learning [inaudible]. You can now train models using Create ML and then use those models either with the Core ML APIs or with Natural Language. The models that we generate using Natural Language and the APIs are highly performed and optimized on Apple hardware across all the platforms. And finally, it supports privacy because all of the machine learning in NLP happens on user's device. So there's more information here. We have a Natural Language lab tomorrow, so we encourage you to try out these APIs and come talk to us and ask us questions about where you'd like enhancements or perhaps some sort of consultation with respect to your app. We also have a machine learning get together, and there's a subsequent [inaudible] Create ML lab that's happening right now. So you can continue coming and talking to us as part of that lab. Thank you for your attention. Thanks.  Hello. Good afternoon and thank you for coming to What's New in WatchOS. In this session, we're going to give you a high-level overview of the new features you as developers, designers and product leaders can take advantage of to create great Watch experiences. And we're also going to dip into the details of the new APIs and interface builder options. My name is Lori Hylan-Cho, I work on the Watch frameworks team, and I am beyond thrilled to be here, to share with you the awesome new options for creating great Watch experiences in WatchOS 5. We've come a long way since our humble beginnings in 2015. And a great Watch experience now consists of several components, of which a Watch app is likely to be only one. Notifications, complications and Siri shortcuts work together to create a Watch experience that allows for brief and meaningful interactions with the right information at exactly the right time. And of course, with Series 3 Watches that support cellular, and the new Wi-Fi options that let you join networks directly from the Watch, more and more Watch wearers are venturing out without their phones. So, it's important to create a Watch experience that feels complete. Let's take a look at the features we've added to WatchOS 5 to help you deliver this kind of a great experience on the wrist. Ask any Apple Watch user what they like most about their Watch, and one of the first things they'll say is notifications. We're making notifications even richer this year with a few key improvements. Creating dynamic notifications with images and text has been an option since WatchOS 1. But if the user missed the notification when it arrived, chances were they'd never see it at all, as the Notification Center would display the static version. This was intentional, as we have more time to display the notification when it arrives than we do when displaying it from Notification Center. In WatchOS 5, we'll do our best to show that beautiful dynamic notification with no code changes or recompilation required on your part. We worked hard to make this possible for its own sake but being able to show dynamic notifications from Notification Center also makes possible the next two features I'm going to tell you about. The first of these is grouped notifications. You probably already know that you can have multiple categories and different notification interfaces for each one. And you might already be specifying a thread ID and the push notifications that get forwarded from iOS to the Watch. In WatchOS 5, this will cause notifications to be grouped automatically in Notification Center as you saw in the previous slide. Specifying a category and thread ID also opens up another possibility. Getting the same behavior as the built-in messages app, where new notifications with the same thread ID that come in when the original one is still on screen are appended to the existing interface. We made this kind of grouping behavior optional to give you a chance to handle being called multiple times for the same notification interface controller. To opt into this behavior, select your notifications category in the interface storyboard for your Watch app. And check the handle's grouping box in the attributes inspector. If the original notification is on screen when additional notifications that have a matching category and thread ID arrive, did receive notification will be called again on the existing notification interface controller. So, you should be prepared to append content to the existing notification interface. Perhaps by adding a blank line and the new body content to an existing label. Or by adding a new row to a table, as you saw on the previous example. And speaking of did receive notification, its signature has changed in WatchOS 5. This version, with the completion handler has been deprecated, in favor of this simplified version. We did this, so we'd have a clear boundary for when you're done processing your notification data. And we'd know that everything is ready to be shown on screen. We encourage you to do only the work necessary to display the notification and to do it as quickly as possible. By the way, it's not necessary for the user to keep their wrist up to see all the notifications with the same thread ID as they come in. If you're handling grouping and the dynamic interface for your notification category, all the messages with the same thread ID will expand to a single platter when the user taps on the group and Notification Center. And if additional messages on that thread come in, they'll be appended live is when the long look was first on screen. So, here's a big one. In WatchOS 5, you can now bring some of your apps' functionality directly into the notifications you send by including elements that let the user interact with content in the notification itself. For example, you could let a user pay for their ride and rate their driver, notify a user that their meter is about to expire, and let them extend their parking time, or give a diner an opportunity to not only confirm their dinner reservation, but to let you know they'll be a party of three instead of four. So, let's see how all this works. If you're creating a new notification interface controller, a dynamic interactive interface will be created for you automatically. If you already have a dynamic interface for your notification category, select the category in your Watch App's interface storyboard and check that has interactive interface box. In both cases, you'll notice that the old dynamic interface is still there, in addition to the dynamic interactive one. The dynamic interface will be shown in WatchOS 5 when the notification first arrives, and now in Notification Center as well, as I mentioned before. The dynamic interface will be shown in earlier versions of WatchOS so, you'll want to keep that one around for backwards compatibility. Once you have a dynamic interactive interface controller you can add buttons, switches and other interactive elements in the object library, which is now available at the top of the screen as a popover menu. You can even add gesture recognizers with your notification, though you should be aware that the system gestures will take precedence over any gestures you add in the same areas. Once you've designed your notification interface wire up the interactive elements to your code, the way you would any other interactive element in your app. As you can see here, I'm dragging my IB action for up button tapped to the button in my dynamic interactive interface. So, the amount of time will increase by 15 minutes every time the button is tapped. And with all this button tapping you may have already forgotten that notifications have always launched your app when tapped. This is still true for a regular dynamic and static notifications. But, we obviously had to disable this behavior for dynamic interactive notifications. Because we have interactive elements in them now. You can still launch your app from an interactive notification, if you need to. You just have to do it explicitly by calling a new method, performed notification default action. Similarly, if you include a button in your notification interface that should dismiss the notification after taking action, as in this case where tapping the extend button should both commit the changes to my rental time and dismiss the notification, you do this by calling perform dismiss action at the end of the IB action function associated with the button. You could also process the changes made in the body of the notification using a standard action button. Since action buttons always dismiss the notification. Wait, you're probably thinking, action buttons are shared between all the notification interfaces. And some of those buttons wouldn't make sense if the interactive elements weren't there. Well, new in WatchOS 5, you can now adjust the action buttons that are shown at runtime. There's a new notification actions property on the notification interface controller that returns the array of actions that will be displayed for the notification. You can set this property to a new array of UN notification action objects in your did receive notification call back. which gives you the flexibility to add or remove buttons to suit your interactive interface. And in an upcoming seed, we're going to allow changes to actions any time after did receive notification is called. Which will give you the flexibility to change the action buttons based on how the user interacts with the elements in the interactive notification interface. So, that's a lot of information about notifications and the options for creating them so far. But I have two more things to tell you about before we move on. Critical alerts are a new kind of notification that will cause a prominent haptic and a sound to play even when your Watch is silenced or in do not disturb mode. And thus, they can be used to deliver extra urgent information. If your Watch app integrates with a medical device or is used by emergency responders, this kind of alert might be for you. Critical alerts require an app entitlements and explicit permission to be granted by the user separate from regular notification. The opposite of a critical alert is one that can be delivered quietly. That is, rather than interrupting the user with an in the moment notification that takes over the screen, you can now choose to send notifications directly to Notification Center. You'll notice that no indicator will appear unless a prominently delivered notification is also in Notification Center. But the swipe from the top of the screen will still reveal the quietly delivered notification. If you choose to deliver notifications quietly in your app, you don't have to prompt the user to allow notifications when the app is first launched. Instead, you can request provisional permission. This gives the user a chance to see what kinds of notifications your app sends before having to decide whether they want to see them as they arrive. Whichever option you choose as a developer, the user still has ultimate control over how they receive your notifications. They can choose to deliver notifications quietly by swiping left from a notification in Notification Center or change their notification preferences in settings. To sum up, notifications are now more dynamic. Users will see your dynamic interface from Notification Center in addition to when the notification first arrives. We're also offering grouping by thread ID for the first time on the Watch. So, you can design notifications that behave like the ones like the built-in messages app. You can now bring more of your app experience directly into the notification your app delivers with interactive controls and action buttons that can be defined at runtime. And you can also choose to deliver notifications with the appropriate level of urgency. To learn more about notifications you can attend one of these sessions. I highly recommend Designing Notifications, that will give you some great tips for how to design effective interactive notifications. Okay, now that we have your user's attention with notifications, let's turn our attention to the features we're adding to make your Watch apps more awesome. The first has to do with local audio playback. Some of you have already made forays into audio playback on the Watch by taking advantage of URL session for downloading files to the Watch. And WK audio file queue player to play them. With WatchOS 5, we're going to make developing audio apps for the Watch even easier, and the resulting experiences more delightful. In WatchOS 5, we're offering a new background mode for playing local audio files. So, you can now focus on your app's main purpose, audio, rather than trying to figure out how to build something else like a workout app just to be able to play audio. We also exposed AVAudio player and AVAudioEngine directly, which means you can use the same methods and properties you're already familiar with if you've been building iOS apps that play audio. In fact, you can share your code between your iOS app and your Watch app by moving your playback related code to a framework. One thing that's different from iOS, is that playing longform audio on the Watch requires headphones or an external speaker to be connected just as when playing from the built-in music app. For this reason, Bluetooth routing is part of the session activation process. If you set your route sharing policy to longform, as you should, will automatically connect to the headphones with Apple wireless chip, like AirPods or Beats Studio3, if they're already in use. Or, will show a route picker to let the user choose other headphones or Bluetooth speakers when you call the activate with options completion API in a session that's new. You could also use the MP now playing info center API to populate the now playing app with information about what your app is playing. Which means your apps info will show in the now playing complication. And you can handle the media remote commands that make sense for your app; from play and pause, to next and previous, to even like and dislike. And last, but not least, it's now possible to control the volume from your custom playback UI with the new volume control view. Available in the object library and interface builder. The control automatically takes on the tint color of your app when at rest and responds with the color changes you're used to from the system volume control when turning the digital crown. There's a whole session devoted to creating audio apps for WatchOS which I highly encourage you to attend to get more details about the new APIs and best practices for working with audio. And we'll also be prepared to talk about background audio at the WatchOS runtime and conductivity lab on Thursday. If the primary function of your app isn't playing audio, but your app would benefit from being able to control audio playing elsewhere on a system, whether it's on a Watch or on phone, as in the workout app, where you can swipe left during a workout to see now playing controls. You'll be happy to hear that were exposing a now playing view that you can embed in your apps. You can find the now playing view in the object library in interface builder. It's designed to fill the screen. So, it works best in apps with page layouts. Note that the now playing view is a systemwide control that's meant to control audio coming from other apps. So, it will show whatever the user is currently listening to, whether it's on Apple Watch or iPhone. While we're here, I want to point out a couple of other new options and one change in behavior. In previous versions of WatchOS, if you added a 38-millimeter asset but forgot to add the same asset for a 42-millimeter device, the asset would be missing on the larger device. Now it will automatically fall back to the 38 millimeter size if the 42 millimeter size is missing. But you can skip worrying about providing assets of different sizes altogether by instead adding a PDF in the universal selection of your asset catalog and setting the new auto scaling option to automatic. This will cause the right sized asset to show up at the right place at the right time. We've also exposed the title textiles in the font menu to give you more options for differentiating your text within your interfaces. These textiles are compatible with dynamic type, so they'll increase or decrease. They'll scale up and down, as the user changes their font size in settings. And the large title style is available in both interface builder and as an API. Since I'm in the process of building an app for skating workouts, now would be a good time to talk about the improvements we're making to workouts in WatchOS 5. We completely rewrote the workout API in this release to make it simpler, more reliable, and more resilient. And we've moved the built-in workout app to use it. We're hoping you'll move your fitness apps to it to. It's now easier than ever to start a workout and collect the right data with a new initializer from HK workout session and the new workout builder API. You create the workout session, get the builder from the session and start collecting data. It's that simple. This is what it looks like in code. You create and configure the session with a health store and a workout configuration, which includes the activity type. Grab the HK live workout builder that's associated with the session and begin collection. The relevant data for the workout type will be collected automatically, even across pauses and resumes, to give you a correct and consistent HK workout artifact with a correct elapsed time. And because no app is perfect, if your fitness app crashes during an active workout session it will automatically be relaunched. Just use the HK health store recover active workout session API and the session and builder will be restored in their previous state. To learn even more about the new workout APIs, as well as options for collecting health and fitness data check out New Ways to Work With Workouts or visit the health and fitness technology lab tomorrow afternoon. Next up, your apps now have a place on the Siri Watch face with Siri Shortcuts. As you saw on the keynote, Siri Shortcuts are meant to help users accomplish tasks that they perform frequently with more ease than ever before. And the Siri Watch face surfaces these, the common tasks, at the right place and time with a little input from you as app developers. I want to focus on how to use shortcuts to make a great Watch experience and how that experience differs depending on whether your Watch app is installed. First, a few words about what makes a good shortcut. As the name implies, shortcuts are about helping Watch users see useful information and perform common tasks quickly. Whether it's launching your app to a specific screen with options preselected, or getting to the awesome outcome your app enables, such as ordering your morning coffee, booking a fine dining reservation, or re-upping your chocolate supply. Think about glanceable information and one or two tap interactions. Shortcuts can be user activity based, or intent based. User activity based shortcuts are great for when the goal is to launch your app with a specific context, such as navigating straight to a screen where the user can log a meal they've eaten, or in this case, delivered. Intent based shortcuts make more sense for tasks that don't necessarily require your app to be launched. Such as placing your regular coffee order. If your shortcut is intent-based, when the user taps your shortcut platter on the Siri face, they'll receive a quick confirmation screen. Did you mean to order that coffee? If the intent supports background execution, it will run without launching the app, when the user confirms. Otherwise, the confirmation will launch or restore the app and hand it your intent. How the user has interacted with your app in the past is taken into account when predicting what shortcuts to show. You should donate intense or user activities as the user performs the main functions of your app. For example, an audio app like Audible would donate an IN media, play media intent when the user starts or resumes playing an audiobook. Shortcut surface on the Siri Watch face based on relevance with the most relevant items appearing at the top of the up next section and less relevant ones at the bottom. Previous user interactions intense performed or donated and user activities that have been marked both eligible for search and eligible for prediction inform relevance. But to appear on the Siri Watch face, you must also give the system explicit hints about when or where a shortcut is relevant by creating a relevant shortcut. A relevant shortcut consists of a user activity or intent-based Siri shortcut, an optional Watch template that defines the title, subtitle, and image that should appear in the shortcut platter, and an array of relevance providers that define the time, location or situation in which your shortcut would be most relevant. We've already talked about user activity and intent-based shortcuts, so let's talk about the other two components of a relevant shortcut. A Watch template is an optional IN default card template, that consists of a title, an optional title, and an optional image. If you don't provide a Watch template, we'll pull the necessary information from the IN shortcut, but you should take the opportunity to provide informative strings so the user understands what they get when they tap your platter. A relevance provider focuses on the date, location or situation in which the shortcut would be most valuable. Think about when and where your shortcut would be relevant. Is it something the user might glance at or tap on at any time during the day? Is it relevant at a specific date and time, or is it based on location? Since a relevant shortcut takes an array of relevance providers it's possible to specify more than one kind. For example, if your shortcut would be relevant at both a time and a place. Again, these relevant providers act as hints to the system. As the user interacts with your content, actual usage will inform its placement. If the user fails to interact with shortcuts that appear on the Siri face, they'll drop in relevance. Once you have relevant shortcuts, the next step is to supply them to the relevant shortcut store so they can be considered for the Siri Watch face. Relevant shortcuts can be supplied by both your iOS app and your Watch app. iOS relevant shortcuts are synced periodically from iPhone Apple Watch and they're merged into consideration with the Watch relevant shortcuts. If your Watch app supports the iOS shortcut, the Watch app handles execution. If it doesn't, or if the Watch app is not installed, the shortcut executes on the phone, over Internet. A subset of relevant shortcuts can surface on the Siri Watch face, without Watch OS support. If they are intent-based shortcuts that support running in the background and can run without accessing encrypted data, which is separate from off. Again, these shortcuts are going to execute on iPhone over Internet. The richest experience for users will always be with a Watch app that can handle the shortcut locally, either by launching the app or by executing the intent in the background. You'll want to update your relevant shortcuts periodically. If the user launches your app, for example, by tapping one of your shortcuts on the Siri face, you'll have the run time to accomplish the update. But shortcuts that provide glanceable data are useful to the user without launching the app. Think about the carrot whether example from a couple slides ago, were you could tell that it was 72 degrees and cloudy just by looking at the platter. You didn't have to tap. To help with this case, we added a background refresh task called WKRelevantShortcut RefreshBackgroundTask. When you get this task before sure to check if your data needs to be updated and then supply new relevant shortcuts. We factor in user engagement when scheduling these background tasks and glances count as engagement. If you have an intent-based shortcut, your intent might update applications when it executes in the background. Since the intents extension is in a separate process from your Watch Kit extension, we're now supplying a new refresh background task called WKIntentDidRun RefreshBackgroundTask to let your main extension run and update any snapshots or complications that may be stale after your intent executed. Now that you have your shortcuts available at the turn of the wrist and a tap, you should know that users can set up shortcut phrases on the phone and use them to execute your shortcuts at any time. These shortcut phrases are synced to Apple Watch. So, you can use raise to speak to say, for example, mint me up and kick of an order of a mint mojito from Phil's Coffee. Your shortcut platter does not need to be showing on the Siri Watch face for your shortcuts to be invoked with a shortcut phrase. So, in summary, use Siri shortcuts to provide relevant information and quick interactions to your users. Create relevant shortcuts on both iPhone and Apple Watch and add them to the shortcut store so they can be considered for the Siri Watch face. And finally, your users will have the best experience with a WatchOS app where they'll be able to do more with your shortcuts directly on the Watch. For more on Siri shortcuts, I recommend checking out one of these sessions or labs, especially the one about Siri shortcuts on the Siri Watch face, which will go into much more detail about the APIs I covered today. So, we covered a lot of ground today. Here's what we talked about. We want you to make your notifications more engaging and actionable by creating dynamic interactive interfaces for your notifications. You can now bring audio into the background with AV foundation APIs, media remote commands and access to now playing info. You can make your fitness apps more robust and reliable with the new workout builder API. And you can provide value on the Siri Watch face by creating relevant shortcuts. If you have any questions about what you've heard today what you built for Watch OS so far, or what you're going to build next come visit us in the labs. We'd love to see what you're working on. Thank you for coming and have a great WWDC.  Hi everyone. Welcome to session 221, TextKit Best Practices. I'm Donna Tom. And I'm the TextKit engineer. And my colleague Emily Van Haren from authoring tools will be joining me today. And we're both really excited to share with you some best practices for working with TextKit. So let's get started. First, we're going to review some key concepts for working with TextKit. Then, we'll dive into some examples to illustrate how to apply the key concepts in your app. And finally, we'll wrap up with some best practices in the areas of correctness, performance, and security. So let's start with the key concepts. Now to make sure we're on the same page, we're going to start at the very beginning. What is TextKit? And your first instinct might be to open a shiny new playground in Xcode and type import TextKit, except if you've ever actually tried this, you found that it doesn't work. And that's because TextKit is a little different than other frameworks you might have used. You don't have to import anything special to use it. The text controls in UIKit and AppKit are built on top of TextKit. And so if you've ever used a label, the text field, or a text view, you've actually used TextKit. And TextKit pulls together powerful underlying technologies such as Core Text, Core Graphics and Foundation to make it simple and seamless for your apps to show text. And every time you use one of these built-in controls, you're using the power of TextKit to show or edit text in a fully, internationalized, localizable manner without having to directly use these underlying technologies or understand the intricacies of complex scripts. And there are a lot of things you get for free too like all of these display features that you see here. And for editing, you'll also get access to all the tech services that are supported by the OS like accessibility, spell checking and more. And you can take advantage of all of these great features without having to write a single line of code and that's pretty awesome. And so with all of this functionality at your fingertips, how do you decide which control to use? So let's talk about that, choosing the right control for your situation. And the options are going to be a little bit different depending on whether you're using UIKit or AppKit. So let's review them separately. All right. Let's start with UIKit. And first you're going to consider whether you need text input. And if you don't need text input, then consider whether you need selection or scrolling. And if you don't need these, then you should use UILabel. UILabels are intended for small amounts of text like a few words or a few lines. And so if you have more text than that or if you need these selection or scrolling capabilities then you should use a UITextView with editing disabled. Now going back to the top. If you do need text input, then consider whether you need secure text entry. And this would be like a password field where the text is obscured and copying is disabled. And so if you need that, then use UITextField because this is the only control that supports secure text entry. Otherwise, think about how much text you expect to be entered. And if you want something that's like a form field input that only needs a line, then use UITextField. And UITextField only supports one line of text entry. Otherwise, if you need more than that, you can use UITextView. And so now here's that same decision process for AppKit. And it's similar to the UIKit process but there's a few small differences. So, again, you're going to start by considering whether you need text input. And AppKit doesn't have a label control. So if you need to display text, use an NSTextField and you can disable both editing and selection to get that label behavior. Now going back to the top here. If you do need text input, again ask if you need secure text entry. And if so, you can use NSSecureTextField. Otherwise, we're going to ask our favorite question, how much text do you expect? So NSTextView is optimized for performance with large amounts of text. And so if you're expecting a lot of text, you should use NSTextView otherwise you can use NSTextField. Now, unlike its UIKit counterpart, NSTextField does support multiple lines of text, but it's still optimized for shorter strings and so you should still use NSTextView if you have a lot of text. Now those of you who have been around the block a few times with TextKit might notice that the flow charts are missing an option and that's string drawing. And you use string drawing by directly calling draw in point or draw in rect methods under NSString or your NSAttributedString. And many of you may be using this for the performance benefit to avoid the overhead of view objects at the kit level. And so if you're going to go this route, please keep the following tips in mind. You want to use it for small amounts of static text. And you want to limit how frequently you call the draw methods. Now if you're calling the string drawing methods a lot, you might actually get better performance out of a label or a text field because these controls provide better caching, especially if you're using auto layout. And if you're drawing an attributed string with a lot of custom attributes, this could also be slowing down your string drawing because the text system needs to validate all of the attributes before rendering and so for best performance, you should strip out extra attributes before drawing and only pass in the ones that are needed to determine the visual appearance like font or like color. And finally, remember that by using string drawing, you'll miss out on all of this free functionality that's offered by the text controls, so you should use the text controls whenever possible. So now you know what you can do with TextKit just by using the built-in controls. But if you want to go beyond what these controls provide, you'll need to find the right customization point within the text stack. And like much of Cocoa, TextKit is based on the model view controller design pattern. And the text system can be divided into three phases that correspond it directly to NBC and that's storage, display, and layout. And so now let's take a closer look at the TextKit objects that make up each of these phases. And we'll start with the storage which corresponds to the model. Now NSTextStorage holds your string data and your attributes. It's a subclass of mutable attributed string and so you can work with it in the same way that you already know how to work with attributed strings. And my colleague Emily will show you some really powerful ways to customize the text storage a little bit later so stay tuned for that. Now NSTextContainer models the geometry of the area where your text will be laid out. And by default, it's a rectangle but you can customize the flow or the shape of the text layout as shown here. And for more detailed information on working with the storage objects, check out these great past WWDC sessions and documentation. And they'll be available from the more information link at the end of the session. And next up is the display phase and that corresponds to the view. And we've already talked about the display phase quite a bit when we talked about choosing the right control. And so for additional information, you can again check out these documentation resources. And they'll also be accessible from that more information link at the end of the session. And finally, we have the layout phase which corresponds to the controller. And NSLayoutManager is the only component in this phase. And let me tell you it is a beast. And I mean that in a good way because it's so awesome at what it does. So it's the brains of the whole operation. It coordinates changes between all of the phases, and it controls the layout process itself. So here's a quick overview of how that layout process works. So text layout happens after the system fixes attributes in the text storage to remove inconsistencies like making sure that all the characters in the string are covered by fonts that support displaying those characters. And so in this example, the Times New Roman font is specified for the entire string, but this font doesn't support displaying Japanese kanji or emoji. And so after attribute fixing, your text storage will look something like this with an appropriate Japanese font assigned to the Japanese characters and the emoji font assigned to the emoji character. All right. So once the attributes are fixed, the layout process can begin. And we can think of layout in two steps: glyph generation followed by glyph layout. And once they're laid out, they're ready for display. But wait a minute. What's a glyph? Let's back up and review that. A glyph is a visual representation of one or more characters. And as you can see here, the mapping between characters and glyphs is not always one-to-one. So here this string ffi has three characters, but it could be represented by a single glyph for the ligature. And you can go in the other direction too. Here we have n [inaudible] which is a single character that can be represented by multiple glyphs: one for the n and one for the tilde. And so going back to our diagram here, we have NSLayoutManager performing glyph generation and glyph layout. And glyph generation is where the layout manager takes the characters and figures out what glyphs need to be drawn. And glyph layout is where the layout manager positions those glyphs for display in your view. And there's a lot more to learn about the layout manager from these past WWDC sessions and documentation. And you can access them from, you guessed it, the more information link at the end of the session. Okay. So now you understand the phases of the text system. And you know the TextKit components that make up each phase. So now let's explore choosing the right configuration of these components to create different effects. So this is your standard configuration. And when you drag and drop a text view from Interface Builder, you'll automatically get one of each component as shown here. And most the time this is all you're going to need. If you want a multiple page or a multiple column layout, you can use pairs of text containers and text views, one pair for each page or column. And you can hook all of these up to the same layout manager in the same text storage so that they share the layout information in the backing store. And if you want different layouts in each of you, you can do that too, just use multiple layout managers. And, again, since the text shares the same backing store, updating that text will update all of the views. Now we didn't go into too much detail about these configurations because there's a great past session that's already done that so check out WWDC 2010 session Advanced Cocoa Text Tips and Tricks. And this will be accessible from that more information link at the end of the session. All right. So we've looked at the built-in text controls. We've looked at the components in TextKit. And we've look at how to configure those components to achieve different effects. And there's a lot that you can do with that knowledge already, but if you need even more, you'll need to extend and customize parts of TextKit yourself. And so now we'll talk a little bit about choosing the right approach for doing that. And choosing the right approach is like building up your text toolbox. It's like going to the store because you need a hammer. And then when you get there, you encounter this giant wall of hammers to choose from. And you want to pick your hammer that can do the job and ideally the least expensive one that will do what you need. And so these are the hammers that are available to us. Delegation is like your standard hammer with the claw on the end, and it's used to perform multiple tasks. So the delegates have a lot of different customization hooks and most of the time they'll get the job done for you. Notifications is like a ball-peen hammer. And this has a ball on the end instead of a claw so it's more specialized and it's better suited for certain tasks, but it's not as versatile as the standard hammer of delegation. And finally, subclassing is your sledgehammer. The sledgehammer is very powerful, and you can use it for just about anything that you would need a hammer for but it's probably overkill for a lot of things. And with that, I'd like to invite Emily up to show us how to use these different kinds of hammers. Emily. Thank you, Donna. So, as developers, we have a collection of controls to choose from, various configurations, and a wide range of customization options to achieve what we need. So our tool chest is stocked full, but how do we know what tools to choose? So we're going to take a look at some examples of apps that harness the power of TextKit. And we don't have to look very far because almost every app that we use displays or edits text. We're going to start by looking at two apps that we're all familiar with and then go through the steps of building our own. So the first app we're going to look at is Apple News on iOS, which is a beautiful app that displays text in a personalized and curated articles. So here's an example of an article that is featured in the spotlight tab. Now the top of the app shows some details about this article. Now how could we use TextKit to re-create this look and feel? So let's consider the flow chart that Donna showed us earlier to pick the control that's best suited for this example. So we have a handful of text controls to choose from, but since we want to display small amounts of text, each on a single line, we'll use a label. Now we can see that there is a ton of customization options in the inspector panel. So we're going to go ahead and change the text to spotlight. We're going to change the font to use the body style. And we're going to enable dynamic type, which allows those with accessibility settings enabled to see text in a font size and style that is appropriate for their needs. Now it's great that we can customize this label in Interface Builder, but we can also see all these properties in Swift. So we can set the text and the formatting properties dynamically at runtime. Now back in Interface Builder, we'll go ahead and add two more labels. Now everything fits pretty well, but we have one more thing we need to do here. So looking back at Apple News, we can see that the text on the right is actually displayed with two different colors. Part of it's black and part of it's white. Now we could achieve this with two separate labels, but if we wanted to use just one label, we wouldn't be able to do this in Interface Builder. So how could we do this? Well, we can take advantage of the power and flexibility of attributed strings. Now an attributed string is a run of characters that can have attributes applied to ranges of characters. Now some attributes you get for free like the default font and text color, but we can override these attributes with our own values. In this case, we're going to set part of our string's text color to white. Now to see attributed string in action, we'll use the add attribute method on NSMutableAttributedString to set the text color to white just for the range that we want. And this time we'll set the attributed text property on our label. At runtime, this looks pretty spiffy. Now UILabels were a great choice for this sort of text. Now if we look at the bottom of the screen, we'll see a headline. Now this is also text, but it's a little bit bigger and it spans multiple lines. Another thing that makes this text different is that it's selectable. So which control should we use this time? Now both text field and text view support selection but text field is meant for usually just one line. So in this case, since our headline can span multiple lines, we're going to use a text view. Now when we put a text view onto our storyboard, we can see that we get a lot of lorem ipsum text by default. So we're going to go ahead and change the text in the inspector panel. We're also going to change the font to look a little bit more like Apple News. And we want to disable the editing feature because the headline isn't really editable. Now UITextView scroll by default because they are a subclass of UIScrollView. But if we want our text view to play well with auto layout, we want to disable scrolling. So this will allow the bounds of our text view to resize to fit the text. Last but not least, this white background really needs to go, so we're going to set it to transparent. Now Interface Builder made it really easy to customize this text view but just like our labels before, we can set all this in code. So here in Swift, we can set the text and the formatting properties dynamically at runtime. So we looked at Apple News to pick the right control, but now we're going to look at a different app that we're all familiar with to choose the right configuration and that's TextEdit. Now TextEdit is an app on macOS that handles display and editing of rich text content. Now what most people don't know is that TextEdit is actually a really thin wrapper around NSTextView. So I want to take a moment to marvel at just how much we get for free with TextKit. So this is the inspector bar, and we get this for free just by checking a checkbox in Interface Builder. And right below it is a ruler view which we also get for free just by enabling it. And everything below that is just a text view. Actually, it's a text view, text container, layout manager, and text storage. Now this is the standard configuration for both NSTextView and UITextView, but the similarities mostly stop there. So, for example, tables are only supported in NSTextView. And marveling again at the power that we get for free, TextKit provides a table editor that does all the heavy lifting for us. Now when we use TextEdit, we're often editing large amounts of text. Sometimes we paste in a lot of lorem ipsum to see that we also get a spell-checker for free. But really what we want to see is that when we use the format menu to choose wrap to page, we end up with it looking a little bit more like a page. We can see that the text container has been resized to match the dimensions of a piece of paper. Now if we scroll down, we can see that the text jumps from the first page to the second. Now the standard configuration doesn't really support layout like this. Sure enough, this layout uses two text views and text containers. Now they're still managed by the same layout manager and text storage, which allows the text to freely jump from one page to the next. Now if you'd like to see more about how TextEdit works, you can actually find its source in the guides and sample codes library. So we've picked the right controls, we've picked the right configuration, but sometimes we actually need to hammer on these to achieve what we want. But how do we decide which hammer to use? So we're going to try and pick the right hammer for the job when we go through the steps of building a journal app together. We'll start by putting today's date on to the window. Now we don't have UILabels in AppKit, but we can make a text field behave like a label. All we need to do is disable editing. Now for the journal entry part of the window, we're going to use a text view. So in the inspector, we can make sure that the text view is editable and selectable and supports rich text and undo. We're going to add a couple of text fields to the bottom of the window as well so that we can show how many words have been written. Now when we run our app, we want the word count at the bottom to change, so let's find the right hammer for this job. Now we can either conform to a delegate, handle a notification or subclass. But in this case, we're going to use a small hammer. And we're going to listen for a notification from text storage. Now we can get the number of words from the text storage. And when we hear the notification, we can update the string value property of our text field. And when we start typing, we can see the word count change. Now if we want to emphasize part of our text, we can use keyboard shortcuts or the menu to apply formatting like bold. But it would be great if we could support modern text formatting like Markdown, which uses control characters to specify formatting. So if we start inserting asterisks before and after, we want it to be bold. But which hammer should we use for this? Well, we want to know when a change happens, and we want to know where a change happens. But notifications don't really give us much information about this change. So we're going to use a bigger hammer and implement the text storage delegate, specifically the didProcessEditing method. Now we can make a new bold font from our existing one. And we can add that font directly to our text storage for the range that we want to be bold. And now when we insert that last asterisk, we can make it bold. Now we're feeling pretty good about this whole Markdown thing so what if we try inserting a code snippet? Now in Markdown it looks like this. And if we add this last back-tick, we want it to look like a code block. It should have a background and a header that says Swift Code. Now this is actually a complex task, so we're going to need two sledgehammers. And the first is a subclass NSTextStorage. Now when we subclass NSTextStorage, we need to implement four required methods. And we can do this by operating on a private instance of a mutable string. Now let's pay attention to the replaceCharacters method. Now we can add an NSTextBlock to our paragraph style. And then we can add that paragraph style to our text storage over the range of that code block. Now NSTextBlock by itself doesn't do any custom drawing by itself. So we'll need to subclass that too. Our NSTextBlock subclass needs to have some padding with some extra padding on the top and a light gray background. We'll override drawBackground and use string drawing to draw the header Swift Code. Now this is actually all we need to do to make a text block look like a code snippet. Now back in our custom text storage, we can create an instance of our new code block instead of using a plain text block. Now, last but not least, we need to tell our text view to use one of our custom text storages, so we'll replace the text storage on the layout manager. Now this is turning into a real WYSIWYG Markdown editor. Now a popular feature of most Markdown editor's is a side-by-side view with an editing version on the left and a rendering on the right. Now we can do this with two text views side-by-side. We'll disable editing for the one on the right. And now we have two text views but we want them to display the same content but look a little different on the right. So we want a configuration like this where we have one text storage but two of everything else. To do this, we will replace the text storage on the right with that from the left. Now let's see what this looks like. Now this is actually really cool. If we add any characters to the left, they'll show up immediately on the right-hand side. Now usually the right-hand side doesn't really show the Markdown characters but since this is a shared text storage, it means we have to hide the characters during the layout process. Now since we need to do it this way, we really only have one option and that's to implement the shouldGenerateGlyphs method on the NSLayoutManager delegate. This will allow us to intervene in the glyph generation process. So we can take the glyphs that are about to be laid out and if they represent a Markdown control character, we can apply the null property to that glyph. Now this will eliminate the glyph altogether during the layout process without changing the underlying text storage. Then, we will use the new glyphs and tell the layout manager that we want to present these glyphs with our new properties. Now this is actually really cool. So the left-hand side shows an editable version with all the Markdown characters included. And the right-hand side shows no Markdown characters all, all using the same text storage. Now building a side-by-side Markdown editor is not something all of us do every day, but it was really good to see how customizable TextKit is with real world examples. If you'd like to learn more about how to use and customize TextKit, check out our amazing programming guides. And with that, I will hand it back to Donna. Thanks, Emily. Those are some really cool examples. And I really hope you'll be able to take some of the techniques that she showed off and use them in your own apps. But now let's shift gears a bit and talk about some best practices for working with text. So on the topic of correctness, if your text doesn't render the way you expect, it could be related to incomplete or incorrect attributes on your attributed string. And so let's take a look at an example to see this in practice. Let's say we have a UITextView with some attributed text that says don't hate. And it says this in the font Comic Sans 24 point. And we want to programmatically apply a bold typeface to the word don't because if there's any font more universally hated than Comic Sans, it's Comic Sans bold. And so at first blush, it might seem reasonable to write code like this. Now here we have our original font. And we're going to use a font descriptor to create a bold version of this original font. Then, we're going to initialize our mutable attributed string using the original text. We're going to apply our new font or new bold font to the word don't and that's going to be the first five characters. And then we're going to set the attributed text property of our UITextView to use this new attributed string except when we do that we'll see that our new bold font applied to the word don't just as we expected but the rest of the string somehow lost the original font. And now those of you who despise Comic Sans might be happy about that, but the result is wrong and so that warrants a sad face. So why did this happen? And to answer that, let's take a closer look at how we're initializing our attributed string. So notice that we're using a plain text string to initialize it, and we're using the initializer with no attribute information. And when you create a new attributed string and you don't provide any attribute information, that new attributed string, we use the default attributes. And the default font is Helvetica 12 point. And so to recap what happened, we started with this original attributed string with the font Comic Sans 24 applied to the entire range. And then we created this new attributed string, and it got initialized with the default attributes. And we applied our bold font to the word don't on this new string, and we ended up with this incorrect result here where the word don't is in Comic Sans bold 24, and the rest of the string is in the default font of Helvetica 12. And so there are two different ways that we could do this correctly and one way is to avoid mixing the plain and attributed text altogether. So by initializing our new attributed string using the original one, we're going to keep those original attributes. And then we can apply our new attributes without getting this reset effect with the default ones. But it's not always feasible to just avoid mixing plain and attributed text. So if you've got to mix it up, you can explicitly supply the attributes when creating that new attributed string from the plain text string. And if we make sure to apply the same attributes from the original text, we'll get the correct result. But you should be aware that this reset effect can happen with any attributes that have default values and not just fonts. And as you can see, there are a lot of attributes with default values. So I'd like to call out the paragraph style here in particular as being a sneaky reset point. And to see why, we'll revisit our earlier example. But instead of changing the font, we're going to change the paragraph style to truncate the word hate because nobody likes hate. So we want our text to look like this, but when we run this code, we'll get a result like this with all of the text in Helvetica 12 and using the default paragraph style with the default line break mode of word wrapping. And, again, this is really great for those of you who loathe Comic Sans because it's been totally eliminated from the string but it's wrong. And it's wrong in a different way from last time. And to understand the difference, let's recall that attribute fixing happens before layout and this is where the system repairs the inconsistent attributes. And so here in our attributed string we have a single paragraph with multiple paragraph styles and that's pretty inconsistent. So when the system fixes the attributes of this string, it's going to take the first paragraph style it finds and apply it to the entire paragraph. And that's how we ended up with our attributed string displaying with the default paragraph style. And the key take away here is to be explicit with your attributes, especially when you're mixing plain and attributed text. So by doing this, you're going to avoid this reset effect with the default attributes. And for AppKit developers, this is actually super important if you're updating your app for dark mode. So by using the explicit attributes with the dynamic colors like NSColor.textColor, you'll ensure that your text is drawn with the correct colors for the context. So moving on. The next topic is performance. If you're working with large amounts of text, a good way to improve your apps performance is to use noncontinuous layout. And to understand what that means, let's revisit our old friend the layout process. We said that the layout process consists of glyph generation followed by glyph layout. And so with continuous layout, the layout manager is going to perform glyph generation and glyph layout starting at the beginning of the text storage. And it goes in order from the beginning to the end. And so if someone using your app scrolls to some point in the middle of your text view, the layout manager has to generate and layout the glyphs for all the glyphs that come before that point as indicated by the red rectangle. And note that this also includes the text that you can't see that's been scrolled off the top of the screen all the way back to the beginning of the text storage. And so if you have a lot of text, that poor person might have to wait a while for your app to finish layout but luckily, we can avoid this situation by using noncontinuous layout. And so as the name implies, with noncontinuous layout, the layout manager doesn't have to do glyph generation and layout in order from the beginning of the text storage. So now when that person, using your app, scrolls to the middle of your text view, the layout manager can perform glyph generation and layout for that middle section right away. So if your text storage has a lot of text in it, using noncontinuous layout is a huge performance win. Great. So how do you turn this on? Well, noncontinuous layout is a property of NSLayoutManager. And so for NSTextView, you can access the text to use layout manager and then you can set that property there. For UITextView, you usually don't have to do anything because this is turned on by default, but there's just one important thing to remember. Since UITextView is a subclass of UIScrollView, noncontinuous layout will require scrolling to be enabled. And this is because when you disable scrolling, asking for the intrinsic content size of your text view is going to require laying out all the text and then you wouldn't get the performance benefits of noncontinuous layout in the first place. And that brings me to a really important point. You should avoid requesting layout for all or most of the text at once when you're using noncontinuous layout since that kind of defeats the purpose of using it in the first place. So if you have only one text container, don't ask for the layout of the entire thing. And don't ask for layout for large ranges of characters or glyphs that include the end of the text. And we didn't dig too deeply into the topic of text performance here because I gave a great talk on this last year at WWDC 2017, Efficient unteractions with Frameworks. And you can access the video from that more information link at the end of the session. All right. Now it's time to talk about everyone's favorite topic, security. So you may have noticed that there have been incidents in the recent past where some people on the Internet have exploited bugs in our software to cause problems for people who use our products. And in response, we're continuing to devise techniques for mitigating these kinds of attacks, but today I'd like to talk about how we can work together to provide a stronger defense against these attacks. So you may have heard of the concept defense in depth. And in case you're not familiar with the terms, defense in depth refers to creating multiple layers of protection to defend against threats. And this concept has been around for centuries. You can see it in the design of medieval castles. The land around the outside is clear of trees so you can see attackers coming. And there's a moat to make approaching the castle more difficult and to prevent tunneling underneath it. And the walls are another defense. They're built tall so that they're difficult to climb. And there are arrow slits in the walls and crenellations at the top to allow defenders to fire on attackers from protected locations. Now any one of these individual protections might not be enough to fend off an attack but collectively they provide a strong defense. And like the castle, we here at Apple provide multiple layers of defense against attacks, but there's nothing stopping you from also taking your own defensive measures in your app or framework. And by doing this, you're adding another layer of protection and improving your product security. Everyone wins. So let's talk about what you can do here. And something I'd like you to consider is setting limits on text input in your app or framework. And now I'd like to emphasize that this might not always make sense to do. So, for example, if your app is an authoring tool like that journal app that Emily showed earlier, it wouldn't really make any sense to set a limit on the length of the text there. So if it doesn't make sense, you shouldn't do it. But in contrast, if your phone app has a text field for assigning a nickname to an account, you probably have some idea what a reasonable limit would be there. And it's a good idea to set these limits because all text input is potentially untrusted. When you allow text input, you allow copy and paste. You don't know what kind of text can be pasted in there. It could be anything. It could be a string with malicious character combinations, or it could just be a string that's really, really, really long. And even though long strings like that may not be malicious in themselves, it could cause your app to freeze or hang. So if you have a text field that's intended for one line of input and someone pastes the entire contents of "War and Peace" into it, which is about 3.1 million characters in English, is that reasonable? Probably not. So this is a great example of a case where it makes sense to impose your own limits. And here are the recommended approaches for setting these kinds of limits. You want to validate the input string before it's set on the text field. And so for UITextFields, you can do this by using UITextFieldDelegate. And for NSTextFields, you should use a custom NSFormatter to implement your validation logic. Oh, and we've also got some additional security enhancements coming your way. So keep an eye out for them in the release notes and come see us at the labs this week if you have any questions. All right. We're just about out of time so let's recap. You know how to choose the right control, customization point, and customization approach and you know the best practices to follow in the areas of correctness, performance, and security. So use this knowledge to go forth and create great things with TextKit. Oh, and before you go, here's that super important more information link where you can find all of the great past sessions and documentation that we've referenced today. And come visit us at the labs on Thursday and Friday. Thank you and enjoy the rest of the conference.  Good afternoon, everyone, and welcome to Integrating Apps and Content with AR Quick Look. My name is David Lui, and I'm so excited to talk to you about AR Quick Look. As you all saw yesterday at the Keynote, we brought this new capability of previewing 3D models in AR space throughout iOS. AR is becoming the tool to see what objects look like in the world, from a new sofa you're purchasing for your living room to that cute animated character you're reading about in your book. AR Quick Look was designed to enrich any application on the OS with AR content with a simpler way to adopt AR previewing for consistent viewing experience. So, in this session, we'll take a deeper look at what AR Quick Look is, how to design your app and web page to take advantage of AR Quick Look, and creating content specifically for it. So, let's get started. So, let me show you what AR Quick Look is with the files app. Let's preview the 3D silver pitcher model and push it into AR. We find the table, and we place the pitcher on the surface. You can see the pitcher on a 1-to-1 scale and get a better understanding of how it fits with your surrounding. On my table, I have an Apple mug, some Post-its, and a kettle, and it looks so realistic with these items. And what's really amazing is that you can see the colorful Post-its reflecting on the lower body of the pitcher, giving this illusion that it's actually really there. So, it's really magical how the virtual pitcher blends in with the physical world. To help get my lovely pitcher in the world, we're using AR Quick Look. The way to preview 3D content in AR. AR Quick Look is built in and deeply integrated into the OS to allow previewing from any application and websites. The deep integration comes from us using, of course, Quick Look technologies and expanding its capabilities to enhance the system experience with AR. The AR portion of Quick Look is available in iOS 12 on ARKit compatible devices and only object mode on non-ARKit supported devices. You'll see this new AR file icon throughout the OS to represent the new 3D file format usdz. Let's invite you to Quick Look, the 3D content in our viewer and see how it looks like in your world. So, let's tap on the icon to launch AR Quick Look. AR Quick Look handles setting up the AR experience like plane detection, object placement, gesture manipulations, and creating contact shadows. But these are only a subset of all the viewer capabilities that together with rendering a 3D model, provides an immersive experience. So, use AR Quick Look if your goal is to preview 3D objects and push them into the world. By adopting our new system viewer, you got a consistent 3D content previewing experience, so you don't need to create your own. It's really easy to adopt and integrate the viewer into a website and application and it could be as simple as embedding a usdz file if your application already supports and uses Quick Look. Just as you saw from the previous slide, AR Quick Look does all the heavy work for you in providing and creating a great AR experience. And it's truly magical how you get an amazing AR experience by just providing the 3D content and we make this super easy for you so you don't have to deal with any AR specifics. So, how about we see AR Quick Look in action with the web demo. So, here, I have the files up, and I have a catalogue of 3D objects. You'll notice that there's these rich and beautiful thumbnails with the AR badge stamp at the top right-hand corner, and that's there to tell you that there's an AR experience behind this. So, I have a lot of great models to choose from, but my personal favorite is this little purple guy at the top right-hand corner. So, Radar is our bug tracking software, and if you don't know, the purple aardvark is our mascot. So, I'll go ahead and Quick Look the purple aardvark into the viewer where we can see her in our studio-like environment, which we call object mode. So, you can really pan around and see the model from various different angles. We can pinch to size an object and make it look a lot larger and really see the fine details that went into this model. So, you can see how there's animation baked into this model, and AR Quick Look supports transform animation and skeletal animation, so the purple aardvark is really lively and animated with personality. And just like a photo, you can double tap on the model to recess the position and the size. But of course, the beauty really comes from pushing my purple aardvark into the world. So, I'll tap on the AR tab, and there, the purple aardvark is in the world. And so we were able to place the model so quickly into the world because we started the AR session the moment AR Quick Look was launched, and we gathered all the feature points of the environment and pushed the objects quickly out into the world. So, right here, you can notice that the controls are hidden for a full immersive experience. Controls like the status bar, on-screen controls and iPhone X, we hide the home indicator. So, you can go ahead and rotate the objects using your standard iOS rotation gestures. You can scale the object down using your two-finger pinch and scale it larger if you want it to be a lot cuter. And so, you can see that animation there where the purple aardvark is just fixing all my bugs and just slurping them up. So, I can go ahead and double-tap to reset the size back to 100%, and you can translate the model by just tapping on it and dragging it to a different place on the table. Or you can tap on it and move the device around for device movement, really seamless interactions. Of course, since this is an AR experience, we really encourage you to walk around and be really engaged and immerse and see that animation, see the model that's pushed out into the world. So, we've also provided a contact shadow. So, you'll notice right there just sitting on the edge, this tells you that the purple aardvark is grounded and placed on that table in the world. And what makes the aardvark look so amazing right now is that we're matching the color intensity and the temperature that's reported by ARKit from the world, and we're using that with our lighting setup. So, I think this is really amazing. I want to remember this moment. So, I'm going to go ahead and tap to show the controls. I'm going to take a snapshot of this. So, right here, I'm going to go ahead and mark up the moment, and this is really amazing, so I can save this later and take a look at this. But with AR Quick Look, we really wanted to make AR accessible to everyone, and we do mean everyone. Accessibility is one of our core values, and so we made AR Quick Look work with VoiceOver and switch control. So, let me show you the VoiceOver flow right now. So, I'll triple click the home button to activate VoiceOver. VoiceOver on. Files. Close. Button. Landscape. So, I navigate over to the AR tab. Home button to the right. AR button. Selected AR. Radar aardvark is now positioned in the world. And what's really cool is now you get audible feedback for when my Radar aardvark is off screen. Radar aardvark is now off screen. AR when is back on screen. Radar aardvark is now on screen. And this is really amazing, and we really encourage developers and users to really think about the full accessibility workflow in your applications and websites as you adopt and integrate AR Quick Look. So, I just showed you AR Quick Look integrated into the files app, but that's one of six first party apps to have adopted and integrate the viewer. The others are mail, messages, notes, news, and Safari. And the great news is with AR Quick Look built into Safari itself, developers can integrate it into their websites by using our notational [phonetic] markup that I'll talk more about later. Lastly, you can adopt a viewer into your iOS application to provide a more immersive experience with AR. So, sharing a 3D model between all these apps requires the models to be bundled up in a single sharable file. To enable this, AR Quick Look supports usdz, which is a new file format for mobile distribution of 3D models. usdz packages all these models and textures into a single file for efficient delivery of 3D content without having to work with reference files. It's based on Pixar's open-source universal scene description format, or short, USD. And usdz is supported on iOS and macOS, in SceneKit and Model I/O, and later in the session, we'll show you how to convert 3D models into the usdz format using the new usdz Converter tool that ships as part of Xcode 10. Now that you know more about AR Quick Look, let's talk about how to integrate the viewer itself. You can integrate AR Quick Look in two different mediums. The first, in an application. Apps that showcase a lot of photos and text can provide better imagery and a really immersive experience with AR Quick Look integrated. And here, we're using Quick Look API to facilitate this integration, to preview usdz files, and load them into the viewer. Second, is in websites in Safari. So, integrating AR Quick Look in news articles and product pages of online storefronts really complements the reading and shopping experience by providing a new way to visualize the content and develop a better understanding of what you're actually seeing, and we'll be writing HTML to allow previewing AR content on the web. So, let's get started with applications and how the Quick Look API is used to provide an AR experience. So, if you're not already familiar, let me quickly explain Quick Look. Quick Look is all about previewing documents like Keynotes, PDFs, images, and now 3D model files like usdz. Quick Look itself is an iOS framework that not only provides support to preview documents, but also will be really to create and provide custom previews for your custom file types. It comes with everything you need to preview documents. You can control the transition and the different presentation modes for Quick Look, like being embedded inline or incorporating a push navigation flow and the standard modal presentation. And what's really amazing is that security is handled for you where the contents of the usdz file is safely read to present a preview for you. Now, there's a dedicated endup session about Quick Look and previewing all file types, including the new usdz file type that I encourage you to check out. So, let's take a quick look at the preview flow. Here's the apps ViewController on the left. It has a grid of thumbnails for various 3D models. When someone taps on one of these thumbnails, I want to show a Quick Look preview of the model for that thumbnail. The first thing I want to do is create a QLPreviewController. I have instance, I have ViewController that allows you to preview documents. And I set myself as a dataSource and a delegate. So it can ask me questions about what should be previewed. With that configured, I present that QLPreviewController. Now, it doesn't appear immediately, however. It first asks me some questions. It starts by asking how many items do you have for me to preview? And for usdz files, the answer is one. Now, it asks me for a URL for the first item to be previewed, and I respond with the file URL for my app bundle for the model that was tapped. Finally, it asks me for the view to use as a source view for the transition animation for when the preview is presented. And here, I want a seamless animation from the view that was tapped to the view that was going to be presented. And so, I return View #3. With all that configured, we're ready to present our preview controller, and so it's going to animate and scale up from the view that was tapped. And this is what it looks like in code. In order to preview and present documents, we need to instantiate a QLPreviewController. It's part of the Quick Look framework. It has a dataSource and a delegate, which I'll get into in a bit. And then we then present the previewController modally. Pretty simple, right. So, let's take a look at the protocol for a dataSource. So, as I just mentioned, we need to conform to the QLPreviewControllerDataSource protocol. This protocol has two required functions; one to provide data to the Quick Look Preview Controller. The first being, the number of preview items in controller. And here, you return the number of items that your Preview Controller should preview. For AR Quick Look, it only allows previewing a single object in one dedicated session, and so we returned the one item. The second function we're implementing is PreviewController previewItemAtindex. And here, you returned the one item that we we're requesting to be previewed. This is the URL where to find a 3D object on disk. Now, this should look very familiar to a lot of you, and this is because we're using a very similar data source and delicate pattern from UIKit like Table Views. So, moving to the protocol for that delegate, there are a few ways to present the preview. AR Quick Look is intended to be presented full screen. To share the same consistent experience with AR Quick Look, I recommend you configure your Quick Look PreviewController's appearance to have the same zoom transition animation. So when the file's up, you can see the nice transition from the thumbnail view to the viewer and back down as it goes down for dismissal. In code, the easiest and recommended way to achieve this effect is to implement PreviewController, controller transitionViewfor item and return a UIView. So, Quick Look does all of the heavy work here for you and uses the UIView to infer the rectangular frame that the animation should start from when presenting and ending when dismissing. If possible, I suggest the final or UIView to have a square frame, to have the best seamless transition. AR Quick Look nicely handles the transition from this view to the full screen viewer on presentation and dismissal, creating this effect where the model just magically appears. And for an overall better experience, I recommend using a UI button as the view and setting the thumbnail as the button's image. That way, you get visual feedback whenever the view is tapped with the button highlight, and this is extremely important so that the user knows that some action is about to happen. And that is how to integrate AR Quick Look into your iOS application. It's never been easier to introduce AR preview and capabilities in your apps today. Now, let's talk about integrating AR content on the web. We really want AR content to be accessible online. So, starting in iOS 12, Safari has built-in support for previewing usdz files in AR. By hosting usdz files on your website, you got a way of delivering previews of 3D objects. So, not only can you read about the object and see it through the image, you can now experience what the model looks like in your world. So, here, I have a modern version of my online garden store with various thumbnails of gardening tools I have listed for sale. For better integration of AR on my web content, I'll use the new HTML markup that I'm about to show you shortly. This opens up and allows customers to preview my gardening tools before they actually purchase it. So, by following this markup, you had a badge that I mentioned earlier, rendered on the image in the top right-hand corner, and this is useful and there to tell you that our system viewer is behind this so you can preview my watering can in AR. So, you can provide your own cover artwork associated to the 3D object, and this can be a portrait shot or nice hero image like I've chosen for my lovely watering can. And with this flow, we support drag and drop and, of course, long-press Safari actions from Safari like Add to Reading List or Copy and Share. And the best part is you get a more immediate and optimized workflow into AR Quick Look. So, you're not following any links, and you get a response without updating your web URL. The HTML Markup is very straightforward and simple, and this is it. So, by adding this attribute, this will turn your webpage into an AR experience. The requirement is an a elements with rel=ar, and this tells WebKit that this is AR content. You then provide the link to the location of the usdz file. Lastly, we have a single child that is the image element, and this is the cover artwork that I mentioned earlier about that you provide as your thumbnail for the 3D content. The badge will appear above your thumbnail image inviting you to preview and it screens the object in AR. And this can also be done with the picture element. Similar to the image element, the rules here are single picture child of the a rel=ar tag. The picture itself can have all the children it needs, from being very simple to having very complicated roles about which child source image it picks to render based off of device, size, and other platform logic. usdz content must be served with the correct media type, and so make sure that the MIME type is set for these files. The registration of an official type is in progress. So, in the meantime, WebKit is affecting both of these media types if you're using Apache. And that's how to integrate AR Quick Look. The goal was to make it really easy to put AR everywhere, on not just websites in Safari but also in apps in iOS. Now, you might be wondering where can I get usdz content. I'll invite my colleague Dave who will talk more about how to create content for AR Quick Look. Thanks David. Well, let's start by taking a look at what it is that we want to achieve with our 3D models. Here, I have a model of a teapot that I've loaded in AR Quick Look, and we can see that it sits perfectly on the ground, as indicated by its shadow. It spins around its natural pivot point, the center of the bowl, and if I place it in the world, we can really believe that it's there on the surface in front of me. We even see the environment reflected in its shiny metal handle. So, how do we achieve this effect with our 3D models? Well, when we're creating our models, there are six things we need to get right, and we'll see how to do each of these in the remainder of the session. We need to set the model's placement, we need to set its physical size to be correct in AR, we need to create any animation we want for our model, add its contact shadow, modify its appearance, and then add any transparency that our model needs. Once these six things are done, we can optimize and export our models for use in AR Quick Look. We'll start with placement, and there are three things we need to get right in our 3D software to make our model appear correctly in the world. The first is that the object should face toward the camera, toward positive z. Secondly, the base of the object should sit on the ground plane, the plane where y = 0. And finally, we should put its natural pivot point at the origin. So, let's see how that looks for a sample model. Here, I'm in my 3D modeling software. I have the origin at the center. I have positive x heading off to the side, positive y heading up from the origin, and positive z toward the camera. Now, if I import a model, if I import my teapot without having set it up, we can immediately see there's a problem here. The model is below the ground plane. If we look at it from the side, we can see this model has been created around the origin. So, let's move it up such that its base is set on our plane where y = 0. And now, it's ready to position in the world on the ground. If we look at it from the front, however, there are still some esthetic improvements we can make here. This kind of looks like a teapot, but it could look more like a teapot. This is what we think of when we think of a teapot, with a spout to the side. When you're thinking how to position your objects for this optimal recognition, think how you would place that object if you put it on a shelf or in a display cabinet. If in doubt, ask a young child to draw the object for you. Chances are the profile they choose is the one you should use. Now, this profile really matters. Not only is it what people will see when your object first loads in our Quick Look, but it's also the profile that's used for the object's thumbnail, and these two deliberately match when the thumbnail is in files or in messages. So, we get that seamless transition between the two. So, choosing a good profile is really important. We'll head back into our 3D software and rotate our model so the spout is to the side. As one final check before we export it, let's take a top-down view and make sure that that bowl is centered on the origin, so it will spin around the point we want it to. The second thing we want to set up for our model is its physical size. Here, I have a gramophone record player, and size matters here. This record player is designed to play 12-inch records, so we need to make sure it's the right size in the world. If I export this model and load it in object mode in AR Quick Look, it looks great because we always scale models to fit in this mode. But if I place it in the world, we can see there's a problem when I put it next to a 12-inch record. It's far too small. Right now, it's set up to play 6-inch records, and they don't exist. So, let's head back into my 3D software and create a reference, 12-inch cylinder. Now, I know this is exactly 12 inches in diameter because I know the units that this project is working in. I can use this reference cylinder to resize my turntable and make sure that the platter is exactly the right size to play a 12-inch record. Having done so, if I re-export my model, it looks exactly the same in object mode because it's still scaled to fit. But when I place it in the world, we can now see it's the right size to play this 12-inch record. We could imagine picking it up and placing it on that turntable. Now, not all objects have a natural size. This little guy is Chatterbox, and he doesn't exist in the real world. He only exists in our imaginations. But he's pretty cute, and he has a fun animation, and he's good to take a photo with. So, we've created him at desktop size, which makes him easy to place in the world in common environments, then people can resize him, shrink him down. They can move him around or scale him up and make him the size they want him to be for a photo. But by making him desktop size by default, he's always easy to place. We saw here that Chatterbox has animation, and this is something we recommend you use where it helps to bring that object to life with an idle animation that makes it feel more like it's in the world. Note that animations always loop, and the animations you could create can use skeletal and/or transform animation to bring them to life. Here's another example. This is a cartoon style chicken character without animation, and if I place him in the world, he looks pretty cool, but he's kind of static. He doesn't really feel like he's alive there in the room with me. So, there's more we can do here. So, if we add a little bit of skeletal animation to our character to make him bob his head, now when we place him in the world, he feels much more realistic, much more like he's there on the table in front of me. Some tips as you add animation to your models. Firstly, choose animations that enhance the AR immersiveness, that make you feel more like that object is there in the room. Now, because people can manipulate your objects, they can retape them, scale them, and move them around, it's important not to move them away from the origin. If I create a car that zooms around in a meter-wide circle, when someone tries to put their finger on that car to pick it up, it's already moved away and zooms somewhere else. So, that gesture is very confusing to somebody trying to manipulate that object. So, try and keep those objects centered at the origin. For the same reasons, try and keep a consistent bounding box throughout the animation as well. Even if the object is at one location, it's much easier for me to manipulate it if it stays in the same place under my fingers throughout the entire gesture. As a result, prefer animations that make sense at a static location. If you have a T-rex character, rather than giving it a walk cycle that would make it look like it was running on the spot, make it roar or stamp its feet or something else that makes sense at a static location. Now, there is an alternative. You can also consider creating animations as self-contained scenes to help with these gesture manipulations. And let's see an example of how that can work. Here, I have a swimming koi fish, and it makes sense for this fish to have some translation. So, we created a scene with an aquarium base, and this means when we place this in the world, if we rotate it, or if we move it, the scene we're manipulating is the scene, not the fish, and so the gesture makes a lot more sense. It also means by having a base to our scene that we have a consistent bounding box throughout the entire animation. Another thing that really brings your models to life in the world is that contact shadow. Now, note here that AR Quick Look provides the contact shadow for you. This means that it can turn the shadow on and off as you transition between modes. It also means it can apply ambient lighting conditions to the shadow as the lighting changes around you. Because of this, don't bake a contact shadow into the models you provide. If you do, you'll end up with two shadows. Note also that the first frame of any animation is used to create the shadow for a model, and sometimes this means it's important to choose the right first frame. We'll see an example. Here, I have a model of a Newton's cradle, a desktop type, and it has a swinging animation, a transform animation to bring it to life. Now, if we freeze this with its first frame, here the first frame has one of the ball bearings out to the side, we can see there's a problem. The shadow we've generated from this first frame also has that ball bearing out to the side. And so, if we animate to the other extreme of this animation, at this point, the shadow doesn't really make sense. Now, because animations loop, we could choose a different starting point, a more neutral one where the ball's in the center and still have the same animation, the same overall effect. But now, by choosing a better first frame, our shadow makes sense throughout the entire animation. One of the biggest things we can modify about our model to make it feel real in the world is the model's appearance. Now, here, AR Quick Look uses a Physically Based Rendering, or PBR shader, and this gives us six things that we can modify about our model's appearance to make it feel more real in the world. We do this by providing textures, images, for each of these six things if we want to. The first is the model's albedo. This is the base color of the model, its underlying color. The second is indicating which parts of the model are metallic. This changes whether they are a conductor or an insulator, which changes how they interact with the physics of light in the real world. We could also specify which parts of our model are rough or shiny by providing a roughness texture. We can add in a normal map. This creates the illusion of depth and variances within the surfaces, the model's surface, without changing the underlying mesh. We can add an ambient occlusion texture to specify where the model casts shadows on itself in the crevices and nooks and crannies of the model. And if our model emits lights, we can provide an emissive texture for things like a TV screen or a computer monitor. Now, if you're just getting started with PBR, we have a session on Thursday about creating PBR-based models for any AR experience, and I highly recommend you check it out. For now, we'll take a look at how these six textures affect AR Quick Look particularly. So, I'll start with a model of a TV. Right now, it has no textures. It's pretty bland. We'll bring in an albedo texture to give it some base color, and it's much more recognizable as a TV now, but it's still very flat. It doesn't feel realistic. The next thing we'll do is to introduce a metallic texture to make the trim around the TV screen and the antenna on top to indicate that they're made from metal. Note here that our metallic texture is pretty much black and white. Things either are a metal, or they aren't. But the metal still doesn't look as realistic as we want it to do, and that's because the metal, like all of the model, is entirely rough. There's no shine, there's no smooth surfaces on this TV right now. To fix that, we'll bring in a roughness texture, and at this point the model really comes to life. The metal around the TV screen and the aerial now looks like metal. It looks like chrome, and the wood on the top of our TV actually looks like it has a varnish or lacquer effect. We can believe it much more realistically as a TV. And roughness is one of the biggest points of creative control you have over the look of your model and how it feels in the real world. And you'll see, we have quite a lot of variance in our roughness texture, even within the same parts of the model, to give more realism and to bring it more to life. Next, we'll add a normal map to add the illusion of variance, particularly in the woodgrain on top of the TV here. The underlying mesh hasn't changed, but it feels more 3D. Also, on the speaker grill. Because we have some crevices and some hidden areas on the front of the TV that would cast shadows on themselves, we'll add in an ambient occlusion texture. The effect is pretty subtle, but it does add to the depth of the model and make it feel more real when it's in the world. And because this is a TV with a screen, we'll add an emissive texture as well to indicate that the screen emits light into the world. The effect of this is best seen when pushing the object into a room. Here, we can see that the main body of the TV reflects the ambient lighting conditions of the room it's in, but the screen is brighter. It's actually emitting light. Emissive texture is a fairly special case but not something you'd use every day. But if the object you're modeling would glow if you turned the lights off, it might be a good candidate for an emissive texture. We can also make parts of our models transparent. This is useful for making objects made out of partly glass, for example. If you do this, be sure to use a separate material for the transparent and non-transparent parts of your model to ensure they render correctly. You do this by providing an albedo texture that has the transparency in its alpha channel. So, a transparent PNG file, for example. Note that transparency is really intended for see-through parts of the model, not for creating cutouts like a leaf edge or a butterfly wing. We'll see this in action for our TV model from before. In addition to the mesh we already have for the base of the model, I'm going to add a second mesh for a curved screen, a screen cover that's made of glass, with a slight amount of dirt on the screen. Here's how it looks in AR Quick Look. We can see that the light reflects on the screen and shines off the curved surface, and if we turn it to the side, we can see the color of the screen there as well where the glass as some grime to it. We achieve this by using this kind of texture for our screen. It's pretty simple. It's just adding that small amount of dirt. But crucially, it has the transparency in its alpha channel to indicate that the screen is see-through, and we can see the main screen behind it. When you are setting up the textures for these individual properties. These are the formats to use. Albedo should RGB, or RBGA, if you're also providing transparency. Normal and emissive should also be RGB. And metallic roughness and ambient occlusion should be Greyscale. You can use any image format supported on iOS, and note that these textures should be square. They should be powers of 2 squared. So, 2k, 1k, 512 pixels, so on. David mentioned earlier that AR Quick Look is available on all devices that support iOS 12, and this is actually quite a range of devices with different capabilities. Now, because AR Quick Look is a system-wide extension, this means it has to share the available system memory with the applications that launch it. And so, there's only so much memory available to display and load these models. To help with this, we recommend that you create and optimize and test your models for high memory devices like the iPhone 7 Plus, 8 Plus, and X, and the iPad Pro 12.9 inch. If you do this, AR Quick Look will dynamically downsample the textures in your model for other devices of different capabilities, if needed. This means you can make the devices, your models look great on those devices but still know they'll work everywhere. Know that the meshes and the animations won't be modified. It's only the textures that will be downsampled. You're probably thinking at this point, okay, so how big can my model be? How big and complex can my textures be? And the answer is there is no one answer. Many factors affect a model's memory usage, its mesh, and its animation complexity, and its textures size and count. But as a guide, for a model that has a single PBR texture, using those six textures we saw earlier on, if you aim for about 100k polygons, one set of 2k by 2k PBR textures and about ten seconds of animation, this is typically good for those high-memory devices we mentioned earlier on. If in doubt, always test your model on a device to make sure that it renders and loads as you would expect. As you optimize your models for use in AR Quick Look, a few things to keep in mind. Do be sure to freeze any transforms in your models and merge any adjacent vertices to tidy them up ready for export. And if you can, try and use a single material and texture set for the entire model. This enables you to optimize how you pack the different parts of that model into those square textures that you're using. And if you don't need a texture, don't include it. If none of the parts of your model are made from metal, there's no need to provide a metallic texture. The default is non-metallic for any model that's loaded. When you're choosing where to spend your texture budget, prioritize the areas that give most realism to that model. For some models, that might be a high-resolution albedo texture. For others, it might be better spent on roughness or the normal map. And remember the pixels actually have a physical size in AR. If you're printing a model of a thimble, and it's one inch across, there's probably no need to use a 2k by 2k texture for it. Those pixels will be tiny in the real world. And although we want models to look great, we also need to balance texture size and quality against file size. If a model is being downloaded from a website, we want it to be a good size for downloading and also for sharing via messages and mail and other means. So, with all of that in mind, how do we make these things? Well for this, we have a usdz Converter. This is a command line tool that will convert existing 3D models to usdz format. It ships inside Xcode 10, and in addition to creating usdz files, you could also use it to map PBR textures to the meshes and submeshes inside those existing models. It's X3 input format, OBJ files, Alembic files, and existing USD files, either USDA or USDC, the ASCII and binary versions of USD. Before we create one, let's take a look at what it is we're going to make. Let's look inside a usdz file. In essence, these are uncompressed zip archives. The first file is always a usdc file. This contains the model's mesh, its animation, if it has some, and any material definitions it needs. And then, the remainder of the files in the archive are any textures, any images, like the ones we saw earlier on. If you're thinking of creating your own tooling to build usdz files, the great news is that this is that this is an open format and Pixar have published the direct specification for usdz on the graphics.pixar.com site. To create one with a usdz converter, first note that we call the tool with xcrun because it shifts inside Xcode. We pass in the name of the model we want to convert, an OBJ file in this case, and the file name of the usdz we want to have the other end. To map PBR textures to the meshes they're in, we use the -g option, followed by the name of one of the groups, a mesh or a submesh that we want to map them to, and then we can provide any number of these textures for that particular group. To map multiple textures to multiple groups, we can use the -g option multiple times, as many as we need, to map all the textures in our model. And if you're not sure what the groups in your model are called, you can pass in the -v option for verbose output. This will print the names of the groups that we find in the model plus useful other information about the conversion process. In addition to making your own models, we've also put together a gallery of example models that show good practices for PBR. If you visit this gallery in an iOS 12 device, you can tap any of these models, open them in AR Quick Look, and place them in the world. This gallery also showcases the Safari integration that David showed you earlier on. So, if you have iOS 12 on a device, visit develop@apple.com/arkit/gallery and give them a try. So, in summary, AR Quick Look is a system-wide way to view AR content in the real world. It can be integrated into your own apps and websites. It uses the usdz file format for 3D model distribution and sharing. It supports PBR, animation, and transparency, and you can use the usdz converter tool that ships in Xcode 10 to convert your existing models to usdz. For more information on the subjects in this talk, check out the session's website. And we also have ARKit labs throughout the week. So, please do bring any questions you have about AR Quick Look, and we'd be really happy to help you answer them. Thank you.  So hello everyone. My name is Doug LeMoine. My background is in interaction design. And here at Apple I'm on a team of people who works with developers like you. As a designer I focus on user interfaces and the ideas to deliver great user experiences to the people who use your apps. Today I'm here to talk about intent. I think there's a thread running through great apps and really great products. And it has to do with a conscious focus on people, on humans, and what they want and what they really need. So I'm going to talk about focusing on the person you want to serve. And I'm going to focus on being intentional. The intent that I'm interested in talking about today is less about a specific design vision or a specific outcome and more on a mindset. How to keep a sharp focus on those people that you're trying to reach and create apps that feel simple. And when an app feels simple users perceive it as natural, an extension of what they know. It feels like it arises from their experience and it flows naturally from what they expect. So I know simple and natural are sort of designer cliches, we say this stuff all the time. Like, what do we mean? I think these terms are shorthand for discussing qualities of apps that create feelings of comfort and confidence. When people feel comfortable and confident, when we understand how a thing works, we can move through it seamlessly without needed to learn or figure it out. So intent is the kind of thing is hard to define but you know it when you see it. So I'm going to show you some examples of apps that I found in the last year that really deliver real value to people and really connect with them. And I'm going to talk about five elements of intentional design that we can learn from these apps and that you can apply to your work. So let's dive in. So before mobile devices came around when you traveled abroad you'd carry a paper map. And the advantage of the paper map is that when you would approach a local they would instantly recognize what kind of help you needed. So mobile maps have made people more independent, right? We probably approach fewer locals when we're traveling abroad. But in the same way that a paper map invites an interaction, there's an app called iTranslate Converse that makes it easy to approach a local with your iPhone in your hand to ask a question or get directions. Some of you may have seen this on Monday night at the Apple Design Awards. I'm going to go through a demo to really bring out what I think is really intentional about this app. So first of all there's very little UI. The screen is essentially one big button. I tap and hold and start talking. So I'll show you that in a second. And to be clear the goal of iTranslate Converse is to make conversation easy. The developers have intended this app to be used together. So two people together, one of whom most likely has never used this app before, probably never even seen it. So I launched this app really for real in in the Naples train station when I was trying to get on an express train to Rome and the platforms were really confusing. So I pulled out my phone, I launched the app, and I walked up to a conductor. So I said is this the express train to Rome? [Computerized Speaking Italian] The app translated it into Italian and the conductor heard. Then I could flip the phone around and again, what I find interesting in this situation that other person has probably never seen this app before. And they speak in a language that I didn't speak but the app is smart enough to know that it's ether going to English or Italian to listen for. So the conductor spoke into the microphone. Platform Nine. And the app translates it for me. So in reality he spoke English. But he did want to see how the app worked and I showed him and he was impressed. I think the real beauty of this is that you can make a connection with a person, right? You can get on the right train like I did, but you also make a human connection in the process. So the developer's intent here is to enable communication between two people in real time. Just touch the screen and talk. So the first element of intentional design is what I'm going to call radical simplification. So how did they do this? Well, I think they considered the context really well, the context that this will be used in. They took into account where and when people would pull out their phone and launch their app. You're probably in a foreign country, like this guy you're jetlagged, lost, tired, your hair is uncombed, your jacket is rumpled. I'm not even sure if that's a map that he's carrying at this moment, it kind of looks like a book. So they remove the UI and allow people to focus on what they need, which is to find something, to ask a question, to get their bearings. And it's amazing how much an experience can change when actions are removed from the UI. So I want to show you another example. This is an app called Vanido. It helps you learn how to sing. So if you're the kind of person who wants to learn how to sing with literally no one hearing your or judging you, this app is amazing. This is the UI. It's so simple. The app tracks the pitch of my voice while I'm singing and it's continually giving me feedback. Am I hitting my notes or not? So let me show you. I'm actually not going to do a live demo. But I am going to sing along [harmonica note] because I've been practicing and I want you to see how it works. [ Cheering ] You can leave feedback in my session. So what I love about Vanido is that it has stripped away what music education looks like, right? There's no charts or notes. It's colorful, it's animated, and it's fun. And you should have heard me sing before I started using this app. It really works. And there's only one interactive element on the screen when you're singing and it's way up at the top left. So you can hold your phone like a microphone and really belt it out without worrying about accidentally exiting your lesson. So this is radical simplification and that's what I mean, that's the first manifestation of intent and intentional design. Being radical means taking risks, right? Removing a bunch of stuff from the UI could be jarring if you don't do it right. But I think there's a way to reduce your risk. So next I want to talk about people and about orienting your design around what people really want. So what do they want? I think this is worth exploring because it's almost never obvious. The first needs that you see when you start kind of looking at people and the users that you might want to serve are superficial and you need to dig past those. They might be the first ones that you think about addressing or they might be the ones that pulled you into the problem in the first place, but you need to keep digging because those superficial needs often mask deeper needs, right, you could solve them but not really solve the persons' problem. So you have to keep digging. The second element of intentional design that I want to get into here is called deep understanding. So knowing the person that you want to reach and the actually problem that they have. The deeper you dig the more likely you are to see where the real opportunity to meet their needs is. And the app that really shows this off, in my opinion, is called Streaks Workout. So it's a workout app but I want to go behind the scenes a little bit. On the surface it looks like many workout apps. It guides your workouts. This is the primary screen and it looks pretty simple. You pick a workout duration and you go. But personally, I'll confess that I don't love working out and I've tried many workout apps that I just don't stick with and I want to thank any developers that are here who have developed workout apps that I've tried because I respect your effort and I'm really trying to get fit so thank you for that. But what I struggle with is staying motivated. It's easy to get bored or burnt out and this isn't an unknown problem, right? Many people feel this sense of inertia when facing something challenging like a workout. So inertia, just to be clear, is a disinclination to move. And that's what I feel. I feel a sense of disinclination when I think about working out. So I want to walk you through how Streaks Workout helps users handle this. So when you first launch the app you select movements that you want to do, these exercise movements on a screen like this. And there's dozens of these things. You can scroll down and pick as many as you want. After that when you want to start a workout there's just one tap. You just pick a duration and you go. And the app does the rest. It auto selects from your movements and it picks a sequence and then you're in the workout. [ Heartbeats ] You guys psyched up? So let's fast forward a bit. Now I'm doing ten seconds of crunches. Crunches ten seconds. I'm actually not going to do the crunches. But as you can see I can't see what's next, right? The question mark means it's random. It's a surprise. Burpees ten seconds. Yeah, I'm not going to do those either. Specifically, what I think works well is that the app confronts the inertia that people might feel by introducing a sense of randomness and removing work. So the developers recognize that each step in the process of doing a workout requires a choice, requires someone to think about what they want and these decisions are not things that everyone wants to make. So you only choose movements once. After that the app does it for you. And you never choose the sequence or the reps so half of the work, half of the potential inertia is gone. You could say that Streaks workout -- it takes the work out -- Of workouts! Really bad. They've taken half of the work out anyway. So this launch sequence reveals the movements that you'll do and it helps each workout feel new. Of course there are probably many of you in the audience that perceive that this app actually lacks essential features. Choosing reps is amazing. Picking a sequence isn't work. If this is you I have great news. There are lots of apps that allow you to do those things. But I'm curious about what inspired this. Whenever I see something unique like this I wonder what's behind it because it's unique and it's implemented really well. So I asked the developers of Streaks Workout. And it's such a common story to hear about how an app originated with an idea that came out of prison. So specifically, their idea originated with how inmates in prison keep workouts new and interesting. So I want to walk you through how that works. So in prison you take a normal deck of cards and at the beginning of each workout you would assign a movement to each suit. So today diamonds are crunches. The numbers on the cards represent the number of reps per movement. So at the beginning of the workout you'd put the deck face down and you'd start drawing and each card would deliver a movement and a rep count. OK, everyone eight crunches. So the original app concept was a literal recreation of this. This is an early comp and they also prototyped shuffling and flipping cards to try to kind of replicate the origin of this prison workout. But they quickly moved past the literal approach. They didn't stick to the literal deck of cards because it carried too many constraints. Four suits would limit the number of movements that you could do in each workout and people would somehow need to remember that diamonds represented crunches in that particular workout and that's work, right? So instead they focused on the value that they wanted to bring. They wanted to help people make a habit of working out. So they acknowledged that people felt inertia when they were going to launch a workout app and they removed as much as possible that might trigger that and they stayed true to the original intent of the prison workout, which is that spirit of randomness and surprise. And that's what I mean by deep understanding. Knowing the person that you want to reach and the actual problem that they might have. So the superficial approach to this workout would have been just a typical workout app, right? Of course people want to get healthy. But what people really want, or what I really want anyway is to get past the boredom and that inertia and they've addressed that for me. So knowing users is nothing new. I'm guessing a lot of you here have personas and market segments and you do surveys and other kinds of user research focus groups. Maybe you create experience maps and customer journeys and user stories. All of those can be useful models. But they have to feel authentic, right? They actually have to feel like people. That can make focus easier and it can help teams kind of come together around a vision. As long as you share a common understanding and you feel a real empathy for the person that you're designing for. Those can work. But too often those artifacts of needs and goals and skills, they just feel artificial. They may not connect with everyone. They feel complicated or distant or corny. So for a moment I want to talk about how much easier it is to make some something when you really care about the person you're making it for. So I first heard about this story that I'm going to tell you in a book. It's by a guy named Alan Cooper, he's a software developer from the early days, and his claim to fame is that he created a prototype that became Visual Basic. And this is an actual pixel resolution of Visual Basic 1.0 on a 4K display. It's pretty amazing. So Alan built this with a small team and as he did it he began to recognize a problem in maintaining a focus on what people really want. And he tells this great story in a book about another invention and that story is a parable of how powerful knowing people can be, knowing the person that you're designing for. So apologies if I got your hopes up about Visual Basic. I'm actually not going to talk about that. I want to focus on this story and kind of extend it a little bit. So the invention that he talks about is the Rollaboard Suitcase. So this is the patent form for the Rollaboard. So nowadays the suitcase might seem pretty unremarkable. For most of us this is just a suitcase, big deal. But it actually hasn't been around for that long. The patent was filed in 1989. People have been traveling forever though. So why did it take so long to design something so obvious? The inventor was a guy named Robert Plath. I'm guessing not a household name, but more on that in a minute. So what was innovative about this? Well, I think the innovations are subtle. It wasn't the wheels. Many patents had been filed for wheeled suitcases before. This is from the '70s. So the wheels are underneath and it's also got some kind of leash thing. Suitcase manufactures had been trying to put wheels on suitcases as far back as the '40s and this actually has some kind of other leash thing, some grabber thing that you can see right there. So clearly people have been trying to figure out a way to not carry heavy things. So if you worked in the luggage industry in 1989, you probably thought you'd already innovated. You weren't worried about this new patent I'm guessing. But we all know what happened, right? The new design became the de facto design of suitcases. And the improvements in retrospect are totally obvious. There's a handle at the top so the whole suitcase it oriented to be upright. It's easy to grab and pull and pivot. That is the evolution of the leash. Second the wheels, right? The wheels are placed on the edge so they're only fully engaged when the suitcase is tilted. And finally the size, it's small enough to lift and place in an overhead bin. So nothing particularly revolutionary but the inventor Robert Plath made each decision to serve a really specific person, a person who traveled for a living. Robert Plath was a pilot for Northwest Airlines. He intimately knew the pain of lugging and lifting heavy things. He did it every day. And he deeply understood what people like him wanted, to move with ease while looking professional, and he made something perfectly suited for that. So what I find really interesting is that at the times, 1989, there were only about 100,000 travel professionals as the Census Data called them, but pilots, flight attendants, people in the travel industry. That 100,000 might seem like a lot, but when you compare that number to the overall market of people who traveled a couple times a year, just like more than 100 million at the time, and that's just Census Data from the U.S., his target user, quote-unquote, was a really specific person that represented only 0.1% of 1% of the market. So what I'm saying is that typical methods of market identification probably would not have led Robert Plath to focus so narrowly. He focused on an extreme use case, a person who traveled all the time. So I'm using extreme intentionally to emphasize that he focused on what we might call an edge case. The market size was really small and his solution was kind of whacky. It looked really different. It implied a new kind of default way of interacting with your luggage. So he focused on the needs of people he intimately knew and that extreme focus allowed the real needs of a lot of people to come to the surface. So his focus gave him the freedom to arrive at an extreme solution. So the third element of intent is extreme focus. So what might seem like an edge case could be what frees you to reorient or refocus your design. Make it more coherent. Make it simpler. And therefore actually reach more people. So let's relate extreme to our work. Is there a way to relate extreme to the work that we do? I think there is and I think being extreme can be really worthwhile but you have to nail it. So you might not immediately associate the words extreme and weather app. But Carrot Weather is a really compelling illustration of being extreme. So on the surface it's a weather app. So let's check the weather forecast. I wonder how many licks it takes to get to the center of a human skull. So right, they're not typical forecasts. The voice of Carrot, which is the robotic voice that you heard, has personality and attitude and a little bit of an edge. You probably think this sunshine means that things are starting to look up for you. They're not. So you might ask is that even a forecast [laughing]? So sometimes when I read these things I just feel like it's being mean to me. It's definitely got an attitude but the key information is super readable and the screen is really nicely organized. The color scheme changes according to the time of day and it's really playful. And it's got those illustrations along the bottom that are sometimes bizarre and often really funny. So the forecast is only one of quite a few unique elements in the app. So I ask the developer whose name is Brian Mueller, to help me understand where this came from and he told me something interesting. He said that when we writes in the voice of Carrot, he's channeling the personalities of people really close to him -- his wife, his sister, and his mom. And these forecasts are a form of conversation with them. So the lesson is that the people that Brian is trying to reach, the people that he's speaking to with these forecasts are actually really specific. And I think that's what makes his apps stand out. The edginess helps him connect with people. And it's not surprising to me that he's had so much success in the app store. The personal touch is what allows his app to deeply connect with the rest of us. Obviously personality isn't the sole province of weather apps. All apps can deliver something like this. All apps can feel handcrafted and special. And that's the fourth element of intent that I want to talk about. Intentionally designed apps feel personal. They create a personal connection. So the point isn't to spice up your copy, right? The point is that spicy copy in the right context feels memorable. And it must be said that the personality of Carrot is probably not for everyone. And I talked with Brian he commented on this. So you can probably imagine the email and Tweets that he gets from people who are surprised or upset or offended by the forecast after they download the app. Like, you know, how dare he insult meteorologists like this? So he offers an olive branch -- To these disgruntled souls. They can adjust the personality from the default, which as you saw was Homicidal. You could go the left for less edge and to the right for more. But the bottom line just to be clear is that edgy text would be less funny or perhaps not funny at all if the app didn't work, right? It's a good weather app with simple navigation and a playful UI. The extreme personality though is what gives life and sprit to it. So when I get a Carrot notification it's similar to the feeling that I have when I go out to a local restaurant or bar where everyone knows my name, right? It's enjoyable, it's personal, and it's comfortable. So we've got four things that characterize intent and the behavior of intentionally designed apps. And I want to flip things around for a moment and talk about what happens when you're not intentional. In other words, where does bad UI come from? So I think the best friends and the worst enemies to designing intentionally are all of the systems and patterns that we've internalized as we've gained experience in the work that we do. This experience allows us to make decisions quickly and to be really efficient in that work. But these familiar patterns get in the way of our ability to reckon with new problems. Sometimes we won't even recognize that the problems are new and this is true of anyone who has tried to make anything new ever, right? Suitcases once had a form. They were rectangular and the long side was down. That form was adjusted but the fundamental elements were always taken as a given. The wheels were tacked on but the overall form stayed the same. For decades smart people considered this form and stopped there. It took someone with a very sharp focus on an extreme audience to break out of this pattern, right, and turn the form on it's head [laughing]. Thank you. So we apply patterns automatically. Our brains make connections quickly and that has benefits, right? It helps us work efficiently. It helps us be consistent, but it also prevents us from noticing the obvious and challenging it, asking fundamental questions, why does this thing need to be this way? So as a UI designer it's really tempting always to draw on familiar UI elements to solve new problems. I'm sure you're familiar with the house icon. It made sense in a desktop browser. A browser is a frame for the Internet universe. A browser home button will lift you out of whatever corner of the dark web that you found yourself in and allows you to get out of there. So that notion of home was extended to iPhone and the behavior is similar. It's a shortcut, it's an easy button, home is a starting place, and unlike in real life, home is a place that you can always return to with a simple gesture. You can go home whenever you want. So home is also a familiar notion in the finder. Here the spirit is more like -- you followed the river to it's headwaters. On the Mac, home represents a container. Everything is in here. You don't need to look anywhere else. you have reached the source. Home in these contexts offers reliability, safety, an ability to return as you navigate outward. And it's clear obvious utility elsewhere on the platform might seem like a reasonable choice as a tab bar icon, right? Except there's a crucial difference in using the notion of home in a tab bar. To be blunt, an app isn't a frame containing god knows what. A typical app offers specific things, specific contents, specific tools, specific actions. And a tab bar is a quick expression of the overall utility of your app. It's a high-level map of your app. So let's use an example that's close to home -- the tab bar of WWDC app. So we all can get a quick understanding of the scope of information that's available in the app simply by scanning the tab bar. We can guess at what content might be in each tab. So tab bar icons and tab bar labels should be as direct as possible. And this directness will then limit what each tab can contain and that's great. Directness creates predictability. So the app may not do everything, right, no app does everything. But people won't have to hunt around to figure out what it does and doesn't do. So when I talk about home I'm talking about a metaphor, homeland, a home base. So let's just dig in to why home doesn't work. First it's overly broad and it feels generic. And I think it's a copout. You're avoiding a decision by naming a tab Home. The perceived comforts of home as an icon in a label are getting in the way of being direct, of clearly and directly communicating the scope of what's in your app. If your app is a meditation app and offers content related to meditating and building a meditation practice, imagine the impression that it would leave on people if not only the tab bar was more direct, but the directness of the tab bar enforced a simplicity and an order on the content that you display. And that content was predictable and directly organized. So indirect labels feel conventional and they feel safe but they mask utility. Like a storage locker you're putting your valuable stuff behind a door that's anonymous and inscrutable. And this is what I mean by unintentional design. No one intends to be indirect. Being intentional will lead to cleaner and clearer communication. And this brings me to my final element of intent. Direct communication. Intentionally design things. Clearly communicate to the people who use them. And the way to achieve this is to draw on what people learn on the platform and in the world. So I want to show you a couple of last quick examples of apps that rely on a familiar form to make interactions feel useful. So the core concept of an app called Tinycards is to bring flashcards on to a mobile advice. You can use them to memorize pretty much anything -- languages, flowers, leaves, almost anything. So the interaction within the app is super direct. People expect cards to have dimension, so that periodic pulse is inviting you to interact with the card. You flip the card over by tapping it. So these cards have two sides. They've respected the metaphor of the card. People appreciate that. So the entire experience of Tinycards is based on quickly flicking through these cards as you commit things to memory. Like for instance the flags of Canadian provinces. Does anyone know this one? Ontario. Let's try it. Oh my gosh. You've done that quiz before, right? So the UI of Tinycards is so clear you don't even think about it. Working with cards feels completely natural and you can just focus on your goal, which is learning stuff. So what Tinycards demonstrates to me is that if you implement a metaphor well, people will instantly become immersed. They'll forget that they're even interacting with a UI. And once they're immersed, you even have the opportunity to extend the metaphor, to really surprise and even delight people. So I want to show you a game. It's called Gorogoa and it extends the metaphor of the card in a way that I feel like creates a truly unique experience. So Gorogoa is a puzzle game. You move cards around a grid and you advance in the game and in the story by getting them into the right positions. But you could see that these cards do things that no actual cards can do. They can be torn off and as you'll see in a moment they can actually stitch themselves together into a scene. Yeah, this is an incredible game. And it's a great example of how you can extend the metaphor farther than you might think and create something truly amazing. This moment right here where you've got this card metaphor that suddenly kind of disappears, that's purely delightful and it's really I think a great testament to how well he's implemented the metaphor of cards. Bending and extending metaphors doesn't work if the result doesn't extend the utility, or value, or delight of your app. That's why the home tab on the tab bar doesn't work, right? It doesn't extend the utility of the app or create any additional value. So the last app I want to show you is an example that combines metaphors from the real and digital world in a really simple way. It's called Rosarium. Yeah. I think the developers are here. It's a rosary that you wear on your wrist and I believe that is Latin and the font is really communicating to me that it's Latin anyway. So the beads are represented by circles. Each time you turn the digital crown you get a slight haptic tab. So when I met the developers they told me that they were inspired by the similarity between the feel of rotating the digital crown and the feel of an actual rosary bead. So the app has translated this real world object, just a strand of beads, into the virtual world in a way that feels really natural. So I'm no suggesting that it's a replacement for rosary beads or any prayer beads because the physical beads can have deep, you know, personal meaning. But I love how they've thoughtfully carried that real world experience into a digital form in a way that's so radically simple and clear. And I think it really illustrates each of the points that I made earlier -- the simplicity and the understanding of people, the extreme kind of focus on a problem and the direct communication of a solution. And this is intent in action. This is being intentional in design. It's a matter of turning off the automatic part of your brain, of slowing down, and challenging the obvious. It requires a level of intention that will enable your decisions to become more conscious and for you to become aware of the patterns and the familiar things that might get in the way and block you and focus on people, right? The people that you want to serve, the person who inspires you, the family member who allows you to be the best version of yourself, the coworkers with whom you share a real need for something better, and you can take that inspiration and use it to discover newer and better ways to connect with people and to create something really great. So you can check out more about my session here. Thank you very much. And please, please stick around for the next talk. The next talk is designing fluid interfaces. It's a great talk. Easily one of the top seven design talks at this year's WWDC. So thank you.  Good morning, ladies and gentlemen. How many of you are here for the first time at WWDC? That's fantastic. It's great to see so many new faces every year. My name is Stuart Cheshire. And we're going to be talking about networking. I'm going to start by covering some topics that affect the performance of your app. There's hardly an app that exists today that doesn't make use of networking. And getting the best performance out of the network is really important. We're going to cover some technologies here that help you get the best performance. We're also going to cover some tips and tricks and guidance about how to make the best use of Apple's APIs, a little bit of news about new technologies on the horizon, and then my colleague Jiten will go into detail on URLSession. Let's start off with a state of the Internet update. Earlier this year, we hit a total of 4 billion people using the Internet. That's more than half of the world's population. And we're used to Internet usage doubling and doubling. Clearly when we passed halfway, the number of people on the Internet can't keep doubling, so that growth is slowing down but that doesn't mean Internet growth is slowing down. There's a lot of growth in machine-to-machine communications, Internet of Things, Smart Homes. There's still a lot of growth in places like India and China. And there is a lot of people who have never owned a desktop computer who may never own a desktop computer. Their primary computing and communication device is their Smartphone. And a lot of those Smartphones are still using 2G networks. I'm sure most of us in this room are fortunate enough to live in places and work in places where we build our apps where we have fast LTE networks. And that can be a handicap because if you build your app so it works well in LTE, it may perform very, very badly on 2G. One of your competitors, somewhere else in the world, who builds an app that works great on 2G is going to be fantastic on LTE. So we have a tool to help everybody mimic some of the properties of these slower networks and that's Network Link Conditioner. You should build your app running Network Link Conditioner right from the start. Don't think you can add in performance at the end because it's too late. Always, always, always run and test your app using Network Link Conditioner and that way if you make a programming mistake that has horrible performance implications, you see it right away and you can fix it right away. Use tools like Wireshark and tcptrace to understand the network performance of your app. It's a lot like using Instruments to look at memory and CPU usage. If you haven't seen tcptrace, it is a wonderful tool that produces graphs like this that let you see at a glance what's going on, on the network. If you want to learn more about that, check out the video from three years ago. IPv6 usage continues to grow. Why is that important? It's important because IPv6 is shown to have better performance than IPv4. And if you care about performance, you want to make sure not just your app but the service your app is talking to support Native IPv6. Some places in the world are doing better in this respect than others. In the US, we're now up to 87% of mobile carriers offering IPv6. Other places like India are doing pretty well too. And let's focus a bit more on India. Here is some data that the networking team at Apple gathered earlier this year about net TCP connection setup time and ongoing round-trip delay on cellular networks in India. The blue line is IPv6. And if we, for example, look at the 75th percentile, we can say 75% of TCP connections over v6 are set up in less than 150 milliseconds. The comparable number for IPv4 is worse than 325. It's more than twice as slow. So if you want fast, responsive applications for your users, get on IPv6 if you're not already. Another technology that improves performance by reducing packet loss and retransmission is Explicit Congestion Notification. We've had this enabled by default in macOS and iOS for some years now so you don't need to do anything on your application to take advantage of this. Do make sure your service supports ECN. In a survey we did of the Alexa top million websites, we found last month we're now up to 77% of the Alexa top million services supporting ECN, which is a big improvement compared to a few years ago. Another technology that helps improve performance and resilience of your connections is Multipath TCP. Quite often, the user may make a connection in their office on Wi-Fi and then they walk outside and they lose the Wi-Fi signal. Now with traditional TCP, the connection is broken. You have to reconnect. You have to start again. Multipath TCP makes its packet routing decisions on a per packet basis not per connection, so it can switch that connection live to a different interface. We talked last year about how to enable this in your application. And, of course, check with your server operators to make sure your servers are supporting Multipath too. We recently did a survey of the mobile carriers that Apple works with around the world and right now 78% of their networks work with Multipath TCP. Only 22% of carriers are still blocking Multipath connections. TCP Fast Open is a technology that lets you avoid the normal round-trip delay of the TCP connection set up. TCP Fast Open lets you put your initial data in with the TCP connection set up packets. You can check out more details of that from our video from three years ago. And check with your server operators to make sure that your servers support TCP Fast Open. Now moving on to some new news. There is a technology that many of you will have heard of called Quick. Quick is a new transport protocol, the first serious candidate in 30 years for a successor to replace TCP. It started off as an experiment by some engineers at Google. That experiment proved successful. It has now been adopted as an IETF work in group item for standardization. Apple engineers are participating in that. In fact, we have engineers right now at the Quick meeting taking place in Sweden. This is not yet ready for prime time. The standard is not finished, but Apple is working on it. As soon as it is ready, you can expect to see Apple API supporting that. Continuing in the theme of performance, we observed some behavior that's very common. Lots and lots of websites and Internet services use pretty short lifetimes on their DNS records. And they do this because if a data center goes down, they want to be able to update the DNS and very rapidly direct traffic to a different data center. The problem with this approach is you're paying a performance cost for something that almost never happens. Data centers very rarely go down. And what this means is every time a DNS address record is expired, your client has to spend another round-trip delay waiting for the response from the DNS server, which is the same as what it knew already last time. So thinking about this, we realized and optimization we could do. If you pass the flag to opt into this new behavior, then when you do a DNS query, if we have a stale, expired answer in the cache, we will give that to you immediately while in parallel, at the same time, doing the normal DNS query we would have done anyway. If the answer comes back the same, as we predict it will almost always, everything is fine, you just saved a round-trip time and got your connection started faster. If the answer comes back as a different address, we will then give another asynchronous notification to your client that there's a new address available which it should also try. And to make use of this, you have to use it in conjunction with Happy Eyeballs algorithm. That means your racing multiple connections in parallel. You're trying IPv4, IPv6, multiple addresses, multiple interfaces. If that sounds like a lot of work and it's hard to get right, you're absolutely correct. It is a lot of work. Stay after the break and we will tell you about some new APIs that let you take advantage of this without doing all the hard work yourself. Now moving on to some guidance. We have seen a common pattern that many developers use SCNetworkReachability as a preflight check. They want to predict the future. They want to know whether the next network operation they do will succeed or fail. And, unfortunately, predicting the future is always a hard thing to do. You may have connectivity now but two seconds from now the user has walked out of the building and you've lost the Wi-Fi signal. So there is no way to guarantee whether a future operation will succeed. And we see this pattern where they check. The preflight says yes. They try it. They fail. They go back. They check again. This also is a lot of work, a lot of [inaudible], a lot of difficult things to get right including networks with proxies. We can handle that for you. The better way to do this is just make a connection using the waitsForConnectivity option. You can learn more about that watching last year's video. What this means is if you want a connection, you tell the system I want a connection. Now if you can, later if not. If the device is in airplane mode, then when it's out of airplane mode your connection will succeed. That is much easier than building the retry loop yourself. There is one case we've seen with developers which does make sense which is if you're going to have the user answer a lot of information in a form, you may not want to waste the user's time if you have good reason to believe that may later fail. If that is the use case you care about, stay after the break because we have a new way to do that that's much better. Security remains important, as always. After ten years of using TLS 1.2, the Internet is now ready to move to its successor, TLS 1.3. It has a number of improved security features. It has reduced connection setup time, similar to TCP Fast Open. That standard is now final. The final draft was approved for publication by the Internet Engineering Steering Group earlier this year. We are waiting for the actual published document to come out of the RFC Editor. And when that does, we'll be turning on TLS 1.3 by default. Right now in your seed, it's not turned on by default. You can use the instructions here on iOS or macOS to enable TLS 13 in your applications. And we encourage you to do this right away because later this year when TLS 1.3 is turned on by default, you don't risk problems with your service not being compatible. So test them right now to make sure everything will go smoothly when the switchover happens later this year. Another element of security that's new is certificate transparency. You've probably heard cases where certificate authorities, either through malice or incompetence, issue rogue certificates to entities that they should not. The solution to this is something called certificate transparency logs. Every legitimate certificate authority now issues a public statement of every certificate it issues. And those are recorded in public logs for anybody to inspect. And this means that if a rogue certificate authority issues a bogus certificate, if it publishes it, they'll immediately get caught. And if they don't publish it, they'll be caught by the client. This is the setup you're probably familiar with. The new entity here is the log. When a certificate authority issues a certificate to a server, it also records that with the log and the log gives the server a signed affidavit that its certificate has been publicly recorded. And then when the client connects, the server can give all that information to the client and the client can verify that not only is this a signed certificate, it is a publicly logged signed certificate. Now suppose we have a rogue certificate authority that doesn't publicly expose the rogue certificates it's issuing. The client will reject that because it doesn't have the affidavit to attest to it being recorded in a public log. Starting later this year, we will be enforcing this. All newly issued TLS certificates must include the verification that they are publicly logged. And if they're not, then the client will reject it. Your apps don't need to make any changes, but if you have tailored certificates for your servers, make sure that your certificate authority is recording them in the public certificate transparency logs. Now we have a bit of news for hardware developers. The Bonjour Conformance Test is a tool that lets you verify that your hardware devices implement Bonjour correctly. You need to run this test if you want to use the Bonjour trademark name and logo on your packaging. You need to run this test if you want to bundle the Bonjour for Windows installer with a Windows application. And if you want to use the AirPrint, AirPlay, CarPlay, HomeKit logos on your packaging, passing the Bonjour Conformance Test is a part of the logo licensing process because reliable Bonjour is an essential part of those products. But more importantly, the value of running the Bonjour Conformance Test is it helps you improve the quality of your products and that makes them more reliable which makes your customers happy which makes your customers not return the product to the store because they can't make it work. And that's what your customers want. That's what you want. And that's what we want. We want happy customers having a wonderful time with products that work reliably. Now I want to cover API choices. Thirty years ago we had BSD Sockets. And it was a great API 30 years ago. But 30 years ago we didn't have mobile computers in our pockets. We didn't have wireless networking. We didn't have IPv6. We didn't have many computers with more than one network interface. If you had an Ethernet port on your computer, that was a fancy computer. Now 4 billion people around the world have a multi-homed IPv6 wireless battery-powered computing device that does power management and goes to sleep to save energy. The world has become a lot more complicated. Many of you may be using third-party libraries which are built on that Sockets foundation. Many more of you may be using URLSession. And you may have assumed that URLSession is also just a wrap around Sockets. Well, not quite. URLSession is actually built using Apple's user space networking code network framework. And starting now, in iOS 12, we are exposing that same API that URLSession uses so that your apps can directly use that for making TCP connections and other appropriate use cases. If you're doing things with URLs and HTTP GETs, URLSession is still your API of choice. But for the things URLSession doesn't cover, we now expose network framework so your apps can use that directly. And if you're the developer of one of these third-party libraries, which are very popular that are built on BSD Sockets, we encourage you to look at the network framework APIs. Move your library over to these improved high-performance APIs, and give us feedback about how that goes for you. So to summarize, we really strongly recommend here and now in 2018 that you avoid using BSD Sockets. Avoid using libraries that are nothing but wrappers around BSD Sockets. And if you are one of the authors of those libraries using these older APIs, look at switching over. Come and meet us in the labs this afternoon and tomorrow and give us your feedback about what it takes to move your libraries to new APIs. And with that, I would like to invite my colleagues Jiten to come up on stage and give you more details about URLSession. Thank you, Stuart. Good morning everyone. My name is Jiten Mehta. And I'm an engineer on the CF network team. Today I'll be talking to you about some networking best practices for your apps. Networking is an essential part of every application. Each year, you guys do a great job of adding awesome features to your apps. And today I'll be talking to you about some simple networking details, details that can help make your apps successful. Our agenda for today is going to cover four categories: reducing latency, maximizing throughput, increasing responsiveness, and making better use of system resources. Before that, let's quickly review URLSession, the API you've been using. URLSession is the recommended high-level networking API available on all Apple platforms. URLSession has first-class support for HTTP/2 and HTTP/1.1. If your app does not use HTTP, we have support for URLSessionStreamTask, an API that allows you to make secure TCP connections to a server over which you can build your arbitrary protocol. That's URLSession. Let's move on to our first agenda item: reducing latency. Let's suppose you and your friends go to a restaurant where the waiter walks up to you and you say, "Can I get a glass of water please?" The waiter say, "Sure," walks away, fetches you a glass of water. Your friend then says, "Can I get a glass of water too?" The waiter says, "Sure," walks away, and fetches your friend a glass of water. Wouldn't it be faster if the waiter took everyone's order at the same time and reduce the number of round trips? The idea to reduce latency is simple. To reduce the number of back and forths when you fetch a resource. Let's see how your apps can do this. First, let's look at some issues with HTTP/1.1. Your app wants to fetch a resource, you can create a URLSession task and call resume. URLSession will create a new connection for you, which involves DNS, TCP and TLS. Once the connection to the server is established, we will send out your request. We will then wait to get a response from the server. This is the network idle time when your app is not doing any kind of networking, waiting to get a response from the server. Once we get a response, we will call your completion block or message your delegate indicating that the load has finished. Let's suppose in the middle of this load your app wants to fetch another resource from the same server. You can create another URLSession task called resume and URLSession will create a new connection to fetch this resource since it does not have an idle connection in its connection pool. If your app wants to fetch yet another resource from the same server, you can create another URLSession task and call resume and we will create another connection to fetch the resource. In this example, I've shown you that we've created three different connections to fetch these resources from the same server. If you notice, we've spent a lot of time opening new connections. Let's see how this would look like if you used a single connection instead. This is a single connection case. We saved a lot of time by not opening new connections, but there is another problem here. The request number two which is the green request has to wait until response number one is fully received. The same problem applies to request number three which is the orange request which has to wait until response number two is fully received. This problem is known as HTTP head-of-line blocking. Consider moving to HTTP/2. HTTP/2 uses a single connection, and it also solves the HTTP head-of-line blocking problem. HTTP/2 multiplexes multiple streams over a single connection allowing you to receive parallel responses in an [inaudible] fashion. Let's analyze this example a little more to see how HTTP/2 performs better than HTTP/1.1. Pay attention to the times when your app wants to fetch a resource and the time when the request is sent out. In the HTTP/1.1 case, there is a significant delay between the time when your app desires a resource and the time the request is sent out. HTTP/2 can significantly reduce this delay and allows us to send the request almost immediately when the app desires the resource. Also pay attention to these gray boxes. If you recall, this is the network idle time when your app is not doing any networking, waiting to get a response from the server. HTTP/2 can significantly reduce this network idle time allowing you to better utilize the bandwidth and load the resources much faster. We just discussed many benefits of using HTTP/2 over HTTP/1.1, but let's quickly summarize them. HTTP/2 solves the head-of-line blocking problem at the HTTP layer. And it also allows you to better utilize the bandwidth. If your apps use URLSession, you don't need to make any client-side changes. Simply enable HTTP/2 on your servers and you will see these benefits. By adopting HTTP/2, you can also get some server-side savings because devices running your apps will now make fewer connections to the servers. This year, we have something new in URLSession that is going to add to the advantages of HTTP/2. Introducing HTTP/2 Connection Coalescing for URLSession. HTTP/2 Connection Coalescing is going to increase connection to use even more. Since your apps are not going to be opening new connections, they will become more responsive to your users. Starting with the [inaudible], HTTP/2 Connection Coalescing is going to be automatically done on for all your apps using URLSession. Now let's see how Connection Coalescing decides to reuse connections. Let's suppose you have an app and that app wants to fetch a resource from menu.example.com. We open a connection with the server, and the server presents us with the certificate. If your app wants to fetch another resource from delivery.example.com, we open another connection and the server presents us with another certificate. This is the old behavior where URLSession would create two connections to fetch these resources from the given host names. But if you look closely, the first certificate presented to us covers all the subdomains under example.com which means delivery.example.com is covered by this first certificate. Also notice that delivery.example.com, it results to the same IP address as the first connection. At this point, it's safe for us to assume we're talking to the same endpoint and reuse the connection instead of opening a new one when we want to fetch the second resource. This saves us time by not opening a new connection and makes the load much faster. HTTP/2 [inaudible] HTTP/2 Connection Coalescing new in iOS 12 and macOS Mojave. Now let's see how using fewer URLSession objects can help reduce latency. All the benefits of connections we use that we just discussed in the previous slides are applicable only if you use the same URLSession object to create your tasks. It's also important to know that every URLSession object has a connection pool and when you create multiple of these URLSession objects, you don't get any benefit of connection to use. It's also important to note that the URLSession objects are fairly expensive to create and have a non-trivial memory footprint. As we have in the past, we continue to advise you to use fewer URLSession objects. Let's move on to our next topic for the day: maximizing throughput. Coming back to our restaurant example. The waiter checks up on you and you say, "Can I get an order of grilled chicken tossed in creamy tomato onion gravy made with a lot of butter?" Now that's a mouthful. Wouldn't it be easier if you just said, "Can I get butter chicken?" The idea to maximize throughput is the same where you reduce the number of bytes that you transmit when you want to fetch a resource. Let's see how your apps can do this. Let's look at a couple of ways to reduce the request size. Pay attention to HTTP cookies. They are not free and have a non-trivial cost in storing and looking them up. Cookies are attached to all the requests that match the domain and path attribute. And it can quickly increase your request size. Please use the domain and path attribute wisely to make sure cookies required by the servers are attached to your requests. Use of smaller cookies when possible, and delete these cookies when you no longer need them. Try to save some state on the server so you can reduce the number of client-side cookies. Also consider moving to HTTP/2 to get benefits of header compression. Let's talk a little more about compression. HTTP compression, also known as content and coding, is simply compressing the data that is shuttled between the client and the server. This allows us to better utilize the bandwidth. The algorithms that URLSession supports and recommends are Gzip and Brotli. Gzip is widely supported and is relatively fast. Brotli support was introduced last year in iOS 11 and macOS High Sierra. Brotli is optimized for structured text and HTML. And it has the best compression ratio on short data. Please enable compression on your servers if you haven't done so already. Let's move on to our next topic for the day: increasing responsiveness. Coming back to our restaurant example. Here you are here in San Jose for WWDC, and you decide to meet up with some old friends. You and your friends are sitting at the restaurant table. Your drinks are here, but you would like some more time to catch up with your friends before the food comes out. You can simply tell the waiter, "Can you please bring out our food after some time? We are in no rush." The same concept can be applied to responsiveness where you mark your tasks with priority depending on the other tasks that you're doing. Let's see how your apps can benefit from this. You might be familiar with these five QoS classes associated with dispatch queues and NSOperation objects. Data [inaudible] the CPU scheduling policy. URLSession is QoS-aware which means it will capture the QoS off the queue on which you call task.resume. And all the messages that it sends to your delegates will respect this QoS. Let's take an example. If your app wants to fetch some data which is not time critical, consider resuming that task on a queue which has background QoS to make sure this task does not contend for CPU with other higher priority work that your app might be doing. Network service type is the property on the URLSession configuration object that allows you to classify your network traffic that helps the system prioritize the data leaving the device. This year, we have a new network service type, the responsiveData. ResponsiveData is slightly higher than the default type but should be used judiciously. An example where you might want to use responsiveData is if you have a shopping app and you are on the checkout page. You might want to mark your payment request with the responsiveData to make sure you get a good response from the server. Traffic marked with the network service type property will maintain this tag across all the hops when on a Cisco Fast Lane network. For more information on this API, please view the WWDC session from the year 2016. Last year, we introduced the URLSession Adaptable Connectivity API waitsForConnectivity. waitsForConnectivity will simply wait instead of failing the load when your task does not have connectivity. In the past, you've been using STNeworkReachability to do a preflight check before you send out your request. But as Stuart pointed out a few slides ago, there is a race condition where the system might tell you that you have connectivity to a server but by the time you create and send your request, you've lost your chance and you're no longer connected to the server. We recommend using waitsForConnectivity which will simply send out your request as soon as a connection to the server is available. Optionally, you can implement the taskIsWaitigForConnectivity delegate method which gets called when your task does not have connectivity. This can be helpful to present the user with a different flow or an offline UI for better user experience. For more information on this API, please view the WWDC session from last year where this API was introduced. Now let's move on to our last topic for the day: making better use of system resources. Coming back to our restaurant example. You like the food at this place so much that you decide to come here for dinner the next day. The restaurant has a delivery service where you can place your order today and they will deliver the food to your house the next day. This not only saves you the time and effort to go and pick up your food but it also helps the restaurant prioritize their work based on your deadline. Let's see how your apps can make better use of system resources to be more efficient. Background sessions have upload and download tasks. These tasks use system intelligence to decide when to start and when to stop a download based on various factors like battery, CPU, Wi-Fi, etcetera. If your app wants to fetch a large file, consider using background sessions. These tasks run out of process which means your download will continue even when your app is in a suspended state. For more information on background sessions, please view the WWDC session from the year 2014. Caching is a great way of reducing latency but it's important to note that caching might result in disk IO. In the real world, we've seen some apps write several gigabytes of data to disk each day which can cause severe flash storage degradation. Please don't cache unique content. Let's take an example. Let's suppose you have an app, a dating app, and you are responsible for the networking code of the app. This app loads user profiles with high-resolution images. It might be wasteful to cache these high-resolution images because the user might swipe left, move on to the next profile, which means that the images that you just cached are probably not going to be requested again. Please consider making client-side changes by adopting the willChacheResponse delegate method to decide what resources should be cached. If you own the servers, please consider using cache control headers to decide what resources should be cacheable. Let's quickly go over some of the key points that we discussed today. Number one, order all your food at the same time when you go to a restaurant. I'm just kidding. Move to HTTP/2 today to get wins like header compression, connection coalescing and no head-of-line blocking. Use fewer URLSession objects to reduce latency by reusing connections. This also reduces the memory footprint so it's better use of system resources. Reduce the request size to maximize throughput. Pay attention to QoS to increase the responsiveness of your apps. And finally use background sessions when you can to make better use of system resources. For more information on this session, please visit this website. Now we'll have a short break. And after the break, we'll introduce you to network framework, a modern alternative to Sockets. I would love to see you all at the networking labs which are going to be held today and tomorrow. Thank you all for being here. And I hope everyone has a great rest of the conference.  Good afternoon. Welcome to Best Practices and What's New with In-App Purchases, my name is Dana DuBois, I'm an App Store engineering manager. In-app purchases are an essential part of how so many apps do businesses in the App Store. Whether you have a subscription model where you're maybe a video streaming service or a newspaper or magazine or you offer consumables, content that the user can buy over and over again like in-game currency or non-consumables, something where the user buy once and uses from then on. It's important to make that user experience great, this is your business after all. So we're going to cover some best practices in order to help you make that experience great. We're also going to talk about some stuff that we have been doing to make this great for you. So first, I'm going to invite my colleague Ross up on stage, he's going to talk about Introductory Pricing, this is a feature we launched last December and it allows you for your subscription business to entice new customers in. I'm going to give a quick update on trials in the App Store. We're going to go over ratings and reviews and how your app can get customers to give feedback into the App Store. I'm going to give an update on how you can use the Apple App Store Sandbox environment for testing your in-app purchases. Ross is going to come back up and talk about processing transactions, this is when you're actually buying those in-app purchases. And finally, I'm going to give you an overview about the in-app receipt. So to begin here's my colleague Ross. Thanks Dana. In iOS 11.2 and macOS 10.13.2 we added the Introductory Pricing feature to in-app purchase subscriptions. This allows you to create a one-time use discount for new subscribers. We set up introductory pricing in App Store Connect and each subscription can only have a single introductory price at a time. When an eligible user purchases the subscription in your app the discount is applied automatically by the App Store. Each user is eligible for a single introductory price based on the subscription group. I'll go into that a little bit more later. Along with Introductory Pricing we added several new types and properties in StoreKit. These APIs reflect the data that you've set up in App Store Connect and you can use them to format your UI to display the terms of the subscription to your users, as well as to determine which users are eligible for the introductory prices. For example, here's what it looks like in the App Store when you set up an introductory price on a promoted in-app purchase. You can see that it clearly naturally lays out the terms of the subscription to the user, the first year for 19.99, then 39.99 for each year after that. This is also a great benefit of promoting your in-app purchases that have introductory prices. Users can see the discount before they even install your app. Today I'm going to go over these new APIs with you and look at a couple ways you may want to set up your introductory prices. First, there's a new property on SKProduct which is called fittingly enough introductoryPrice. This is a new class that's called SKProductDiscount. This class contains all the information about introductory price you've set up. And as you can see it's an optional property that's because only subscriptions can have introductory prices and not all subscriptions will have them. So let's dive into SKProductDiscount. SKProductDiscount reflects the data you've set up about the introductory price in App Store Connect. It has a price and a price locale property and these behave the same as the properties with the same names you're familiar with on SKProduct. Also as a subscription period property, this is a new class called SKProductSubscriptionPeriod and this reflects all the data about the billing and renewal terms of the introductory price. It achieves this with two properties of its own, the first is unit which is an enum that can be day, week, month or year. The next is number of units. So for example if you have a unit of month and a number of units of 2 that means that the introductory price renews every 2 months. And back on SKProductDiscount there's also a numberOfPeriods property. So this reflects the number of those subscription periods the introductory price is valid for. Say if you have that 2-month introductory price, the number of periods is 3, that means the introductory price is valid for a total of 6 months. Finally, on SKProductDiscount there's a payment mode property. This is another new enum that has three values. The first value is payAsYouGo, this behaves like a normal subscription where the user pays once per renewal period. The renewal period for these does have to be the same as the renewal period of the base subscription you've set up the introductory price on. So if you had a month-long subscription you couldn't have a 2-week long introductory price in this case. You can use these types of introductory prices to offer your users a longer introductory duration but with less friction up front. For example, if you have a 3-month renewing subscription that's billed at 9.99 you could set up a 6-month introductory price that renews twice at 1.99 each. The billing period would look like this, the user purchases the subscription at the introductory price of 1.99 and receives 3 months of access. After that they can renew at 1.99 for another 3 months of access. When that's over they'll renew at the normal subscription terms of 9.99 for 3 months. Here's what the data from StoreKit would look like in this case. You can see the subscriptionPeriod has a unit of month and the number of units is 3, so it'll renew every 3 months number. And then the numberOfPeriods is 2 giving us that total of 6 months of introductory. The payment mode is payAsYouGo. And the price for the introductory price is 1.99. The next payment mode is payUpFront. With this type of introductory price the user pays just one time and receives the entire introductory duration. This is not limited to multiples of your subscriptions renewal period. So in this case, you could have a 1-year renewing subscription and offer a 1-month introductory price. So if we can take our previous example of our 3-month auto renewable subscription, this time we'll offer a payUpFront introductory price for 6 months. Charging 3.99, so it's about the same overall cost as the previous example, but this time the user will pay 3.99 upfront and receive the entire 6 months. After that's up we'll renew with the normal terms of the subscription at 9.99 for every 3 months. Here's what the data from StoreKit would look like in this case. The subscriptionPeriod's unit is still month, but this time we have 6 units. The numberOfPeriods is 1 because it's a payUpFront introductory price. The final payment mode is freeTrial. This behaves the same as the old freeTrial behavior where the user pays nothing and receives the entire introductory duration. This is available under durations varying from 3 days all the way up to 1 year. Excuse me. Also in iOS 12 and macOS 10.13.2 we've added the subscriptionPeriod property to SKProduct itself. This way you have access to the billing and renewal terms of all of your auto renewable subscriptions not just the introductory price. Also, in iOS 12 and macOS 10.14 we've added the subscriptionGroupIdentifier to the SKProduct. This is important because introductory pricing eligibility is based on the subscription group not the subscription itself. Each user is eligible for a single introductory price per subscription group. This is because subscription groups are meant to offer the same features or content just a different at renewal periods and price points. For example, many apps offer both a yearly and a monthly option for the same content with the yearly one being less-expensive over time. Now it wouldn't really make sense for a user to receive an introductory price to both the monthly and the yearly option for exact same content. Since introductory prices are applied automatically by the App Store at the time of purchase it's important that your app correctly determines the user's eligibility when displaying the price. For more information about determining eligibility you can see the engineering subscription session which is just after this one at 3 o'clock. Now with these new APIs you'll have access to all the information you've set up in App Store Connect for your subscriptions, so your app can reflect any changes you make without updates to your binary or to your server code. I'd like to invite Dana back on who has a few tips and improvements for you. Thank you Ross, so Introductory Pricing is a great way to attract new customers if your business model is a subscription. But what if it doesn't make sense for your app and your app's business model to offer a subscription service? Well, starting today we're excited to announce that you can offer free trials for non-subscription based apps. This will allow you to have a try before you can buy experience. So how do you do this? Well you take your payUpFront app and you make it free in the App Store. And then you add a non-consumable in-app purchase to unlock. So if your app is 9.99 in the App Store you're going to want to make a non-consumable in-app purchase of 9.99. But it's important to let the user decide when to begin the trial period, so to do that you need to make a second non-consumable in-app purchase at price tier 0. This will be a free non-consumable that the user will use to begin their trial period. The naming convention of that free non-consumable should detail out how long that introductory period is. So in this example it's 14-day trial. Before the user begins their free trial, they should be clearly informed of how long that trial is, in your app's UI what the ultimate cost is going to be to unlock at the end of that free trial. And finally, what features or content is going to be lost if they let the free trial expire and choose not to purchase the full unlock. That information needs to be presented up front. So this gives you a quick overview of the change, I'm going to talk more about how you can implement this inside your app when I go over receipts. Right now however I'd like to give an update on requesting ratings and reviews. Whether or not your app has in-app purchases if it's in the App Store you care about the feedback your users are going to give. And there are ae few ways the App Store and StoreKit can get you that information. First, in iOS 10.3 we introduced the SKStoreReviewController. This is a powerful API and it allows an app to give a quick and easy prompt for the user to select a star rating or maybe even type in a review and get right back to using your app. But with great power comes restrictions. First, we limit the number of times that this prompt can appear per app, per device and per year. Second, if users choose never to see these rating prompt they can just go into App Store settings and disable them altogether. So with those 2 restrictions in mind what are some strategies you want to use for when to determine is the best time to call into the review controller. Well first and foremost, don't interrupt. If the user is in the middle of playing that level and fighting the big boss at the end of it it's not a great time to ask them to rate your game, wait until they're done. You should also wait until both they've had enough experience with your app to actually have a good opinion about it, as well as wait until they've had a positive experience. Maybe they just finished fighting that boss, maybe they just ordered food and it got delivered through your app, wait until that positive experience. Finally, I mentioned before that we limit the number of times we will prompt this review her app, per device, per year, but that doesn't mean we do any sort of rate limiting. So if within one session you call the SKStoreReviewController 3 times and the user keeps clicking not now they will see that 3 times in a row. It's up to you to add your own rate limiting to your app. So how does this look in code? Well as I've been saying, wrap any call to the StoreReviewController around your own app's business logic. Here's what you're going to want to check, is this a good time to do it, are they in the middle or have they just completed a task. Have they just had a positive experience and have you prompted recently, that's where you want to have your check. As soon as all of those pass you simply call SKStoreReviewController. requestReview. It takes no input and it gives back no output, it's a simple but powerful API. We've gotten immense response from developers about these APIs, I'm happy to share a few with you. Zappos have said they're seeing 10 times the amount of reviews that they used to using the SKStoreReviewController and it's giving them the confidence to know that they're delivering the right things to their customers. Frosty Pop, maker of such great games like Kingpin Bowling and Ninja Attack have said that they're getting greater visibility in the App Store and because of a larger sample size they have a much greater and more relevantly engaged user base giving feedback. I hope this type of feedback convinces you that you shouldn't be using your own prompts to request review you should move over to the SKStoreReviewController, it really has been making tremendous gains. As I'm sure you saw yesterday, we announced a beautiful new Mac App Store and we know you guys are all going to rush off and make great new Mac apps. And so with that we're excited to announce that we brought the write a review API to the Mac starting with Mojave. So I mentioned there were a few ways you can get users to write reviews for your apps. Well another way of doing it is you can deep link right into the App Store. We also introduced this in iOS 10.3 and we're also bringing that to the Mac this year. It's basically just a link right into your product page, you tell the App Store bring up the write a review sheet and the user can fill it out right then and there. As an alternative to the SKStoreReviewController this works much better from any sort of UI you might have where the user has to take an immediate action. So for example, if you had a button inside a settings page within your app basically asking the user to kindly write a review this would be a great time to use a deep link right into your product page. So how does this work? Well pretty simple, you take your product page URL and you add action=write-review to the end of it. This informs the App Store to bring right up the write a review sheet. If you don't know how to get a product URL for your app linkmaker.itunes.apple.com is a great facility for finding that. You can get a lot more information about ratings and reviews, including how to respond to your customer reviews right on the developer website. I'd like to do a deep dive into the App Store Sandbox next. So what do I mean by the Sandbox? Well first what don't I mean, I'm not talking about the app Sandbox, that part of the operating system in kernel that limits what resources your apps have access to. What I'm really talking about is a dedicated environment, it's an entire copy of the App Store commerce engine on the server side that's there for you to test your apps and in-app purchases. StoreKit knows when to go to the Sandbox environment based on how your app is signed. So for example, you're building your app in Xcode, it's signed with the developer's certificate StoreKit knows right then and there take all of your requests to the Sandbox environment. If the user downloads the app from the production App Store in that case it's going to be production signed. StoreKit knows to go right to the regular production environment. So what makes the Sandbox environment different? Well first we don't charge, this is an environment for you to test your in-apps. You're going to test them over and over and over again, there's no reason to charge. Second, we have specific dedicated Sandbox accounts that you create in App Store Connect for your developer to use in the Sandbox environment, they're not the same as your regular iTunes in-apps accounts. We also as I mentioned before have a completely different backend environment which means there's a completely different URL from when you're doing server to server receipt validation. This is important to know if you're taking that development test receipt from your device, sending it to your QA server and wanting to have that go to the Verify Receipt endpoint for validation. I'll talk more about that a little bit later on. We also offer some test modes right in StoreKit. So for example, SKReceiptRefreshRequest can take in an argument to get an expired receipt. This way you can actually test hey what happens if I get an expired receipt later on. We also have an ability for you to simulate what would happen if a kid is asking their parent for an in-app purchase. We have simulatesAskToBuyInSandbox. Probably one of the biggest differences is that when you're working with in-app subscriptions we can track down how long it takes to auto renew. If you have an annual subscription in your app it would be ridiculous for us to make you wait a year to test your annual subscription renewing. A general rule of thumb is 1 year in real time equals 1 hour in Sandbox. You can also see that if you have a monthly subscription that's only 5 minutes. We also limit the number of auto renews in Sandbox to 5. So when you buy that initial subscription purchase you'll have 5 automatic auto renews working up this cadence and at the end it'll just stop. This is to simulate what would happen if a user goes into managed accounts and disables that subscription, they just turn it off or what we call voluntary turn, they've decided they don't want to use your service anymore. So how do set all this up, how do you work with the Sandbox environment? Well first you go into App Store Connect, you create your users. You create your products that you're going to sell, you're going to need to get that up on the server side before your app starts working with that. You build and sign your app in Xcode just like you do all the time. And you launch your app and you go find a product to buy. And then when prompted you sign in with your Sandbox account. Now if you're paying attention you might have thought I missed a step. What would happen if I'm already signed in to my production app or iTunes Store, don't I need to sign out before using a Sandbox account? Well starting in iOS 12 that's no longer the case. Down here we've separated out your production account from your Sandbox account, you can manage it separately just like how we use the receipt to decide which environment to use we use the -- I'm sorry, just like we use the certificate to know which environment to use we know to use the Sandbox account when you're in development mode. So look forward to that in iOS 12 I think it's going to make a huge difference for carrying these test apps on your personal devices for long-term testing. So now I'm going to invite Ross back up and he's going to talk about some best practices, some things you want to keep in mind when processing your transactions. Ross. I would like to go over a few common questions and scenarios we see and discuss the best way to handle them. First things first, you should add your Transaction Observer to the default gaming queue as early as possible in the life cycle of your app. This is a common issue we see where apps don't add a Transaction Observer until the user navigates to the in-app purchase UI or even until they begin a transaction. In fact we recommend adding it right in the application didFinishLaunching WithOptions method of your AppDelegate. Well why we recommend this. Well the Transaction Observer is StoreKit's way of communicating changes in transactions the user is making in their app and pretty much all of these changes are important. And it's a good user experience and generally good business to make sure you handle any transaction promptly. There are a few cases when transactions can become interrupted. For example, if the user leaves your app in the middle of a transaction, maybe they got caught playing games in class. Your app could be terminated by the system later on or even by the user and then when it's opened back up again StoreKit won't know to continue the transaction until you add the Transaction Observer. If the user decides to buy something else this could result in confusion when they receive two prompts or they could receive a prompt out of nowhere when you finally add the Transaction Observer again. Not great [inaudible]. Another common case where the user needs to leave your app in the middle of a transaction is if they have to edit their billing info, this happens all the time. You'd have to leave your app and you want to make sure to smoothly continue the transaction when they do return. Finally, it happens your app can crash, in this case you want to make sure to smoothly continue as well. Now there are some other reason as well that you want to make sure to add this as early as possible. There are several types of transactions that can actually come from outside your app. For example subscription renewals come through the Transaction Observer. When an auto renewable renews successfully you'll receive a transaction in the payment queue. You definitely want to make sure to receive this as early as possible so you don't interrupt the user's service when they actually have paid for it. Also promoted in-app purchases, users click on these in the App Store and then the transaction is handed off to your app. So you want to make sure to have a smooth transition there. Finally, if you set up promo codes for your in-app purchases these are redeemed from inside the App Store and sent to your app as well. Another question we get all time is when should I call Finish Transaction. Well the general rule is you should call Finish Transaction after a transaction has completed successfully and you have downloaded and delivered all the content to the user or after the transaction has computed unsuccessfully. To get more specific, let's go through all the transaction states and talk about how to handle them. The purchasing state you don't need to do anything at the moment, just continue to observe the payment queue and wait for the state to change. In the purchased state the transaction has completed successfully. You should download and deliver all of the content to the user and then call Finish Transaction. In the failed state the transaction has completed unsuccessfully. You should inspect the error and handle it appropriately, maybe update your UI, record analytics, whatever you need to do, and then you should call Finish Transaction. And it's also important to note here that transactions come back in the failed state if the user cancels them. So it's very important that you do inspect the error and make sure it's not the canceled error because you shouldn't show any UI in that case. You know the user just canceled they don't need to know that it failed. The restored state, this is very similar to the purchased state. It indicates the transaction has completed successfully, so again you should download and deliver all of the content to the user and then you should call Finish Transaction. Finally, the deferred state, this is similar to the purchasing state except it means that the transaction is waiting on some outside action to continue. For example, if a user has Ask to Buy turned on, so Ask to Buy is a feature that allows parents to manually approve or decline transactions made by their children. Any user can have Ask to Buy set up, but it can happen to any of your in-app purchases so it is important that you handle this case appropriately. Speaking of Ask to Buy, if your transaction is deferred due to Ask to Buy the user will receive a message from the App Store saying that their parent has been notified to approve it. If it is approved and then it will be returned to your Transaction Observer in the purchased state. You should let the user know it's been approved, of course deliver the content, and call Finish Transaction. If it's declined it will be returned to your Transaction Observer in the failed state. So here you should let the user know that it was declined and then call Finish Transaction to finish it up. However, if there's no action taken within 24 hours then the transaction fails silently, this means that nothing will be returned to your Transaction Observer. Furthermore, all Ask to Buy transactions within that 24 hours are consolidated into a single ask. So the important takeaway here is that you should not wait on deferred transactions or even expect them to come back at all, you should make sure you don't lock the UI and allow the user to continue using your app as much as possible. Another important tip when working with in-app purchases that have Apple hosted content via SKDownload is that you should always finish downloading the hosted content before calling Finish Transaction. This is because once you call Finish Transaction all SKDownloads will be canceled and no longer valid for redownloading. If you do this on accident then you have to call Restore Transactions to get a new download to finish. So even if your download fails you should withhold calling Finish Transaction until you retry it and you actually successfully download it and deliver that content to the user. Finally, if you're using Receipt Verification you should always make sure to do this before calling Finish Transaction as well. This is especially important for consumable in-app purchases because consumable in-app purchases only appear in receipt as long as they're unfinished. Once you do call Finish Transaction they will no longer appear in their seats and you won't be able to validate them. So if you're using Receipt Verification make sure that it's a real Apple signed transaction the user paid for, always do this before calling Finish Transaction. I'd like to hand it back to Dana to discuss some tips for managing the receipt. Thanks again Ross. So what is the receipt? It's a lot like its name implies, it is a record of your app in in-app purchases. It's a lot like the piece of paper you get when you buy some retail good. It's stored on the device, it's a file, it's a file that we actually get from the App Store. It comes from the server, StoreKit doesn't make it and we store it on the device and your app can actually read it. It's signed by the App Store so that's how you know it's authentic. And finally, it's for your app and for that device. The receipt is an important way for making sure that that app is valid for the device it's trying to run on. How do you validate the receipt? Well there's two things you can do. First, you can look at on-device receipt validation, this is where you can just use cryptography right on your device to validate that receipt and unlock content right then and there. Alternatively, you can do server-to-server validation. This is where you take the receipt, send it up to your trusted server, and it'll manage it for you. In this case it'll call itunes.apple.com/verifyreceipt, send that receipt over and the App Store will validate it for your trusted server. This is great if you have a subscription service on the backend. However, I'm going to focus more for the purposes of my talk for on-device validation. If you have a subscription service I highly suggest you check out engineering subscriptions here in Hall 1 today at 3 o'clock right after this talk. But I do want to make one point about server-to-server validation. As I said, it's completely correct, it's completely appropriate to take that receipt from the device and send it up to your server. But you should never send the receipt from your device directly to Verify Receipt. You don't control either end of that connection, you don't control the user's device and what connections it makes and you don't control the other end with the App Store. There's no way to make a trusted connection from your app right then and there. You should send it, always send it up to an intermediate server. So what is the structure of the receipt? Well it has some purchase information, the certificates and signatures. We give you an API to pull it off the file system. And as I said it's a single file. We use opensource standards, PKCS Cryptographic Container and ASN.1 to store the metadata within the receipt. These are all well documented and well published open file formats. And there are tons of options out there for verifying and reading the receipt, OpenSSL, asn1c, there's tons more, you can even build your own. How do you read that receipt no matter what API you're going to use? Well you call bundle.appStoreReceiptURL, this returns a URL to your app, but again it's just a path to the file in the file system. There you're going to want to read in that content, get that blob of data into memory, and if you're doing server-to-server validation send it up to your server, otherwise you can process it right there on the device. When you are processing it if you're using OpenSSL here are some tips you want to follow. First, build a static library not a dynamic one, it much more secure if you use a static library. Finally, if you're going to include the Apple Root CA Certificate, which is what you need to validate the receipt you can grab that online, there's tons of documentation out there, but watch out for the expiration of that receipt. So as I mentioned before there's tons of solutions out there for this, but keep in mind convenience comes at a price. This is your business, you need to understand and know the risks that come to any solution you implement. If you're using some popular third-party solution out there and there's an exploit within there your app is going to be vulnerable to that as well. It's your revenue stream, make sure you understand what products you're choosing. So I mentioned that there's a signature and there's certificates within the receipt. One best practice is don't check the expiration date of those certificates within the receipt against the current clock. First of all, the user can actually change the clock on the device, but more importantly the receipt isn't necessarily invalid just because the certificates within it are expired. What you should be comparing it to is the dates of the transactions within the receipt. As long as all the transactions occurred before the certificate is expired it's a valid receipt. So let's dive in and get a sense of what is inside the receipt. That purchase information contains just a bunch of types and attributes. Here you can see there's a bundle identifier and a bunch of other values that go along with it. If you're going to check that the receipt is for your application first you want to check that bundle identifier and you want to check type 3, which gives you the version that goes with the app. But a best practice here is always use hardcoded values, don't just read from your app's Info.plist. It's too easy to change the Info.plist to match an invalid receipt to spoof your app into running. Now that you've matched that bundle identifier and version to the receipt you're going to want to make sure that it's valid for your device and the way to do this is to use attribute 5. Attribute 5 is shawl SHA-1 hash of a couple values, including the bundle identifier, the device identifier, the thing that represents the hardware that is running your application, and an opaque value. This is a bit of cryptic or [inaudible] entropy, it changes over time and we store that in the receipt as type 4. You're going to take those 3 values, generate your SHA-1 hash and compare and if it matches what you generate with what's in the receipt you know that that receipt was generated for your app on that device. Well what happens if you get an invalid receipt or it just doesn't exist for whatever reason? StoreKit gives you APIs to request a new receipt from the App Store. Again, the receipt comes from our commerce backend so you're going to need a network in order to be able to complete this operation. In order to make sure that we are using an authentic receipt from an authentic user a sign-in may be required. When you get an updated receipt you need to be careful to avoid any sort of continuous loop of validate and refresh. If you get an updated receipt check it once and if it's still invalid error out then. So what does this look like in code? You simply call SKReceiptRefreshRequest, you set your purchase queue delegate right there on the ReceiptRefresh, you call start and as that receipt gets updated we'll call back and let you know when it's down. On macOS you can also call exit 173, that 173 code will tell the operating system and tell StoreKit to refresh a receipt on your behalf. There the app will get relaunched once a new receipt comes down. As I mentioned earlier, we're now allowing non-subscription apps to do free trials and the receipt is a great way to understand is the user in the middle of a free trial or have they bought the full unlock non-consumable IAP. To do this you're going to want to look for type 17 within the receipt. This includes all of your subscription consumable and non-consumable IAPs. Within type 17 there's a couple subdictionaries of data. Type 1702 is going to be the product identifier that's associated with your non-consumable IAP. This is where you're going to want to check to see is it my non-consumable that represents the beginning of the free trial or the non-consumable that represents the full unlock of the app. You're also going to want use type 1704, this is where you can check the purchase date of when that consumable was actually made. So what's the algorithm you're going to want to follow? Well first, you're going to iterate over all those in-app purchases, all of those type 17's. If you find any within the receipt that matches the product identifier that you've represented the Full Unlock of the app you're done. You know that the user has purchased the Full Unlock of your app, give them the access to it. Alternatively, if you find a Free Trial Product Identifier, the product identifier that represents the user beginning their free trial you're going to want to look at the purchase date associated with that free trial. Here's where things get a little tricky. You can't always trust the clock on the device, again the user can change that. So you might want to use your own backend server or some sort of time server to make sure that it's still within your free trial period. There's also device check APIs which I highly recommend you check out that help you determine whether or not the user has previously completed their free trial window on that device. Lastly, if neither are present this is your signal to display that free trial upsell to begin the free trial, but again you need to be careful to make sure you represent the length of that free trial, the content that they're going to lose if they don't buy that Full Unlock, and the ultimate price of the Full Unlock. If you already have an app in the App Store and you're selling it or you have a subscription model and you want to move to a new free trial there's some things you can also use the receipt for to make that easier for you. So if you're paid up front and you want to move to a subscription or if you're paid up, paid up front and you want to move to a free trial use type 19. This will tell you the original version that the user purchased that app with from the App Store. Even if they delete the app and redownload it over and over again that type 19 will tell you exactly what version they bought their app with. If the user did originally pay up front make sure to give them the functionality that they bought. Just because you move to a subscription model if they bought it when it was a paid for app in the App Store you need to give them that access to that content they originally paid for. So again use type 19 within the receipt to know what version they bought that app with. So we went over a lot of things today, we talked about Introductory Pricing, this is a great means of enticing new customers into your subscription, you can show content right on the App Store if you're merchandising your subscription IAPs. I also talked about how you can now offer non-subscription free trials within your apps. We talked about SKStoreReviewController and how developers have seen such a great response to that and how we brought it to the Mac App Store this year. Talked about the Sandbox environment and how now we manage your Sandbox account separately from your regular iTunes and App Store production accounts. Ross gave a great overview of things you need to keep in mind when processing your transactions, including making sure to observe early and knowing when the right time to call Finish Transaction is. And finally, I gave you some good points on how to manage that in-app receipt for on-device validation and making sure your free trials go smoothly using the receipt. For more information I highly suggest you stick around for Engineering Subscriptions if you have a subscription business model. We're also going to be at the labs today at 4 o'clock and then Thursday morning at 9 a.m. We'd love to get your questions and help you out as best we can. Thanks so much for coming.  Good morning, everybody. My name is Pavel and Marin and I will be talking to you about ClassKit today. Our talk is split in three parts. In first part, we will give you a general overview of ClassKit. We will show you how data flows through the system. In the second part of the talk, we will list all the classes we have in our system and we'll mention a few interesting things about each one class. And in the end, Marin is going to show us how to adopt ClassKit in an existing cap. So, let's get started with the Overview. ClassKit is an education-based framework which means that it is designed to work with in a school environment. It allows you to define assignable content from your app, so that teachers can assign it to students as homework. Then it allows you to report progress on those pieces of content that were assigned by teachers. And this is done with privacy in mind. What I mean by that is that you can report progress on any assignable piece of content within your app, but teachers will only see progress on the things that they have explicitly assigned to students. So, why would you adopt ClassKit? Well, for one, this will get a better student experience. Teacher workflow, because they will be able to know exactly what's assignable within your app and assign it to students. It gives teachers insights within how students are doing in your app. And this enables personalized learning. If a teacher knows how students are doing within your app exactly, they'll be able to better cater future homework assignments. Lastly, well, it also gives you a competitive advantage. This is because with teachers while the type of information is surfaced back to them, chances are that they will be advocating for your framework to -- for your app to be used within the school. Speaking of schools, there's a related technology called Apple School Manager, and school admins and IT use this to create managed Apple IDs for everybody within their organization. So, every teacher will get a managed Apple ID and all students will get managed Apple IDs. Admins also use it to create classes within a school, so for example, in mathematics class we will have a teacher with -- which is a person with a certain managed Apple ID and students, which are people with other managed Apple IDs. This is also where schools manage all of their content and devices. And as we mentioned for competitive advantage, this is where schools will be able to enable student progress reports in feature of ClassKit. Also, this is where schools are able to purchase apps in bulk. And any app that has enabled ClassKit support, we'll have works with Schoolwork checkmark next to its name. So, Schoolwork is the new education app that is coming out soon. Students use this app to view handouts that were assigned to them. And teachers use it to create homework assignments. And homework assignments, we call handouts within our system. Handouts is just a collection of tasks. For example, a piece of content might be a task within a handout. So, this is also the place where teachers will view progress reports of how students are doing within your app for a given handout. So, let's actually take a walk, what the life cycle of a handout is. But before we can create the handout, your app actually needs to be able to declare what pieces of content are assignable. Within our framework, this is done using a cross code CLS Context. And what a CLS Context is, is that it allows you to represent your content as a tree structure. So, there is one main app context redefined for your app. And this acts as the root context of the content tree -- or the context tree. Also, all of your content is just a descendant of the main app context. So, we ask that you define your content as accurately as possible because the sooner you define your content, the sooner it will be available to teachers in Schoolwork to assign to students. Assuming that you've created your context tree, a teacher will be able to go to Schoolwork, they will be able to tap the Create New Handout button, which will create a new instance of a handout. Then they will point the handout to a specific piece of content within your app, and once they've done that, they will assign the handout to a student. Once a student receives that handout, they will then tap on the handout which will open your app. At that point, your app should create the necessary contexts. And for a better student experience, we ask that you navigate the student to the same piece of content. Marin is going to show us later how this is done in code. So, assuming that now the student started working on the homework assignment, your app starts reporting progress back to students and all of this progress will be sent to the teacher's device and bundled in what we call a progress record -- a report, or activity report. So, teachers, once this is done, will have access to that report from within the assigned handout. So, this entire flow involves you having a teacher managed ID, a student managed ID, and maybe a couple of devices or even one device from which you'll be logging in and out, so you can test. But with all that, we can actually make this a little better and we created something that we call Developer Mode. What Developer Mode does is that it allows you to act as a teacher, so that you can create handout assignments, assign them to students, and view progress reports for your students, and it also allows you to act as a student, so you can consume those handouts and your app can report progress back to teachers. There's also a way to reset the development data and I personally had to use this button quite a bit, to be honest. With that, let's take a look at the classes we have in our framework. At the very top, we have a Data Store. It is used to maintain the Context Tree, within our app. This is also where the main app context is exposed as a property. And the Data Store also gives tracks of all modified objects within the system. So, if you'd like to save those object, you can call CLS Data Store, Save with Completion. I'd like to mention also that there is one shared instance of the Data Store that you use to interact with. So, next, let's talk about contexts. We saw that contexts are a three hierarchy, and Marin is actually going to go in detail later about contexts, how to use them and mention some interesting properties about them, but I'd like to concentrate on Context Identity. So, identity of a context in ClassKit is two things. The first one is the Identifier Property, and the Identifier Property allows you to uniquely identify contexts across siblings. What I mean by this is that it's okay for you to have a context with the same identifier, multiple contexts with the same identifier, as long as they don't have the same parent, which will be fine. So, that [inaudible] is something we call Context Identifier Path. Context Identifier Paths are what we actually use to uniquely identify context within the tree, within the context tree. And what Context Identifier Paths are, well, it's just an area of Context Identifiers. The way we use them is that we traverse the Context [inaudible] following the path, until we reach a note there -- the last note within the path. Let's actually take a walk how this looks in action. In this example, we have a context card that's pointing to Section 1 Context within some chapter, within some book. And we also have our Context Tree. To find Section 1, we will first visit the App Context. Then we'll visit the Book Context. We will find the [inaudible] of the Book Context with Identifier Chapter 2. We will visit that one, and we will do the same for Section 1. Since Section 1 is the last thing on our path, then this is what the path is referring to. So, we have a few ways to look up contexts with our system. One is using absolute context paths, and to do that, you would call the CLS Data Store Context Matching Identifier Path Method. The completion block of this method will return all contexts among the paths. So, for example, it will be an array of the Up Context, Book Context, Chapter 2 Context, and Section 1 Context. If for some reason, Chapter 2 in Section 1 was missing, then that array in the completion block will just continue to Up Context and the Book Context. And that gives you a chance to create the missing context as something else. There's also a way to look up contexts using relative paths. This is useful if you already have a reference to a context, but you would like to find a descendent of it. And the method for that is called CLS Context Descendent Matching Identifier Path. The difference is that here you will either get the context [inaudible] if it doesn't match the path. There's also a genetic way to [inaudible] contexts and this is done using CLS Data Store Context Matching Predicate. On the example from the slide, we're showing you how to find all children of a given context. So, there's also a Data Store delegate. This is used in conjunction with the path quitting methods that we just saw. The definition of the decoration of the delegation is on the screen. It's usually useful to use this when your contexts are downloaded on demand to -- when all of your context is not available, so you want to create things on demand. The way it works is that as we start querying for a path, for our contexts among the path, if you don't find a context on that path, we will call the Create Context for Identifier Method under Delegate that you've defined, giving you a chance to create the missing context. And if you do create a context, we will take the context and go add it to the correct place on the tree for you. So, as I mentioned, it's useful for app for -- with dynamic content. Let's actually take a look at how this work in action. Again, we have the same identifier path, but in this case, our tree is incomplete. And we also have a delegate. So again, we will visit the App Context. Then we'll visit the Book Context. Then we are going to try to visit the Chapter 2 Context, but it doesn't exist. So, we are going to ask a delegate to create it. If a delegate creates it, we will then attach it to the correct place on the tree, and we'll visit that context. Then we're going to build the same thing for our Section 1. Again, since section 1 is the last thing on the path, that means that this is what a path is referring to. Next, let's talk about Activity Objects. So, Activity Objects are actually the objects which you will use to report progress back to teachers. And they actually translate to Activity Reports. Activity Objects are always associated with contexts. You can officially allocate one on your own. And the way you would create a new one is by calling CLS Context Creating New Activity, which will return a new activity object associated with the receiver of the method. If you would like to see if there's currently an activity associated with your context, you can do so by querying for the current activity property on the CLS Context. I'd like to mention that every time you co-create a new activity, this is the same thing as starting a new attempt at that activity, meaning that a teacher would receive a new progress report. Let's take a look at how would you add progress to an activity. One way is to directly set the progress property. And that one is to add a progress range, from a start to an end. Also, know that setting the progress property directly is the same thing as adding a progress range for the start of zero. I'd like to also mention that it's perfectly fine for you to add overlapping progresses arranges or even the same progress arranged multiple times. Underneath, we take care of the details to make sure that the correct progress is reported to teachers in the end. So, there's one more object that we should talk about and that's Activity Items. What an Activity Item is, is a quantity that you would [inaudible] to represent to teachers as part of your report. Each activity can have one primary activity item, and this would be the main piece of information you would [inaudible] to teachers that's apart from progress. For example, this is useful for a final score of a quiz. You can also have multiple additional activity items, which you would use to add and show extra information to teachers. Like for example, how many hints were given to a student, answer to individual questions, and etcetera. So, Activity Itemization and Abstract Class, and in our system, we have defined at the moment, three subclasses. One of them is CLS Quantity Item. This is useful for simple scalar quantities like hints, experience points, and things like that. We also have a score item which is useful to represent X out of Y quantities. For example, final score of a quiz might be useful for this. And we also have a Binary Item. And this one is used to answer Yes or No questions, type of questions. Let's take a look at how would you add a primary activity item to an activity. Well, you will do that by first creating an activity item. In the [inaudible], we're showing you how to create a Score Item with a title of Total Score. And then to associate it as a primary activity item [inaudible] activity, you will set it -- you will set the primary activity item property of an activity. To add an additional activity item, you will again create a new Activity Item. In the [inaudible] size, we're showing how to create a quantity item with a type of hints, and then some quantity. To associate it with an activity, you would call CLS Activity Add Additional Activity Item after passing the item you just created. At that point, this activity item will be associated with activity. So, there's some best practices when dealing with activity items. One of them is to always save the subclass for a primary activity item. What I mean by that is that imagine a teacher assigns the same context to two students. You know, Student A device, you set the primary activity item as a score item. On Student B device, you set the primary activity item as a binary item. Well, if you do that, we can't actually generate an aggregated report back to teacher because there's no clear heuristic of how to convert from a score item to a binary, or vice versa. Because of that, it is best to always have the same subclass of an activity item as the primary activity item. Also, please provide clear, concise, but descriptive titles to your activity items. This is because the titles that you set on your activity items will be visible to teachers in their report. And please make use of additional activity items. They're a great way to provide the extra information that teachers will actually need to truly understand how students are doing within your app. With that, I'd like to invite Marin to the stage who is going to show us how all of this works in action. Hello, everyone. My name is Marin, and I'm an engineer on the ClassKit Team. And I would like to show you how to adopt ClassKit into a preexisting app. So, to do this, Pavel and I went ahead and built a sample app. The sample app is called Quizler. It's a simple, math, quiz-taking app. The very first screen that you get asks you to select what type of math you'd like to be quizzed on. So, let's go ahead and select the addition quiz. Once we do that, we'll get presented with another view that asks us if we'd like to view the scoreboard of all the users' high scores, or if we just want to start the quiz. So, let's go ahead and tap Start Quiz. Then, we get presented with each one of our questions. We're just going to answer them, and we get a total score at the end. So now that we understand what this sample app does, let's talk about the steps that we'll take to adopt ClassKit. So first we will talk about what type of activity -- context data makes sense for our sample app. After we do that, we'll discuss what type of student generated activity data, might also make sense. We'll make sure that we support deep-linking. Now, there's two ways to do this. The first way is to use universal links. So, if you already adopted universal links within your applications, you can simply just set the universal link property that lives on CLS Context. Now, our sample app does not support universal links, so I'm going to show you the second approach, which is to use NS User Activities Continue Restoration Handler. Then we'll make sure that we test our implementation using Developer Mode and the Schoolwork app. So, now let's talk about what type of context structure might make sense for this app. As a first approach, we might decide to do a one-to-one map of RUI to a Context Tree. And if we did that, we would end up with a structure that looks something like this. At the top level, we have our main app context, which is our Quizzler app. Beneath that, we have our Addition Context and a Multiplication Context. And then beneath each one of those, we have the Scoreboard Context and then the actual Quiz Context. Now, Pavel told us that a context is a part of our application that a teacher will assign to their students. So, if we keep that in mind, let's first talk about the Scoreboard Context. Well, what is the Scoreboard Context? It's all the users' high scores. Now, does that really make sense for a teacher to assign to their students? Not really. So, we're going to go ahead and remove that. Now, we're left with a structure that looks something like this. So, next let's focus on the Addition Context. What is the Addition Context within our application? Isn't it really just the addition quiz? It's really just one thing. It's not two. So, those really should be combined into one. So, let's go ahead and do that and then the same goes for the multiplication context. Now, we're left with a structure that looks something like this. Now, Pavel and I talked about it, and we decided that we may add a subtraction quiz and a division quiz in the future. And if we did that, we could easily add those as siblings. Now, when you all are thinking about which structure makes sense for your applications, don't just think about your current feature set, but also consider what you might be doing in the future, and make sure that your structure will easily grow with you. So now, let's talk about the context themselves. We want to make sure that we have clear titles, and that's because the title is the one piece of information that both students and teachers will have to know what this context represents. We also want to define our context as early as possible. For our sample app, we have static content. And so, we'll know what context to write as soon as the app launches. So, that's when we'll define ours. Now, we might also want to always display our context in a particular order for teachers that makes sense. So, for our sample app, that might mean our addition context should probably also be displayed above the multiplication context. So, to do that, we'll take advantage of the Display Order Property that lives on CLS Context. Now that we've determined what type of context data we're going to write, let's talk about the student generated activity. Here, I have a screenshot of the Schoolwork App, and this is a sample of what some activity data might look like. For our quiz, I think it definitely makes sense to track the total amount of time that a student spends taking this quiz. So, to do this, we can go ahead and call the Start and Stop Methods that live on CLS Activity. Now, it might also be nice to show what the total score was that the student received. And if we think about it, the total score is probably the most important piece of information about this whole quiz. So, we probably want to highlight it within the Schoolwork UI like we have here. Well, to do this, we can go ahead and create a CLS Score Item, and then we'll set it as the primary activity item. That way, it will get highlighted within the Schoolwork UI. Now, it might also be nice to indicate whether an individual question was correct or not. So, to do this, we can go ahead and create CLS Binary Items and we'll add each one of those as an additional activity item on our activity. So, now that we've determined what type of ClassKit close we're going to write, let's go ahead and see what that actually looks like in action. So here, I'm mirroring my display for you and have Xcode on screen. The first thing that I'm going to do is select my project's target. Once I do that, I'm going to select the Capabilities Pane, and we're going to find the ClassKit APIs right here. Then, all we need to do is make sure that we toggle that on, and we're all set to start writing some ClassKit code. So, first we're going to write our context and we said that we could do that as soon as the app launches. So, let's go ahead and open up the App Delegate. And in here, let's go ahead and create a function that will publish our context for us. Here we have our published context function, and what we're going to do is we're going to instantiate an instance if CLS Context, passing in the type of context that this is, giving it a unique identifier, and then a nice, clear, and concise title. Then we make sure to set our display order because we want our addition quiz to display above our multiplication quiz. Then, we're going to do the same for our multiplication quiz and [inaudible] a context and make sure to set the display order. Once we do that, we're going to create a dictionary of our context that we will need to create. Then, we're going to grab the parent context that we want to add these to. Since our structure is flat, we know that our parent context is always going to be our main app context. Now, let's issue a query to see if any of these contexts already exist. So, we're going to create a predicate, looking for all the context where the parent is the parent we just defined. Then, we're going to issue that query to the CLS data store's shared instance. For all the context that match, this predicate we just defined. This will return an array of context. We're just going to iterate over all of the context that we know already exist. Then, for each one of those contexts that's already there, we're going to remove it from our dictionary of context that we need to create. Then, we're going to iterate over the remaining contexts that do not exist, and for each of one those contexts, we're just going to add it as a child context to our parents. Then we call Save to Save our changes. Now, what we need to do is call this function when our app launches. So, let's go ahead and run this code to test that our contexts were actually created. Here, I have a device and I'm mirroring my display for you. So, awesome. Our Quizzler app launched. But everything looks the same. So, how do we know that these contexts were actually created? Well, this is where we use Developer Mode and Schoolwork. So, I'm going to hit the Home button, and open up the settings up. Now, I'm going to scroll down and I'm going to look for the Developer Settings. When I select the Developer Settings, I'm going to see some ClassKit APIs. If I select the ClassKit APIs, we can see that we can switch and act as the role of the teacher. So, I just make sure I select that. And now, we can hit the Home button and open up the Schoolwork app. Now, when the Schoolwork app launches, I first get a Welcome screen. I'll go ahead and dismiss that. And then if we look in the top right-hand corner, I can see that I have a plus button. That's how I know that I'm logged in as a teacher. If I tap on that plus button, I'll get the Create a New Handout view. And I can add an activity. Then I can select Apps, and we can see our Quizler app is showing up. If I select that, awesome, our contexts are there. So now, we've been able to validate that our contexts actually got created. I'd also like to point out that the addition quiz is displaying above the multiplication quiz. We've also now been able to validate that our display order was set correctly. So, let's go ahead and select the addition quiz. That's going to add that context to this handout. And now, let's just go ahead and send this handout to our class. I'm going to tap on the To field. Select my class. And let's just give a title to this handout. Now, I'm going to go ahead and hit Post. What this does is it sense this handout to my class, and it authorizes the context that's on this handout. So now, that context can start recording progress data. We can see that my handout posted successfully, and I can tap on it. Here we see the context that I've added, and I see my nice, app icon is right there next to the context name. If I tap on the icon, it should navigate me straight into the addition math quiz. So, let's go ahead and tap on that. Uh-oh. Schoolwork launched our app, but this is not the addition math quiz. Oh, yes. That's right. We forgot to add in deep-linking. So, let's go back to XCode and add that in. So, I'm going to go back to the App Delegate, and let's add in that Continue Restoration Handler. So here, the Continue Restoration Handler will process some user activity. We're just going to grab that User Activity and get the Context Identifier Path. Once we have the Context Path, we're going to instantiate our own internal quiz model that's associated with this context. Then, we're going to make sure that we call our same published context function, and that so, if the context has not been created, we'll make sure to create them. This will return an optional error, and we'll make sure to handle any errors. Then, we're just going to sync back up with our main thread and instantiate our storyboard and also instantiate the correct view controllers for this quiz. Then we're going to set our quiz on our view controller, and push on the appropriate views. So, now let's go ahead and run this code and test that our deep-linking is working. So, I'm going to switch back to my device, and then we're going to navigate back to the Schoolwork app. Here, let's tap on the icon one more time. Awesome. It launched us straight into the addition math quiz. So, now we're ready to start writing our user generated activity data. So, let's go back to XCode, and let's navigate to the part of our code that gets called when a quiz first starts. So, here, we said that we wanted to start the timer for our quiz. So, what we're going to do is we're going to issue a query to our CLS Data Source Shared Instance. And then we're going to query in our main app context for all of the descendants, that match the identifier path that's associated with this quiz. This will return an optional context. We're going to grab that context, make sure that we called the Become Active, and then we're going to instantiate a new instance of CLS activity. And that's because when this part of our code gets run, we know that the student is taking a new attempt at the quiz. Then we're just going to cash that activity and a property that we have defined. Then, we call Start on Activity to start our timer. We also said that we wanted to report the answer for an individual or question. So, I'm going to navigate to the part of my code that gets called when a student taps on an answer. So here, what we're going to do, is we're going to get that same currently running activity that we just created. Once we have that, we'll instantiate a CLS binary item, giving it a unique identifier, and then a nice, clear title. Then we'll make sure to pass in the type of binary item that this is. Now, we're going to set the value of whether the student got this question correct or not. Then we're going to add this binary item as an additional activity item on our currently running activity. The last thing that we said we wanted to do, was stop the timer and set the total score. So, we're going to navigate to the part of our code that gets called when a student ends an attempt. Here, we're going to get that same currently running activity. Once we have that, we're going to create a CLS score item, passing a unique identifier, title, and then we're going to pass in the score the student received out of the total maximum possible score. Then, we're just going to add that score item as the primary activity item on our activity. Notice, every time I'm doing this, I'm always setting the same subclass of CLS activity item. It's always a score item. Then, we're just going to call Stop to stop our timer and save to save all of our changes. So, now let's go ahead and run this code to test that our student generated activity data gets set. So, I switch back to my device, and now we have to switch and act as the role of a student. So, I'm going to go ahead and go back to my Developer's Settings. And the settings are all up here. We can tap on student and now I'm switched to act at the role of a student. Now, we can go back and open up the Schoolwork app. Here, if you look in the top right-hand corner, there no longer is a plus button, and that's how I know I'm logged in as a student. And we can see that I have the handout the teacher assigned to me. If I tap on that, we can see the context that I need to complete, and I can tap on the icon and it navigates me straight into the quiz. Now, I can an alert saying that progress data is going to be recorded on my behalf and sent to my teacher. So, I'll dismiss this. Then we just go through, answer all of our questions, and we get our total score at the end. So, we can see we got 100%. So now, let's go back to the Schoolwork app. Here we can see our activity data showing up. We can see the time and the total score. Notice the total score is highlighted right there in the main part of the [inaudible]. Now, we've been able to validate that our activity item was set correctly. I can also tap on the cell, and I get a popover of all the activity data I've written. I can see the total score and in light gray, I see the title of Total Score that I set. And then at the bottom, we can see the time, and then each individual question. And then also in light gray, we can see the title that I set for each one of those individuals questions. So now, we've been able to validate that our activity data was getting written. So, with that, I would like to reintroduce Pavel back on stage to go ahead and summarize everything that we've seen. Thank you, Marin. So, I'd like to mention some best practices about ClassKit. For one, declare your contexts as early as possibly so that they're available to teachers to assign. And also, Marin showed us that not everything [inaudible] needs to be a context [inaudible]. If it makes sense, take advantage of CLS Data Store Delegate. And also, please take advantage of additional activity items. They really are a great way to provide extra information that teachers might need to understand what the student is doing within your app. There's also some general best practices about education apps. One of them is to StoreKit dependence. And this is because in-app purchases don't really work within a school environment. That one is to support purgeable storage. This is a good idea in general, but it's real important in school environments where shared iPods are common, and space is a premium on those. Lastly, implement setting access via Managed App Config. Doing that, makes school admin's jobs a lot easier to configure devices. You can find links about all of this and the going details at developer.apple.com/education. Now, with that, have a wonderful rest of WWDC. Thank you for coming.  Good afternoon. Welcome. My name is Ken Ferry. Today Kasia Wawer and I are going to talk to you about performance in Auto Layout. Last time I was up here talking about Auto Layout was in 2011 when we first introduced it. So it's really great to be back up here chatting with you all today. OK, so Auto Layout. This is how we position content on iOS on the Mac. The main object involved as we know are views and constraints, constraints giving the relationships between views. When it comes to performance, I think the challenge here is that if all you've said is the distance between these two buttons should be 20, it can be hard to understand the step-by-step process that the machine goes through to do that and that means it can be hard to understand the expectations around performance and understand what's fast and what's not and generally how things are working. So that's our goal for this talk. It's to really understand that, to have a good feel for how things are going to work. So we're going to start things off by first showing some of the work that we've been doing in this release for iOS 12 around performance. We've been doing a lot of work, that's we get to give this talk. When that's done we're going to move on to trying to build that step-by-step understanding that I was talking about. So we have good intuition and good expectations around performance. To do that we're going to do something very unusual for us, which is to go into internals. So enjoy that please. Last, if you only ever rely on our intuition for performance, it's not going to work out very well. So we will then look -- Kasia will take over and we'll analyze code and we'll sort of show how to merge your intuition with, you know, practice. But let's get to it. So first as is traditional for an Apple presentation, we're going to look at a bunch of numbers and brag. Here we have, what we're looking at here is a benchmark. So the way we approached this work is we went out and looked a bunch of third party apps as well as our own apps and we tried to isolate what we saw happening into test cases that we could then benchmark. So this one here, what we're looking at, is UICollectionView with self-sizing cells and on the whichever side looks bad is iOS 11, which hopefully looked janky and bad. And on iOS 12 it's perfect. It's hitting full frame rate. So that's just one of the cases we looked at. Here are some more, just another sampling. We have a lot. These ones are all time. So what you're looking at is that the gray bars are iOS 11. How much time it took on iOS 11 and the blue are iOS12. So what you take from this is that we found a lot of places where we could make things really a lot better. And that will only improve your apps. That will make things quite a bit better for you I hope. This is going all the way up and down the stack. So some of it is in the real guts that affect just absolutely everything. Some of it's moving into UI kits. Some of it's up in the client layer so for in how people use Auto Layout. So if you look at for example that UICollectionView benchmark that we were looking at that's all of those. It does include stuff that's in the real guts but it also includes a lot of really important changes in just how UICollectionView uses Auto Layout and is more performance due to it. Which is a good segue to the rest of the talk, which is how you can do that too. So how to use it properly. When we were going through these I think a lot of the time the reason why we were able to make all these improvements is that we have a good mental model for how things are put together and how it performs, how it works. We want to help you develop that model as well. So to frame this we're going to go through an example case, some client code, that is not -- that has some issues and we're going to discuss why. So your code may or may not have this particular issue, but we did choose what we thought was the most common thing that we saw when we went through all these client apps. But even if you don't have this particular issue, the work we do to go through what's happening should be meaningful to everybody and probably new to almost everybody. So let's do it. This is the case we're going to go through so we're going to produce this layout, obviously very simple. Oftentimes I think you would build this in interface builder. That's a great idea. It is such a good idea that it would completely prevent the performance issues that we'd like to go through. So let's say we didn't do that. Let's say that we built it like this. First let's just walk through -- before we try to analyze it, let's walk through what this code is doing. First, we are overriding the UIView method updateConstraints, whatever that does. So we'll talk about it. Next, we have an Ivar called myConstraints. And we are taking everything in that variable and we are deactivating all those constraints. We are then creating constraints, which implement the layout that we were just looking at. That's fairly straightforward. It's using visual format language here. We're then activating those constraints, installing them, and last we're calling super.updateConstraints was an important thing to do because the UIView level implementation that this method does do work. OK, that's the basic structure of what it's doing and it does work, it's functions. But let's talk about what it's doing more concretely now so that we can understand the performance. So the first thing to understand is what exactly is updateConstraints, this method we're overriding. Well, it's one component of the Render Loop. The Render Loop is the process that runs potentially at 120 times every second. That makes sure that all the content is ready to go for each frame. OK, so it consists of three phases -- Update Constraints, Layout, and Display. First every view that needs it will receive updateConstraints. And that runs from the leaf most views up to the view hierarchy towards the window. Next, every view receives layout sub views. This runs the opposite direction starting from the window going down towards the leaves. Last, every view gets draw if it needs it, that kind of thing. OK, what are these for? Why do they exist? Well, they all have the exact same purpose and they have exact parallel sets of methods. And that purpose is to avoid wasted work, which I can explain by example. So a label, a UI label needs to have constraints that describe the size of its text, OK? But there are many properties that contribute to that size. There's the text property itself, there's the font, and the text size, etcetera. One way to do this would be that every time one of those properties changes go re-measure your text. However, that would often be pretty inefficient because you usually change a bunch of these right in a row. When you're first setting up a label, you're probably going to call a bunch of these property setters and if you're re-measuring text on each one, all the intermediate ones are wasted, you really just want to measure at the end. And that's what the Render Loop gives you. Because what you can do instead is that inside a set font you can just call setNeedsUpdateConstraints and then you're guaranteed to get update constraints at the end before the frame goes to the screen. And that's what it's for. So the couple things to understand from this before we move on is number one it runs a lot, 120 frames a second. Number two they're parallel. So you can use that for intuition as well. If you feel like you understand the layout pass or have some feel for that, same deal when you're thinking about UpdateConstraints or you're thinking about display. And then the last thing being that the whole reason it's there is to avoid wasted work, to defer work and possibly skip it entirely. All right, so having looked at that we are now in position to analyze the rest of this method. See how we are -- every time it's called we're deactivating constraints and then activating them again new ones. We are saying this is analogous to layoutSubviews. So if we wrote the exact same code in layout Subviews that is the analog, that would be as if you -- every time layoutSubviews was called you destroyed all your Subviews, created them from scratch and then added them again. And I think a lot of people have the completely accurate intuition that that's not going to perform very well. So the thing to really get is that it's the same. Whatever intuition you take from that apply it to updateConstraints as well. When you are ripping down constraints like that you're doing a lot of extra work. So how do you fix it? Well, you need to use -- as we were saying, you need to make sure that you're not doing it more than once. It's for deferring work. So it should be something like this, we say did we already do this work? If we did then just don't do anything at all. If we haven't done it yet, then sure set up those constraints once. And that will perform well, OK? So this is again, this is actually the most common error that we see in client code, which is we call it churning the constraints. Unnecessarily ripping them down and putting them back up. OK, great. We are going to do more but stepping back for a second now to think about the Render Loop for a little bit. The Render Loop is great if you actually need it. The purpose again, it's really useful for avoiding that redundant work. But it's also dangerous because it runs to often. It's very sensitive code. So in a case like this usually what you want to do about sensitive code is not -- like, you should take care if you're writing it but you should also try to minimize how often you write sensitive code because, you know, you're probably going to screw it up. We all do. So in this case, in fact you might be, you should really think again like could I just do it once and not put it in updateConstraints? And a good way to do that is use Interface Builder. If you can use Interface Builder you should. It's great for all sorts of reasons. It puts you on a good path. OK, so that's great. We've now talked about that. I think we have a better understanding for why that's problematic, at least somewhat by analogy sub use. But for the purposes of this talk we want to do better than that. We don't just want to say this is bad. We want to really understand it and understand the process. So to do that we're now going to peel back the covers and start to really see what really happens. So when we activate these constraints, when we add the constraints, what is the process that occurs? Let's diagram it out at a high level. So if this is the view that we're adding the constraints to, this view is in a window. Hanging off the window is an internal object called the engine. And the engine is the computational core of Auto Layout. When the constraint is added what will happen is that we make an equation, which corresponds to the constraints, and we add that equation to the engine. The last object to understand in the diagram is that the equation is in terms of variables where a variable is like, you know, if I hand you an equation and I say solve for X, X is a variable. The things that we need to solve for in this case is the frame data of a view. So there will be four variables for every view, which is the min X, the min Y, the width, and the height. OK, so let's go into this process. So this was the layout we were going to do. We're going to focus just on the horizontal constraints for simplicity, but we're going to follow through the process. So the first thing that happens, as we said, is we make these equations, which look like this. These are pretty straight forward. The most interesting one is I think the space between the two text fields, which looks like we're saying it looks very, very similar to what you say with the constraint but it's somewhat lower level because it's in terms of these variables. OK, then each of those equations needs to get added to the engine. And we're actually going to follow along that process again with the goal being to have a good feel for the performance characteristics. What is happening when we do this? So the engine is trying to solve for these variables, which is something you may have done in algebra and it actually looks exactly the same. So let's follow it. So first equation comes in, says the first fields minX is 8. Cool. Its width is 100, fine. OK, when this one comes in we say the second field's minX is equal to the first minX plus the width plus 20. What would you do in algebra if somebody asked you to solve for these variables? You would substitute out for the ones that you already had in there. And that's exactly what's going to happen. If you are profiling, you'll see there is a genuine method in the engine that contains the word substitute as well as another 140 characters because we are Cocoa Code Programmers. But and that's what it will do. And then, you know, and the last equation comes in and this looks done. it looks like that was all the work that had to happen at least in this case to solve for those variables and that's true. That's what I want to understand at this point is that the work that happens is both not very complicated. It corresponds very, very, very closely to what you would do if you were doing it by hand. And it's also not very expensive. It's just sort of substituting out like this. That's the work it does. OK, so now we have sort of solved for these variables in the engine but that's not layout. So let's finish the process. What happens for the rest of the process is that whenever the engine sort of assigns a value to one of these variables, it's going to inform the view that the variable came from and say, this thing changed. What will the view do in response to that? Well, if you think about it for a minute it will call it Superview and say hey, setNeedsLayout because it needs to move. OK, that was all happening as part of the update constraints phase. We now just receive setNeedsLayout, so at some point it will move on to the layout phase. Then, OK, so the last piece of the puzzle is that we'll receive, UIView will receive layout Subviews will do is it will copy that data from the engine into the frame. So it will just say engine, what are the values for those variables? Engine will tell it and it will just call set Superview of that view we'll call setBounds at setCenter on that Subview. And that is the entire process. So just step back and think for a second. Like, that is the step-by-step process of Layout. If you can try to internalize that and get a feel for it, you're going to have a much, much, much better feel for performance expectations around this stuff. In fact, let's see how that's going right now, because now when we look at this and we look at this method that we were looking at that where we're deactivating constraints and we're reactivating constraints, think about what we just did and think about what the engine is going to be doing. It's going to look like this. Which we call churning [laughs]. So each operation it's doing is not super expensive, but it's doing a lot of them and it's just completely unnecessary. This work is wasted. So if you can feel this in your heart, if you can really feel that this is what is happening when you do this, then you're going to be in good shape. Then that's -- you're going to be in the same position we are to go through and really get a good feel for this. OK, so I hope that's great. There's one other big topic that we want to cover though. If we want to really have a good performance model is this idea that you only pay for what you use with Auto Layout. And having looked at this, I think we're in a good position to understand what that means, OK? To do this, let's say we double the situation we had before. So we have four text fields in two sort of independent hierarchies. Now something you can do is you can make a constraint that crosses the hierarchy like this. That goes -- that you can say, well text field one should be aligned with text field three even though they don't have the same Superview. I think sometimes people have the impression that because this possible, it means that things are going to be generally quite slow because anything could affect anything at any time and so it's just sort of a giant ball of mud and performance probably sucks. OK, but having looked at what we've looked at, let's see what happens in the common case where you don't have this because most of the time you don't. Most of the time views are only constrained to their parent and to their siblings. What you'll see there is that since we have these two independent blocks, that will give, if you look inside the engine it will be two independent blocks of equations that completely don't interact with each other, that don't have any overlapping variables. What that will do, is that because they completely don' t overlap, they just don't interact. And if we have one of these it will take some amount of time to deal with. If we have two of them it will just take twice the time because they have nothing to do with each other. Three of them, three times, etcetera, the point is you're going to see a line. You're going to see linear performance, which is the best you can get. That's perfect marks for this kind of thing. So I want to stress this again, the reason why it's linear is because there aren't any dependencies between these pieces. If you do have a dependency, then it will tie those blocks of equations together and that will be somewhat more, you know, more computation to deal with but that's only if you use it. And of course if you do have something like that, you know, if you're doing it by hand of course it's going to be a little bit more expensive that's what you expect. You're doing something more complicated. So it's kind of this usual thing that we often aim for in Cocoa, which is that the simple things are simple and the complex things are possible. In this case it's more like they cost a little more. But you're not paying for it if you're not using it, which is actually the right way to think of the whole engine in terms of intuition again, you can think of it as a cache for layout and as a dependency tracker. It's very, very targeted. It understands which constraints affect which views, and when you change things it just updates exactly the things that are needed. And this has implications on how you write code too. Sometimes we see -- one issue we sometimes see is people taking great pains to avoid making constraints because they have the impression it's going to be expensive. But actually, it's very, very targeted. As long as the constraints that you're making correspond closely to the problem that's being solved, it's pretty unlikely that whatever you do, if you tried to dodge it, it's going to be more performance. Oftentimes we'll see people doing very complicated measurement and adding things up and sort of trying to pull information out and then push it back in and that's almost always more expensive than just directly expressing as a constraint what you're after. Now the converse side of that is that sometimes we see hierarchies that look like something like this where we see lots and lots of constraints and lots of priority and it's really not clear what's happening and this is a -- usually this is a telltale sign of this being the situation that there's actually just two completely separate layouts that someone has in mind and we're trying to sort of pack them together into one set of constraints and do it all in one. And that's also not a real good idea. So that will -- that creates a lot of false dependencies, places where it seems like things interact that they really don't. It's also nearly impossible to Debug, if you haven't noticed. So the overall advice is try to model the problem as directly as possible. Kasia is going to walk through this kind of case where you're switching between different layouts and show that a little more explicitly. But that's the general advice. Just use it in a natural way. It's better for both performance and for understandability. OK, so that most of what we have to say. But since we're trying to build an overall mental model of the performance characteristics of Layout, I want to at least make sure we touch on all of the major features. So there are some other things you can do. And let's discuss. So you can say that some particular view should be at least 100 points wide. You can use inequalities. What does that cost? Very, very, very little. Compared to just saying it's equal to hundred points wide. Since we went internals a little bit, it's going to coast exactly one more variable. That's it. You can also call set constant. The example use case for this is something like I have a gesture recognizer and I'm kind of trying to drag a view around and what I'm going to do is every time I receive a call from the gesture recognizer I'm going to take its translation and I'm going to pump it into a constraint by calling set constant on that constraint with that translation value. OK, what that's going to do is we talked about how the engine is a dependency tracker. This exploits that to the maximal degree. So that's sort of a very, very, very fast one step update of just exactly what has to change due to this one constraint changing. So that's a performance optimization. That's why we even have this method set constant. Last to talk about it priority. So here you can say, you know, you can say this view should ideally be 100 points wide, but if something else prevents that just please be as close as possible. This does incur some more work, some amount of work. So let's talk about that a little bit more. Another way to think about that is to say that the width of that field is going to be equal to 100 plus some error and please minimize the error. That's what you're asking for. So there is an error minimization phase I didn't discuss before. So when the view asks the engine as part of layout subviews and says, hey what's the value for these variables? The engine needs to make sure that all of those error terms have been minimized first. And this is actually, this is -- I'm not going to go into how this works but I am going to talk a little bit about performance characteristics and I'm also going to say that's super neat. So you might want to look this up. This is the simple X algorithm. This is what we're really doing. It's super old. It was developed during World War II. What you might note is before computers. In fact, the people who used to be called computers, before there were machines that were called computers, this is kind of what they're doing. They're doing it by hand, which does give you some feel for the performance characteristics. It must be pretty fast if you do it by hand. And it is. It's pretty much the same stuff we've been doing. It's more substitutions. That's how you should think of it. Anyway, but it does -- you know, when you use priority it does cost at this level so that's just something to be aware of. OK, and other than that it's just same as before. So that's what I wanted to talk about. So that is our attempt to build this intuitive understanding of the performance characteristics around Auto Layout. So quick review of what we talked about. Try not to churn your constraints. That's when you're doing all this work that just doesn't matter. So don't do it. When you do work with constraints it's basic algebra and that algebra is happening when you add constraints, when you remove constraints, when you call set constant, that's the primary times. And then also, you know, when we have this error minimization phase. The way to think about what Auto Layout does is that it's a cash for your layout, we saw the engine sort of contains all those solved values and it's a dependency tracker so that when things change we can update those values in a super, super targeted way. Which leads to our last point, which is that you only pay for the features that you're using. That's what we talked about. You know, that's your intuition. And for the rest of the talk I'm going to turn it over to Kasia because if you, again, if you only rely on intuition, things are not going to go well. So she's now going to go into some analysis, avoid we talked about and putting that intuition into practice. So please enjoy. Ok let me get to my slide here. Thank you, Ken. Hi everybody. My name is Kasia Wawer. I am a member of the iOS Keyboards Team and we use Auto Layout and we love it. So I get to talk to all of you about building efficient layouts. All right, let's go back to Constraint Churn real quick here. Constraint churn as we heard happens when you change your constraints but the actual views don't need to move so you're sending extra work to the engine and enough of that can affect your performance. So you tend to want to avoid it. So let's talk about how you might run into this problem and how you might get out of it. So we're going to work with a spec here. This is for a social media type app. There's an avatar view that shows you who is sharing. There's a title, a date, and a log entry view and for that you're going to need some spacing, you're going to need some sizing and you're probably going to need some alignment too. But this is actually not a pure social media app. It is a semi social media app, where you can choose whether you want to share things. So there's also optionally a view that says that you've shared and who you've shared with. And no social media app would be complete without the ability to share cat pictures. So that's another layout that you might have to put in. And maybe you don't even want to share that cat picture because it's just too good, you want to keep it to yourself. So we have four very similar layouts. They're not the same and there's going to need to be some adjustment when these table view cells come on to the screen. If I didn't mention it these are in table view cells. And let's say that you are working on performance in this app and you ran it for the first time and this is the scrolling performance you got. And there are a lot of hiccups there, especially on the scroll back to top. So you're like, OK, how do I improve this app? What's going on? So I get to introduce something new today, a sneak peek into something we're working on. This is not actually available in the beta but stay tuned because we're going to be introducing an instrument for layout. And, OK. I'm glad you are excited. That's good motivation. Anyway, let's take a look at what's here. The top track is your standard how much CPU is being used. And this is sort of your canary in the coalmine view. If there are a lot of peaks here you have an indication that you might have something you need to look at in your layout. And if it's pretty flat, probably your performance problems are originating somewhere else. Below that we will be specifically tracking constraint churn. The height of the bars in this instance correspond to the number of views that are experiencing constraint churn. So when you see a big piece there you know a lot of views are affected. We're also going to show you how to remove and change constraint instances and finally sizing for UILabel and other text views. This one says UILabel because that's what's in this app. It's also going to track other types of text views as well. So this was taken with that app scrolling, so what do we look at here? There are several peaks in the CPU view but let's zoom in on this one because right below it I see a big jump in constraint churn and that's a little concerning. So if you highlight this view and go down to the detailed view in instruments, what you'll see is a list of the views that are affected by churn by view description. And we are grouping them by Superview so that in an instance of say Table View Cells, it's easier to see that it's happening over and over in a specific context and not different ones. So in this instance we see that the avatar view and three labels are experiencing churn. And since I am the one who ran this through the instrument, I know that these labels correspond to the Title Label, Date Label, and our Log Entry Label. That's almost all of our views in this cell. That's a little concerning. Let's see what happened. All right, back to our spec here. So look into the code and find that UpdateConstraints is being overridden. And in that method when anything changes or when UpdateConstraints runs at all, we're removing all of the constraints and then adding back the ones that we think we still need. Well, everything landed back in the same place where it started. So that removal just is contributing to performance issues. So in the instance of the social label here, social avatar thing, being added and removed, we don't actually need to pull it all the way out. When you look at the constraints around this view, you'll see that they don't actually interact with anything else, just that particular view. So here you can use, you know, this neat little feature called setHidden, maybe you've heard of it. And because it's not affecting any of the views around it, it's just going to disappear, it's constraints stay in place and this is a very, very, very cheap way to hide and show views, rather than removing them from the hierarchy. So that's fine. But this is a really simple example. What about the image view? All right, so for the image view, again we might we might want to try removing all constraints and then adding back the ones we already had plus the image view ones. And again, everything is landing in the same place so we're experiencing churn. Well, in a situation like this how I want you to think about it is to look at groups of constraints. So let's start with this group that I'm highlighting here in green. These constraints stay the same in every one of our layouts. Once we're doing the hide and show on the sharing view that doesn't need to change, the avatar view never moves, and the labels never move other than the log entry label being able to get longer. So those green constraints should be added when you create the views and then left in place. Don't touch them. They want to stay where they are. But now we have the four constraints that are controlling the image view. So what do we do with those? Well, let's stick them in an array and let's also take the constraints that are going to be there when there's no image. And I very creatively named these imageConstraints and noImageConstraints so you can keep them apart. And let's, when we're getting to the point where we're going to be putting in this image view or taking it away, let's see what layout we're currently in. Deactivate the noImageConstraints if we need to and activate the ones for the image. If we don't have an image coming in, you know, all of our other constraints are already activated, we just have the one that we're adding. Now I put these both in arrays despite the fact that this is a single constraint because it simplifies things in my code. I don't need to check and see whether I'm dealing with an array or a single constraint, I'm always dealing with an array of constraints. Your mileage may vary though. So the nice thing about this is that if you are tracking your constraints properly like this and you know that you want to add this image view live in front of the user, you can deactivate these noImageConstraints, activate the ImageConstraints and call Layout in needed inside a View Animation block and it's going to animate nicely into your view. If you tried to do this with deactivating all of your constraints and putting them back in, it would look very interesting. Let's say it that way. All right, so now that we've debugged this and we're working with groups of constraints instead of throwing everything at it, let's look at what it will look like. This is what it looked like originally just to remind you. Let's scroll to the top. It's very bad. And this is what it looks like after we've debugged it. And that is much smoother. Thank you. But wait there's more! I actually took this video on iOS 11. This is not taking advantage of any of our performance improvements in iOS 12. This is just the client code doing things more efficiently. In iOS12 it also looks great. And of course fabulous [laughing]. Yes, it's great. So how do we avoid constraint churn? Well, avoid removing all of your constraints. That's usually going to put you into a situation where you have to put a bunch of them back and that's going to land you in a position where you're relaying out frames that don't need to -- or relaying out views that don't need to be laid out again. If you have a set of constraints that going to be common to all of the potential layouts in your app, add them one and then leave them alone. This is a good use for Interface Builder and the initial layout of your app. Changes to the constraints that need changing but don't change the ones that don't need changing. Kind of tautological but it seems good. And then you have a neat trick now for hiding views instead of removing them when that makes sense. All right, so that was constraint churn in the instrument. We also have that view at the bottom that said UILabel sizing. UILabel sizing is tracking the amount of time it takes for the label to calculate its size. So let's talk about intrinsic content size. I'm going to take a walk over here. OK, not all views actually need intrinsic content size. Some views do. Views with non-view content like to return a size for their intrinsic content size based on that non-view content. Two examples of this are UIImageView, which uses the size of its image to calculate its intrinsic content size and UILabel, which measures its text and uses that to return its intrinsic content size. There's nothing really magical about intrinsic content size, it's used to create constraints by UIView. It makes sizing constraints for you and that's it. You can define all of your sizing in constraints yourself and skip this whole thing. There are a few circumstances where it needs to be overridden, that's what it's there, as we saw there are these couple of things plus some other examples in UIView subclasses. But a lot of the times it gets overridden because the assumption that it's either faster or more exact and it is neither of those things. However, there is a circumstance where overriding it might help your performance. Text measurement can be expensive. In our app here the UILabel sizing did not take very long. It was very short durations. So messing around with that is not going to improve the performance of that that much. But if you have a text intensive app and you're seeing a lot of time happening in the UILabel text measurement or you have text view text measurement or whatever else you're using, you might be able to help it along if you have some additional information. If you know the size that the text needs to be without doing all that text measurement, you can return that size and intrinsic content size, or if when you're going to be putting this view on the screen the constraints are going to be fully defining the size regardless of the size of the text inside of it. For instance, the constraints are always going to make it larger than the amount of text you have. Then you can go ahead and return no intrinsic metric for the width and height in intrinsic content size. And what this will do is tell the parent, hey I already have my size, don't bother to do the text measurement. So obviously this only works if you're not trying to detect measurement yourself, but it can help some apps improve their performance. So I wanted you to know that this little trick exists. And we can't talk about intrinsic content size without talking about system layout size fitting size because people often conflate the two even though they're kind of opposites, so that's kind of unfortunate. Intrinsic content size is a way that you communicate size information to be put into the engine. System Layout Size Fitting Size is a way that you get sizing information back out of the engine. They're actually kind of opposites. So this is used in sort of mixed layouts where there's some reason that you need frame information from a view that manages its subviews using Auto Layout. Not very frequent usage but is there and can be used. I want to tell you how this method works because it might be a little more expensive than you think. When you call System Layout Size Fitting Size an engine is created. Constraints are added to this engine, the layout is solved, and then the size of the top views frame is returned, and then the engine gets discarded. So each time you call this method an engine is being created and discarded. While this is fine for small uses, if you're doing it a lot you can see how it might start to build up over time. So be cautious when calling System Layout Size Fitting Size. One of the uses that we sometimes see people do is forwarding that call from their self-sizing collection or table view cell to a content view. And when you do that you're actually overriding some optimizations we've made to make that scrolling, scrolling in that view faster and you're adding extra engines. So if you're currently doing that and your scrolling is no good, maybe look into that. All right, now we come to my very favorite topic in the world. Unsatisfiable Constraints. OK, so what are unsatisfiable constraints? If you haven't run into this before, this is what happens when you do something like, hey this view should be 50 points wide, also it should be 200 points wide. Well, that's not really going to work. These are not actually quantum phones. You know, I can't speak to the future but so the engine has to kind of calculate the fact that there's no layout available and break a constraint in order to generate some sort of layout for you. When it breaks that constraint it sends a very detailed log to your debugger, possibly you've seen it, that says, hey unsatisfiable constraints. Here's the constraint that I broke and here's all the other ones that affected, that caused me to have to break it. So this can sometimes affect performance directly and it can also mask other issues. So it's a good idea to get them debugged. And Mysteries of Auto Layout, Part 2, had some good debugging information so you might be interested in checking that out if you have been having trouble with your unsatisfiable constraints. OK, guys you've graduated. Congratulations. You are all Auto Layout experts and, you know, I hope that you really enjoyed learning about the internals of how it works. Now you know better how to think before you update constraints and understand the process they go through, you've got some tuning information with size and priority and inequality, and you have faster layouts in iOS 12 so that's awesome. We're going to be in the Labs tomorrow if anybody has questions. And we've got the link here for information in related sessions. Enjoy the rest of your week.  Good morning. Good morning, and welcome to Managing Documents In Your iOS Apps. I'm Brandon Tennant, a software engineer here at Apple and the Document Management team. And, later I'll be joined by my colleagues Thomas Deniau and Rony Fadel. So, today we're going to give you an overview of what document management on iOS is. And, then we'll share a few new things in this area. Then, Thomas will come up on stage to show you how to use the Document Management API in your apps. And, finally, Rony will join us onstage to show you a few tips and tricks for raising the bar in your application. So, let's get started with a refresher about what document management on iOS is. It's a collection of technologies. First, there's an API for application developers. For cloud vendors, there's also the File Provider API. And, there's also the Files app. In iOS 11, we introduced UIDocumentBrowserViewController. And, also significantly revamped UIDocumentPickerViewController. Using this new controller in your app, your customers will be able to manage and browse their files from all of their favorite cloud vendors. There's also powerful Search that searches across all of these cloud vendors. They'll be able to favorite Folders, and those favorite Folders will appear in the sidebar. And, they'll be able to organize their files using Tags. Recently used files also appear in the Recents tab. So, by using UIDocumentBrowserViewController in your app, you'll be giving your customers a consistent experience, by bringing all of the power of Files into your app. It eliminates the need for you to write your own document browser. This allows you to focus on perfecting the experience your customers will have when viewing or editing documents in your app. To get started with this, see last year's session, "Building Great Document-based Apps in iOS." There's also API for cloud vendors. This is called the File Provider API. You use this to write file provider extensions, and also custom UI actions, including the account sign-in screen. For application developers, you don't need to use this API for people to access your files. So, when you write a file provider extension, it will appear in the Files app, and in UIDocumentBrowserView Controllers, under the Locations tab-- under the Locations section. Also, because sign-in is implemented using FPUI ActionExtensionViewController, you'll want to use this so you can reduce friction for your customers by making logging into your service easier. Let's go through a few examples of that. First, in this example, what you can do-- or, what the user is able to do, is sign into their service, without being interrupted from their workflow. They don't have to leave the app. They can just sign in and continue going along their business. An example of what to try and avoid, would be something like this, where there's redundant UI elements or branding onscreen. You can see the example of two nav bars, or two Cancel buttons. And, another example to try and avoid, is having a simple Sign In button, where all it does is launch to your application. This interrupts the customer's workflow. So, try to avoid doing that. Lastly, we've seen great adoption from all of you developers and cloud vendors over the course of the last year. So, there's a lot of applications that use this technology already. So, now is a great time to try adopting UIDocumentBrowserViewController in your application today. And, if you're a cloud vendor that hasn't yet already provided a file provider extension, now's the time to do so. So, let's talk about a few new things. First, for cloud vendors, we've seen some of the difficulties in making a file provider extension. And, what we're happy to announce today is there's a new tool for you. It's called the File Provider-Validation tool. This tool will guide you-- will help-- will run tests, and show you the issues with your file provider extension, and guide you towards fixing those. It's available for download today from developer.apple.com. And, when you download it, you'll get a couple of things. You're going to get a set of source files that you'll need to add to your file provider extension project, and an iOS application to install and run on your iPad. Once you've modified your file provider project, installed the app on your iPad, and run it, you'll be greeted with a screen that looks like this. Your file provider will be listed-- will be shown on the left side, and when you tap that, there'a Run button that you can press to run our full suite of tests against your file provider extension. When you've done that, you'll see a list of successes and failures. You'll want to tap on the failure to dig deeper, and find the issues in your code. We'll be able to help you through this at the labs on Thursday and Friday. Also, an exciting new thing this year announced was Siri shortcuts. And, we're happy to say that Siri shortcuts are coming to file provider extensions. What this does, is it surfaces recently used documents that were opened or created, and it makes them visible either in Search or on the Lock Screen. You'll also be able to-- your customer will also be able to make shortcuts that are synced across devices. And, it's important to note that they're synced across devices, because in order to support this, your file provider will need to use unique identifiers for each file across all of the devices for a given customer. If this is true, or you can make this happen, all you need to do to support Siri shortcuts in your file provider extension is to add NSFileProvider UsesUniqueItemIdentifiers AcrossDevices to your file provider extension's info.plist file. Once you've done that, and submitted to the Store, you're done. For application developers, there's nothing you need to do. For information about Siri shortcuts, check out yesterday's session, "Introduction to Siri Shortcuts." Also this year, we're happy to announce that we're releasing the Particle Sample app. This was sample code that we used in last year's session to demonstrate how to use UIDocumentBrowserViewController in your app. It looks like this, and it uses UIDocumentBrowserViewController as its main viewController. It defines a file format. It implements state restoration, and imports assets using UIDocumentPickerViewController. You'll be able to download this today. The link is available in the WW app, and you'll also be able to search for it online. And, that's Particles. So, with that, I'd like to introduce Thomas onto the stage. Hello, everyone. I'm Thomas, a software engineer on the Documents Management team. And, I'm going to take you through how you can leverage the document management features of iOS in your app. Together, we're going to discuss what the Document Picker and Document Browser are, when and how to use them. Then, we'll add a document picker to our demo app, Particles. And, finally, we talk about document types, and how to configure them properly in Xcode. So, what does interacting with documents mean nowadays? Of course, it means that you need to provide UI to let your customers organize their documents. Starting with the ones in your app container. But, because most documents are in the cloud nowadays, it means that you need to let them access documents stored in the cloud, and in other apps. How do you do this? Well, we have two solutions for you. The Document Picker and the Document Browser. What are the differences between these two? Well, both let you browse files stored locally on your iOS device, and in various cloud services. And, both let you access files from other apps. However, they have different use cases. You [inaudible] Document Picker shortly, for a short period of time, for a quick user interaction with the files you want to reach. The Document Browser, on the other hand, is supposed to be the main document browsing UI of your app. Let's start with the Document Browser. As Brandon mentioned, the document browser is what you see when you launch any of the UI applications. If your app is document-based, you should probably use a document browser to display these documents. And, if you choose to do so, this is what your customers will see when they launch your app. They immediately get to the files, which is what matters. The Document Browser is full screen, and it lets your customers open and organize the documents that your app handles. As Brandon mentioned, it has all the features of the Files app. So, Search, Tags, Favorites, Recents, all of this comes for free, and we do all of the editing for you. It can be customized with your own buttons, such as the [inaudible] button in the top right corner, or to match your application's look. As you can see, numbers and keynote look very different, yet both are based on the Document Browser controller. Of course, once your customer opens a document, you get to present your own UI on top of the browser to present your documentation UI for example. Now, because the Document Browser is the entry point of your app, it is best to make it the root view controller. However, we got a lot of feedback over the last year from you, saying it was not always easy to make it the root view controller. If that is the case, we want to clarify the guideline we gave last year. The Document Browser can also be presented [inaudible] full screen, likely shortly after launch. However, it is still meant to be the starting point of your app. In document-based apps, what we should see first are the documents. So, we do not recommend making it possible to get out of the browser. If you think you need to hide the browser, maybe what you might be looking for is Document Picker, which we are going to discuss next. How to get started with a new Document Browser controller. You can start a new app in Xcode, and use the document-based app template. And, this template gives you Document Browser [inaudible] based app. If you have an existing app, and want to add a DocumentBrowserViewController to it, you can drag a Document Browser to your storyboard, and use the Initial View Controller checkbox in Xcode. Once you have the Document Browser controller, you can customize it. You can add your own buttons to the bars and menus. And, you can set your own colors and themes to match your app's look. You can also customize the open and close document animations so that you get a nice zoom-in transition from your document's [inaudible] to your own document addition UI. To learn all about this, please watch last year's session, where we have covered this in detail. And, as Brandon mentioned, we have made the sample app that session last year, available as sample code today. Now, the Document Picker is a different thing. You use it to let your customers open documents or assets stored in the cloud, or stored in other apps, but it is only shown to let them choose a file, and then it goes away. The Document Picker is what we use in Mail. When you want to attach a file to an email, you tap the Add Attachment button, and this sheet pops up. This is a document picker. You choose a file to attach, and the sheet goes away. Same thing in iWork. If you want to attach a PDF, insert the PDF in your Pages document. You're going to tap this Insert From button, and the same sheet pops up. You choose a file, it goes away. So, if you need to display user interface to let your customers open-- view things from the cloud, and then hide it once the file is chosen, the Document Picker is a great option for you. Why do we need to give you the Document Picker? Because as we have said, documents might be in multiple locations. They can be in the cloud, or in other app containers. And, by default, you do not add access to these. So, you need something to let you access, copy, and move documents between these locations, and this is what the Document Picker does. It goes away once the files get chosen, so if you want something which stays onscreen for a long time, you should check out Document Browser instead. What can you do with Document Picker? You can use it for multiple things. You can use it to access files in the cloud directly. Now, creating copies, you can use it to move files to the cloud to implement a safety cloud feature. And, getting copies of documents is often confusing, but if you really want to, you can use it to copy things from and to the cloud. So, let's say we want to let our customers access a movie they've stored in the cloud. This is a great use of Document Picker controller. You create a Document Picker controller, and you specify the types of file you want your users to choose as a first document. And, the second document is one of the constants we have described on the previous slide. Then, you set the Document Picker delegate, because this is how we get notified that your customer has chosen their file, and you present it. Once the file is chosen, you're going to get called back at this delegate method. You get [inaudible] of your files in this method. You can use them directly. And, [inaudible] single URL, because there's a property you can set in the Picker to let your customer choose multiple files at once. So, let's go ahead and add a Document Picker to our demo app, Particles, from last year. This is the app [inaudible] last year. It is based on the Document Browser to let me choose a file to open. As you can see, I have Recents, I have Tags, I have Search, I have Browse to list all my sources. All of this comes for free. Now, I can go ahead and open this file, and I can tweak this nice particle system. Now, my customers love this app. But, they would like to make the particles a little nicer. So, they've asked the designers to design some better-looking particle images, and the designers have shared these images over iCloud. So, how do we get these images to customize them? What we're going to do, is add a Choose Image button in the navigation bar here to pick a file from iCloud, and customize these particles. So, you can open your storyboard, and here I am [inaudible] the controller. I'm going to drag a new bar button item to that editor. And, I'm going to animate Choose Image. [inaudible] Choose Image. OK. Now, I get to write some code to present to the document picker of the controller. So, I can drag an action from my button to the source code, and I agree to call it Choose Image, and [inaudible]. Connect. And, now I have twice my code. So, let me display this in a bigger window. Here I am in my chooseImage method. I'm going first to create a Document Picker controller. And, I specify the type of files I want my customers to choose. So, here this is UTTypeImage, and the mode. I'm using .import here because I want to embed the particle image in my document. But, in many cases, you will use [inaudible] here instead. You set your document picker delegate, and you present it. When the Document Picker gets invoked, and the file is chosen, it going to call you back. So, we need to implement the document picker [inaudible] at method. I don't know if you noticed, but I have not set a property to choose document files. So, I'm going to check I have only one URL here. Get the first URL, and create an image from it. And, set it in our particle system, which is [inaudible]. Let's run this again. It [inaudible] my app and will open my simulator shortly. So, here I am. I am in my simulator. I can open my file. And, I have this new Choose Image button in my navigation bar. I can tap it, and my Document Picker comes up. And, as you see, again, we have all the pictures of the Files app. We can search our files, you have Tags. You can browse. All of this comes for free. Now, my particle image is [inaudible] shared to me over iCloud, so they show up in [inaudible]. And, again, use this little star image here. Pick it, and my particles are little stars. This was easy. And, when you are back at work, you will be able to add a Document Picker to your app in no time. It simply has very little code, and does not depend on any third-party library. Your customers simply get access to their files, no matter where they are stored. Now, you noticed that during the demo, we have again talked about file types. And, many among you probably don't know what these are. So, let's talk about file types. Document Types let the system know which files your application knows how to handle. And, they're important because they let iOS open your app when the file is tapped in the Files app. They also let it show your app in the Share sheet. And, finally it lets iOS choose the proper icon for your documents. So, if I configure my types properly, and if I want to [inaudible] extension, I'm going to see this when browsing particles files in the Files app. If I don't, I'm going to see this instead, which is not too good, and my customers won't be able to open my files from the Files app. So, let's configure this together. There are two steps when you want to configure document types. You need to declare the type first, if it is not declared by iOS already, and then you need to claim it to tell iOS you can view or edit files of this type. Let's start with type declaration. The first question you need to ask yourself is, do you need to declare it? Because in most cases, you'll be dealing with common file types, such as videos, images, PDFs, presentations, spreadsheets. If that is the case, iOS most likely already declared this for you. And, to know if this is the case, you can check out the documentation at this short URL. If that is the case, great, you do not have anything to do, and you can jump to claiming the type. The other common case, is when you wrote an app which defines its own file format. You invented your own file format, and you get to define what that type is. Then you should declare your type as exported, which tells iOS you are the authority for this file type. This is a situation I'm in, in the Files app-- in the Particles app, sorry. My particles file format is my own. So, I can open Xcode to configure this type. And, in this exported UI section, which lets me edit my info.plist, I specify the type identifier, which is a [inaudible] string that I choose. So, [inaudible] comes the example that [inaudible] sample code, the particles, and I need to declare a list of parent types in the Conforms To field. What are parent types? Well, types form a tree, which we call the [inaudible] tree, and you can think of it like a [inaudible], like Swift. For example, JPEG images and HEIF images are both kinds of images. And, .image conforms to our root type for user-created, or user-visible, documents, which is called public.content. Now, public.content has other children, like, for example, public.spreadsheet. You can find this list of types in the documentation I pointed to earlier. Now, we need to find a spot in this tree for my type, Particles. Since the particle system is not a common file category, there is no [inaudible] category for me. So, I can't just make it conform to public.content directly. And, I need to conform to public.content either directly, or indirectly through public.spreadsheet or .image if I want my files to show up in Recents and in Search. Then you have a second tree, which we call the physical tree, which lets iOS know how the files are represented on-disk. And, here this is much simpler, because you only have two choices. In most cases, you will be dealing with a single file on disk for your document. If you need to group multiple resources together, and you have the file package, you're going to use com.apple.package. But, in most cases, you have a single file disk, so it's public.data. This is because I am in with particles, so I'm going to conform to public.data. Now, because this is a tree, .data and .package both iterate from our root type from the physical hierarchy, which is public.item. But, you don't care about this, you just need to choose between .data and .package. Therefore, you have two parents, public.data and public.content. So, in my Xcode UI, I'm going to write, Conforms To public.data, public.content. The last thing I need to do is tell iOS which file extensions are associated with my type. This is how iOS will know that .particles files are of the type [inaudible] defined. To do this, I add a UTType classification property to my [inaudible] properties. In this dictionary, I add a key called public.filename-extension, and [inaudible] file extension that you want to associate with your type. So, here I have a new item, of particles. And, with this, our declaration is done. There's just one more thing I didn't mention. It is if you-- if this was the case where we were declaring our own type. But, you might also want to use type defined by another application. If that is the case, you also need to declare the type so that iOS knows about it, in case that app is not [inaudible]. And, because this is an imported type declaration, this means that if iOS finds an extra definition of that type somewhere else, it will prefer that definition over yours. This is also what you need to do if you want to use a common file type which is not declared by iOS yet. Now, do not freak out, it is the same UI as what we've just went through, except that it is in the imported section instead of exported. Once you have your type defined, you can claim support for it, still in your info.plist, or the Xcode UI. To do this, you specify the type identifier that you have defined, and the only other thing that you need to do is define a rank. And, this lets iOS know what to do when you tap on the file and multiple apps can open this file. For this, you have three choices. The first is if it is your own file type. You choose "owner." And, to be clear, this should only be used if you happen to have invented your own file type. In most cases, you want to use "default," if you can edit files of this type, or "alternate" if you can read files of this type, but not edit them. And a list of [inaudible], rules are different on macOS, so if [inaudible] macOS up, please check out this other short link. When claiming support for a type, please be as specific as possible. We have just claimed support for a very specific file type, so .particles file type. But you can also claim support for categories, such as any image using public.image. This is fine, but you need to be as specific as possible to make your app show up exactly what it needs to show up. Do not claim support for [inaudible] types such as public.data or public.content, because this will make your app show up in places where your customers do not expect to see it, and it will confuse them. So, what can you do with all of this information? Well, first, if you have a need to access files in the cloud, you should use the Document Browser or Document Picker to access these documents. Most cloud vendors have adopted our API's, so if you use one of these viewControllers, your customers will be able to access their files no matter where they are. And, you do not have to write code for each cloud vendor. If you wrote your own custom document browsing UI before iOS 11, now's a great time to switch to UIDocumentBrowserViewController. It is packed with features-- it's packed with features, so please let us do the hard work, and just get all the features in your app for free. Finally, please make sure your document types are configured in Xcode properly so that your app shows up exactly where it needs to be, and help customers find it. And with this, I'm going to hand the stage over to my colleague Ronny. Thank you, Thomas. Hello, everyone. I am Rony Fadel, a software engineer on the Documents Management team. So, Thomas has explained how easy it is to adopt a document browser and document picker API's in your app. Now, that you've adopted these API's, we'd like to cover some best practices to make sure that your app is a well-behaved citizen on the iOS ecosystem, the day you submit it to the App Store. So, now that we've implemented a document browser-based app, or added picker support, now we're ready finally to pick some documents. Your app might want to access documents directly, avoiding the copies in import mode that Thomas has described, or if you want to edit your document directly. For this you'd need to invoke the Picker in Open Mode. In this example, we're in the Particles app, and we've created and presented a Picker in Open Mode. We're going to pick an image file to change our particle system's image. Once we've picked a document, we receive a callback at one of these two delegate methods. The first one, if we're picking our document in the Browser, or the second one if we're picking it in the Picker, like our example. We see the document here, and we see actually the Document Picker here, serving our app the URL that we've just picked. If we attempt to access the document directly, we risk running into a permissions error. Let's try to understand why this could happen. As you may know, your apps on iOS are Sandbox. The App Sandbox is a security feature, whereas every app has unrestricted access to documents in its own app container, and by default, access is restricted to documents in other containers, namely in other app containers and in cloud service containers. So, this is a security feature that guarantees that other apps and processes don't just go snooping around in your app container, accessing your documents. And, always make sure that if they do access them, access these documents, that is done with user consent. So, since access to documents in other containers is restricted, how can your app gain access to these documents that your customer has picked? What we have to do, is to extend our App Sandbox to include these documents. Here's again the URL returned by the Browser or the Picker. This URL has a security scoped resource attached to it. You can think of this resource as a permissions token granted to you by the system, and accessing this token would allow your app to access this document. We can start accessing this document using the following URL API's. When we call startAccessingSecurityScoped Resource, your app gets access to this document, and so you can start displaying or editing the document. And, once you're done using it, the document, you should call the stopAccessingSecurityScoped Resource API on that URL. And, as you can see here, access is again restricted to that document. So, here's a simple recipe to guarantee that you will always interact successfully with Sandbox documents. We walked through this example where we make these calls. First, we make the startAccessing call on that URL, and so we can start interacting with that document. And, you notice in the defer block, we make the stopAccessing call once we're done using the document. You notice that these calls are balanced, and that we only call stopAccessing if startAccessing returns "true." We keep resource access time as small as possible, because when we make this startAccessing call, we consume limited system resources, reserved just for your app. And so, we need to relinquish these resources once we're done using that document to make sure that we can access additional documents within our process. If you know for sure that this document exists inside of your app container, you do not need to make these calls. But, in case of doubt, make these calls. If they're not actually needed, they'll simply be a no-op. Great. Now, we know how to gain access to our documents. We can go ahead and pick that document, like the particle image in our example. And, start accessing it. As you can see here, the cloud service that is hosting this document, as well as other apps also can access this document, and sometimes concurrently with your app. What we need here is a system-wide mechanism to make sure that access to this document is always coordinated, so that processes don't go stepping on each other's feet, accessing, reading, and writing to this document. On iOS, this mechanism is called file coordination, and it is exposed via the NSFileCoordinator and NSFilePresenter API's. It acts as a system-wide multiple reader/single writer lock, and it's also the way for your app to instruct the system to download this document if it's not available locally. Because as you may know, a lot of these documents could be in the cloud, and not on the device. So, you might find yourself writing a lot of boilerplate and error-prone code to handle sandboxing, to handle file coordination. If you're simply interested in displaying and editing your documents, the solution is much simpler. And, on iOS, it comes in the form of UIDocument. UIDocument has been available since iOS 5. It is the recommended way for your app to display and edit documents. It handles sandboxing for you, so there's no need to make the start- and stopAccessing calls. And, it handles file coordination for you. So, there's no need for you to manually coordinate access to these documents. For more information on file coordination and UIDocument, we invite you to check out the "Building Document-Based Apps" session from WWDC 2015, which covers these topics in great detail. Alright. We can now access our documents, and we are sure to read and write to them in a consistent state. We can edit our particle system. And now, what we're going to do is just switch apps for just a second, check out the WWDC webpage, and just jump back to our app. Hmm. As you can see here, we were in the editor, but now we went back to the browser. Your customer always expects to be brought back to where they were when they were using your app. Let's try to understand what happened in our previous example. Here's a diagram of our app's lifecycle. We started off in the foreground while we were editing the app particle system, and we then were taken to the background when we used the App Switcher to switch to Safari. The system suspended our app while it was in the background. It turns out that at the time, our system was under some memory pressure, and so the system terminated our app. When we used the App Switcher to switch back to our app, our app was relaunched. What we failed to do here was restore the UI state. And, that time was a good time to restore the UI state, to bring back our customer to where they were. So, how do we implement state restoration? A simple and easy way would be to persist the URL we've received from the Document Browser, or the Document Picker, and restore it at app relaunch. That wouldn't work though, for two reasons. First, the document that the URL points to could have been moved or renamed. And so, we'd end up with a broken link at app relaunch. And second, the security scoped resource that is attached to our URL is not encoded when that URL is persistent. And so, when we relaunch our app, we again lose access to this document, even if that document wasn't moved or renamed. The correct solution is to use a security scoped bookmark, that we get from our URL. And, as you can see, the bookmark correctly references the document, even if it has been moved or renamed. To save the security scoped bookmark, we must first get it from the URL, using the bookmarkData API on the URL. And, we will then persist it to disk. Once our app is relaunched, we can restore the original URL using the URL resulting bookmarkData call. Alright. Now that we have all the ingredients to save and restore UI state, let's jump to a quick demo to see it in action. So, here we are in the simulator, running the Particles app as seen previously. And, what we're going to do is just take our app to the background. And now, we are going to simulate the fact that the system is killing our app, by killing it in Xcode. If we launch our app again, we see that we jump back to the browser. So, now what we're going to do is implement state preservation and restoration, to be able to jump back to where we were. So, I'll move to Xcode. First of all, in our app delegate, we'll declare to our app that we're interested in state preservation, and then restoration. And, to do that we first return "true" in application shouldSaveApplicationState. And also, return "true" in application shouldRestoreApplicationState. Second, we'll go to our storyboard, and give our DocumentBrowserViewController subclass a restoration ID, so that the system restores this object. And in this example, we'll simply use the storyboard ID. Next, we're going to implement state preservation and restoration in this documentBrowserViewController subclass. So, first I start by implementing encodeRestorableState. We get the editorViewController, if it exists, and get the document URL from it. Once we have that URL-- I'm going to make this a bit bigger. Once we have this URL, we start accessing it to extend our Sandbox as described previously. And, the defer block, we stop accessing once we are done using it. If we can start accessing this document, we simply create a bookmarkData block from this document, and encode it. And, finally, we don't forget to call super. Alright. Now that we know how to encode our restorable state, we'll implement restorable state decoding. And, to do this, we start implementing decodeRestorableState. And, we'll do the inverse steps. So, first we get the bookmarkData from the coder. And, if it exists, create that document URL using the URL resolvingBookmarkData call. And, we simply present the document at that URL. And, last but not least, we don't forget to call super. Alright. So, let's run our app. And so, our app will now be launched in the simulator. As you can see, we can start using our document. And, again, going to take our app to the background. Simulate the system killing our app. And, we're going to relaunch our app, and as you can see here, we're back to our editor. Awesome. We can now access documents, read and write to them without risking data corruption, and we can now even restore state and bring our customer back to where they were when they were using our app. Even if our app was killed in the background. So, if you open the files up, and we tap on Share in the callout menu, on documents that your app can handle, you notice that our app is now available in the Share sheet. However, it says "Copy to" instead of "Open in." What this means is as soon as we tap on our app icon in the Share sheet, the system will make a copy of this document, and serve this copy to our app. And, practically this means that the customer would start editing a copy of the document. To allow your customers to open and edit the original document that we wish to access in your app, you need to adopt an iOS feature called Open in Place. If you do adopt Open in Place, you see the "Open in" your app in the Share sheet, instead of the previous "Copy to." Adopting Open in Place is very simple. If you use the document-based app Xcode template, you'll get this behavior for free, as Open in Place is already enabled by default there. Otherwise, you'll need to add the LSSupportsOpeningDocumentInPlace to your info.plist. Once you've declared to the system that your app handles Open in Place, we must access the document in the following UI application delegate method. Again, if your app is document browser-based, typically in this method, you make the revealDocument call on the documentBrowserViewController. Once the completion handler is called in this method, you present the document in the completion handler. This is already done for you in the Xcode template. We can now tap on our app icon in the Share sheet, and go straight to our app. As you see here, we report progress in the UI, since the document could be in the cloud. And, since that download could take awhile, we also support user cancellation, so we can cancel opening this document, and jump back to the browser. Here's how you can implement progress supporting in your app. If you call the documentBrowserViewController revealDocument API that's been described previously, you get that UI for free. So, you'll see a loading UI on top of the icon in the browser. Otherwise, you may opt to display your own progress UI. If your app uses UIDocument, you should know that UIDocument conforms to progressReporting. And so, to display your own progress UI, you simply access the progress property to display your own progress. Alright. Our app is now awesome. And, we're finally ready to ship it. So, what's the takeaway today to make your apps excellent at managing documents on iOS? Adopt UIDocument when displaying and editing documents, as it eliminates a lot of boilerplate and error-prone code. However, if you already have document editing code that doesn't use UIDocument, adopt the start- and stopAccessing best practices to make sure that you interact successfully with Sandbox documents, or Sandbox-- with the app Sandbox. And also, coordinate accesses to your documents to make sure that you read and write to them in a consistent state. Implement state restoration to always bring back your customer to where they were, when they were using your app, even if your app is killed in the background. Adopt Open in Place, so that your customer always edits the original document, and doesn't need to start editing copies of that document. And, also report progress, especially in a world where most of these documents could be in the cloud, and not available locally. So, what did we cover today? We have shown you how easy it is to adopt the Document Browser and Document Picker API's. Give them a try in your app today. We learned about best practices so that our apps become excellent at managing documents, and for you cloud vendors, we talked about the new Siri shortcuts, and how they allow your app-- sorry, your documents that are frequently accessed to be available throughout the system for your customers. And, we talked about the fileProvider validation tool, and gave guidelines on how to implement an effective authentication flow in your fileProviderUI extensions. For more information about this session, check out the session webpage at the following URL. Also make sure to check out this year's "Introduction to Siri Shortcuts" session. Thank you for your time. Have a great afternoon, and see you at the labs. Thank you.  Well, good afternoon and welcome to a tour of UICollectionView. My name is Steve Breen-- I'll set this over here-- and I'm a frameworks engineer on the UIKit team. And I'm joined on stage today with my colleague, Mohammed Jisrawi, also on the UIKit team. So today we're going to do something a little bit different. We're going to build an app from some specs we receive from our designer, Breanka [phonetic], and this is going to leverage many of the capabilities of UICollectionView. Now, while we work through all the tasks required to build our app, we're going to touch on a wide range of topics regarding UICollectionView including layouts, updates, and animations. So we've got a ton of ground to cover, so let's jump right in. All right, so here's the first spec we got from our designer. It looks like a friends feed, imagine that. A great little columnar layout. It looks pretty straightforward, okay. Okay, this looks great. So here we've got this really fancy looking mosaic layout, which is the contents of our friends feed. Okay, so Mohammed, since you're going to be writing all the code for us and walking us through how to use these features, what are your thoughts on these designs? Well, you know, I'm seeing these for the first time, but they seem like great candidates for collection view. I think we can have a lot of fun with this one, especially. Yeah, this one looks great. All right. So before we dive into code, and Mohammed starts walking us through this, we need to cover three core concepts we need to understand about CollectionView before we dive right into that code. So let's do these now, and we're going to talk about the layout, data source, and the delegate. Okay. So, first of all, let's chat a little bit about the layout. So if you open up the definition for UICollectionView for the very first time, and you're familiar with UITableView, you notice right away that there's a lot of familiarity in the API. You got a delegate and a data source. All these things look pretty familiar, but the layout concept, that's pretty unique and distinct to UICollectionView. You can really think of it as UICollectionView's super power. It allows CollectionView to abstract away the visual arrangement from your content separate from the content itself. Layout is all about the where content is displayed. Now, each individual item is specified by the UICollectionView layout attributes, for attributes like such as bound, center, and frame, things like that. You can think of it as a set of properties you can use to define these items that are displayed. You can even customize these by printing your own subclasses of UICollectionView layout attributes and include these in your designs. Okay. So as the user is scrolling through the content on screen, the layout is considered to be immutable. Now if you need to change this, like for example you're going to change the appearance of the layout, you would use the invalidation mechanism, which Mohammed will walk us through in a little bit. Okay. Now, one great thing about layout being a separate abstraction is we can transition from one layout to another layout and get a great animated effect when you move between layouts, and layout A doesn't need to know anything about layout B. They just declare what the layout is going to be, and the transitions occur. Okay. So CollectionViewLayout is an abstract class, and as such, it's not meant to be used directly, but rather subclasses of CollectionViewLayout are meant to be used. Fortunately, we provide one. UICollectionViewFlowLayout, and you're probably familiar with this class if you used CollectionView before. Now there's a lot of customization points on CollectionViewLayout including some properties that we'll talk about in a little bit, but you can also customize it using a delegate, and we'll talk about the CollectionViewDelegate in a moment, but CollectionViewFlowLayout will specify additional things that extend that CollectionViewDelegate. Okay, so what's Flow all about? Well it's a line-based layout system, and because of this, it can cover a wide range of different designs that you might receive. All right, so what does line base mean? Well let's go through this. So the best way to explain what a line based system is like is to give an example, so let's do that. Okay. So here we see, we've got a vertical scrolling collection view, and we're going to mimic what flow layout does when it lays out this content. All right. So here's our first item, and we start at the top leading edge, and we start laying out our items along a line. Now look at this line. This line is orthogonal to the scroll axis. We're scrolling vertically, so the line is horizontal. Okay, now notice here, we filled up the available space for items on that line, so now we're going to drop to the next line and continue laying out our content. And finally, we drop to that last line, and bingo, we got all our content. Now if I dropped some guides on here to show highlight where those lines are horizontal, let's talk about some definitions that we have for ways to customize flow. First up is the notion of line spacing. And as you see the arrows here, the line spacing is going to be the space between these horizontal lines. Similarly, inter-item spacing is the space between the items along the layout line. And we have two properties on flow layout that let you specify the minimums for both of these. Okay. So let's cement our intuition a little bit and rotate the whole thing, pi over 2, and let's start over at the top leading edge. Now, this one is scrolling horizontally, right, so we're going to have a vertical layout line, and when we get to the bottom of this area, we filled out that line, back up to the top. Okay. This pattern is pretty familiar now, right. There we go. There's all our content. Now we have our vertical layout lines. So now in this orientation, our line spacing is this. And our inter-item spacing is this. It's key to remember when you're working with flow layout. Okay, so that's layout. Let's talk a little bit about the data source, and if you work with TableView, this should look very familiar. It's a very similar pattern. They share very similar APIs. Okay, so if the layout is all about the where content goes, the data source is the what. The content itself. Three core methods to think about here. The first one is optional, number of sections in CollectionView, and this one if you don't provide it, we'll just assume you mean one. Similarly, we have number of items in section, and this is going to tell you the number of items in each individual section, because they can all have different numbers of items. And then the last one, sell for item index path is where you provide the actually content you're going to display to your users. Okay. So that's the data source. Okay, the final of our three topics we're going to talk about before we dive into code with Mohammed is the Delegate. Okay. So use of the Delegate is optional. Now, CollectionView is a subclass of UIScrollView. So we use the same Delegate that's provided by the ScrollView superclass, but we extend it. So if you need to modify scrolling behavior, you can do it also against this same Delegate and also work with some of the UICollectionViewDelegate methods that provide things like fine-grained control over highlighting and selection as the user interacts with your content. And we also got an API that let's you know, hey, something came on screen. WillDisplayItem and DidEndDisplayingItem. Okay. So those are the three core concepts we really need to talk about before we dive into code to get started with UICollectionView. So let's switch over to the dev box with Mohammed and have him show us how it works. Mohammed. All right, so the first of our two screens that column layout is a great use case for using CollectionViewFlowLayout. We can probably accomplish everything we need there, and it would be a great way to get us started using UICollectionView. So now while we could accomplish the entire goal of our design with a flow layout, I'm actually going to subclass CollectionViewFlowLayout because we're going to do a little bit of extra customization. So I'm going to start out by creating an instance of my ColumnFlowLayout class, which I've already prepared. I'm going to use that instance to create my CollectionView. I'm going to take that CollectionView, and I'm going to set some view properties, like auto resizing mask, background color, and since it's the ScrollView, I can set some of it's ScrollView properties as well. This is all just to get it to look and feel the way I want it to look in our app. After adding the CollectionView to my view hierarchy, I'm going to register my PersonCell class using its unique identifier with CollectionView so we can get our cell design to app. And then I'm going to set the view controller as the CollectionView's data source so we can give it some information about how many cells it's going to display and what sort of data it's going to display in it's cells. And then I'm going to set it as it's delegate as well, so we can handle cell selection. So now that we've gotten set up, we need to actually conform to these two protocols. So let's start out by conforming to the data source, and we have two required methods we need to implement here. The first of these is number of items in section where we can just return the number of people or the number of items in our people array to get our data model objects displayed. The second method we'll need to implement is CellForItemAtIndexPath where we'll dequeue a cell from the CollectionView using our unique identifier, pass a person and object that we get out of our people array, to the cell to actually display our data, and then return the cell. And to wrap things up here, we'll just need to implement one optional method from the Delegate protocol, so we can handle selection. So I'm just going to add DidSelectItemAtIndexPath where we'll just instantiate our FeedView controller, which is going to be our second screen if we don't already have an instance, and then we're going to pass it a person object so we know whose images to display, and then we'll push it onto our navigation controller. Okay. So let's build this and switch to the simulator to see. All right. [ Applause and Cheering ] Okay. So we have our CollectionView on screen here, and we have some cells. You can see them, though they're kind of squished. They're not the right size, so we're going to have to do some of that customization we thought we might need to do. So let's go back to Xcode, and let's pop open our column of class here, our ColumnFlowLayout class that we've put together, and let's take a look at what we need to do here. So I already have a stub override of the layouts prepare method. Now, UICollectionViewLayoutsPrepare method is called whenever the layout is invalidated, and in the case of UICollectionFlowLayout, our layout is invalidated whenever the CollectionView's bounds of size changes. So if our app rotates on a phone or if our app is resized on an iPad. So this is a great place to do any customization that takes the size of the CollectionView into account. In our case, we want our cells to be some function of the CollectionView's width. And we can let the CollectionView know how big we want our items to be by saying it's item sized properties. So I'm going to go ahead and do that here. So I'm just going to set my CollectionView's item size to a CG size with a width that is the width of the CollectionView's bounds, inset by its layout margins, and then we're going to give it a height of 70 points just to match our design. And since we're already here, I'm going to do a couple of other little things here just to get things to look nice. I'm going to apply a section inset with some padding at the top that matches our inter-item spacing, and I'm going to set the layout section inset reference property to from safe area so everything is neatly tucked within the CollectionView safe area insets. Okay. So let's go back to the simulator one more time and see what our properly constructed layout looks like. All right. That looks great. That looks just like our spec. I think our designer is going to be really happy. And if we rotate to landscape, we see that our cell is a great size, and so we know that our invalidation code is getting called again in prepare. Now while this is okay, you might be thinking, it's not the best that we can do here. It doesn't look as great as it can. We might want to do something more interesting here like display multiple columns since we have the space available to us. Now flow layout makes it really easy to do this. If you remember during Steve's explanation of how flow layout performs it's layout earlier, flow layout will automatically try to fit in as many items as it can within a line before it wraps to the next one. So using that, we can kind of figure out that layout-- that we can get multiple columns if we change our item size. So if we head back to Xcode, to our layout here, and if we just change how we're calculating our item size here. So I'm just going to remove this fit here, and I'm going to replace it with something that does a little bit of extra math. So I'm just starting out with the same available width that I had before. This is the bounds inset by the margins, and some arbitrary definition of what a minimum column width is, it's 300 points. And then taking both of those values and using them to calculate a maximum number of column that I think I can fit within the available space, and I'm taking that and dividing the available width by it to calculate an optimal cell width, which might be more than our 300 points. I'm then passing that to the CT size that I'm using as my item size. Okay. So let's go back to our simulator again and see what our updated layout looks like. Okay. So everything is the same here. We didn't break it. It's a good start, and if we go to portrait, there are our multiple columns side by side, just what we want. What do you think, Steve? That looks great. We got a great adaptable columnar base layout. Not a lot of work. No. What's next in our design? Well, now that we've eased ourselves in with our friends list, it's time to start thinking about that fancy mosaic layout for [inaudible]. Oh, yeah. That's going to be great. Yeah. Let's switch back over slides, and let's chat a little bit about that. Okay. So let's take a look at this layout or design here and see what we can do. So our first inclination, I don't know about yours, but mine would be can I use Flow. I've got it. It's ready to go. Let's try to use it. So let's zero in on this design a little bit and see if Flow is going to make sense for us. And this particular region where these three photos are, I'm going to zoom in on that real quick. All right. So now in this instance we have a very large photo on the left and then a vertical stack on the right. So in the flow universe, since it's line based, we're going to lay out that large left item, move over to the next item where it's got room, try to lay out another item, and then jump to the next line. But we're not done. We've got that vertical stack to contend with. So this really is not going to work for Flow because it turns out it's not really a line-based layout, this fancy layout of ours. But going through this exercise is useful, so, you know, let's start with flow first. Okay. So in this instance, we're going to create our own custom layout. Oh, no, we're scared, right. Nope, it's not complicated. We've got four basic methods to deal with here, and I'm going to bring up one additional method that gets an honorable mention. Okay. So four methods. Here we go. Our first method I want to talk about is the CollectionView Content Size. Now, recall before we mentioned CollectionView is a subclass of UIScrollView, and one of the features of UIScrollView is that you have a visible region with a large content area, and you get that great iOS experience of moving your content around inside that, and so CollectionView needs to know how to tell the ScrollView, hey, here's how big my content is. Okay. So how do we get this size? Well, if you imagine a rectangle that encompassed all the content that the layout is going to define for your CollectionView, we want the size of that. Okay, so that's CollectionView Content Size. Next up we have two methods that are in the business of providing layout attributes. The first one is LayoutAttributesForElements (in Rect). Now this is called periodically by CollectionView when it needs to know what is needed to display on screen as the user scrolls through your content or displays for the first time. So this query is by a geometric region. Okay. It's companion API, LayoutAttributesForItem AtIndexPath, as you can imagine, it's just looking for a single item. Give me the attributes for that. Okay, so we're going to see more when Mohammed walks us through this, but for these two APIs, it's important to note that performance matters. Okay, so the fourth of our four core custom layout subclass items is going to be the Prepare method. Now Mohammed has already chatted about this a little bit. This is called every time the layout is invalidated. So this is a great time to compute anything, such as your layout attributes you might want to cache and also your content size, which is going to be asked for pretty soon afterwards. Okay. So our honorable mention APIs, so let's talk about this one. This is a Should Invalidate Layout For Bounds Change. So this is called every time the bounds in the CollectionView changes. Okay, so again, it's a CollectionView is a UIScrollView subclass. So what do we mean by a bounds change? Well, when a ScrollView bounds change, the origin can change during a scrolling, and also the size can change when the application size changes or the CollectionView size changes. So this is going to be called during scrolling. Hence the oh yeah emoji. This is called very often. So making the right decision here is important. Okay, so the default implementation in UICollectionViewLayout returns false. So if you need this to do something different, here's your chance. And as a way of an example, UICollectionViewLayout will return false if the origin changes. Okay, so if the user is just scrolling through your content, we won't invalidate. Let it by default. But if the iPad rotates, the phone rotates, and your app changes to a different size, it'll return true. Now a slight exception to this is things like floating headers and footers, right. We have to recompute those while you're scrolling your content. That'll do a custom invalidation to take care of those things. Okay. So enough theory. Let's switch back to our development machine and have Mohammed walk us through what this is going to look like in code building this fancy custom, UICollectionViewLayout. All right, let's dive right in. So I've already put together another layout subclass that we're going to use for this layout, and you might notice that it's a subclass of UICollectionViewLayout directly, not a subclass of CollectionViewLayout, and this is for the reasons that Steve explained to us earlier are UICollectionViewLayout doesn't really meet the needs of our custom mosaic design. So the first thing I'm doing here is I'm setting up a couple of instance variables that I'm going to use to hold onto some key pieces of information that I can refer to later. The first of these is a content bound CG rect, which I'm going to use to keep a representative bounds of all the items within my CollectionView. And the second is a cached attributes array, which I'm going to use to hold onto my layout attributes so I can refer to them quickly when performance matters. So we're going to start out by implementing our prepare method again for this layout. Prepare is the ideal place to do the bulk of our layout work because it's getting called once per invalidation. We can set up our layout here and then avoid having to do any heavy layout work or any heavy layout math in the methods that are called much more frequently. So we're doing a couple of things here. First, we're resetting our cached attributes and our content bounds just to clear out any stale information from previous invalidations. Next, we're doing a few things for every item in our CollectionView. The first of these is actually preparing the attributes, and now I'm not going to go too deeply into what that entails for our specific layout because this is going to be different for you. This is where you are going to calculate the sizes and positions and transforms, etc., for your cells to match your design needs. But there are a couple of key things that we're going to do here after we are done with the attributes. The first is, we're going to cache them. We're going to put them in our cached attributes array so we can grab them quickly later on. And the second is, we're going to union their frame with our content bounds rect so that our content bounds are kept up to date. So now that our prepare is up and running, we need to implement the remaining methods in our layout that we need to get everything working. So the first of these is CollectionView Content Size where if we had done our job right in Prepare, we can just return our Content Bounds as size. Next is should invalidate layout for bounds change. Now since our layout doesn't have any elements that require us to invalidate while we're scrolling, so no floating headers, no floating footers or anything like that. We only really want to invalidate when our CollectionView's bounds of size changes. So we'll just return true if our new bounds of size is not equal to our CollectionView's bounds of size, our current bounds of size. After that, we'll implement LayoutAttributesForItem AtIndexPath where, again, since we've prepared all the attributes in our Prepare method, we can just grab the specific attributes that correspond to the RequestAtIndexPath from our array. And finally, we're going to implement LayoutAttributesForElements InRect. Now this method is called periodically by the CollectionView with different query rects, which may be bigger than our CollectionView. Our CollectionView is just asking for a set of attributes that match a certain region. It's our job to return an array that contains all the attributes that correspond to all the items that are going to appear within that rect in our CollectionView. So we can answer that question here simply by filtering our cached attributes array on the frame of the attributes. So if our attributes have a frame that intersects our query rect, we can return them. Okay. So let's switch back to the sim and see what our layout looks like. So I'm going to select one of these feeds, and there you go. We have our layout. Our images are nicely loaded in this fancy mosaic configuration, and if we rotate to landscape, you can see that our cells have resized so we've updated everything correctly, we've invalidated, which is great. So this looks like our spec, but that scrolling performance isn't great, is it? No. No, it's pretty bad, huh. So you might already have an idea of what's going on here. Let's switch back to the code and see what might be happening. So if we take a look at our layout attributes or elements in rect here, remember that this method gets called frequently during scrolling. So this function here, which is filtering our entire array, you might imagine can get really expensive as the number of items in our CollectionView increases. So the more photos we have in our app, the slower our scrolling performance is going to be. So if you find yourself in a situation like this, it helps to step back and think about the nature of your layout and think about whether you can find any optimization opportunities. So our layout kind of demands that every cell apps next to or below it's preceding cell. So this means that our attributes are already sorted within our cached attributes array by their frame's minimum y value. So we have a sorted array, so we can speed up our search by doing something like a binary search as opposed to our linear filter that we're doing now. So let's remove our slow implementation here, and let's replace it with something that should be much faster. So I'm going to step through this bit by bit, don't worry. So the first thing we're doing here is we're calling into binary search function that we've already prepared, which takes in a range of indices within our array and our query rect. If it finds a set of attributes with a frame that sits within our rect, it'll return the attributes as index within our array. Then starting from that index, we can build up the set of the rest of attributes for our query rect simply by looping up and down in our array and picking up attributes until we exit our query rect, until we find attributes that are outside our rect. And this should be much faster. You have thousands of items in your array. You're not going to loop through the array thousands of items, thousands of times. Okay, so let's go back to the sim again and let's see what our faster scrolling algorithm looks like. Let's pop this open and give it a flick. It's way faster. What do you think, Steve? Much better. Okay, great. So we've got these two great layouts. What's next? So we have our two screens. That just leaves our update animations for our friends list here. Oh, great. All right. Well let's switch back over to slides, and let's walk through that totally cool update animation I think our designer called it. All right, so we've got a video here. Let's run through this and see what this totally cool update animation looks like. Okay. So we have some elements here. We see that last item is getting refreshed. I guess somebody posted a picture, and then we got another item there, it looks like, yeah, that third item smears is not going to be here. Okay. So we've got three basic operations happening, right. We've got a reload, a move, and a delete. Why don't we switch back over to the dev machine, and Mohammed, why don't you show us how this works? Sure thing. Okay, so we're doing multiple animated updates at the same time. So you might be aware of a great tool that UICollectionView and UITableView provide us, and it's the Perform Batch Updates API, which basically allows us to pass the collection view a set of updates that can be performed at the same time with animation. So I'm going to add a call to CollectionView PerformBatchUpdates, and note that I'm doing both my data source updates and my CollectionView updates in the closure here. This is really the best way that I have of coordinating my updates and keeping things neatly in sync and avoiding inconsistencies. So, first I'm just updating my last item in my data source. I'm removing the second to last item, picking up the last item, moving it to the top, and then I'm asking the CollectionView to perform the animations that I want. Okay. Let's go back to the sim again and see what our update looks like. So I have wired our update code through this update button at the top right corner, and uh oh. What's going on? Oh, that's embarrassing. What's going on here. You know, I've been writing iOS for a long time. I've seen this movie before. Yeah, it sucks when it happens on stage though. You know, we're running out of time here, so why don't we just call reload data, and we can come back and do the animations for V2. Really? You know, we could do that, but then we'd lose that totally cool update animation, and our users expect these lively interfaces, right? Yeah, yeah, you're right. You know what? They deserve better. Ah, I like the way you think. All right, let's switch back over to slides real quick, and let's see if we can save our totally cool update animation. You've seen this before. All right. So first of all, let's dig into this debug exception and see what it's trying to tell us. All right. So it's saying here we're attempting to perform a delete and a move from the same index path, 0-3. So if I remember right, that was the fourth item. We did a reload and a move on that. We didn't delete it, we deleted the third item, 0-2, right. I don't remember deleting them. So, yeah, what's up with that? All right, but before we do this, let's go back and take a peek at the PerformBatchUpdates API and talk about some high-level principles. All right. So as Mohammed mentioned earlier when he introduced this API, the purpose of this API is that we can commit multiple updates at the same time and have anything animate together and get that great experience. And as he also mentioned, it's super important to perform your data source updates alongside your CollectionView updates inside that CollectionView update closure. Now, what I'm saying for CollectionView also applies for TableView. So if you got TableViews in your apps, all this information is going the same direction. Okay. So let's make some observations here. The CollectionView updates, when you do inserts, moves, and deletes, the ordering of those do not matter in your update's closure. Put them anywhere you want. Now however your data source updates, when you're changing the structure offer your data source, which is backing that data source, or does matter. Okay, so this is best served by showing an example, so I'm going to take a example of two arrays that have three elements in them, and we're going to strengthen our intuition on this and show a delete and an insert, but we're going to do the first run through with the delete first and the second delete second. We're going to reverse the order, just to kind of strengthen our intuition. I do this all the time, draw pictures, right. All right. So we deleted the first item, and now we're going to insert at index one. Okay, on the second example, we reverse the order and do the insert first and then the delete. So our intuition holds, indeed we get a different result. This is probably not a good thing, right. So let's contrast this with the CollectionView updates. Now here I have two sets of CollectionView updates on a submit via batch updates, and I've left out the data source updates, just to keep the slide tidy. But I've got an insert and a delete on the first one, and the second one has a delete and a insert, and the order is different. This will give you the exact same result. We're all engineers. We want to know why, why is that? Well, let's talk about that. How does this happen? Why is the ordering not important for the update sent to the CollectionView, and of course it is for your data source. Okay. So let's walk through these operation by operation. So the first one to delete, this is process in descending index path order. Now let's talk about the index paths. So first of all, if you can think about what's happening on a PerformBatchUpdate, before the batch update starts, your data source is in a before state. Now once everything is done in the batch updates, you'll be an after state. Okay. So for delete, the index paths always referred to the before stage. So that's delete. So insert is processed in ascending index path order paths. So the index paths refer to in the insert are always referring to the final state or the after updates stage. Okay, a move is this mixture of the two, right. You have a from and a to index path, and the from is in the before state, right, and the to is in the after state. Reload. Now reload is a little bit of a super command if you will, right. It actually decomposes down into a delete and an insert. And the index path specified in a reload is speaking about the before state. Okay. So this insight now that we understand what reload is really doing can kind of tell us a little bit of what's going on with our error in our app because of the delete on the reload on the last one is conflicting internally with the notion of moving that item, okay. So we can address this in a minute when we get back to code. Okay, so I'm not going to go through these, but you can reason about these later. Just put it up here as reference that these are the things that'll cause CollectionView to go bonkers. Don't do it. And how can we take all this knowledge and simplify it, distill it in such a way that we can always apply our data source updates from a given set of CollectionView or TableView updates and make sure everything is in sync. All right. So these four basic rules. So first of all you want to decompose those moves and to delete and inserts. Easy. And then combine all your deletes and inserts into two separate lists, process the deletes first in descending order on the index paths, and then finally apply those inserts in the ascending index path order. Do this, and you're good to go. What about reload data? And I know Mohammed said we could just hit that and we're done, and everybody laughed so I'm pretty sure I yelled and that's the case, but the thing about reloaded data is you don't get those great animations, and this is really a sledge hammer approach. So, and we really prefer the apps to be lively and animated and feel great for our customers. So, and this is used in special cases. Okay, Mohammed, let's switch back over real quick and see if we can get this fixed in code and save that totally cool update animation. All right, time to redeem myself. Yes. So let's use the guidelines that Steve just shared with us to fix our update animation. So let's remove our existing implementation here. And if you recall, our update consisted of a reload, a delete and a move, and our reload and the move were at the same index path. They started at the same one. So that's really where our conflict is. So we'll need to start out by separating those two. So let's take our reload out into its own call to perform batch updates, and here I'm just updating my data source, again, same as before, and calling reload items on the CollectionView. I'm just performing it in a UI view performed without animation closure because if you look closely at our spec, it's actually nonanimated, that initial reload. Okay. So next up, we have to take care of our remaining updates, that delete and the move. And let's reason about them for a second. We have a delete at index two, and then we're moving the item at index three to index zero. So if we break down our move using the guidelines that we just learned about, that becomes a delete at index 2, a delete at index 3, and an insertion of the item from index 3 at index 0. So now we have two sets of operations. We have deletions and insertions. We can process them accordingly. First, we'll perform our deletions in descending order. So we'll do our deletion at index 3 first, and will hold onto the person from there so we can insert them later on. And then we'll delete the item at index 2. Then we'll need to process our insertions in ascending order. We just have one, so we can just go ahead and insert it. And finally, we'll ask the CollectionView to perform the animations that we want. Now, note, I'm still calling move here. I didn't break it down into it's component actions because we still want the collection view to play the right animations, and if we've done the right thing with our data source, the CollectionView will do the right think in terms of animations. All right. Let's go back to the simulator and see what our update looks like when it works. All right. Here goes nothing. Wow. Great! I'm going to reload and take a slow-mo victory lap just to-- there we go. That looks exactly like our spec, doesn't it. That's great, awesome. All right. Well let's wrap it up. We covered a ton of content. Can you switch it back to slides? And I'd like to issue a bit of a call to action. So if you've been nervous or anxiety building that custom layout, take the stuff we just applied today and go back and dive in and create those custom layouts and build really great CollectionView solutions. And if you got a lot of reload data sprinkled all throughout your apps and you're losing out on these gray animations. They'll inspect those things and see maybe why you didn't understand or something wasn't quite jived about why it was happening and fix those spots. Okay, so more information, you can see the link on the slide here. And we also have a CollectionView lab tomorrow morning at 9. If you have any questions or comments about your CollectionViews, please swing on by and chat. Mohammed and I will both be there. And thanks very much for coming out, and I hope you enjoy the rest of your conference.  Good morning and welcome to What's New in Swift. This has been an exciting year for Swift and the Swift Community. And over the next 40 minutes Slava and I are thrilled to give you an update on the major developments there. The session is roughly broken in to two parts. First I'm going to give an update on the Swift open source project and the community around it. And then we'll dive into Swift 4.2, which is available as a preview today in the Xcode 10 beta. Since late 2015, Swift has been an open source project on GitHub. With a vibrant community of users who discuss, propose, and review all changes to the language and standard library. In that time over 600 individuals have contributed code to GitHub as Swift open source project. And together they have merged over 18,000 pull requests. Since Swift.org was launched Swift has been available for download on Swift.org both for downloadable tool chains for Xcode, as well as various version of Ubuntu. These are both development snapshots and official releases. Now we want Swift to be supported on all platforms so everyone can use it. And a critical part of that is extending the testing support provided to the community. Various folks in the open source project are working on bringing Swift support to other platforms. And we'd like to support those efforts. About a month ago we extended the public continuous integration systems to support community hosted CI notes. So if you are a member of the community interested in bringing Swift to another platform or effort, you can now seamlessly plug in your own hardware support to bring testing there. This is a nice prerequisite for supporting Swift in other places. We've also invested tremendously in the community around the Swift open source project. This is the community that discusses all changes to the language. About six months ago we moved from using mailing lists, which were very high traffic to forums. This was at the behest of the community. Various people were concerned that they wanted to be able to engage in the project at a level that worked well for them but found it difficult to do so. With the forums you can now engage the level that works well for you. The forums have also worked so well that we wanted to extend their utility out to supporting the general open source project. If you maintain a Swift open source project such as a popular Swift library, you can now use the forums for use to discuss things on that project such as discussions with your users or development. We've also looked at ways to continue to extend Swift.org to be of general use the community. This week we've moved to hosting the Swift programming language book to swift.org. Located at docs.swift.org, this will be a natural place for us to extend more documentation for use by the community. Now the really exciting thing about Swift is that people are really thrilled about using it. And they're talking about it in a variety of places. At Podcasts, Meetups, conferences. And we, Apple, thought it was very important for us to engage in those places because that's where a lot of the discussion is happening. Over the last year we have made a very conscious effort to engage in those conferences and present technical presentations on things that we're doing with Swift, or how does Swift work, or how you can get involved in the open source project. And this is something we're very committed to continuing going forward. One of these efforts I'd like to talk about is a event co-located next to WWDC on Friday and that is the try! Swift San Jose Conference. There there will be a workshop with members from the community to help on board people who are interested in contributing to the Swift open source project. And there will be members from Apple's compiler team there to also facilitate those conversations. So that's all about the community update. Let's talk about Swift 4.2. I think the natural place to start is well, what is this release about and how does it fit into the bigger picture? Swift updates occur -- some major updates occur about twice a year. And Swift 4.2 is a major update over Swift 4.1 and 4.0. Now there are in broad strokes two themes to this release. One is a huge focus on developer productivity. You can see this in a variety of ways. The faster builds for projects. But also just through and through massive improvement to the core tooling experience from the Debugger through the Editor. And the community has also focused on language improvements that aim to improve common developer workflows, remove boilerplate. And Apple has continued invested improvements to the SDK so that the Objective-C APIs better reflect into Swift making better use of the language and improving your use of our APIs. And the other side there's been a massive effort on under the hood improvements and changes to the runtime towards this binary compatibility goal, which culminates in Swift 5, which will be released in early 2019. So what is binary compatibility? Binary compatibility means that you can build your Swift code with the Swift 5 compiler and layer. And at the binary level it will be able to interoperate with other code built with that compiler or any other compiler layer. This is a very important milestone for the maturity of the language. And what this will enable is Apple to shift the Swift runtime in the operating system, which means apps can directly use it, meaning that they no longer need it included in the application bundle. So this is a code size win but it's also important that it impacts things like startup time, memory usage, it's an overall huge win for the community. If you're -- we've been very transparent on the progress towards ABI stability or binary compatibility. You can follow along on the ABI stability dashboard on Swift.org. Today's focus is on Swift 4.2, which is an important waypoint toward Swift 5. Let's talk about source compatibility. So just like in Xcode 9, Xcode 10 shifts with one Swift compiler. So if you're using Xcode 10, you are using Swift 4.2. However, just like in Xcode 9, the compiler also supports multiple language compatibility modes. Now in all the modes you can use all the new APIs. You can use all the new language features. What these gate are source-impacting changes. The first two are ones that existed previously in Xcode 9. They're there to provide an out of the box experience that you can build your Swift 3 and Swift 4 code without modifications. The Swift 4.2 mode is almost identical to the 4 mode except it gates those SDK improvements that are talked about. That's it. Just some previous versions of Xcode, there's Migrator Support that you can find in the edit menu to mechanize most of your changes. I want to give an important disclaimer about the Swift 4.2 SDK changes. Later Xcode 10 betas will likely have further SDK improvements. This is to provide opportunities to incorporate you feedback from the betas of how these APIs should be improved and how they reflect into Swift. This means if you migrate to 4.2 early, you should expect there are going to be some changes later. Or you can hold off and migrate later. It's completely up to you. Now with Swift 4.2 we think we are rapidly converging on what Swift code is going to look like going forward. This is an important phase in the maturation of the language. And thus we really think it's important for everyone to move off of Swift 3 and embrace using Swift 4.2. There are important code size improvements there and just overall improvements to the language. And this is Xcode 10 is going to be the last release to support that Swift 3 compatibility mode. So let's talk about some improvements to the tooling. In the State of the Union we mentioned that there are some significant improvements to build improvement for Swift projects over Xcode 9. And so these numbers are run on a 4-Core MacBook Pro i7. Let's look a little bit closer at one of them. This project is a mix and match of Objective-C in Swift. It started out as an Objective-C project and started incorporating Swift. This is a very common scenario. Now what this Build time improvement doesn't really underscore is how much faster building this Swift Code actually became. So if we just focus on how much faster the Swift code built, it actually builds three times faster than it did before. And so that's why the project has that more modest 1.6x speedup. And what you will see is that the overall build improvements will depend on the nature of your project, how much Swift code it's using, the number of cores on your machine, but we've in practice have seen from many projects it's a 2x speedup. And the win comes from observing that because within a Swift target you have cross-file visibility, right, that's one of the great features of Swift where you don't need header files. There was a lot of redundant work being done by the compiler. And what we've done is we've retooled the compilation pipeline to reduce a lot of this redundant work and make better use of the cores on your machine. And that's where these speedups come from. If you're interested in more details there's these two great talks later this week that dive into how the build process works under the book including more details about where this performance win comes from. Now this big win comes from debug builds. I want to focus on how this is surfacing in the Xcode build settings. Recently we separated out compilation mode from optimization level. Compilation mode is how your project builds. So for release builds the default is whole module compilation that means all the files within your target are always built together. This is to enable maximum opportunities for optimization. It's not the amount of optimization done but the opportunities for optimization. And for Debug builds the default is incremental. That means not all the files are all built, re-built always all together. So this is a tradeoff in performance for build times. Optimization level for Debug builds continues to be no optimization by default. This is for faster builds and better Debug information and the release builds are optimized for speed. We'll get back to the optimization level in a few minutes. All right so this separation of compilation mode and optimization level nicely highlights and interesting stopgap measure that various folds discovered that when they combined whole module compilation with no optimization that they sometimes would get faster Debug builds. And this is because that combination reduces a lot of that redundant work that I talked about before that we have no made great efforts to eliminate or significantly reduce. The problem with this combination is, is it impedes incremental builds. So anytime you touch a file within a target the whole target gets rebuilt. Now with the improvements in Xcode 10 to Debug builds, we feel you no longer need to use the stopgap measure and we have observed that the default incremental builds are at least as good as this combination or better. Especially since they support incremental builds. Let's talk about some important under the hood runtime optimizations and this is all part of that march towards binary compatibility. Swift uses automatic memory management and it uses reference counting just like Objective-C for managing object instances. On this slide I've illustrated in comments where the compiler inserts, retains, and releases. This is how it behaved in Swift 4.1. When an object is created there's a +1 reference account associated with it. What the convention was if the object is passed off as an argument to another function, it's the obligation of that function call to release the object. So it's basically you're passing off the responsibility to the call to release it. This provided some performance opportunities to shrink the lifetime of some objects to like their smallest range of use. However, code often looks more like this where you're passing the object off several times to different APIs. And because you have this calling convention, you still have this dance where the initial reference count is balanced out with the final call. But the intermediate calls are expected to have these extra retains and releases because that's what a convention is. This is really wasteful because the object is really just going to be alive during the entire duration of this function. So in Swift 4.2 we changed the calling convention so that it was no longer the callee's obligation to release the object. This means all these retains and releases go away, which is a significant reduction in retained release traffic. This has two implications. It's both a code size win because those calls are gone and it has a runtime improvement. Another important optimization we did was to string. And Swift 4.2 string is now 16 bytes big where it as previously 24. We feel this is a good tradeoff between memory usage and performance. It's also, however, still large enough to do an important small string optimization. If the string fits within 15 bytes then the actual string is represented directly in the string type without going to the heap to allocate a separate buffer to represent the string. This is obviously also a memory win and a performance win. This is as similar to an optimization that exists within a string. We can actually represent larger strings. Finally before I hand it off to Slava we'll talk about the language improvements. I want to talk a little bit more about the efforts to reduce code size. I talked a little bit about that calling convention change, which reduces code size. But we've also introduced a new optimization level, Optimize for Size. This can be useful for applications that care very much about app size limits such as from cellular over the air download limits. Swift is a very powerful language with static knowledge about what your program does. And so compiler has many opportunities to do performance optimizations such as function call inlining, speculative devirtualization, which trade off a little bit of code size for more performance, but sometimes that more performance isn't really needed in practice. This is the result of applying Osize to the Swift Source Compatibility Suite, which contains an assortment of projects from GitHub, frameworks and applications. And what you'll see is a wide range depending on what language features are used about 10% to 30% reduction in code size. Now this, when I talk about code size I'm talking about the machine code that is generated as a result of compiling your Swift code, not the overall app size. The overall app size depends on assets and all sorts of other stuff. In practice we observe that runtime performance is usually about 5% slower. So you're trading off for a little bit of performance. For many applications this is totally fine. So it really depends on your use case. But if this is something you're interested in we encourage you to give it a try. With that I'd like to hand it off to Slava who will talk about all the great language and improvements with Swift 4.2 Hey everybody, I'm Slava Pestov. I work on the Swift Compiler and today I'm going to talk about how the new language features in Swift 4.2 allow you to write simpler and more maintainable code. So before we start talking about the new language changes, let's review the process for making improvements to the language. So as Ted mentioned, Swift is an open source project, but it also has an open design. This means that if you have an idea for improving the language, you can go and pitch it on the forums and if the idea gains enough traction and crystalizes into draft proposal, you can submit it together with implementation to the core team for review. At this point a formal review period allows members of the community to give additional feedback and then the core team makes a decision as to whether to accept the proposal. If you go to the Swift Evolution website, you can see a list of all the proposals that were accepted and implemented in Swift 4.2. And if you look at this list of proposals there's a lot here. There's more than I can cover today. But one thing I really wanted to emphasize was the large number of proposals that were both designed and implemented by the community. What this means is that these proposals address common pinpoints in the language that came up in the real world and you came up with the idea for fixing these pinpoints and you contributed these improvements back to Swift so that everybody benefits. Thank you. So for the first improvement we're going to see how to eliminate a common source of boilerplate when working with enum's. So let's say I have to find an enum. And I want to print every possible value that this data type can have. So in Swift 4, I had to define a property perhaps with a list of all the possible cases. And if I add a new case then I have to remember to update that property, otherwise I just get incorrect behavior or runtime. And this is just not very good because you're repeating yourself to the compiler. So in Swift 4.2 we've added a new CaseIterable protocol and if you state a conformance to this protocol, the compiler will synthesize an all cases property for you. OK, that was short and sweet. For the next improvement we're going to see how to eliminate another source of boilerplate. This time it's when you're unable to make your code sufficiently generic. So in Swift 4 we have this contains method on sequence. And this requires that the element type of the sequence is Equatable so that it can find the element that it's looking for. And of course I could call this within an array of strings because string is Equatable. But what if I call it within an array of arrays. Well array of Int, the element type here is not equitable, which meant that I would just get a compile time error. And you might ask, well why doesn't the standard library make all arrays Equatable. But that doesn't make sense either because if the element type of the array is not Equatable, like a function perhaps, then you can't really make the array Equatable either. But certainly if the element type of the array is Equatable then I can define an equality operation on arrays that just compares the elements pair wise. And this is what conditional conformance allows a standard library to do. So now array gets an implementation of Equatable for the case where the element type is equitable. And in Swift 4.2 this example we saw earlier now works. And in addition to arrays being Equatable the standard library defines a number of other conditional conformance. For example, optional and dictionaries are now Equatable when their element type is Equatable and the same works for Hashable, Encodable, and Decodable conformances. And this allows you to compose collections in ways that were not possible before. So here I have a set of arrays of optional integers and everything just works. If you want to learn more there's a session later this week where you can learn about conditional conformance and some other generics improvements in Swift 4.2 that I won't be covering today. So what about defining your own Equatable and Hashable conformances. Well, a common pattern in Swift is that I have a struct with a bunch of stored properties and all those stored properties are themselves Equatable. And then I want to make the struct Equatable just by comparing those properties of the two values. In Swift 4 previously you had to write this out by hand. And this is just boilerplate. If I add a new stored property to my struct, I have to remember to update this implementation of Equatable and it's easy to make a copy and paste error or some other mistake. So in Swift 4.1 we introduce this ability to synthesize the implementation of equality. If you emit the implementation than the compiler will fill it in for you as long as all those stored properties are themselves Equatable. This also works for Hashable. Now what about generic types? So here I have a data type whose values are either instances of the left type or instances of the right type. And I might now want to make left and right constrained to Equatable because again, I want to be able to use this either type with functions, errors, and other non Equatable types. But certainly I can define a conditional conformance so that if left and right are both Equatable then either is Equatable. But I can do even better than this. Notice how the implementation of equality here there's only really one obviously correct way to do it. You have to check that both values have the same case and if they do you check their payloads for equality. So you might guess, well the compiler should be able to synthesize this for you and it can in Swift 4.2. And this also works for Hashable. So now I can have a set of either Int's or strings as one example. OK. Now, there are cases where you really do have to implement equality and Hashing by hand. So let's look at one example of that. Let's say I have a data type that represents a city and it's got a name, it's got the state that it's located in, and it has the population. And let's say that for the purposes of this example I only have to compare the name and the state for equality and if I know those are equal I don't have to check the population. So if I let the compiler synthesize the implementation of equality here it's going to do unnecessary work because it's going to be comparing that population field. But I certainly write it out by hand and maybe in this case it's not too bad. But what about Hashable? So if I want to calculate the Hash code of the city object, then I'm going to calculate the Hash code of the name and the Hash code of the state and I have to combine them somehow. But how do I do that? Well, I can use an exclusive or operation or I could use some random math formula that I found on the Internet or just came up with myself. But neither one of these is very satisfying and it feels like these Hash combining functions have a lot of magic to them. And the cost of getting it wrong is pretty high because the performance properties that you expect to get from a dictionary or a set really rely on having a good high-quality Hash function. There's also a security angle here. So if an attacker is able to craft inputs that all Hash to the same value and send them to your app over the Internet somehow, then it might slow your app down to the point where it becomes unusable creating a denial of service attack. So in Swift 4.2 we've added a better API for this. Now recall the Hashable protocol in Swift 4 and 4.1. It has a single Hash value requirement that produces a single integer value. In Swift 4.2 we've redesigned the Hashable protocol so now there's a different Hash into requirement. And instead of producing a single Hash code value, Hash into takes a Hasher instance and then you can feed multiple values into the Hasher, which will combine them into one Hash code. So going back to our example of the city data type, all we have to do is implement Hash into by recursively calling Hash into on the name and the state passing in the Hasher object instance that we were given. And the Hash combining algorithm in the Hasher, it does a good job of balancing the quality of the Hash code with performance and as an added layer of protection against denial of service attacks, it uses a random preprocess seed, which is generated when your app starts. And we think that it should be pretty easy to migrate your code to using the new Hashable protocol and we encourage you to do so. The one caveat to watch out for is you might have some code where you're expecting that Hash values remain constant from different runs of your app or that if you iterate over a dictionary or a set you're going to get the elements in the same order. And this is no longer the case because of that random preprocess seed. So you will need to fix your code. And to make this easier we've added a build setting, the Swift Deterministic Hashing Environment Variable, which you can enable in the scheme editor to temporarily disable that preprocess random seed. OK, so let's talk about generating random numbers. So how do you generate random numbers in Swift today? Well, you have to use imported C APIs. And this is really not ideal because they are different between platforms and they have different names, different behavior, so you have to use build configuration checks. But also they're quite low level and these common operations that are not quite so obvious to implement. For example, if I want to get a random number between 1 and 6, then I might think to just call this Darwin arc4random function and then calculate the remainder of dividing by 6. But that actually gives you a result that is not uniformly distributed between 1 and 6. So in Swift 4.2 we've added a new set of APIs to make this kind of thing easier. First of all, all the numeric types now define a random method that takes a range and returns a number uniformly distributed in that range. This uses the correct algorithm and it even works for floats. For higher level code we've added a random element method to the collection protocol. And just like min and max this returns an optional so that if you pass in an empty collection you get back no. And finally there's a shuffled method on collection where this gives you an array with a random permutation of the elements of that collection. And we think the default Random Number Generator is a good choice for most apps. But you can also implement your own. So there's a random number generator protocol and once you write a type that conforms to this protocol you can pass it to all these APIs that I talked about which have an additional overload with a using parameter that takes a random number generator. OK, so we saw these build configuration checks earlier. Let's talk some more about them. Well, this is a pretty common pattern in Swift. You have a little piece of Swift code that is shared between iOS and macOS and on iOS you want to do something with UIKit. On macOS you want to do something similar in AppKit. So if you want to do this today you're going to write a #if compile time condition check and then you have to list out those operating systems where UIKit is available. So but what you really care about is not that you're running on this particular operating system, but that you can import UIKit. So on Swift 4.2 we've added a has import Build Configuration Directive so you can better express your intent. And with the new features of Swift 4.2, I can actually improve this code further. So let's say that I'm also going to explicitly check for AppKit and then if neither UIKit nor AppKit is available, for example if I'm building on Linux, I'm going to use the new #error build directive to produce a friendly compile time error message. OK, now here's another similar source of boilerplate. So if I want to compile something conditionally when I'm in the simulator environment, then today in Swift 4 I have to copy and paste this ugly thing everywhere I want to perform that check. In Swift 4.2 you can use the new hasTargetEnvironment condition, to again better state your intent and just explicitly ask the compiler, am I compiling for the simulator or not? And while we're at it, let's replace that FIXME with a #warning build directive to produce a message or compile time so that I don't forget to fix my FIXME. OK, so that about wraps up all the features that I'm going to discuss today, but I have a couple more things to talk about. Let's unwrap, Implicitly Unwrapped Optionals. That's a horrible pun. OK, so Implicitly Unwrapped Optionals can be a little bit confusing and let's first review the mental model for Implicitly Unwrapped Optionals. How do I think about them? Well, so since Swift 3 they're not the type of an expression. Don't think of it as a type. Instead, think of Implicitly Unwrapped Optionals as an attribute of a declaration. And what the compiler does when you reference such a declaration is it will first try to type check it as a plain optional and then if that doesn't make sense in the context where it's used, it goes ahead and unwraps it and then type checks it as the underlined type. So let's look at an example of the first case. So here I have two functions, the first of which produces and implicitly unwrapped optional integer and the second of which takes a value of any type. And I'm going to call the second function with the result of the first function. Now in this case I can store an optional Int inside of an Any and so no forced unwrapping is performed. The value simply becomes a plain optional. Let's look at an example of the second case now. Here, the first function now produces -- sorry, the second function now takes an integer. So when I call the second function with the result of the first function then I cannot pass an optional Int where an Int was expected. So the compiler has to insert a force unwrap and then it all works because now I have an Int and an Int And this mental model makes Implicitly Unwrapped Optionals very easy to reason about. But until recently the compiler had some edge cases where it did not always follow this model. So recall that you cannot have an implicitly unwrapped optional that is part of another type. And this is still the case in Swift 4.2. I cannot have an array of implicitly unwrapped Int's. However, in Swift 4 previously, there is some edge cases like this. I could define a type alias where the underlying type was implicitly unwrapped Int and then I could make an array of this type alias and I would get very confusing behavior from the compiler that made code hard to understand. So in Swift 4.2 we've re-implemented Implicitly Unwrapped Optional so that it exactly matches the mental model I outlined earlier and this confusing code example now generates a compile time warning and the compiler parses that as if it was just a plain array of integers, of optional integers. Now, most code will not be affected by this change to Implicitly Unwrapped Optional, but if you were accidentally relying on these edge cases I encourage you to check out this blog post on Swift.org that goes into a lot of detail and has a lot of examples about what changed and how. OK, now there's only one more thing here today. Let's talk about memory exclusivity checking. So if you recall, in Swift 4 we introduced something called Memory Exclusivity Checking, which was a combination of compile time and runtime checks that restricted certain operations from being performed. In particular we banned overlapping access to the same memory location. What does this mean? Well, let's look at an example. So here's a piece of code that implements a data type for operating system paths. And this is represented as an array of path components. And there's a withAppended method. This method adds an element to the array, then in calls a closure that you pass in and then it removes that element from the array. And this code is totally fine, it's a valid Swift 4 code. But let's look at this usage of our path data type. So here I have a path that's stored and a local variable and then I call withAppended on it and inside the closure I access that local variable again printing it. So what the problem here? Well, it turns out this code is actually ambiguous because when I access that local variable inside the closure, it's already being modified by this withAppended method, which is a mutating method. So the ambiguity is that do I mean the original value of path as it was before I called withAppended or do I mean the current value that is being modified whatever that means. Well, in Swift 4 this was a compile time error because it was an exclusivity violation. And one way to address this is to resolve the ambiguity by telling the complier, hey I really want the new value so I'm going to just pass it in as a parameter to the closure instead of capturing it. OK, but now look at this example. So this is almost the same function except that it's generic, it's prioritized by the return type of the closure. And in this case we can have the same kind of ambiguity by accessing the path value from inside the closure. But previously Swift 4 did not catch this error at compile time. In Swift 4.2 we've improved the static exclusivity checking to catch ambiguities like this in more cases. And in addition to improving -- OK, and you can also fix the ambiguity in the same way by passing it as a parameter to the closure. In addition to improving the static checks, we've also added the ability to use the runtime exclusivity checks and release builds. And this has some overhead but if your app is not performance critical, we encourage you to try this out and leave it on all the time. In the future, we will get the overhead of these dynamic checks down to the point where we can leave this enabled all the time and it will give you an extra level of protection just like array bounce checking or integer overflow checking today. And there's a lot more in Swift 4.2 that I didn't talk about today. And we encourage you to try it out on your existing apps. We also want you to try out the new features and if you have any questions please come to the labs and ask us. Thank you.  Good afternoon, welcome to Optimizing App Assets, my name is Will, engineer on the Cocoa Frameworks group and today make my coworker Patrick and I are going to go through some of the best practices for optimizing assets in your application. In this day and age, many apps and games offer great user experience by incorporating high fidelity artwork and other type of assets in their application. And by doing so they're able to attract a large number of audience, as well as they're engaging. We want this to be true for all of your apps as well and that is why we're here today to showcase some of the best practices with Asset Catalog and also more importantly, on how you can better deploy the assets in your application to your users and how to translate that to the overall user experience. And throughout this talk we're going to touch on a variety of different aspects through the traditional, design, develop, and deployment workflow. But first I'd like to spend a little bit of time to talk about a topic and that is image compression. Image compression is at the heart of the Asset Catalog editor and is the last step in the Asset Catalog compilation pipeline. And is greatly related to some of the other optimizations that happen throughout the pipeline. By default Asset Catalog offers a variety of different types of compression types and is also by default able to select the most optimal compression type for getting any given image or texture asset. While that may be sufficient for most projects, it's still a good idea to understand what are some of the options offered and more importantly, to understand what are their trade-offs, as well as what are the implications on your project. Now before I dive into the specifics of any image compression, I'd like to talk a little bit about another optimization in Asset Catalog that has a huge implication on all of the compression that we do, and it's called automatic image packing. Traditionally, before the inception of Asset Catalog, one way to deploy assets in your application is just to dump a bunch of image files in the application [inaudible] of your project. It's important to be aware that there are many drawbacks, as well as trade-offs when doing this approach. There are two sides of downsize that you have to be aware of. The first comes from the additional disk storage that comes with doing so. Traditional image container formats uses extra space to store metadata, as well as other attributes associated with the underlying image. Now if your application has a huge number of assets, and if they have similar metadata, the same information gets duplicated over and over on disk for no real benefit. Additionally, if most of your assets are fairly small then you do not get the full benefit of most image compression. The other type of drawback comes mainly from the organizational overhead that you have to pay for. It is very hard work with a large cache of loose image files and it's also much harder to interact with them from the NSImage and UIImage family of APIs. Last but not least, you also have to deal with the inconsistency in image format, as well as other image attributes as well. For example, in your artwork collection you can have a mix of images but some of them support transparency while the others do not. The same applies to other attributes, such as color space and color gamut. As the catalog is able to address all these problems by identifying images that share a similar color spectrum profile and group them together to generate larger image atlases. This way you do not have to store the same metadata over and over for all of your image artwork. And you also benefit better from all the underlying image compression. Now let's take a look at a real-world example. Here on the left-hand side of the screen there are a dozen image artwork. These may look familiar to you and that is because they are taken directly from one of our platforms. Now these image artwork are all fairly small, but still the overall size add up to over 50 kilobytes. This automatic image packing Asset Catalog is able to identify that all of these image artwork share very similar color spectrum and if so it'll group them together to generate one single larger image atlas. This way the overall disk size gets reduced to only 20% of the original size. That is an 80% size reduction saving and that is huge. It's also important to be aware of these optimization scales very well. The larger the amount of our asset in your application, the more benefit you're going to get out of this optimization. So that's automatic image packing. Now let's talk a little bit about lossy compression. Lossy compression is all about trading minor losses in vision fidelity for the large savings that you gain from the underlying compression. So it's really important to understand what are the scenarios in your application where lossy compression is most applicable to. Typically we recommend you use lossy compression for image artwork that have fairly short on-screen duration. For example, that will be artwork that is shown on the splash screen of your application or through animations and effects. Now it wouldn't be exciting for me to just stand here to talk about lossy compression without introducing a new lossy compression in the Asset Catalog. So I'm very happy to announce that this year we're extending support of high-efficiency image file format in Asset Catalog. If you followed our announcements from last year, you know that we introduced high-efficiency file image format on all of our platforms, as well as in the Asset Catalog editor. This year we're taking it one step further, we're making high-efficiency image file format with default lossy compression in Asset Catalog. Thank you. Now let's have a quick recap of some of the benefits that we get from high-efficiency image file format. The most important thing to know that it's able to offer much better compression ratio than compared to some of the existing lossy compression that we already offer. One that you may be already familiar with is JPEG. There are many benefits that come with this high-efficiency image file format, such as support for transparency out of the box. And more importantly, it's important to be aware that Asset Catalog is able to automatically convert image files from other formats to high-efficiency image file formats, which means that as long as your image assets are tagged to this lossy compression there are no extra required, no extra action required on your end. This all happens automatically in the Asset Catalog compilation pipeline. For more in-depth information on high-efficiency image file format, I suggest you refer to our session from last year. Now let's shift our focus to lossless compression. Lossless compression is the default compression type and it's used for the majority of application assets. Therefore, it is really important to understand how you can get the most benefit out of lossy compression. Typically image artwork can be categorized into two groups based on their color spectrum profile, and they each benefit differently from any lossless compression. Let's take a look at that. The first category of images are commonly referred as simple artwork. And they're referred this way because they have a fairly narrow color spectrum and a fairly small number of discrete color values and that is because of the simplistic designs. And they're best represented as many application icons. On the other hand, the other type of image artwork are referred as complex artwork. Again, both these types of image assets benefit differently from lossless compression. And generally speaking, any lossless compression will do really great to either one of them because they're optimized for it. We realize that both of these are really important in many projects. And we also want to have all of your assets to be deployed through the best lossless compression possible. So I'm very happy to announce this year we're introducing a new lossless compression in Asset Catalog and it's called Apple Deep Pixel Image Compression. Thank you again. Apple Deep Pixel Image Compression is a flexible compression that is adapted to the image color spectrum. What that means is that it's able to select the most optimal compression algorithm based on the color spectrum characteristics of any image artwork. This year not only we're extending this new compression to all of you guys, we're also enabling it on all of our platforms, as well as first party apps. And by doing so we're able to observe on average 20% size reduction across all of our built projects, which is a pretty big deal. Now let's look at some numbers. Here's a chart that shows you the overall size of all the Asset Catalogs from some of our select platforms. And it is immediately obvious that we're able to see about up to 20% size reductions across all of our platforms. When it comes to lossless compression, compression ratio is only half the story. Because of the fact that lossless compression is used for the majority of your application artwork, decode time is just important as well. Apple Deep Pixel Image Compression is also able to offer up to 20% improvement in decode time. So that was lossless compression. Now I'd like to shift gears to touch on two separate but strongly connected subjects that have a huge implication on all the optimizations and compressions that I just talked about, and their deployment and App Thinning. Here's a quick recap of what App Thinning is. App thinning is a process that takes place in the App Store that generates all variants of your project targeting all the device models, as well as versions of your deployment target. When we take advantage of App Thinning is to have the deployment target of your application to a version that is lower than the latest version of the platform you're targeting. This way you'll be able to reach more audience. App thinning is able to take care of generating all the variants of your project and deploy the most optimal one across all of your user base. This year if you build your project with Xcode 10 and the iOS 12 family of SDKs, your project is automatically going to benefit from all the optimizations and new compressions that I just talked about. However, if you back deploy your application to an earlier version the new optimizations are not preserved. And that is because App Thinning has to generate variants that are compatible to the earlier versions of the targeted platform. This isn't ideal and more importantly, we really want all of your assets to be deployed in the most optimal manner. So I'm happy to announce this year we're introducing a new version of App Thinning, called OS Variant Thinning. With OS Variant Thinning your application can still target those that are on earlier versions of your target platform, say in this case from iOS 9 all the way to iOS 11. And for those that are running on the latest version of iOS, OS Variant Thinning is able to generate a special variant of your project that has all the latest optimizations and compression types. This way everybody is able to get the most effective version of your project and everybody's happy. So that was App Thinning and backward deployment. Now I'd like to walk you through an example of how you can exercise the same App Thinning expert workflow locally within Xcode. It is a fairly simple process, all you have to do is go to Xcode archive button. This will simply instruct Xcode to generate all variants of your project. Once that is done simply click on the Organizer button and that will bring up a window that shows all the variants generated for your project. And here's a window for the garage band project that we took to perform this exercise. The first thing that Xcode is going to ask is to select a type of distribution method that you can distribute all the variants that it just generated. For the purpose of this exercise, simply select Ad Hoc Distribution. And on the next window, in the App Thinning field, simply select all compatible device variants. This will instruct Xcode to export all the variants that are targeting all the supported device types. Once this is done Xcode is able to synthesize a report that summarizes all the variants that it just generated. And there are a few key data points that you can extract from the report to help you better understand the deployment of your project and again help to answer a few key questions such as how many variants are generated from my project, what do their sizes look like, and are there any rooms left for the optimization and fine-tuning for any particular variant. And it turns out that actually half the numbers generated for the garage project that it just exported and let's take a look at that. So here on this chart it's going to show you the sizes of all the variants generated for a select set of device models. And these are the sizes generated for the iOS 11 and earlier versions of the variants. Now because garage band is a fairly large project, with tens of thousands of image artwork the sizes of the generated variants range from 90 to over 100 megabytes. And here are the numbers for the iOS 12 variants. And again from this graph it is immediately obvious that we're getting about from 10 to 20% saving sizes in size reductions. Now if this number looks familiar to you by now all of these optimizations are from all the optimizations and compressions that I just talked about. So that is image compression. Now I'd like to hand it to my colleague Patrick to talk about design and production of your application asset. Thank you Will. So that's great, so you just heard about some amazing ways that you can get your assets improved just by using Asset Catalogs in Xcode. I'm going to talk a little bit more about a few other things that you can do with just a little bit of effort in Asset Catalogs to really optimize your application's assets. So and I'd like to begin with design and production because this is really where it all begins. So assets as you know they come from many tools, many different workflows, many different sources but they have one thing in common, they ultimately all came from humans at some point. And it really pays to be organized in terms of understanding that process of how those assets come into your software workflow and to pay attention to some of those details that can really pay big dividends in your application efficacy. So the topic I'd like to talk about first is color management, often overlooked but still quite essential. So on disk an image asset is just a bunch of boring bytes right, it doesn't really mean anything until you apply color to it. How does it get the color, how does the system even know what each of the numbers on those bytes means? Well the answer is it comes from the color profile, that is what actually gives each one of those color [inaudible] a value and an absolute colorimetric value, it tells the system how it should look. As such, I want to emphasize that it's really important to maintain those color profiles in your assets as source artifacts. These are vital pieces of metadata that keep the designer intent intact on how that asset was delivered. And resist the temptation to strip those profiles out because you think they're just extra metadata that you know take up a bunch of payload. These are source artifacts that are checking into your project, let the tools worry about optimization for deployment. So why is any of this color stuff important? Well the answer is our devices have a broad range of displays with different characteristics and something needs to make sure that the actual colors in your assets match appropriately and look appropriate and get reproduced appropriately on all those different displays, that's the job of color management. This is a computational process, it can be done either on a CPU or at times on a GPU, but it is some work. Now Asset Catalogs come into play here because what they will do is at build time in the compilation process they will perform this color matching for you. And this is really great because it means that computation is not happening on device when it really doesn't have to. And your assets are ready to go on device and ready to be loaded and ready to be displayed without any further ado. And as a bonus this extra processing we do to do this color management at build time eliminates the profile payload that you might've intended to strip earlier and replaces it with an ultra-efficient way of annotating exactly what color space we now have and the pixels on disk, so that's color management. A related topic I'd like to talk about here is working space. Now by working space I'm really referring to the environment in which these assets actually got originated in the first place, this is the designer or maybe as an engineer are working on some artwork yourself, you're working in a design tool, you're creating content. It's important in these contexts to use consistent color settings for all the design files that you have for your project. This actually is a good practice and it actually has positive technical benefits because it ensures a consistency between how you organize everything across your application. There are two specific formats that are most talked about and most recommended for creating working design files. sRGB 8 bits is by far the most common, a very popular choice and it has broadest applicability across all of our devices and your content types really. However, if you're working on a really killer take it up to the next notch vibrant design like this wonderful flower icon here for example you may want to take advantage of the wide color characteristics, capabilities of some of our devices and use the wide color, use a generated wide color asset. For this we recommend you use Display P3 as your working profile and 16 bits per channel to make sure you don't lose anything in executing that design. Now Xcode and the platforms, runtime platforms have a wide range of processing and management options to handle this wide color asset. I'm not going to go into too much depth here, but I encourage you to refer to and look at the Working with Wide Color session that I did two years ago where I went into some depth on these topics and it gives you some more background for this. Also, new since last year there's a great treatment of working with P3 assets up on the iOS Design Resources section of the developer.apple.com website. Okay now let's get into some actual software art here. Okay so you may have, your UI typically has to adapt to a lot of different presentations and layouts, this can commonly call for artwork that actually needs to stretch and grow to adapt to those layout changes. How do you accomplish this with artwork? Well the most common approach is to identify a stretchable portion of the image and the unstretchable portions of the image. Why is there a difference? Well considering, this is a crude example here on the slide, but imagine that we had a beautiful shape to the overall asset and like round corners that you wanted to preserve at all possible sizes like a frame. You want to make sure that you don't stretch those pieces, the blue pieces in this slide but you can stretch the yellow pieces. So traditionally the way this is done is with the modern design tools is to slice all these items up, identify all these regions, and distribute them as individual assets. Then the programmer would reassemble these in the final design size using a draw 3 or a nine-part API for example. Now this works fine and has been a tried and true practice for many years, but it does have a downside. Reassembling those images at a final size is a pretty CPU intensive task and it can be a bit complex and inefficient and it's not really a good fit for modern GPU UI pipeline like core animation. What's a better approach? A better approach is to take a single image and just provide the stretching metadata for it that identifies what the stretchable portion is. And that really enables the most optimal smooth GPU animation of that resizable image. And I'm happy to tell you that Asset Catalogs makes this really easy to do and it's called the Show Slicing editor. It's really easy to work with, you just click the Start Slicing button and then you start manipulating the dividing lines here which actually lets you identify the stretchable portions of the image and the unstretchable portions of the image. In this example the left and right end caps and then that middle slice that's orange is the one that's the stretchable piece. Now you may notice there's a big piece of this image that has got this white shading over it, what is that all about? Well that's actually a really interesting thing, that part of the asset is not actually going to be needed anymore because we can represent any possible size with the three remaining pieces. Okay why is this important? Well the nice thing is now that Xcode knows this at build time we can actually just take the pieces we need and leave the rest behind. So that large section we don't have to include that in the bytes on disk that we actually ship in your app, that's great. And it also means that it has a secondary benefit and this is a more subtle one, but I really like this. It means that you can tell your designer to feel totally comfortable delivering assets at sort of their natural size and don't have to worry about pre-optimizing them to be the smallest possible things so that it's efficiently deployed right. That shouldn't be the concern of the designer it's actually much more meaningful over the long-term to put something in the source code that's easy to look at and obvious what it is and let the tools worry about these deployment details. So in addition to the graphical inspector and graphical way of identifying the stretchable portion there is of course also the Show Slicing Inspector which where you can have fine control over these edge insets and also control the behavior of the centerpiece when it stretches or tiles. All this of course adds up to keeping the stretching metadata close to the artwork which will then yield enormous benefits the next time, which inevitably happens, the designer comes up with a new update to your design. Now you can update everything in one place and don't have to remember the five or six places in code where you might have a hang code of the edge insets previously, now it's all tied together in one place. Thank you. Okay next up I'd like to talk about vector assets. So because our displays on all of our products have a variety of different resolutions you're probably very used to delivering 1x, 2x, 3x depending on what platform you're targeting, distinct assets. And that's fine and works really well, but it's kind of a mess to have to deliver three or two or three assets every time for a single design for no other reason than just resolution. What if you can actually get away with this with just one asset? Well you can and we've been supporting vector assets in Asset Catalogs for a number of years now in the PDF format. And with Xcode Asset Catalogs you can actually supply a PDF and Xcode will generate and rasterize that PDF into all of the applicable scale factors that your app is currently targeting depending on platform. And that's really great because it means you don't have to worry about paying any cost at runtime on device to render an arbitrary, potentially arbitrarily complex PDF vector asset. So it gives you some peace of mind about using vectors. Now you may have a scenario where actually you want to present your assets in some circumstances at a different size or scale than the most common natural size that the asset was designed for. Well new since last year in iOS 11 and Xcode 9 we now allow you to preserve the vector data so that when that image is put into an image view that is larger than the natural size of that asset it'll go ahead and find that original PDF vector data, which by the way we've linked it out and cleaned of any extraneous metadata and profiles as well so it's nice and tight and it's slim as possible. And we'll go ahead and re-rasterize that at runtime but only if you're going beyond the natural size, otherwise we'll use that optimized prerendered bitmap. So this is great because it means your app might be able to more flexibly respond to dynamic type and automatically your images will look more crisply when you resize your UIImage view. That's vector assets. Okay next, I'd like to talk a bit about designing for 2x, 2x commonly known as retina is the most popular and common display density that your apps are probably being experienced on. And it's great right, it was a huge step forward, however, there are still cases where you can have designs where a stroke or an edge might land on a fractional pixel boundary and result in a fuzzy edge. It's still not high-resolution enough that you won't notice a difference between a sharp edge and a fuzzy edge. And this can still be a challenge in designing assets at times. Well what are some techniques that can be used to address this? One common design technique is to turn on point boundary snapping in your vector design tool, set up a grid at one-point intervals and turn on snapping so that when you adjust your shape or your control points that you know that they can snap, when they snap to a boundary you know that that's going to be a pixel boundary and that's great. But there's still some cases that you might have with a design where some of the edges are still perhaps landing somewhere in between one and two and you're not sure, but you'd really like to know, especially on a retina 2x device what's going to happen there and can I optimize further for the actual display density. Well what you can do is you can actually use a 2x grid, make your asset twice as nominally big in your vector design tool and make that grid now be a one-pixel grid where every two points, every two units is going to be one point for retina. And then adjust your assets and use the point snapping to adjust your strokes and edges to fit there. Okay that's great, now what do you do with this thing once you've got it, it's too big right, it doesn't work? Yes it does, all you have to do is just drop it into the 2x slot in the Asset Catalog scales bins and that will automatically enable Xcode to process that artwork, realize it's actually a 2x piece of artwork, it's slightly too big, one point is not equal to two pixels of retina, but rather the other way around. We'll do all the math, we'll render all the right rasterized bitmaps for all the other scale factors and handle that for you. Freeing the designer to use that 2x grid which can be rather helpful. Of course, if the automatic scaling that we do is insufficient or still presents problems in some areas, you can have ultimate control of your results as always by dropping in hinted bitmaps into the appropriate scale factor bins and we'll go ahead and use that and prefer that over the generated PDF rasterizations. Okay so that's a bit about design and production ends of things, now let's talk about cataloging and sort of the organizational aspects once you're in Xcode. So those of you who have played around a little bit with Xcode Asset Catalogs it can be a bit overwhelming to see how many things there are in front of you there and what you're supposed to use and how many options there are. Well I'm here to tell you, you really should only use what makes sense, what makes sense for your project and what makes sense for the content that you're working with. There's a lot of options here and we've made a very powerful engine and organizational scheme here that has lots of capabilities, but you really need to fit it to the need that you have and use the simple, start with simple first and then go from there. So I'd like to talk about two organizational techniques that can help in this area. The first is bundles, now why would I be talking about bundles in an asset talk that seems rather incongruous? Well I'm really trying to address large projects. So if you have a large project where there are perhaps multiple frameworks involved, maybe you even work with multiple teams. Dealing with assets can sometimes be a pain if you have to pour them all into the main app bundle and have to manage them all there and make sure names don't conflict and appropriately sourced to the relevant parts of your application. One of the ways you can solve this problem is by building those assets into multiple bundles because Xcode will always generate a unique Asset Catalog deployment artifact per bundle or target. So for example, consider creating an artwork only bundle as an example and this can be for a good reuse strategy for example to have a single consolidated component that contains all your artwork, that has a consistent namespace, that can provide images to the rest -- to the other components of your application. How do you retrieve these? It's simple, you just use the image constructors like UIImage named in bundle compatible with trait collection that gives the bundle argument. On the macOS side of course there's the NS bundle category image for resource. And keep in mind, that each of these bundles provide a unique namespace, so within them the names have to be unique. But across bundles you feel free to use any naming convention you like. So speaking of namespaces, there's another feature I'd like to call attention to and another challenge with large projects. Now in this case the problem I'm addressing is large collections where they might have some structure in them right. So let's imagine you have an app that deals with 50 different rooms, each one of those rooms has a table and a chair in it and there's assets for each of those. In your code you'd really like to refer to them as table and chair, that seems like the most natural thing but unfortunately there's 50 of them what are you going to do. One alternative is to just generate a naming convention of some form and figure out how to demux that in your code, that's not ideal. One solution that Asset Catalogs can offer is to use the provide namespace option. By checking this box after organizing your artwork into a folder, we will automatically prepend the folder name into each image's record in the Asset Catalog, which you then use to retrieve it. This can be a nice way to organize large structure collections of assets. Okay so we talked about cataloging, now let's talk about some exciting stuff around deployment which is really where the exciting stuff starts to kick in. So Will talked about App Thinning, I'd like to give some overall perspective on what we try to do with Asset Catalogs in App Thinning. So overall what you're trying to do is you're providing all the content variants for your application, you're adapting your content to the needs of the various devices your app runs on. The most common technique for doing this is you know split across product family iPad or iPhone, tv or watch or different resolutions, 3x and 2x. You provide all those content variants to effectively adapt your content and then App Thinning is responsible for making sure that we just select the right subset of that content that's appropriate for the device that your customer is running the application on. Well I'd like to talk about a different way you can approach the same sort of content adaptation and that is performance classes. It's a different way of looking at the exact same problem. What if the entire product mix the way your application saw that continuum was instead segmented by performance capability, not by other characteristics? Well this is what you can do with Asset Catalogs. There's such a broad range of hardware capabilities between the supported devices that we have, even if you go back a few iOS, I mean all the way from say an iPhone 5 up to the latest iPhone 10, that's a huge range of performance capability. Wouldn't it be nice to take advantage of that and avoid the needing to constrain your app to the least capable device that your application needs to support? That's the goal here to be able to have your cake and eat it too and to do that you can solve it with adaptive resources. I'm going to tell you how now. So there are two main ways that we divide the performance continuum. The first is memory classes and this is perhaps the most important one. We have four memory tiers, 1 GB through 4 GB and that corresponds to the installed memory on the various devices and again this is across our entire product mix, it doesn't matter what it is, it's in one of these bins. The second access of collection is graphics classes. Now these actually correspond to two things. One, they correspond to Metal feature family sets, which if you're a Metal programmer you may be familiar with, this is the GPU family concept. But they actually correspond also exactly with a particular processor revision in your device. So Metal 1 corresponds to Apple A7 all the way through Metal 4 which is the Apple A11 processor. And we allow you to catalog and route assets to each of these particular graphics classes. That can be pretty powerful by itself either one of those, but where it gets really interesting is when you can combine these two traits together to form a full capability matrix that you can really finally calibrate how you want to adapt your assets to this hardware landscape. Now how does this work? I'd like to explain this to you by walking through a simple example and this is really key to understand how we do things, it helps you understand how you might be able to use it. So in this example, we've provided three specialized assets, one any any which is just the backstop for the lower capability devices. And then we provide two optimized assets, one for 3 GB devices with Metal 3 or better and one for 2 GB devices with Metal 4. So let's imagine that I'm selecting the asset from the context or the viewpoint of an iPhone 8 Plus. So I'm 4 GB, Metal 4, that's where I'm starting and I'm searching, I'm finding nothing in 4 GB memory tier. So next, I'm going to go drop down a memory tier and look for anything that can be found in 3 GB memory tier. I do that and I find this asset here and I'm going to select that. Now what's important here is that I have selected this asset at 3 gigabyte Metal 3 even though there is an asset that actually matches exactly to my GPU class. But because we prefer and scan through things in memory priority order before we do graphics classes, we're going to select this first. This is really important because we have decided that memory is really the most important way that you can characterize the overall performance of a device, so we're going to prefer that as we go through the selection matrix. Okay that's how it works, how do you think about using it. Memory really represents the overall headroom of your device and is really the best aggregate indicator of capability. So it's a really good choice to use with larger or richer assets, more detailed stuff, things that are bigger on disk, things that are going to take a little more memory when they're expanded in in memory for rendering. Just a richer user experience is usually associated with higher memory. Now higher graphics is a little more subtle since that tracks the raw processing capability, both CPU and GPU, of the device so it's better for more complex assets. Maybe you use a shader that takes advantage of certain features that are only available on certain GPUs or not or you put an asset that requires a little more processing than others. I'd like to give two simple examples as food for thought on how this could work. And the way I'm going to give the example is by talking about NSDataAsset. NSDataAsset is a very simple piece of Asset Catalog but it can be very powerful. All it is is a way to provide a flexible container that you can put in your Asset Catalog with content variants around arbitrary files. This doesn't have to be an image, it doesn't have to be a very specific format, this can be anything. But you can use this with Asset Catalogs in App Thinning to route arbitrary data to these different performance classes. So that's an example, consider a cut scene video in a game. So you might provide, have a nice video that you put in sort of the mid-tier of the performance spectrum and then you might have a really awesome high resolution, maybe it's even HDR who knows video that you put in the really capable quadrants of that capability spectrum. And then on the lower end you put a still image or a very simple image sequence that's not going to take any time or excessive resources on those devices and still give those customers running those older devices a nice and responsive user experience. So that's one example. Another more intriguing example is plist, well why I put a plist in an Asset Catalog, it seems like there's much better ways to deploy plists than Asset Catalogs. Well when you use it in conjunction with NSDataAsset for example you could consider using a plist to tune your application with different configuration parameters that scale according to the performance class that you cataloged that plist in, in your NSDataAsset. So for example if you have an app that renders a crowd for example you could set the size of the crowd based on the capability of the underlying hardware and your code would automatically be self-tuned based on what device it's actually running on at the moment. So that's an interesting idea about how to use performance classes. Next, I'd like to talk about Sprite atlases. So Sprite atlases were introduced a few years ago in support of SpriteKit applications in SpriteKit games. Now, but I'm not going to talk about them in the context of SpriteKit based games, I'm going to talk about them in the context of regular applications. Now they have some attributes that are very similar to what Will talked about with automatic image packing, you're taking all of the related images in that Sprite atlas and packing them into a single unit, they get loaded at once, and then all the images that you reference that are contained within that atlas are just lightweight references to locations within that atlas. So that's great. But the key thing is that you don't really need to use SpriteKit to access these, you can just use this as a grouping mechanism because the one difference that Sprite atlases have over automatic image packing is you get to control the grouping and you to assign a name to it. So you can deal with it, you can have a little bit of control and sort of organize things that way. But you can still access the images contained within using the standard UIImage and NSImage APIs, and the names within that. In addition, there is an intriguing way that you can use SpriteKit framework even though you're not building a SpriteKit application by taking advantage of the SKTextureAtlas preload texture atlas named API if you have a case where you have a large number of images that have to be loaded fairly quickly and have to be used right away. So what this API will do is it'll preload or load from disk, decode, get ready and warmed up in memory asynchronously with a callback completion handler for a set of named atlases. So this is great, but I will caution you, do not use this API indiscriminately because it does exactly what it says it's going to do and that means it's going to potentially consume a large amount of I/O and memory to load all those images. So please be sure that you're about to use them right away and that it's the right choice, otherwise a jetsam awaits you. So another powerful thing about Sprite atlases is that any image within the Sprite atlas has all of the regular features of any other normal image set within code Asset Catalogs, including all the cataloging features, all the compression settings, and all the App Thinning features. We will take care to automatically split and collate all of the images that you provide appropriately, split them by pixel formats, by different device traits, and different compression types. And make sure everything gets baked appropriately and then thinned appropriately so that the data gets routed to the right device in the right way. So those are some interesting details about deployment, we're in the homestretch here. So I'd like to remind you of the important things about optimizing app assets here. First and foremost, I think Xcode Asset Catalogs are really the best choice for managing the image resources in your application. This year you get 10 to 20% less space on disk just by using our new compression algorithms. No matter what deployment target you have, your iOS 12 customers will get those benefits thanks to the improvements in App Thinning, which now optimizes for the latest OS going forward. And we have a bunch of cataloging features that you can use to adapt the resources of your app to the devices your customers use. For more information please look at this link and hope you all had a nice day. Thank you.  Hi everyone! Good morning! Thanks, everyone, for being here today. My name's Jesse Pease. And I'm an engineer on the Health Engineering Team. And today, I'm going to show you how to make your very first app. So today we're going to step out of our comfort zones. We're going to explore, and we're going to start building our dream app. You know that one that you've had in the back of your mind while you're waiting in line for your coffee in the morning? Or that one that you drew out on your napkin on the flight out here to WWDC? Or that one that you tell your friends about and make them promise not to tell anyone else? That one. That's the idea we're going to start working on today. As Joseph Chilton Pearce said, "To live a creative life we must lose the fear of being wrong." And when we start building new ideas, we're bound to run into bumps in the road. But our goal today is to show you all the building blocks you need to make your dream app a reality. We're going to start off today by organizing our ideas. Then we're going to learn how to navigate Xcode. And then build a simple game using Swift. Now familiarity with a programming language will help you today. But if you're new to Swift or looking for a helpful resource, I recommend downloading the App Development with Swift book from the Apple bookstore. Or checking out the Swift Playgrounds app for iPad. Later [inaudible] is going to show you how to add multiple views within your app and then persist in display data for your users over time. So Tono [phonetic] and I were recently at a unicorn petting zoo. Yes, we actually have these in California. And we were after having this experience countless times of waiting in line. We decided that we needed to make something to keep ourselves occupied while we wait. So we decided to make a game. Something quick and fun to keep us occupied for a short amount of time. And I said, "Well, of course if we make a game, it has to have unicorns." But then we needed something that people would want to avoid. Maybe poop. And so we decided to make a whack-a-mole-type game with unicorn and poop. And then incorporate scores and eventually a leaderboard to keep track of our scores over time. Because we have a little bit of a competitive nature. So let's take a look at what this game play looks like. Upon pressing start game, a random emoji's going to appear somewhere randomly on the screen. Either a unicorn or a poop. At that point, the user has one second to click that button. Now, if they hit the unicorn, then they get one point. Then if they accidentally tap the poop, then the game is over. But in addition to this, if they miss the unicorn, the not-so-lonely unicorn, and so they automatically lose the game. So the goal is hit all the unicorns. Don't hit the poops. Let's take a look at this. Get it! Nice! Get that unicorn. Oh! There we go, one more. Oh! Oh, shoot. Moving too quickly and accidentally hit a poop. Let's jump right in. So for those of you who aren't familiar, Xcode is a tool we use on macOS to help build iOS, macOS, watchOS, and tvOS apps. You can download it from the App Store if you don't have it already. So when we open up Xcode, we first want to select create a new Xcode project. After this, we see all the different type of app templates that we can create. In our case, we're going to select single view app. But notice here that you could also select a page based app, a sticker pack app, or even a game. But our game is very simple, so we will just stick with single view app. And then hit next. From here we type in our app name, which in our case is Disappearing Unicorns. And our ex-- , our team name is Example Team. And hit next. After we choose a place to save our app, we're dropped into the project settings. Now we won't be making any changes here today, so we'll go ahead and look at the other important files in our Project Navigator on the left-hand side. The first one is the AppDelegate. An AppDelegate is created for each app, and it helps us manage the lifecycle of our app over time. There's certain methods that can be called when we're going into the background or coming into the foreground. But we won't be making any changes with our AppDelegate today either. Next I'm going to select the main storyboard file. The storyboard is where we create the UI or the user interface for our app. And where we can add all of the UI elements like buttons, labels, or images. But it's not just where we create that one single view. It's also where we lay out all of the logical flow for our app over time. The story. Next we'll select the view controller. The view controller helps us control out view, and this is where we'll write the logical code for our app. Well let's talk about how we create our views within our storyboard. Let's navigate back over to the main dot-storyboard file. And at this point, make sure not to select the launchscreen.storyboard. The launchscreen storyboard is what we use to create the view for when our app is being launched or loaded onto the screen the first time. You'll notice in the upper right-hand corner of our storyboard here that there's an object library button. This is a button with a circle and a little square inside. This is where we can take things like buttons or labels or images and drag them onto our storyboard. On the left-hand side, you'll see the outline view for our app. And if you click the little disclosure triangle, you'll see all the different things that are currently on our storyboard. Now you're thinking to yourself, "There's nothing on this storyboard. Why is this helpful?" Well, in the future, you may have lots of elements on your storyboard. And eventually you may not be able to find something because it could be under something else. Or maybe you've accidentally created duplicates. So this is a great place to go if you're looking for all of the elements that you've put on your storyboard. Alright. Let's go ahead and jump into our demo and start making our app. So here I'm going to go over to my computer where I already have Xcode launched. Okay. And I have my Disappearing Unicorns file. Here I'm navigating to the main storyboard. And let's go ahead and start off by adding all of those UI elements that we need to create our game. So I'm going to go up to the object library. And I can scroll through here and see all of the different things that I can put on there. We have labels, buttons, text fields, activity indicators, even images. But let's start off with that start-game button. So here I'll search for button. And click and drag the button to the center of my storyboard. Here I'll double click to change the text to start game, exclamation point. But this is a little bit small. So in order to make this larger, I can go to the upper right-hand corner of Xcode and open up the Inspector pane. And then select this little slider button which is the Attributes Inspector. This is where we change attributes of the items on our storyboard. In this case, I'm going to make my font much bigger. From size 15 to size 50. And hit done. Now to quickly create copies of items on our storyboard, I can hold down the option key and click and drag on my storyboard to quickly create another copy of this button. And so in this case, I'm going to change start game to the emoji for a unicorn. And to pull up the emoji keyboard, I can hit the control-command-space key and select unicorn. Great! Let's go ahead and make a quick copy of this as well for our poop. Control-command-space and poop. Awesome. And one more button for our leaderboard button. So click, option, drag. Double click leaderboard. Uh-oh. Not all caps. And change our font here from size 50 to size something much smaller. Maybe 25. Awesome. Now we need one more element on our storyboard which is a label for our points. So go back to my object library and search for label. And then click and drag this onto the screen. Here I'm going to change this text to be zero. And then change the font to something much larger like size 90. Awesome! There we go. Now we have all of the UI elements that we need to create our game. Let's go ahead and start connecting our UI to our code. Now to have both the storyboard and the view controller code up at the same time, I can open up this Assistant Editor. It's the button with two overlapping circles. Awesome. Now there's a bunch of different types of connections that we can make between the objects on our storyboard and our code. Now the first type is called an outlet, and an outlet lets us refer to our user interface within our code. Let's start out by creating an outlet for this start game button. To create an outlet, I select the start-game button on the storyboard, hold down the control key, and click and drag over to my storyboard. Sorry, over to my view controller code, and let go. At this point, I change my connection from-- type-- oh, it's already outlet. Perfect. And I'm going to name this start game button and hit connect. Awesome. Now we have a connection between our code and our storyboard. And we can double check that this connection was made correctly, by seeing this little gray circle to left of start-game button. If we hover over it, it highlights the item on our storyboard that is connected to this code. Now, in the interest of time, I've already written the code for our other outlets. But you'll notice that to the left of them, they have open circles. This means that they haven't been connected to our storyboard yet. To make the connections, we just click inside the circle to the left of good button in this case. And drag over to our unicorn button which is the button we want to be associated with this line of code. And let go. We'll do the same thing for bad button, leaderboard button, and points label. Awesome. Now all of these connections have been made. Now another type of connection we can make is called an action. And an action is a piece of code that's linked to an event that can occur within our app. In this case, I want an action method for whenever my start-game button is pressed. So to create this connection, I could hold down the control key once again with the start-game button and click and drag over to my view controller code. And let go. This time I will change my connection to type action and name this start pressed. And I want my event to be a touchup inside event, and hit connect. Awesome. You'll notice that there's this closed circle once again. And when I hover over it, the start-game button is highlighted. Now I've already written two other action methods for both our good pressed and our bad button pressed. But I actually want to make a different type of connection this time. I want to create an event or an action event for when I press down on my button. Because I want my user to get the point immediately when they touch the button. Not when they've touched down and then back up. I want to select the object that I want to be associated with this action. In this case, my good button or my unicorn. Then go back to my Inspector pane and click on this little button with the circle with a little arrow inside. This is called our Connections Inspector. From here you'll see all the different types of sent events that I can create a connection for. In this case, I want to create a connection for the touch-down event. I'll click inside this open circle, and drag over to the action method that I want to be associated with this event. And let go. I'll do the same thing with my bad button. Touch-down, click, and drag over to bad pressed and let go. Okay, great. Just two more things that we want to do before we can build and run our app for the very first time. The next thing we want to do is we want to do a little bit of set-up for our app. And we can do this in a method called viewDidLoad. And a viewDidLoad is created for a view controller at the very beginning automatically. And in here, what I want to do is a bit of setup. In this case, I want to hide some things. The first thing I want to hide is that points label. I don't want that on the screen the very beginning of our game. So I'll just type points label dot-- hmm. How about hide? Oh look? Looks like there's a property called isHidden. And if I hit enter, it'll autocomplete into Xcode for me. To see what isHidden does, I can hold down the option key on my keyboard, hover over isHidden and click. When I do this, I can read a bit more about what isHidden is, and what I can do with it. In this case, I see that it's both a getter and a setter. Which means that I can set isHidden equal to true, and it should hide my points label at the very beginning of my game. Now I also need some variables to keep track of my gameplay over time. And I've already written these, so I'll quickly add them using a shortcut. One of them needs to be, it's already initialized, but I actually need to populate it with some data. And that's this game buttons list. Now you're thinking to yourself, "Jessie, if we only have a unicorn and a poop, why would we need to create a whole array of buttons?" Well, in our case we want to make our code more extensible which means make it easier to build on over time. Because in the future, we may have more than just the unicorn and the poop. We may have a thumbs-up emoji. We may have a frowny-face emoji. So in this case, we want to say that game buttons equals list of good button and bad button. Okay. I've also written some helper methods that will help us with our gameplay. I'm going to add those now. One of them is this method called set up fresh game state. You'll notice that it makes sure that the start game and leaderboard button are unhidden. They're onscreen. It hides my gameplay buttons, and it sets up some initial values for my points. So here in my viewDidLoad, I also want to call up set up fresh game state. You'll notice here as I'm typing, Xcode is autocompleting all of the things that I could possibly put as some code. In this case, if I hit enter with set up fresh game state already highlighted, it'll automatically populate it for me. Okay. One last thing we want to do, and then we can build and run our app. Just for sanity, in my start pressed method, action method, I'm going to put a print statement here. And say start game button was pressed. And I expect that every time I press that start-game button, this line of code will be logged to my console. I can show my debug area by opening up this middle button here with the line on the bottom of the square. I'm going to hide my Inspector pane. I don't need that anymore. Okay. It's finally time. We can build and run our app. Now in the upper left-hand corner, you'll notice that my app name, Disappearing Unicorns, with a little arrow to an iPhone 10. If I click on iPhone 10, I can see all the different simulators that I can run my app on. Now a simulator is a simulation of a device on my Mac. And in this case, I want to select iPhone 10. Now if I had plugged my phone into my computer, I could also build and run my app on my own device which is pretty cool. So with iPhone 10 selected, I can click this play button here. This will build and run my app in my simulator. Here we go. Oh, nice! Okay, so when I press this start-game button, I should see that the start-game button was pressed is logged in my console down here. Awesome! We've just created a UI, and we've connected it to our code. Let's jump back over and start talking about the game logic for our app. Okay, so in the very beginning of our game, we know we have that start screen, and it has two buttons on it. It has that start-game button and that leaderboard button. But we're just talking about the gameplay right now. So we're going to wait for the start-game button pressed. And once that button is pressed, we drop into playing mode. And at the beginning of playing mode, we get a random emoji in a random location on the screen. Either a unicorn or a poop. At that point, we set a timer for one second to allow our user to have time to interact with our button. Now at the end of that one second, if the user did not press a unicorn, then the unicorn was lonely, and they lose. And if they successfully avoided tapping a poop, then they get another round of the game. Now we also want to say what happens if they actually do press one of the buttons during the game. Well if they press the unicorn button, we give them another round of the game and give them a point. Otherwise, if they accidentally select the poop, then game over. Okay. Let's talk about what this code looks like. We'll start off with the start screen. Now we're going to break up our code into logical blocks. And in our case, we're going to use methods to do this. So that it's not just one long file full of lots of code. And it's easier for us to look at very specific parts. The first method we're going to write is called start new game which we're going to call from start pressed. This is going to do some setup for the beginning of a game route. The first thing that happens in start new game is that we need to hide that start-game button and hide that leaderboard button. Then we need to update our points label and set it to zero. And we can even do some cool things here like dynamically change the color of our points label throughout our code. And here I'm going to set that text color to magenta during active gameplay. Finally at the end of setting up my start new game, I'm going to call the first round of my game. Okay? Let's look at that. In our one-game round, we first update our points label, make sure that it's up to date. Then we display a random button. Either that unicorn or that poop in a random location on our screen. I've written this helper method display random button that chooses a random x and y coordinate to place our button. Then we set a timer for one second. Now, if this timer goes off after one second, then the code inside the block of the timer will get called. And in that case, we want to check if the current button that's on screen is a unicorn, then the game is over. Because the user left a lonely unicorn again. Otherwise, if they successfully avoided the poop and that's still onscreen, they get another round of our game. Now let's talk about what happens if they did press one of our buttons. Well if they pressed the good button, we want to give them another point. We actually want to cancel our timer. And we're going to do this by invalidating it so that it doesn't go off, and then we'll call another round of our game. And if they press the bad button, well let's hide that button and get it out of the way so that they can't interact with it again. We'll cancel the timer again by calling invalidate on it. And then call game over which is a helpful method that I've written to help end the game. Okay, let's add this game logic into our game. Going to go back over to our Xcode project. And in this case, I don't need my storyboard up anymore. I'm just going to be making changes to my view controller. So I can select here the standard editor, and then select my view controller from my Project Navigator. Now, we've already written our code in our slides. So I'm going to just quickly add it into our view controller using some shortcuts. So here we have start new game where we hide our start game and our leaderboard button. Set up some initial stuff with our game points, and then call one round of our game. Then we need our one-game round. And here we update our points label, display random button, and call our timer. And finally we need to put the code and for the logic in our both our start pressed. And our good pressed. And finally our bad pressed. Okay. We have all the code we need. Let's see how it goes. Here I'm going to click this play button again to build and run our app in the simulator. Okay, I'm nervous. Let's go. Oh! Got one! Where's another? There's a lot of poops. There we go. Okay, I could seriously play this all day, but I probably should let [inaudible name] show you how to incorporate a leaderboard. Whew! So I'm going to try and lose on purpose here. Oh, there we go. Okay. Wow, yeah. That's great. That was incredible! We just made a game in less than 30 minutes. Just think about what you could do if you had more time. So we first started out by learning to navigate Xcode. Then we created a simple UI using our storyboard and connected that UI to our code. And finally we wrote game logic using Swift. All in less than 30 minutes. Now if we had more time, let's think about some things that we could do to take this further. Well we just used simple UI elements like buttons and labels. But you could use a framework like SpriteKit to create more imaginative buttons that have elements like gravity or physics associated with them. We could also add MusicKit integration and have Apple music songs playing in the background during active gameplay. Or we could read from sensors and change the speed of our game based on the user's movement. Here are three talks from previous WWDCs that I highly recommend you check out. Introduction to SpriteKit from 2013. Introducing MusicKit from last year. And Creating Immersive Apps with Core Motion from last year as well. Thank you so much. I'm going to welcome [inaudible name] on the stage to show you how to add additional screens to your app. Good morning, everyone. I hope you've enjoyed building the game with poops and unicorns. My name is [inaudible name] and I'm here to help you add some awesome features to your apps. Along the way we'll learn about APIs and frameworks that are commonly used across iOS. First, let's take a look at the features we want to add to our game. We would like to introduce a leaderboard where we can see names and the points for each person who's played our game. When we tap on a name, we want to see more information about that person. As we build our app today, we'll focus on three main areas. One is the data. Where should we store information about our users? How do we retrieve this data? Second is the user interface. What does the leaderboard look like? What use should it have? And finally, we'll talk about the logic that's used for managing [inaudible] for passing data, and for doing other tasks within our apps. If we break down our app into these three categories, then we would actually end up following a really popular architectural pattern known as the model-view-controller. The model-view-controller represents the data in the model. The UI is represented as the view. And the controller is the logic that communicates with both the model and the view. This pattern helps us group together similar tasks. And it makes it easy for us to make changes to one part of our app without impacting the others. In iOS, the controller can be represented as a subclass of the UI-view-controller class. The view can be a subclass of the UI view class. And the model can be a subclass of NSobject. When we build our app today, we will take a closer look at the different parts of the model-view-controller. Let's now jump into a demo and build a model to store our data. Here's the Disappearing Unicorns project that we made with Jessie. Let me right click on the Disappearing Unicorns folder and say new file. Select [inaudible] class which is what most iOS classes are. I'll click next, and I'll pull my [inaudible] class data. This will be a subclass of NSobject which is a good parent class for my data. Let's hit next and create. In this class, we want to store information about the player. So let me use a shortcut to add some code I wrote before. We have variables to store the name, points, rank, and image for each person. In addition to this, I've added some initializers to set up this data. But we don't need to worry about those. Let's go ahead and add one more class by right-clicking on the folder again and going to add files. This is the game data class that I've written already. Let's take a look at the methods that have been implemented in this class. First, we have the save points for name method. This is used to save the total game score for any given person. Next we have the player data for rank method which will give us all the information about a given player at a certain rank. And finally, we have a property to figure out the total number of people who played our game. We will not look at the implementation details for these methods today. What's important is to understand that we created a separate set of files to hold our data. When you build your own apps, I would encourage you to think about your data as a separate entity and keep it independent of the user interface. Just like how we've done here. When it comes to actually saving and retrieving the data on the back end, we have a few different options. One is to use core data. Core data is great for managing objects on the disc. It provides you with solutions to validate, query, filter, and organize your objects. If however your data is on a web server somewhere, then you could use NSURLSessions to download and upload the data. Another great option is to use CloudKit for storing data on the cloud. You could even use a third-party cloud provider to manage your cloud data. Now that we've set up our data, let's move onto the next section where we build our user interface. Here's what we want our leaderboard to look like. I'll break this down into the different viewers on the screen. On the top we have a navigation bar. Below that is a table view which has rows which are known as table view cells. Within each cell, we have an image view on the left, and some labels on the right to display text. Besides the leaderboard, we also need to set up the details page. This one's pretty straightforward. It has an image view and some labels. So now that we know what views our UI is made up of, let's go back into another demo and actually set up this user interface. I'll select the main storyboard which we built with Jesse. Here we have a single-view-controller which was used to manage the entire screen of content for the game. Now we want to add more screens for displaying our leaderboard in the details view. To do this, we can add another view controller from the object library. I'll search for view controller. Notice that there are a few different options for view controllers. Since I want my leaderboard to look more like a table, I'll select the table-view-controller and bring it into my storyboard. The table-view-controller automatically has a table view. Let's select the table view and look at the Attributes Inspector on the right. The content type here is set to dynamic prototypes. I'll change this to static cells. Static cells are great for displaying static data. And these can be easily set up in the storyboard. Now let's select a single cell and change its style from the Attributes Inspector to subtitle. This gives me a title and a subtitle where I can display the name and the points on my leaderboard. I also want to add an image to the left of the row. To do this, I can simply type in the name of an image in the Attributes Inspector's image field here. I have imported an image in my project just as a sample. Let me add that. Okay. So now my cell is all set up and we have a good outline for our leaderboard. Let's go ahead and add the details UI. I'll go back into the object library, and this time we'll take a blank view controller and bring it into the storyboard. Inside this view controller, we will add an image view. Let me set the image from the Attributes Inspector. Wait a minute. This is a little scaled. Let me change the content mode from scale to aspect fit. Much better. Now I'll go back to the object library and add some labels. These are going to be used for storing the name. I'll make a copy. Next one is for the rank, and finally another label. Going to make a copy. Another label for the points. Okay. Now that both our view controllers are set up, there's one other thing to do. We need to tell our app to actually display this leaderboard or the details view after we do a certain action in the app. Since we want the leaderboard to be displayed after we tap the leaderboard button. I'll create a connection between the leaderboard button and this table-view-controller. Let's hold down the control key on the keyboard, and drag the mouse from the leaderboard button to the table-view-controller. When I release, I see a menu with a few different options. These are the different ways in which we can present the second view controller on top of the first one. For now, I'll select the show option which is a commonly used segue for such apps. Notice that a connection got created between the two view controllers. This is known as a segue. Now, I want to display the details-view-controller when I tap on the cell in the leaderboard. So let's again hold down the control key and drag our mouse to the details-view-controller. Select show one more time. Now we are ready to run our app for the very first time. Let's look at the simulator. If I tap the leaderboard, I see a good mock-up for my leaderboard. I can tap on the cell and we have our details view with more information. This is looking good. But how do I go back to my leaderboard or the game? Let's go ahead and fix that. To do this, I'll select the main view controller which had the game. And then go to the editor menu in Xcode, select, embed in, navigation controller. This adds a navigation controller to our app. The navigation controller is a special type of view controller that is used for managing the back and forth transitions between our view controllers. This also adds a navigation bar at the top of our view controllers. So from the Attributes Inspector, if I set a title to-- for my leaderboard, it will now show up in the navigation bar. Now that we've added all the viewers that we needed to add for our UI. Let me finish this up with adding an app icon to this app. Let's go into our project navigator. Select the blue folder that says assets. And go to the app icon image set. Now I have some icons that my designer created. I'll select these and drop them into Xcode. There's a specific way in which you need to create these icons. And we have guidelines on developer.apple.com. This time when we run our app, I'll close this-- we will see an icon on the home screen. Isn't it cute? Thank you. Let's go back into the app. And tap on the leaderboard. This time we're able to go back and forth between all our views. This is great! Let's now- I'm glad you like that. Let's now rotate our phone and see what happens. Oh no. My labels are all clipped, and my image is too far to the left. In order to make my layout look right-- let's put that back. I'll use a technique known as Auto Layout. Auto Layout is a set of rules that we define to tell our app how we want our views to be placed. So each rule in Auto Layout is known as a constraint. Now we'll add some constraints to this image view in our details-view-controller. Look at the buttons at the bottom right of the storyboard. We'll click on the align button, the one with two horizontal bars and say horizontally in container. Add one constraint. This tells our app that we want our image view to always be in the horizontal center. I'll also click on the button to the next of this align button called add new constraints. The top-most text box here represents the distance between my image view and its closest neighbor on the top. So if I set this value, it will make sure that my image is at least, that my image is like ten points away from the nearest neighbor on the top. Which could be the edge of the screen too. Let's also set the width and the height. Now our image view is all set to use Auto Layout. I'll quickly add similar constraints to all the labels. We can do this in one go by selecting them altogether. This time, if we run our app and rotate it, we should be able to see our view laid out properly. There's an even easier way to validate our layout. We can go to the Device Configuration pane at the bottom left of the storyboard. And change the orientation here to landscape. Now I can see my view in the landscape orientation. And my image is looking, my image is in the center, and my labels are all seen on the same page. You could even change the device from the Device Configuration pane to see how this would look on an iPad or a smaller phone. With that, a UI is all set up. Let's quickly recap what we learned in this session, section. We started with a single-view-controller which had the-- which managed the entire screen for our game. We learned that we can create multiple view controllers to display multiple screens of content. If we do create multiple screens of content, then we need to tell iOS how to go from one view controller to another. We do this by adding connections that are known as segues. We created a table-view-controller which is great for displaying lists of data. We also added it inside a navigation controller. The navigation controller helped us go back and forth between the table view and the details page. This pattern of using a table view along with the navigation controller is commonly seen across iOS apps. Here are just some examples. To learn more about view controllers, check out the documentation on developer.apple.com. We also learned about Auto Layout which is a great technique for building good user interfaces for different screen sizes and different orientations. I would recommend checking out these WWDC sessions to learn more about Auto Layout. Finally, we can hook up our data model with our UI and write in the logic to bring everything together. Let's take a look at another demo. We need to save the score at the end of each game. To do this, I will select the view controller that we wrote with Jessie. In the game over method, I will call a method on my data model that we wrote before to save the points. The-- this save points method takes two parameters. One is the total points which we've already saved in the game points variable. The second parameter is the name of the user who's playing the game. So to recap, I've just added this little bit of code in our view controller that we created before. The save points method on our game data model takes two parameters. The first is the game points which we had already stored in a variable. The second parameter is the name of the user who's playing the game. In a real-world app, you would probably want to create a profile screen or a settings screen where the user can enter their own name. But for the purposes of this demo, I'll just hardcode this to my own name. Now that we've saved the score, we need to write some code to actually display the right scores and the names on the leaderboard. So let's go ahead and create another class by right-clicking on the unicorn's folder and going to new file. A [inaudible] class again. This will be a subclass of the UI table-view-controller. Remember when we created the leaderboard on our storyboard? It was a table-view-controller, so I use a table-view-controller class again to put in some code for that leaderboard. Let's call it the leaderboard-view-controller. Hit next. And create. This already has some code that has been generated for us by Xcode. These are methods that developers find useful when building their own table-view-controllers. We will implement two of these methods. First is a table view number of rows in sections. This method is asking us for the total number of rows that we want to display in our table view. In our case, the total number of rows will be the same as the total number of players that have played the game. And we can get this information again from our data model. So the number of players variable will give us the total number of rows. The second method that we need to implement is the cell [inaudible] at index path method. This method is called every time a cell needs to be displayed on our leaderboard. The index path argument tells us the row number for the cell that will be displayed. And in here, we're expected to configure the cell just like the comment says. So let me pull in some code to set up this cell. We are using our game data class again. And the index paths row will tell us the rank of the player. Since we want to display the players in ascending order. Once we have the player rank, we can send it to the player data method on our game data class and get more information about that player. Using this information, we can set up our cell. The text label is the title. The detail text label is the subtitle. And the image view is the image that will have the photo. Now instead of creating this game data instance twice, I'm just going to put it at the top of the class so it can be reused. Let me also delete the number of sections method that was added for us by default, because we don't need this today. Now that we have written the code for the leaderboard, let's open the main storyboard and the Assistant Editor on the right. I'll rotate the specs so we can see it properly. We need to tell Xcode that the code for our leaderboard here is written in the class that we just created. To do this, I'll select the view controller for the leaderboard. Go to the Identity Inspector, and change the class name to leaderboard-view-controller. After this, I'll select the table view in the storyboard. Go to the attributes Inspector and change the content type to dynamic prototypes. This means that the cells are now being created dynamically in the code that we wrote. We will also select a single cell. And in the identifier section in the Attributes Inspector, I'll copy over the reuse identifier from the cell [inaudible] index path method in my class. By doing this, I'm saying that I'm willing to reuse the cells in my table view. This will help improve the performance of my table view. Finally, we've written the code for this leaderboard class. We still need to write code for our details-view-controller. Luckily, I've already added some code that we'll import into our project. This is the details-view-controller class. Let's jump to it from our jump bar. Here we have some IBOutlets for the different views in our details-view-controller. Remember when we wrote the game, we connected these IBOutlets to the views on the storyboard. But we're not able to do that right now. This is because we haven't told Xcode that the code for this view controller is right here. Let's do that again by going to the Identity Inspector and changing the class name to details-view-controller. Now we can connect the IBOutlets easily. By doing this, we can set the views from our code like I've done in the view load method here. Let me hide the main storyboard for a moment. We also have a player info variable here. And we are using that player information to set up our views. You might be wondering that we didn't really send this player information anywhere. So how do we get all this data? We actually need to ask our leaderboard about the person whose name was tapped. So we'll go back in the leaderboard scoreboard and figure out which person we need to display more information about. I'll jump to the leaderboard-view-controller. At the bottom of this class, we have a method that was written for us by Xcode called the prepare-for-segue method. This gets called while we're transitioning between one view controller and another. It will be called for us as soon as we tap on a cell in our table-view-controller. So in here, I will get more information about the player based on the rank again. And I can figure out the rank because there is a property on the table view called index path for selected row. This tells me the row number for the cell that was selected. We also get a segue parameter in this method. Remember how segues connect two view controllers? So the segue's destination property will tell me which view controller we're navigating towards. Once we have an instance of the details-view-controller which is the one we're going towards, then we can set the player info with the details we have. Finally, we are ready to run our app one last time and see how everything comes together. So now we'll play the game. And we go to the leaderboard, and we can see all the information coming in from our data model. I made only one point, so I'm at the end. Let me see what my rank is. Rank eleven, not too bad. At least my app is all set up. Now that we've built our app, let's take a look at some next steps. Before you release your apps, it's important to test them. The exit test framework can be used to write unit tests. You should also check out the App Store review guidelines. These are meant for creating a good experience for users and developers alike. At some point, you'd need to enroll in the Apple Developer Program. This will help you create the certificates and profiles that you need to distribute your apps. Finally, submit your apps for review. Once they're approved, go tell your friends and family and the whole world. That you've built an app. To learn more about these next steps, check out the resources section for this session on your WWDC Apps. To summarize, I would encourage you to explore Xcode. It has many tools and templates that make iOS development really easy. You can easily set up the user interface using the storyboard. Try some views and see what your app might look like. Think about what data you might need, and how you would store it. It's important to create good user experiences for all devices. This might mean optimizing performance for older devices or taking care of different screen sizes. Follow the guidelines and best practices to make a good app. If you've done all this, then you're officially an iOS app developer. Congratulations! And have a great WWDC.  Hey guys! Good afternoon. Welcome to Adding Delight to your iOS App. My name's Ben. And, my name is Peter. And, we're going to show you six pro tips to make it magic. We're going to start with external display support, bringing your app's experiences to the big screen. Next, we're going to go through a brand-new programming pattern for you, called layout-driven UI. Then, we're going to show you how to get your customers to your delightful experiences as fast as possible, with laser-fast launches. We're going to focus hard on smooth scrolling, and keeping things feeling great. Continuity is one of the most magical experiences on iOS. And, we're going to show you just how easy it is to adopt Handoff in your applications. And, finally, we're going to teach you some Matrix-level debugging skills, in debugging like a pro. We have a lot to cover, so let's get started. iOS devices are defined by their stunning, integrated displays. And, you can bring your app's experience even further, by adding support for an external display. We've built a demo app to help illustrate this. Built right into iOS, is Display Mirroring, which replicates the entire system UI on the external connected display. Here's our demo app. As you can see, it's a simple photo viewer. When you tap on a photo thumbnail, the photo slides in, and it's full screen. And, this entire experience is replicated on the external display. To take full advantage of the size of the external display, we can rotate the iPhone to landscape to fill it up. And, this is great, with no work on our part, we were able to get this experience. But, we can do better than this. Built right into iOS, are APIs that allow you to create an entirely custom, second user interface on this externally connected display. Let's take a look at a couple of examples of apps that have done this. Keynote is a great example. On the external display, you remain focused on the primary slide at hand. But, on the integrated iPhone display, you can see presenter notes, and the next slide, tools essential to any presentation. Or, maybe you have a game. But, typically you'd have soft, overlaid controls. Well, you could create an entirely custom interface to control your game, and put that on the iOS device's display, and have your full, unobstructed and immersive gaming experience on the external display. When designing your applications for an external display, there are some key things that you should think about. Aside from the obvious size differences, your iPhone is personal. And so, you should consider the kind of information that you show on this display as private. Whereas, the external display will typically be situated in an environment where many people can see it, such as a TV in a living room, or a projection system in a conference hall. So, you should assume that the information shown on this display is public. Additionally, while the displays built into iPhone and iPad are interactive, the external display is not. So, you should avoid showing UI elements, or other interactable controls on the external display. So, let's apply this kind of thinking to our demo app, and see what we can come up with. Here's our optimized version for the external display. As you can see, we're now showing the selected photo full size on the external display. And, on the integrated display, we're showing just the thumbnails, and a new selection indicator to show the photo that is currently being shown full screen. While simple, this is a really powerful use of this design. To show you how we built this into our demo app, we're going to cover three topics. Connectivity, behavior, and connection transitions. Let's start with connectivity. How do you know if you have an external display connected? UIScreen has a class variable, screens, which contains a list of all the connected displays, including the device, built into the iPhone. So, if there's more than one element in this array, you know you have an external display connected. Additionally, because the external display can be connected and disconnected at will, UIKit will post notifications to help you know when this happens. So, you should listen for the UIScreen .didConnectNotification, and the UIScreen .didDisconnectNotifications. And, bring up and tear down your UI accordingly. Peter, can you show our developers just how easy it is to set up a second user interface? Ben, I'd be happy to. Let's jump into our code for our UIScreen connection callback. Here, we'll set a local variable to the last screen in the screens array. We know that this is the external screen, because we're inside of our didConnectNotification callback. Next, we'll make a new UI window to show on this external display. And, we'll assign its screen property to the screen. Next, we're going to want to make sure we set up this window. We factored this into a function, but all we're doing here is making a root view controller, and sticking it on the window, the same way we'd do for the onboard display. And, finally, we're going to mark this window as not hidden to show it on the external screen. So, that's connection. Now, let's look at disconnection, which is even easier. So, here we are inside of our UIScreen .didDisconnectNotification handler, and all we have to do here is hide the window, and nil out our local reference to it, to free up any resources. And, that's it. We've implemented screen connection and disconnection in our app. Wow, Peter, that was really easy. The next thing you're going to want to think about is changing your app's default behavior for when it has an external display connected. Let's look at an example of some code from our demo app. This is the code that's called when we tap on a photo in your collection view. When we're in single display mode, we create our photoViewController and push it onto our navigation stack. But, when we have an external display connected, we're already showing that photoViewController full screen in that second UI, so we just tell it to present that photo. Really easy. The third thing you should think about when designing for an external display is you should handle connection changes with graceful transitions. Let's go back to our demo app to illustrate this. Here you can see our demo app is currently showing a photo full size. And, we haven't yet connected the external display yet. Watch what happens when we plug it in. What happened here, is we popped our viewController back to the thumbnail view while simultaneously showing that previously selected photo full size on the external display. And, it's these graceful transitions that really help preserve the context, and help your customers understand where they are in your app's flow. So, that's external display support. It's really easy to set up. Just consider the different display contexts when designing your application, and be sure to handle connection changes gracefully. To learn more about this, check out this talk from WWDC 2011. Thank you. Layout-driven UI is a powerful way to write your app to make it easier to add features, and easier to debug. Layout-driven UI helps us deal with the number one cause of issues in iOS apps, and that's managing UI complexity. I'm sure you've been here before. I know I have. You add some code, and a gesture callback. You add even more UI update code in a notification callback. And, more when you get a value trigger from a UI control. And, suddenly your app is in this weird, and hard to understand state. And, you have to follow these strange orders to reproduce these unusual bugs. And, as you add more features to your app, the problem gets worse and worse. If we instead follow a simple recipe, and push these UI updates into layout, we can get rid of these bugs, and make it much easier to add features. Let's walk through the recipe for adding layout-driven UI to your app. The first thing you should do, is you need to identify and track all of the state that affects your UI. Then, every time that states changes, you should dirty the layout system by calling setNeedsLayout. Finally, you'll want to update your UI with this state in layoutSubviews. And, that's it. So, what I love about this recipe is just how easy it is to follow. And, if we apply layout-driven UI to our app holistically, while considering the three core components of an iOS app, layout, animations, and gestures, you'll find that our implementation of all three of these works harmoniously, in a really awesome way. Let's start with layout. Layout is the process by which you position your application's content onscreen. But, we're also recommending that you do all other UI updates in layout. Let's look at a simple sample app that we wrote to highlight this. Ben, can you take us through this app? Sure, Peter. So, there's a really simple sample app, with this cool guy in the middle. He shows when we're feeling cool. When we're not, he hides away. But, we're feeling quite cool right now, Peter. So, let's bring him back in. Great. So, while this is a simple example, it's really important to walk through, so we can understand how layout-driven UI works. So, let's go through a skeleton of this app, and follow through the layout-driven UI recipe that Ben showed us earlier. So, here we've got our managing view that hosts this cool guy view, in our coolView, which I wrote ahead of time. So, Ben, what's the first step in our recipe? Well, we need to identify and track that state that affects our UI. So, remember what Ben said. The cool guy is there when we're feeling cool. And, he's not there when we're not. So, I guess we'll have a variable called feelingCool. OK. Ben, what's the second step in the recipe? Well, now, every time this state changes, we need to dirty the layout system by calling setNeedsLayout. But, we need to make sure that every time this state changes this happens. And, this state could change from various places in our application. So, Peter, how can we ensure that we're always dirtying the layout system when there's changes? I'm happy you asked, because I think I've got a good idea for this. We can use a feature called Swift property observers. These let us run code before or after a property is set. So, we can use the didSet property observer to call setNeedsLayout. This is a really excellent use of Swift property observers in your app. OK. So, we're almost done. Ben, what's the last step in the recipe? Well now, Peter, using this state, we need to update our UI in layoutSubviews. OK, easy. We'll override layoutSubviews, and we'll updated the isHidden property of our cool guy view based on the value of feelingCool. And, that's it. That's all you need to do to add layout-driven UI to your app. Now, while this works really well for this simple example, it works well for some more complex ones earlier. Ben and I were up really late last night playing this new macOS The Gathering trading card game, which is sweeping the Apple campus. We built this fun little deck builder app to help us win the tournament next weekend, which we're really going to do. It lets you pick up and drag these cards around, and you can fling them into this little deck area. And, it's really fast, and fun, and fluid. We can pick up two cards at the same time. I think with this app we can really show you how layout-driven UI works, and importantly, we can beat Ben's officemate next weekend. So, let's walk through the other two core aspects of an iOS app, and how we can apply layout-driven UI to them, starting with animations. Animations are the hallmark of any great iOS experience. The life-like motion of your user interface truly makes your apps feel alive. UIKit has some great API available to help you create these delightful animations. The UIViewPropertyAnimator API is a really powerful tool, and it was turbocharged last year with a bunch of new features. To learn all about how to use it, check out Advanced Animations from UIKit from WWDC 2017. In addition to this, the tried and true UIView closure API is also a great way to make these animations. So, great. We can use UIViewAnimations with our layout-driven UI-based app. One thing to keep in mind, is we're always going to want to use the beginFromCurrentState animation option. This tells UIKit to take the current onscreen position of your view, even if it's mid-animation, when doing animation. And so, this lets us do these really wonderful, interruptible interactive animations. Let's look at an example in our macOS The Gathering trading card game app. So, here we've got a variable that tracks what cards are in our deck. And, using those Swift property observers we talked about earlier, we're dirtying the layout system every time this changes by calling setNeedsLayout. Next, when we want to put a card in the deck, all we have to do is add that card to this array, which will dirty our layout, and then inside of an animation block, using this beginFromCurrentState option, we call layoutIfNeeded. This will trigger a call to our code in layoutSubviews, which will move all of our views around, play the appropriate animation state transitions. And, what's really excellent about this, that I really want to highlight here, is notice how we didn't add any special case for these animations. We just kind of got this animated layout for free by doing our layout inside of an animation block. This is really, really awesome. So, that's how we can add animations to our layout-driven UI app. Finally, let's take about the third core aspect of an iOS app, and that's gestures. And, of course, we can't talk about gestures without talking about UIGestureRecognizer, UIKit's awesome API for adding gestural interactions to your app. UIKit provides a number of great concrete subclasses of UIgestureRecognizer. Everything from pans to pinches to swipes to rotation. You should be able to create the kind of interactions you want, using these. And, they're highly customizable as well. If you want something really crazy, you can always subclass UIGestureRecognizer yourself as well. When looking between the built-in UIKitGestureRecognizers, it's important to understand the difference between discrete and continuous gestures. Discrete gestures tell you that an event happened. They start in the Possible state, and then they don't pass go, they don't collect $200. They move immediately into the Recognized state. These are really useful for fire and forget interactions in your app, but won't tell you at every phase during the interaction. The other type of gestures are continuous gestures. These provide a much greater level of fidelity to you. Like discrete gestures, they start out in the Possible state, but as they begin to be recognized, they move to the Began state. As they track, they enter the Changed state. And, at this point you're receiving a continuous stream of events as the gesture moves around. Finally, when the gesture is complete, it moves to the Ended state. One of our favorite types of continuous gestures is the UIPanGestureRecognizer. And, there are two great functions available to help you get the most out of it. translationInView will tell you where the gesture is tracking, relative to your views. And, velocityInView will tell you how fast your gesture is moving. And, this is really powerful for handing off velocity between a gesture and a subsequent animation. To learn all about how to build really great gesture interactions using these, check out Building Interruptible and Responsive Interactions from WWDC 2014. So, I also love UIPanGestureRecognizer. And, we used it to help build that card dragging behavior you saw earlier. Let's look at how we did this using layout-driven UI. Again, we have a local variable, which tracks the offsets for each of our cards that the gesture has applied. And again notice every time this variable changes, we're triggering setNeedsLayout, using one of Swift's property observers. Next, inside of our panGestureRecognizer callback function, we're just going to grab the current translation and view out of the gesture, and associate this gesture to one of our cards. We'll then increment the offset for this card in this dictionary. And finally, in layoutSubviews, we'll make sure to update the position of our card views based on their offsets from this dictionary. Notice again how we're not really doing anything special, other than -- besides the traditional layout-driven UI case. We just have this piece of state that happens to be driven by a gesture, and we're responding to it in layoutSubviews. In fact, if you follow this pattern throughout your app, you'll find that it makes a lot of these types of interactions much easier to adopt. So that's layout-driven UI. Remember the recipe. Find and track all of that state that affects your UI. And, use Swift property observers to trigger setNeedsLayout whenever this state changes. Finally, in your implementation of layoutSubviews, make sure to update your view state based on this state that you tracked. Thank you. The iOS experience is all about being responsive. And, you want to get your customers to your delightful experiences as quickly as possible. There is one step between them tapping on your icon, and being delighted. And, that one step is your launch time. To help you optimize this, we're going to take you through the five high-level components that make up the anatomy of a launch. Starting with number one, process forking. Peter, what can we do in this phase of the launch? Well, for process forking, it's really complicated. You're going to want to read the Man pages for fork and exec, and familiarize yourself with POSIX fundamentals. No, no, I'm just kidding. iOS will take care of process forking for you. We'll deal with number one for you. Let's look at number two. [ Audience Laughter ] Dynamic linking. At this phase, we're allocating memory to begin executing your application. We're linking libraries and frameworks. We're initializing Swift, Objective-C, and Foundation. And, we're doing static object initialization as well. And, typically we see this can take 40 to 50% of the typical launch time of an app. And, one key thing to remember is at this point, none of your code has run. So, it's vital that you understand how to optimize this. Peter, do you have any great advice for our developers? I'm happy you asked, Ben. It's important that you take great care when optimizing the linking phase of your app's launch time. Because it takes up such a large amount of your launch time. My first piece of advice for you is to avoid code duplication wherever possible. If you have redundant functions, objects, or structs, remove them from your app. Don't repeat yourself. Next, you're going to want to limit your use of third-party libraries. iOS first-party libraries are cached, and may already be in active memory if another application is using them. But, third-party libraries are not cached. Even if another app is using the same version of a library as you, we'll still have to pull in the framework if your app uses it. So, you should really limit your use of these third-party libraries as much as possible. And, finally, you're going to want to avoid static initializers, and having any behavior in methods like plus initialize, and plus load. Because these have to run before your app can do any meaningful work. To learn more about this vital part of your launch time, check out App Start-up Time: Past, Present, and Future from WWDC 2017. The next phase of your launch is your UI Construction. At this point, you're preparing your UI, building up your viewControllers. The system is doing state restoration, and loading in your preferences. And, you're loading in the data that you need for your app to become responsive. So, Peter, what can we do at this phase of the launch to optimize? You'll want to optimize the UI construction phase to be as fast as possible for your app. This means returning as quickly as possible from the UI application activation methods. WillFinishLaunching, didFinishLaunching, and didBecomeActive. Because UIKit waits for you to return from these functions before we can mark your app as active. Next, you're going to want to avoid any file system writes during your application launch, as these are blocking, and require a sys call. Hand in hand with these, you're going to want to avoid very large reads during app launch as well. Instead, consider streaming in only the data you absolutely need right now. And, finally I encourage you to check your database hygiene. It's always a good idea to stay clean. If you're using a library like CoreData, consider optimizing your schema, to make it as fast as possible. And, if you're rolling your own solution with SQLite, or similar technology, consider vacuuming your database at a periodic interval. For example, every time your app gets updated. Thanks, Peter. The next phase of the launch is when we create your very first frame. At this point, core animation is doing all the rendering necessary to get that frame ready. It's doing text drawing. And, we're loading and decompressing any images that need to be shown in your UI. So, Peter, do you have any more sage advice for this phase of the launch? Oh, do I. When preparing your first frame, it's really important that you take great care to only prepare the user interface that you absolutely need during launch time. If your user hasn't navigated to a particular section of your app, avoid loading it. And, instead, pull it in lazily when you absolutely need it. Also, you should avoid hiding views and layers that should not be visible when we first navigate to your app. Even when views and layers are hidden, they still have a cost. So, only bring in the views and layers that you absolutely need for your first frame. The final phase of your launch is what we call extended launch actions. These are tasks that you have potentially deferred from your launch path to help you get responsive faster. So, while your app may be responsive at this point, it may not be very usable yet. This phase is really all about prioritizing what to do next. Bring in the content that needs to be onscreen right now. And also, if you're loading content from a remote server, be sure to consider that you may be in challenging network conditions. And, have a placeholder UI ready to go if necessary. So, those are the five high-level components that make up the anatomy of a launch. We've one more thing for you today. ABM. A: Always. B: Be. M: Measuring. Coffee is for quick apps. It's essential that you understand where your launch time is going. So, measure it regularly using the Time Profiler. Any time you change code in the path -- in your launch path, you'll want to remeasure. And, take statistical averages. Don't depend on a single profile to check your launch time. So, laser-fast launches. Get responsive fast. Use only what you need. And, measure, measure, measure. Thank you. Scrolling is a key part of the iOS user experience, and a huge part of the experience inside your apps. iPhone and iPad are magical sheets of glass that can transform into anything your app would like them to be. So, it's important that you work to help preserve this illusion of your app's content sliding on this magic sheet of glass. At Apple, we've got a phrase that we like to say, that your app should feel smooth like butter. But, from time to time, you have some hitches and stutters that make it feel less like butter, and more like peanut butter. And, you've seen this before. Your app feels choppy or stuttery. Ben, what are some causes of the slow behavior in apps? Well, Peter, this slow behavior that you're describing is really that we're dropping frames. And so, we need to understand why that might be happening. And, there are two key areas where this can happen. The first is you could be doing too much computation. And, the second is you could be doing too much complex graphics compositing. Let's look at each of these in turn, starting with computation. How do you know if you're doing too much computation? Well, the Time Profiler, built into Instruments, is the ultimate tool for this. It can give you down to the line measurements for just how much CPU time your code is using. It's a really powerful tool, and we encourage you to go and check out Using the Time Profiler in Instruments from WWDC 2016. So, once you've identified these hot spots, using the Time Profiler tool, we've got some great tips for you to optimize them. The first is to use UICollectionView and UITableView prefetching. These are pieces of API that will tell you while the user is scrolling towards particular cells, and can give you an opportunity to preload that data. There was a wonderful talk given by two very handsome presenters on this in 2016 that I encourage you to watch. The next tip that I have for you, is to push as much work as possible off of the main queue and onto background queues, freeing up the main queue to update your UI and handle user input. Ben, what kind of work can we push off the main queue? Well, there's some usual stuff that you might expect, like network and file system access. These should never be running on the main thread. But, maybe some other stuff that you might not expect, like image drawing and text sizing. UIGraphicsImageRenderer, and its distributed string, both have functions available that are safe to use on a background thread, that might just help you move some of that complex computation off of your main queue. Wow, Ben, those are great tips. I never would have thought to do that off of the main queue. So, let's say I've run the Time Profiler. I've used prefetching, just like those guys told me to. And, I've pushed as much work as possible off of the main queue, but my app is still slow. Surely this isn't my problem, right Ben? Well, Peter, we may not be out of the woods just yet. While we may have optimized our computation, we could still be having problems with our graphics system. Fortunately, there's another great tool available here. The Core Animation instrument is a really powerful way to see exactly what your frame rate is doing. And, also at the same time, looking at your GPU utilization. It's another really powerful tool. And, to learn all about how to use it, check out Advanced Graphics and Animations for iOS Apps from WWDC 2014. Once you've identified that your app is graphics-bound, we've got some great low-hanging fruit for you to investigate. Usually, you have a graphics-bound app due to overuse of one of two things: visual effects, or masking and clipping. Visual effects, like blur and vibrancy, are expensive, and so you should use them tastefully within your apps. You should also definitely avoid things like blurs on blurs on blurs, as this will cause the GPU to work in overdrive, slowing down your app. Also, you should avoid masking and clipping wherever possible. Instead, if you can achieve the same visual appearance by just placing opaque content on top of your views, then I would encourage you to too that, instead of using the masked view, or masked property of UIViewer CA Layer. So, that's how we can optimize smooth scrolling performance. Make sure to run the Time Profiler, and Core Animation instruments on your apps. Use prefetching, and push as much work as possible off of the main queue. And, try to use visual effects, and masking and clipping sparingly. For even more information about profiling, check out this great talk from WWDC 2015. Thank you. Continuity is one of the most magical experiences across Apple platforms. And, Handoff is a fantastic way to truly delight your customers. The ability to take a task from one device, and seamlessly transition it to another device is an absolutely awesome experience. Handoff works between iOS, macOS, and watchOS. It doesn't require an internet connection because it uses peer-to-peer connectivity, and best of all for all of you, is it's really easy to set up. So, how should you think about using Handoff in your applications? Well, let's go through a few examples of how we do it in some of our Apple apps. Let's say I get a message from my handsome co-presenter, and I want to reply on my iPhone X with a humorous animoji. Well, I can get right back into that conversation, right from the App Switcher on iOS. Or, if I'm editing a document in Pages on my Mac, and I have to run, and I want to hand it off to my iPad, I can do so by tapping the icon in the Dock. Or, again, if I'm casually browsing photos on my watch, and I find one from a previous WWDC, and I just want to go and look at all the photos in that album, I can get right back into my Photo library on my iPhone, without having to go search for that one photo. Handoff is really powerful, and it can save your customers so much time when moving from device to device. So, we're going to show you just how easy it is to adopt. And, it's all built on top of the NSUserActivity API. NSUserActivity represents a current state or activity that you're currently doing. In this case, we're composing an email. When this activity is created, all of the nearby devices that are signed into the same iCloud account all show that Handoff is available. On the Mac, you'll see an icon down in the Dock. When you click on this Mail icon, the activity will be transferred over to the Mac, and Mail can launch and continue exactly where you left off. So, let's look at the code necessary to set this up. On the originating device, you start by creating your NSUserActivity with a given type. And, this type represents the kind of activity that your user is currently doing. You then set a title, and set isEligibleForHandoff to true. And you then want to populate your userInfo dictionary. And, you need to fill this with all the information necessary to be able to continue the activity. In this case, our example is a video, and we're including a video ID, and a current play time. Finally, you'll want to set this activity to your viewController's userActivity property. This will cause it to become the current activity, whenever that viewController is presented. And, that's all you need to do on the originating device. On the continuing device, first of all, your app needs to declare support for the type of activity that you created. Then, you need to implement two UIApplicationDelegate callbacks. The first is application willContinueUser ActivityWithType. And, this is called as soon as you click or tap on the icon to initiate the handoff. At this point, we don't have the NSUserActivity object ready yet, but you know the kind of activity that's going to be continued, so you can begin preparing your UI. Very shortly after, you'll receive applicationContinueRestoration handler, which will contain the fully reconstructed NSUserActivity object. From that point, you can set up and continue the experience right on that device. If you've got more information than can fit in a userInfo dictionary, there's great feature of NSUserActivity that you can use, called continuation streams. All you have to do is set the supportsContinuationStreams property to true. Then, on the continuing device, you'll call the getContinuationStreams method on the NSUserActivity, which will provide you with an input and an output stream. Back on the originating device, the NSUserActivity's delegate will receive a callback providing it with input and output streams as well. And, through these channels, you can do bi-directional communication between the originating and the continuing device. But, you're going to want to finish this as fast as possible, because the user may be moving the devices apart to leave. For more about streams, check out the Stream Programming Guide on developer.apple.com. Now, this is great for moving things that wouldn't be appropriate to put in the userInfo dictionary, like images or video content, such as in our email handoff example earlier. But, for document-based apps, the handoff story is even easier. Because you get much of this behavior for free. UIDocument and NSDocument automatically create NSUserActivity objects to represent the document that is currently being edited. And, this works great for all documents stored in iCloud. All you have to do in your applications is configure your info.plist accordingly. In addition to app-to-app handoff, we also support app-to-web browser handoff. If you have a great web experience to go alongside your native app experience, and the continuing device doesn't have your native app installed, you can handoff to Safari, and continue the activity right in the web browser. Handoff also supports web browser-to-app handoffs. You need to configure a list of approved app IDs on your web server, and then you need to add an associated domain entitlement in your iOS app. And then, the user can seamlessly continue from your web experience to your app on iOS. For even more about this, check out this great Handoff talk from 2014. So, that's Handoff. Go out and implement it in your applications. Truly delight your users, and as an added bonus, the NSUserActivity API is used all over the system experiences. In things like Spotlight search, and the new Siri Shortcuts feature. For more about these, check out these talks, from previous WWDCs. Thank you. You write amazing apps and experiences. But, from time to time, you have problems that you need to investigate. And, for that, we're going to teach you some Matrix-level debugging skills. But first, a word of warning. Before we give you this red pill, and show you just how deep the rabbit hole goes, I want to let you know that the methods that we're going to show you in this section are great for debugging, but must not be submitted to the App Store. If you do, your application will be rejected, and you'll have a bad day. So, with that warning, let's get started. We're going to start with a detective mindset. How you should approach problems that you find in your program. Next, we're going to talk to you about how to debug issues with your views and view controllers. We're going to teach you about LLDB, and how you can use it to identify state issues in your app. And finally, we're going to look at some techniques for some great memory issues that you might run into that make you feel less than great. So, let's start with a detective mindset. When you're looking at a problem in your program, you want to make sure to verify your assumptions. What do you expect your program to be doing? And then, verify that it's actually doing that. This can be a great step, when you start to debug issues in your app. Once you're pretty sure which of your assumptions is being violated, you can start by looking for clues. You'll use the tools that we'll show you during this section to poke and prod at your objects and structs. And then, you're going to want to test your hunches, by changing state in your app, and verifying that you found the issue. Let's start with a sample bug that's a real bug. One of the great things that I get the privilege of working on here at Apple, is the screenshot editor. Recently, we were debugging an issue, where my screenshot pen tools were missing, which is pretty bad. Ben, are there any tools we can use to help diagnose this issue? Absolutely. Built right into Xcode, is the View Debugger. You can launch it by simply clicking on this icon in the bottom toolbar. And, Xcode will show you a 3D representation of your entire view hierarchy. As you can see here, our pencil controls are still there, but they're being occluded by this full screen view in front of it. So, we need to go and look at where we're building this UI, and see what's happening with the ordering there, Peter, I think. That's great. The Xcode View Debugger is a wonderful tool for debugging view issues in your app. There are even more tools that you can use to help out with this. UIView recursiveDescription, UIView parentDescription, and the class method UIViewController printHierarchy are great tools for debugging view and view controller issues in your app. Again, they're also great things to not include when you submit to the App Store. It's important to note that these are Objective-C selectors. So, before using them, you'll want to put your debugger into Objective-C mode, using this command. We're going to walk through each of these debugging methods, step-by-step, and how they can help you, starting with UIView recursiveDescription. So, UIView recursiveDescription will print the view hierarchy of the receiver -- the subview hierarchy. And, some associated properties to help you understand the layout attributes. Let's take a look at an example. We have another bug in our screenshots UI with a missing view. So, we're going to call recursiveDescription on our viewController's view. Now, this might look like a wall of debug text, and that's because it is. But, we know what we're looking for. Our screenshots view is there. We can see it. And, on inspection, we can see that it is currently hidden. So, we need to go and look at everywhere we're setting the hidden property on this view, and really understand why it's not showing. In addition to recursiveDescription, UIView also has parentDescription, which will walk up the view hierarchy to the parent views, until it reaches no -- to a nil parent. It'll print the same kind of debugging information. RecursiveDescription and parentDescription are great for UIView issues. But, sometimes you have a problem with UIViewControllers. And, for that you can use the great class method, UIViewController printHierarchy. Recently we had a bug in our screenshot editor, where one of our viewControllers had not yet received the viewDidAppear message. And so, it hadn't set up its state appropriately. By running UIViewController printHierarchy, we can get an output of all of our presenting viewControllers, our presented viewControllers, our parentViewControllers and childViewControllers, and even our presentationControllers. It's Controllerpalooza. So, let's run printHierarchy in our screenshot UI. Here we can see our viewController hierarchy. And, when we inspect the viewController that we're having the problem with, we can see that it's stuck in the appearing state. So, we had missed a callback. And so, we need to look into our app to where we're calling this callback, and then we found the issue. So, great. Using these methods, you can identify view and viewController issues. But, sometimes you have a more fundamental issue with your app. And, for that we can use some great state debugging tips that we have for you. LLDB's expression command can let you run arbitrary code in the debugger. Think about that. Any code that you can run in the source editor, you can write right in the debugger, and run while your program is running. This is so useful for debugging. You can call functions on your structs, get properties on your objects, and better diagnose what your program is doing. For even more about debugging, check out this great talk on how to debug with LLDB from 2012, and how to debug with Swift from 2014. There's some great functions that you can run inside of LLDB with the expression command, that we're going to teach you. And, the first one, is dump. Dump will print all of your Swift objects and structs properties. Let's go through another bug that we have in some of our custom UI. We have a view with a number of subviews, including some labels, and an imageView. And, right now one of our labels is missing. So, we're going to run dump on our parent view, and take a look at what's going on here. So, we've found our missing label. It is here, but it's -- if we bring up and look at the imageView that's alongside it, we notice that the frame of these two things, they both have the same origin. So, what's likely happening here, is that the label is obstructed by the imageView. So, we need to go and look at our layout code again, I think. In addition to dump for Swift objects, if you still have some Objective-C code lying around, NSObject also has the ivarDescription method. This will print all of the instance variables of your Objective-C objects. We have another bug in our screenshot's code, where our crop handles aren't working for some reason. If we call ivarDescription on our screenshot's view, we can see by looking closely, that our cropEnabled ivar is currently set to no. So, we have a good place to start investigating this bug. That's great. Using dump and ivarDescription are great ways to diagnose problems with your app. Another wonderful debugging tip and trick that we have for you is breakpoints. Breakpoints let you pause the program at arbitrary states of execution, and run commands. And, using the LLDB command line, or the Xcode UI, you can even add conditions before these breakpoints are run, and commands to run every time the breakpoint is hit. Breakpoints are an essential part of your debugging workflow. And, you can use the expression command, and dump, and ivarDescription, with the breakpoints that you set up in Xcode. I really encourage you to use breakpoints next time you're debugging an issue with your app. But, sometimes we don't have an issue with views or viewControllers. We don't have an issue with state, instead we have a really hairy memory management issue. Ben, are there any tools we can use for this? Well, I'm glad you asked Peter, because yes, there is another great tool built into Xcode. The Xcode memory debugger. This tool will help you visualize exactly how your application is using memory. Peter and I were debugging an issue the other day, where we had a leaking viewController. And, we could see that here that it's being held onto by a block. By enabling Malloc stack logging, we were able to see the full backtrace of exactly when this block was allocated. By zooming in, we can see that this block was actually created by that viewController. And so, that block is holding onto that viewController. But that viewController is also holding onto the block. And, there's our retain cycle. Wow. Great! The Xcode memory graph debugger is such a great tool for diagnosing issues like this in your app. For even more, check out the Debugging with Xcode 9 talk from 2017. So, that's how you can debug your app like a pro. Make sure to think like a detective whenever you encounter problems with your program. Use the Xcode view debugger, and memory graph debugger to dive deep on view- and memory-related issues. And, use LLDB's expression command with dump, and all the other great debugging methods we talked about here. Thank you. So, we've covered six really exciting topics this morning, this afternoon. But, we've barely scratched the surface. We encourage you to go out and check out the talks that we referenced throughout this presentation, and add even more delight to your applications. For more information, check out our page on the developer portal, and thank you. We hope you had a great conference. Thank you. Hey, everyone. Welcome to the AR design session. My name is Grant Paul, from the human interface team at Apple. And, I hope you've been having a great WWDC this week. So, for this session, we're going to start off by, I'm going to talk about how to design great AR apps and games with the user interfaces and interactions. And then, Omar's going to come up, and he's going to tell you all about making 3D models that look great in AR, and feel great in AR. So, before we get to that, I want to quickly talk about the basics. So, if you're new to it, you might be wondering exactly what it is that we mean when we talk about AR. But, even if you're an AR expert, I want to talk about what we mean for this session. So, AR, of course, stands for augmented reality. And, let's break down what that means a little bit. And, we can start with reality, because it's a little bit easier to get into. So, reality means is that AR deals with things in the real world. And, that's a little bit different than other things we might have done on our devices, that take place on the device, or they take place on the internet. But, with AR, things happen in the real world. They happen in the room around you, in the environment you're in, or in your place on the map. Where you are. But, the important thing is, it's a little bit different. And the other part of augmented reality, is augmented. And, that can mean a few different things. So, augmented can mean -- it can mean augmenting what you know about the world. It can mean getting information that the device can understand about the world, and giving that to you. It can mean placing virtual things out into the world, giving the virtual this physical context. And, it can mean taking something that is real, like your face, when you put an animoji on as a mask. Taking something that is real, and enhancing it, augmenting it. So, that's what we mean when we talk about AR, when we talk about augmented reality. And, with that, I want to get to the first half of this session. I want to start talking about how to design interfaces, and design interactions for your AR apps and your AR games. And, the first part of that is to talk about how to get people into AR. How to help guide people into AR. So, I'll show you how iOS 12's built-in AR experiences help guide people into AR. And then, how you can take those principles, and apply them in your own apps. Next, we'll talk about the different ways you can present content in AR. The different possibilities that ARKit opens up, and also some tips and tricks that are great no matter what kind of AR app you're making. And finally, we'll talk about interactions in the world. It's a little bit different when you're building an AR app than when you're building a 2D app, for what kind of interactions make sense, and what kind of interactions you're going to want to use. So, we'll figure out what still works great in AR, and then, where we might need some new kinds of interactions. But first, I want to get started with talking about how to get into AR. And, what I'm talking about here is after somebody's downloaded your app, after they've found the AR experience in the app, they've opened it up, what I'm talking about here is when ARKit needs to understand the world. Because for every AR app, ARKit needs some level of understanding of the world in order to start the AR experience. In order to get it going. Because every AR app needs to -- needs that understanding in order to place its objects into the world, or show the user that information. And, the way ARKit builds that understanding of the world, is by getting you to move your device, by having you move around. And, that's a little bit different from other places that you might have looked through a device, seen a camera preview, in the past. Like, for example, when you're taking a photo, you just need to point the device where you want to frame the shot. But, in AR, you really do need to start moving, looking at the same place from different positions, and looking at it from some different angles. So, the trick here is to let people know what they need to do. Let them know how to move, that they need to move their device. And, the way to do that is to give them a fixed reference. Give them something to base that understanding off of. And, let's talk about that. Let's take a look at an example. So, this is the game Euclidean Lands. And, what it's showing here, is the device moving around inside of a room. And, that's really great, because you can see here, without any text exactly what it is you need to do, that you need to move the device within that room, and just turning the device to look at a different angle isn't going to be enough. So, without any text, it's really clear from that fixed reference of the room, exactly what you need to do. And, usually, most of the time, that is all you're going to need. ARKit is even faster to build that understanding of the world in iOS 12. So, in even more cases that is all you're going to need. You just need to start moving, and you're ready to go. But, of course, there are some situations that aren't quite as well suited to AR. Maybe you're in a dark room, or it's really reflective, and it's not as easy to -- for ARKit to start building that understanding. So, it can take ARKit a little bit of time to get ready. It might not happen immediately. And, in those situations, if you're still being told to move your device, you're still looking at something that says to you, move your device around, people might start to wonder, why is the app not working? Does the app not understand the movement that they're making? And, it might get -- start to get confusing. So, what you need is some kind of feedback that lets people know that they're not doing anything wrong. They're doing exactly what they should be doing to get into AR. And, they should just keep doing it. So, let's take a look at an example. So, this is how iOS 12's built-in AR apps help guide people into AR. You can see the device moving over the fixed reference of that surface, by showing you that you need to move it. You can't just keep it in one place, or rotate it. And then, once you've started moving the device, the surface transitions into a cube. And, that cube spins with the movement of the device, giving you real, connected, direct feedback that you're on the right path, and you're doing the right thing. And then, once ARKit has built that understanding, it's all ready, the cube spins away, and you're ready to go in your AR experience. So, that's how the built-in apps in iOS 12 help guide people into AR. And, your apps should follow those same principles. They should help people know that they need to move their device, and give them that feedback that they're doing the right thing. But, your apps don't need to follow that same style. They don't need to look like line drawings. AR should feel like an integrated part of your app. It should feel native to your app's style. And, it shouldn't feel like something that's attached on, or this flow is something that's added on after the fact. So, the important part is to help people down the right path, but it should feel like part of your app while you're doing that. And, the last thing I want to talk about for helping get people into AR, is that it's important to balance your instructions, your guidance, how you're helping people get into AR with being really efficient when people already do know what to do. So, if somebody already knows what to do, they're just going to start moving their device right away. They don't need any kind of instructions. They don't need anything to tell them to do that. So, don't make them sit through any kind of guidance. Since ARKit is so much faster, in a lot of cases, they'll start moving, ARKit will understand the world, and they'll be ready to go right away. Alright. So, that's getting into AR. That's how you help guide people into your AR experiences. Now, let's talk about how to present your content in AR. The different ways that you can show things in AR, and tips and tricks for all kinds of AR apps and AR games. And, I want to start with some of the more radical possibilities of things you can build in ARKit. Because not every AR experience has to look like looking through your device, seeing a camera preview of the world, and then placing objects out into space. There are other options as well. So, one of those options is to build an AR experience entirely in two dimensions. You don't need to show a camera preview. You don't need any kind of 3D graphics. You're using that information that ARKit gets about the world to present a great AR experiences entirely in 2D. So, let's take a look at an example of that. This is the game "Rainbrow." And, in Rainbrow, the way you control you character is by moving your eyebrows up and down. [ Game Music ] So, there's no need of any kind of camera preview here. There's no need for any 3D graphics. It's a lot more fun entirely in 2D, but it's still a great AR experience. So, that was AR in 2D. But, another way, another thing you can do with ARKit is you can build things that I would consider to be full virtual reality experiences. And, what I mean by that, is that the experiences take you to a different place. They really make you feel like you're somewhere else. You can walk around the environment. You can move around within it. You can look in all directions. And, to me that makes something really into a virtual reality experience, even if it is through a device. And, there's some benefits to that. You don't need any extra equipment. It works everywhere. You don't need to wear a headset on your head. It works everywhere you are with the device you already have. You don't need any kind of trackers or anything else. And, there's some benefits to looking through a device rather than being fully immersed. Like, you're never going to accidentally walk into a wall because you always have your peripheral vision. So, ARKit can be a great way to make virtual reality experiences. We'll take a look at one. So, this is the virtual reality experience called "Enter the Room." And, it was developed by the International Committee of the Red Cross. And, in this experience, once you walk into the room, you can look around in all directions. You can move closer to things. You can inspect them. You can move farther away. And, the sound comes in, and it feels like it's coming in from outside the room, from all around you. And, that makes it into a really powerful experience. And, that's what you can do with virtual reality using ARKit. So, those are some of the more radical options of things that you can build with ARKit. We can also look at some tips and tricks that make sense no matter what kind of app it is that you're building. Whether it's a game, or a productivity app, these can work for everything. So, the first thing I want to talk about here is showing text in AR. Because text is really important. All kinds of AR apps have different reasons to show text. If it's a game, maybe you want to show the title of a level, or some instructions. Or, maybe for other kinds of apps, you want to label something in the world, label a virtual object. Maybe show some annotations. But, the important part, no matter what reason it is that you're showing this text, is to keep that text readable. Make it really easy to read. So, the simplest way to show text in AR, is to put it out into the world, to put it in perspective. And, you know, that can look really cool. But, it also has some drawbacks. And, one of those drawbacks is what happens when you look at it from an angle. The letters can, sort of, squish together. It can be a little hard to read. And, another issue is what happens when you take a step back, when you're looking from further away. The text can get really small. It's like trying to read a piece of paper from all the way across the room. So, if you're showing titles, or other kinds of things that maybe people already know, or could get in another way, it can be really cool to show that in perspective. But, if you're showing something that people really need to read, need to get information from, you might want to try a different option. So, a different option that you can use, is to show your text in screen space. And, what I mean by screen space, is that the text is always the same size. It's always facing you. It's always head-on. And, that makes it really easy to read. It doesn't have those problems of what angle you're looking at, or how far away from it you are. But, the important part of showing text in screen space, is that it's still attached to a position in the world. It's still fixed to the place in the world that you attach it to an object, or attach it to some physical feature. And, that makes it really feel like part of the AR scene. So, screen space text is a great way to label things, put some annotations into AR, but still make them really readable. So, here's an example of screen space text. And, this is from Measure, which is part of iOS 12. When Measure shows the measurements, it shows them in screen space. So, no matter what angle you're looking at those measurements from, or how far away you are, they're always incredibly readable. So, screen space text is great for readability. But, you should still try and keep how much text it is that you're actually showing in AR to as little as possible. And, the reasons for that is when text is placed in the world, you always have to keep pointing your device at it in order to read it. If you turn your device to a more natural reading position, the text is going to go away. So, if you have more detailed text, if you're showing details about some kind of object, or something in the world, you should show those details on the display. And then, people can use all that experience that they built up using iOS and reading things on their devices, to read those details directly on the display. And, when you're showing those details, when you're coming out of AR, it's important to have a transition. Because those transitions can make it really clear what it is you're looking at. What text, what object it is that those details are referencing. What it is that you're getting details about. So, let's look at an example there. And, this is also from Measure. In Measure, when you tap on a measurement, it comes out of AR to show the details about that measurement flat on the screen. And that's great because it's really easy to read. You don't have to point your device or your phone at the measurement in order to read it. But, it's also really clear, because of the transitions. The transitions show you what measurement it is that you're looking at details for. And, they comes out of the measurement, so you're never going to be confused. So, transitions. They're really great when you're coming out of AR, and going back into AR, for showing details on the display. But, they're also really important when you're just showing objects in AR. And, that's because it makes it feel like there's one version of the object. It makes it feel like that object has its own identity. And, that's important, because things in AR are so physical. They have that physical sense. When you see them in AR, they look real. And, things in reality, you can't just copy them. You can't just make multiple copies. So, it's important to keep that same principle when you're showing objects in AR. So, this is what happens when you Quicklook at objects in AR. And, it shows a great example of keeping that identity. When you switch from the object tab to the AR tab, the object stays in place. It always stays visible. It doesn't disappear and then come from somewhere else. And, even when you're deciding where to place the object here, it always stays visible on the display. So, it's really easy to see that there's one version of this object. And, even when you go back into the app that you were Quicklooking the object from, it still shows the object transitioning back to where it came from. So, it feels like there's one object moving between different parts of your app. And then, into the world, and back out of the world. It doesn't feel like there's multiple copies. Alright. So, that was a bunch of information in a row about the different ways that you can present your content in AR. So, let's do a quick recap of that. So, first, we looked at the different ways you can make AR experiences, and we talked about using AR to create entirely 2D experiences, with no camera preview, and no 3D graphics. We talked about how VR can be a great way to build experiences with ARKit, and make you feel like you're somewhere else, that can be really powerful and really immersive. We talked about using screen space to show text in AR, to make it really readable from any angle, and readable from any distance. We talked about showing your details on the screen, so that they're easy to read without pointing your device at some specific place in the world. And that you can use all of that same knowledge that you've built up reading things on iOS. And finally, we talked about transitioning into AR, and out of AR, for showing details flat on the display. But, also to give objects that sense of identity, that sense of physicality that's so important in AR. Alright. So, that's presenting content in AR. So, different ways that you can present it. And some tips for different types of content that you're going to want to show in your AR apps. Now, let's talk about how we can interact with that content. Let's talk about interactions that make sense in the world. And, let's start with touch, because touch has been really important all the way back to the beginning of iOS. Multi-touch was there right from the start, and it's been the most important way to interact with our devices. And the reason touch is so important, the reason touch is so great, is that it enables direct manipulation. And, direct manipulation is when you interact directly with things on the screen, like they're physical objects. You're not using controls to scroll or to pinch to zoom. You're interacting directly with the content. It's like there's a physical thing that you're manipulating. And, that's even more important in AR. Because in AR, as I said before, things are really physical. Objects feel like they're real. They feel like part of the real world. So, it's really important to use direct manipulation to make it feel like you're interacting directly with those objects. And, direct manipulation is also great because it uses gestures that you already know, that you have experience with from iOS. Because those gestures will be the same as any other content on iOS. They're things that you've been using for probably a long time. So, the first one there is how you can move objects in AR with direct manipulation. If you want to move objects, you just put your finger down and drag them to a new place. And, it feels like you're picking up the object, because it stays under your finger. You get this physical connection to the thing that you're moving. Another gesture you can do, is scaling objects. So, in AR, things start out at their physical size, at their natural size. But, if you want to change that, you can pinch the object to make it bigger. And, you can pinch out on the object if you want to make it smaller. The important things to think about when you're scaling objects in AR, is to give some feedback when you're doing it. Because the change is really, really big when you take an object and scale it up to 4 times the size. So, it's important to give people feedback, so they absolutely know what happened. And, the second thing is to make it really easy to go -- to take the object back to its natural size. Back to the size that it would be in the physical world. So, you can snap back to 100%, maybe with a haptic, to make that really easy. Another thing you can do is rotate objects, by putting two fingers on the display, and twisting to rotate. And, that can be really great, but with all of these two finger gestures, another thing you should think about is your tap targets. Because things in AR are always moving as you're moving the device, and they can get really small if you get further away, or if you scale them down. You should make sure to use really generous tap targets. So, it's easy to land two fingers on the object. And, be sure to use the center of those two finger touch positions to figure out which object to interact with. Because you might not be able to land two whole fingers, even on a generous tap target in AR. Alright, so direct manipulation is really great in AR. It's really important because AR is so physical. But, it's also not quite enough for most AR apps. Because if you have a lot of objects, it can make it hard to touch the right one. As I said before, the objects are always moving on the display as you're looking at different places in the world. And, they're staying fixed to that place in the world. So, it can be a little hard to aim for those objects on the screen. But, the fundamental reason, the number one reason, the real reason that touch is not enough for AR apps, is that it's fundamentally two-dimensional. The surface of the display is two-dimensional, and you're touching on that surface. That is what made it so great, multi-touch so great for flat, 2D apps in iOS. But, AR content, it's placed into the world. It's part of the world. So, that means we need some way to interact with that content also in three dimensions. Because the world is three-dimensional. The answer there is moving your device. Because moving your device, it's natively three-dimensional. It's three-dimensional inherently. You can move up and down. You can move left and right. You can move forward and back. You can turn in all directions. You can stand up and move across the room if you want to go a little bit further. But, the important part is that moving your device is fully three-dimensional, and that really makes it the number one, the primary interaction in AR. In fact, I would say that moving your device is more important than touch for AR apps. In fact, it's so natural, it's built in to every AR app by default. The way that you look at different things in AR is by moving your device to look at it from different angles, and different positions. So, it's really natural, and it's also really powerful. And, moving your device, it accomplishes many of the things that you would have done in a 2D app by using multi-touch. So, in a 2D app, if you want to see more content, the way you do that is by scrolling on the display. You scroll down to see something new. And, that's great in a 2D app, but in AR, you do that in 3D. You see different content, by moving your device to look at the content from different positions, and from different angles. So, it solves that same problem of wanting to see more, but it solves it fully in three dimensions. Similarly, in a traditional 2D app, if you want to see something bigger, you pinch in on it to make it bigger. You pinch to zoom. If you want to see it smaller, you pinch out. You pinch to zoom out. But, in AR, if you want to see something bigger, you can just get closer to the thing you're looking at. You can just move closer in. And, if you want to see more things at once, you want to see things from a wider angle, you can just take a step back, and look at all of the content at once. So, moving your device, it also replaces what you might have done by pinch to zoom to see more content in a 2D application. So, movement is really great. It can replace some of those things you might have used multi-touch for. You can also use it to build totally custom interactions for your AR apps. And, those can be really natural. Just like using your device to look around at different things, or to get closer and further away. So, let's take a look at an example of that. This is Swiftshot, the multiplayer AR sample game that you might have seen in the Keynote, or you might have actually tried it. In order to fire a slingshot in Swiftshot, the way you do it is you move close to that slingshot. You don't have to pick which slingshot you're going to fire from a list. You don't have to reach out and try and aim for it on the screen. You just move close to it, and when you want to fire the slingshot, you just pull back and release. So, it's incredibly precise, because you can fire in three dimensions. You have three dimensions of precision while you're moving that slingshot, while you're pulling it back. And, that's more than you could ever do using touch. So, moving your device, it's not only really natural, but it's also more precise than you could ever do using touch in AR apps. So, moving your device is great, but sometimes we need to combine the best of touch and the best of device movement to create the absolute best interactions. So, let's look at some examples of that. And, let's start with combining direct manipulation with moving your device. So, Quicklook in AR also offers a great example of combining device movement and direct manipulation in AR. If you want to move an object, of course, you can just drag it to the new position, the same as you saw before. Touch on the screen and move it to a new place. But, you could also touch down to pick up the object and then turn the device and release in a new place. And, that's great, because it gives you the full three-dimensional control of moving your device to pick where you're going to place it. And, it lets you move objects to places that you can't see on the screen, by picking them up and turning. But, it still keeps that sense of physical interaction. That sense of direct manipulation that you had from picking up the object directly. So, if you support any kind of moving objects in AR in your apps, you should definitely support picking up an object with direct manipulation, and moving the device to find a new place to put it. Alright. So, moving your device and direct manipulation is one way to combine touch. But another way to combine touch with moving your device is through indirect controls. And, what I mean by indirect controls, are controls that are flat on the display. They're not placed in the world. They're not attached to anything. They're in a consistent position on the display, and you can learn that position. And, that's really great, because that means they get out of the way. Once you've learned that position of that control on the display, it stays in one place. You can rest your finger above it while you're speeding your time focusing on the rest of the app. So, let's look at an example of that. Here's Zombie Gunship AR. You're flying in a gunship above a horde of zombies, and you really need to aim at them. You want to focus on aiming. You don't want to focus on moving your finger, trying to figure out where you need to aim your finger on the display in order to fire. Instead, you can just rest your finger above the fire button, and spend all of your time using that full 3D precision of moving your device in order to aim where it is you're firing, to aim at those zombies. So, indirect controls are really great combined with moving your device. But, they're also really great because they let apps work one-handed. In AR, no matter what, you always have to use one hand to control where it is that you're looking, at least one hand. So, if you want to build a one-handed AR experience, you're going to need to use really reachable controls that are really easy to access. So, an example of that is Measure. Measure uses an indirect control, the plus button, the add button at the bottom of the screen to add points. And, that control is in a really reachable spot. So, even while you're using one hand to focus the reticle on the center of the screen, to precisely place your measurements, you can keep a finger placed above that plus button to easily add your measurements, to easily place those points. So, using indirect controls can be a great way to make it not only easy to use AR experiences, but one-handed ones as well. Alright. So, that's how to pick interactions for AR. You can use direct manipulation to give that sense of physicality, and physical interaction. You can move the device, the primary interaction in AR. And, you can use really reachable indirect controls to focus on the content, rather than on controls or buttons interacting with it. And, that's what I wanted to talk to you about today. Getting into AR, guiding people down the right path, and giving them that direct feedback that they're doing the right thing. The ways you can present your content in 2D and VR, and how you can show content to make it easy to read, and give objects that physical identity transitioning in and out of AR. And, the interactions that you can use in the world, especially and primarily moving your device. So, now I want to bring up Omar, who's going to talk about making your models, your 3D models look really great in AR. Thank you. Thanks, Grant. Hey, everyone. I'm really excited to be up here to talk about some best practices that you need to keep in mind while you're developing your content for your AR experiences. So, we have a lot of different pieces of information to cover today. And, whether you're an engineer, designer, manager, or an artist, we wanted to provide you with a utility belt of tactics and definitions, so that you are best equipped to craft your own exceptional AR contents to delight people with. So, to begin with, let's start with some essential concepts to keep in mind while you're developing your AR experience. AR is incredible. Having the ability to take anything you can imagine, and place it into the real world is absolutely magical. And, because of this, people tend to expect a lot out of their AR experiences. They expect your 3D content to render at a smooth and consistent rate. It's incredibly distracting when you're really engaged with the content, and you start to move a bit closer to it, and then, you want to appreciate those fine details, and all of a sudden, wham! Poor optimization has caused performance to tank, and now it's as if you're watching a slideshow. So, to ensure your -- the most smoothest performance at all times, and to keep people fully engaged with your AR scene, your app needs to render at a recommended target of 60 frames per second. Now, it's really important to maintain this target throughout the entire experience. Really stress test your content. View it from every possible angle. You know, move in close, move back from it, and just make sure that the performance does not ever degrade. You know, maybe someday in the future, batteries will run for days. But, today, make sure that your experience is able to have as minimal of an impact to battery life as possible. Don't give people the chance to blame your app for draining their battery. The more power you save, the more you want people to come back to your experience and try it again. I mean, I don't know about you, but whenever I see a battery indicator in this stake, I seriously feel like I'm going to have a panic attack. And, we definitely don't want to have our AR experiences cause widespread battery chaos throughout the lands. Remember, only you can prevent excessive battery drain. I like to look at AR as having the power to take anything you can imagine and to transport into the real world. People want to explore your content, so you definitely want to bring your A game. Take the time to craft those nuanced details into your 3D content. Build a cohesive story and style. And, remember that every little detail, every little touch is an opportunity to surprise people. So, let's say we wanted to make an AR experience to be about an aquarium. Even in its most abstract form, I think I would be really hard-pressed to find anybody who would believe that this marshmallow blob represents a fish. On a positive note, though, our app will pretty much rock the performance numbers if this little guy's flopping around. So, let's try that again. Ah, now this is much better. That is one properly dead-looking fish. See how it exhibits some nice details, that once it's running in AR will really want to entice people to move closer to it to see all those nuanced details and explore its features. We should strive to maintain this level of quality with all the fishes swimming in our aquarium, or just floating in the top in this case. Finally, it's important to remember that people really want to use your app in a wide variety of environments. You want to avoid having your content stand out in real world locations where potentially the lighting ambient conditions could conflict with the story that you're trying to tell. So, when working with your assets, try to avoid using colors that are either too bright or too dark. And, make sure that you light your AR scene in a way that it casts even lighting on all the objects that you're planning to render, and at no matter what angle you view them from. You want your AR content to work whether it's day or night. Now, spoiler alert, we're going to go over some really nice features in ARKit that will enable you to really delight people when they see your AR content blend and react seamlessly to the real world environment. So, as you're building your AR content, one great tool to help evaluate your progress is by using one of our recent announced iOS 12 features, AR Quicklook. Throw some of your assets up on iCloud drive, view them using the Files app on iOS, and quickly have the ability to project it into AR. Heck, you can even show off your masterpiece by throwing it onto a website so that your friends can look it up, and you can view it anywhere, right from Safari. It's pretty awesome. Definitely go back and check out the session with Dave and David, who go over details of best practices of how you use AR Quicklook, and I'm guarantee you, it'll probably change your life with how you develop your assets. So, now you've taken some time to consider what people expect from our AR experience, let's quickly plan out what type of app we're going to build today. You know, it's always a good idea to start thinking this through before you begin creating your 3D content. As knowing what you want to make will help narrow down how best to optimize your content and your assets for AR. So, you're sitting at your desk, and suddenly you're struck by inspiration. You just came up with the most absolute brilliant AR experience ever. Alright? So, let's step back and ask ourselves a couple questions first. Does this experience really need to render hundreds of AR objects, or will we focus on a single hero asset? How much detail do we actually want? And, what graphical style best represents what we're trying to convey? Have we really paid attention to Grant earlier and are thinking about the level of interaction we want people to have with our experience? Having a clear answer to questions like these will help determine where best to put your rendering budget when you're developing your app. For example, imagine you're building an AR experience similar to the IKEA Place app, where people can preview different pieces of furniture by being able to place them in their home, or in our case, outside on their patio. Now, the stars of the show are actually the different furniture pieces, so you need to present highly detailed objects that closely mirror the real world counterparts. In this case, it is a good idea to make sure that you spend a little extra time in your rendering budget on these single hero assets, because their level of quality can essentially make or break a sale. On the other hand, you decide that you've had enough of accidentally stepping on those little, tiny, plastic bricks of agonizing pain that your kids start laying around the house. So, in order to preserve your sanity, you build an AR experience so they can play around with as many of these blocks as they can possibly imagine, similar to Playgrounds AR. And, save yourself from ever having to experience brick pain again. In an app like this, where you potentially have a lot of objects being rendered and interacted with, you want to basically create very simple, low-poly models with a flat colorful material, so that you can have a ton of them onscreen, and still have really good performance. So, now that we've asked ourselves these important questions, it's time to set up our AR canvas. Just like painters like to set up their canvas in space before they begin work, we would like to have a couple suggestions of how to set up your project up front, in order to position yourself for optimal success. We are big fans of creating a focus square to determine where to start placing your AR content. And, if you're using SceneKit, right there on the bottom of the screen, you have the option to actually activate the statistics panel. This will allow you to see your current frames per second, as well as how many polygons there are visible on the screen at any given time. Which should be very helpful as you start to build out your app and put all the different elements into it. So, now that we have a starter scene up and running, I was thinking, what will be a good example AR app to help get over these best practices? So, I'm not really an outdoorsy guy, but coming to California, I find that here a lot of people are. So, I've been trying to connect with nature. You know, go camping, maybe make a campfire for once, and yet it really didn't happen. So, instead I thought, let's just throw it into an app, and see how it goes. And we're going to call it CampfiAR. I know, it's perfect, right? Now, we can work on building out a detailed, single object, and bring all the joys of being outdoors without the fear of any of those bugs or fresh air. We decided to render with a stylized, semi-realistic, and playful graphical style. And, apply unique details to the use of careful application of some key, physically-based material properties. These choices mean that we could potentially use a lot of polygons to render the content on the screen, but why deprive people of the ability to spend multiple hours staring at our beautiful campfire? Let's avoid going down that route, and work towards optimizing our scene by using a few tricks of the trade. So, we'll begin by focusing on the foundational structure of 3D objects, the mesh. And describe the typical development flow that will allow you to create highly detailed models, but still maintain a low poly count for all the models in your scene. And, for those of you who might not know, poly count is essentially the number of polygons, typically triangles, that a mesh is composed of. So, one of the first things we like to do, is lay out the basic structure of an AR scene by using these simple meshes. We find using this type of white-boxing technique to be really helpful for testing out some basic interactions, as well as seeing how well the objects fit into the real world, what kind of scale are they at? You know, actually, I think this campfire looks really great. I think we're going to call it a day here. Let's just call this done and ship it. Thanks everyone. I'm off to the afterparty, and -- wait. So, that didn't look like a campfire to you guys? Oh, alright. Sorry. Really? My bad. Let's jump back to actually building out this campfire mesh. I want to give a quick high/low review of what a mesh is, and the basic data structures that comprise it. So, now you can think of a mesh as being a collection of triangles that are arranged in 3D space that will form a surface for you to apply materials on. And, the corners of these triangles are actually made up of points called vertices, which hold different pieces of information, such as its position in space, UV coordinates for texture application, as well as a very important property that we'll go over later called the normals. Well, since I got busted for trying to ship early, I wanted to redeem myself by building out one of the most gorgeous campfires in the world. Take a look at the details of this camp. Look at that fish and those branches. You can see all the intricate details of the scales, as well as the etchings found on the bark. But, man, my performance has tanked. And, that poly count has gone up to almost about a million polygons for this screen. So, I'm already in hot water, and I don't want to get in any more trouble, so we better go back and fix this. As I'm concerned about the impact that this will have on battery life, as well as how people are able to perceive and interact with this AR scene. So, let's see what we can do to, kind of, help reduce the number of polygons here. Most 3D authoring tools have specific functions that make it easy for you to reduce the actual complexity of your models. Here, we've reduced the number of polygons associated with the high-density model of the fish. But, notice that, as we zoom in, many of the details have been lost. But, don't fret. We can use certain material properties later on that will bring back a lot of those missing details. The key here is to build a solid foundational mesh that uses a minimal amount of polygons. So, let's put aside the high-density mesh for now. And, let's focus on building up this low-density mesh as we move forward. Alright, so I'll admit that this isn't looking quite as good as before, but man, you can take a look at that performance. Not only have you saved a crazy amount of overhead by reducing the number of polygons found on the screen, but we're able to add a bunch of 3D objects to this scene to make it even more robust. And, if you recall, in our previous high-density mesh, we were running at about 30 frames per second. But, now we're back to 60 frames. And, we were close to about, I think about a million polygons, and now it's down to 9,000. This is incredible. Because of this, we are well on our way to having those performance specification that we desire. A really solid frame rate with minimal impact to the battery life. So, now that we have this optimized model in our campfire scene, let's see how we can bring back some of those details that we lost by working on what some of these different material properties and techniques that will ensure that our model looks as good as possible while still maintaining that great level of performance. So, you may have heard this time before, physically-based rendering thrown about regarding modern 3D rendering. It's a pretty complex topic that will take a lot more time than we actually have in this session to go over. But, the basic concept describes the ability to take the application of your different material properties onto your mesh, in order to have it react realistically to the simulated lights found in your AR scene. And, moving forward, all the materials that we'll be discussing will conform to this shading technique. If you want further details about this concept, there's a great WWDC 2016 talk that goes over details about physically-based rendering as it applies to SceneKit, called Advances in SceneKit Rendering. Now, with that said, let's start talking about our first material property, albedo, or what is sometimes lovingly referred to as the base or diffuse color property. So, let's jump back into CampfiAR. Previously our base measure's looking a little bit boring, with the gray material associated with it, but after you apply albedo, you start to see that it looks a lot better. But, the campfire is still missing a lot of those small, finite details that were originally found in that high-density mesh. As you move closer to the campfire, you'll notice that all the surfaces are relatively flat. And, this is something that we'll definitely correct later, but first let's dive into the albedo property a little bit more. Think of the albedo as basically being the base mesh of the various objects in your AR scene. This is the material property that you typically use to apply textures to the surface of your model. If you recall, your mesh contained different vertices, that held different pieces of information. The ones you see here are actually called UV coordinates, which help map out how pixels from various texture maps are actually applied to the model. And, after we've worked with these textures, we've applied the abel material to this property on the fish. So, now we've essentially applied a texture to our fish, I want to remind you about the fact that you never know where people are never going to experience your app. You want to be able to have your content fit in as many different scenarios as possible. So, you want to take care in selecting the right albedo value that are neither too bright or dark. As, you want this to work in a wide variety of different situations. So, our fish has got a skin, but we're still missing a lot of details from this, and the other objects found in the scene. So let's go back into how we can start bringing back a lot of those details through the use of the normal material property. So, as we jump back into CampfiAR, let's see how we can bring back some of those details that we moved from optimization. This can be done through the use of a special texture called a normal map, which you can see here as the blue tinted maps that are now applied to our AR scene. These maps allow you to add those fine surface details back into your models, without the need to add any additional geometry. Now, after applying the normal maps, you can see how the fish exhibits some scales, as well as making the branches show just a tiny bit more detail. Also if you take a look at the statistics panel, you'll notice that there was absolutely no change in the number of polygons related to this model. It's magical, isn't it? So, how do we make this normal map? Let's see a closer look at one of the branches, and see what we can do to make this work. In most modern 3D modeling applications, artists have the ability to generate these normal maps by projecting details from a high-density mesh over to a low-density one. So, here you can see what the normal map looks like on the branches after they've been generated from this high-density mesh. And, after applying the normal map, you start to notice all those nice details that were originally lost come back into this model. But we still are able to maintain that high performance, of the low-poly mesh. So, you may be wondering why the normal map looks kind of strange, well the colors of the normal map actually represent visual representations of the vector data. And, determine how the normals on the surface model will be offset in order to change the way that light is being reflected, and our key to making this effect work. That was a bit of a mouthful, so let's dive a little bit more into this property, because normals we feel are a really important topic. And, we want to spend a couple more minutes just going into how truly spectacular these can be. The art of manipulating normal vectors are one of the key tools that AR creators have in order to add a lot of the significant details back into their model. So, what the heck is a normal vector, and are there strange vectors as well? Well, no there's no strange vectors unless you forgot your high school trig, but normal vectors lie perpendicular to the surface of the mesh, and are associated with each mesh vertex. So, why do we need these normals? Well, in order to see our object, you need to place simulated lights into your 3D engine. Normal vectors allow 3D engines to calculate how these lights are actually reflected off the surface of these materials. Similar to how light behaves in the real world, and are essential to making sure that your AR scenes mimic reality. What's interesting is that by modifying these normals, you can trick the engine to thinking that your surface is actually more detailed than it really is, without need to add any additional geometry. If you take a look at this example, you can see a simple sphere, being rendered with flat shading. What this means, that the normals associated with each face of the mesh are pointing in the exact same direction, as seen by the 2D diagram. Now, when you -- when light reacts to this surface, you'll be actually able to notice all the different polygons comprising this mesh due to being evenly lit across each face. Here, though, we're using the exact same model, but leveraging a technique called smooth or phong shading. Notice that the normals are actually gradually changing as you move across the surface of the polygon. With the engine calculates the reflection off of this model, it'll give the impression of a smooth, curved surface due to the gradual interpolation of these normals. What's really awesome is that these two models have the exact same number of polygons associated with them. But, through this normal manipulation, the object will seem to have a much more smoother and detailed surface, without the need to, again, add or change any of the geometry associated with this mesh. Whew. Alright, so we've gone through enough about normals. Let's, kind of, go over what it takes to add a little bit more shiny to your scene. CampfiAR is definitely starting to look better. But, some of these normal maps [inaudible] but some of these parts seem a bit dull, especially the objects that you expect to be shiny or reflective, kind of like the kettle or the fish in this scene. What we see here is the results of applying a metal map to our AR scene. A metal map is used to determine which object surfaces should exhibit reflective properties. Once the material property's activated, notice how shiny and reflective the area's we've designated as being metallic are. And, on the kettle as well as the scales of the fish. So, let's focus specifically on the kettle. We'll begin by taking the original albedo map, and then apply a metal map to the metalness property of this material. After the application of the metal map, the 3D render will actually designate the surface to be reflective in the areas on the map that were actually white. And, despite begin called metalness, it doesn't necessarily mean that your object needs to contain metal. Rather, we're just letting the 3D engine know that this object should exhibit, kind of, a reflective surface. Now, it's best to use the metalness map on your model, like our kettle here, when it has both a mixture of metallic and non-metallic surfaces. It's a simple grayscale map, where the level of metalness ranges from being black, for non-metal surfaces, to white, for metallic surfaces. And, it allows for a single material to be used to represent both reflective and non-reflective surfaces on your single object. However, this kettle's a bit crazy reflective. And, doesn't really quite provide the look that we're hoping for. So, in this case, we want to potentially vary the amount of reflectivity, as well as simulate the fact that all surfaces are not perfectly smooth. And, they actually might exhibit slight small, microabrasions to their surface. And, this is where the roughness material property comes into play. So, returning back to CampfiAR, you can see how the reflective surfaces are a bit too smooth. As we layer on the roughness maps, you can see that we are going to modify both the kettle and the fish to adjust the way that they're reflecting. And then, after we apply the roughness material property to both of these objects, you can clearly see the reduction of the reflectivity. This combination of roughness and metalness properties are another important concept to focus on. So, let's dive a little bit deeper into the roughness material property. Use roughness to simulate micro surface details, which in turn will affect the way light is being bounced off that surface. If the roughness property is set to completely smooth, then light will bounce off of your surface like a mirror. As you increase the roughness of the material, light reflects over a wider range of angles. Here, we're slowly scaling a constant roughness value between no roughness to max roughness on the kettle itself. And, this is a good way of being able to simulate the concept of having microsurfaces, and blurring reflection to the point where you might not see any reflectivity depending on which range you put your value to. So, for the kettle, we've taken the original metal surface, and instead of just applying a constant roughness value to it, we actually applied a roughness map. And, this will help designate the surfaces where we will be scattering the light more often, and more than others. Once we've applied the roughness map, we get to see the real final look of the reflectiveness of this kettle, which is a lot less shiny as before. Now, a combination of this metalness property and this roughness property really make your reflective AR models look phenomenal. Roughness can be used to tweak how much of your objects will reflect the environment. And, can really be used to add a lot more realism to your metallic surfaces. You can use this roughness map to add additional minor details as we've done here, to make our kettle look a little more scuffed up. Now, to close out materials, we have two more material properties that I want to discuss to further refine your models, and have a good balance between performance and aesthetics. Ambient occlusion is a material property that is used to provide your model with self-shadowing, which in turn can lead to adding additional depth and details to your AR models. Now, while normal maps are great for applying significant amount of details back into your AR model, you can use ambient occlusion to really hammer in those details. Here, we're visualizing the ambient occlusion maps for CampfiAR, and it's a bit of a difficult effect to demonstrate, as it's meant to be relatively subtle. But, see if you can notice the additional shadows on the logs, as well as certain areas near the bottom of the kettle. In our case here, it's kind of like playing Where's Waldo? with shadows, so let's focus in on the logs in the scene. So, here we're showing you the normal maped logs before. Now, there's some great details found in the ridges, but we can definitely improve some of these areas. Now, as we look at the ambient occlusion map, you can see how we've added some regions of self-shadowing. So, lower portions of the log, as well as around the small embedded stumps. And, after we apply the map to our ambient occlusion property, I hope you can see the benefits of adding those baked detail shadows without the need of using expensive dynamic lights in your scene. When working in AR, we recommend that you actually bake your ambient occlusion into a map. Which is what we've done for CampfiAR. Rather than use the alternative, such as screen space ambient occlusions, which is a camera-based post-process effect, and can potentially lead to poor rendering performance in your scene. And, last but certainly not least, be frugal with the use of your transparency in your materials materials. If you must use transparencies, we recommend that you use separate materials for objects where you see a combination of transparent and non-transparent surfaces. In general, when working with AR content, the use of a lot of transparent surfaces can potentially have a huge impact on performance, especially if you are having transparent surfaces that are, kind of, stacked in front of each other when you're viewing them. This is known as overdraw, and it's something that you definitely want to avoid when you're working in AR. Whew. Alright. I hope everybody's still with me, as that was quite a lot to go through. So far, we've focused mostly on how the AR content will react to the simulated spotlights found in our 3D engines. But now, it's time to focus on some ways to make our content seem like it's actually part of the real world. A fantastic option to use to compensate for varied lighting conditions is to leverage one of ARKit's well known features, and light estimation. Let's begin by activating this functionality and see how it affects our kettle. Notice that how when the ambient light changes in intensity in the real world, a similar adjustment is made to the ambient light in our AR scene. The way this works is that ARKit analyzes each frame of the video, and uses them to estimate the lighting condition of the real world. It is an absolutely magical feature that helps assure that the amount to light applied to your AR content matches what you see in the real world. Now, that we have magical light wizards living in our AR scene, let's discuss shadows. Shadows in AR are really, really hard to get right. Your shadow needs to be able to work in a wide variety of situations. Remember that people can be potentially using your app anywhere. And, if your AR shadows differ from the ones that are seen in the real world environment, it can be relatively jarring for your experience. Here, we have tried to incorrectly cast a dynamic shadow by using a directional light as a sharp angle in our 3D engine. Shadows are a great way to make objects actually feel grounded in the real world, but in this case, it doesn't really match the shadows being seen in the surrounding environment. It's like we're purposely trying to defy the laws of physics here. Instead, we suggest that you place your directional light directly overhead, and play a little bit around with the intensity of that effect to make sure that it actually feels a little bit more subtle. This will allow your shadows to work in a lot more different situations, and a lot more different scenarios. An alternative to this is actually using a method where you can create your own drop shadow, rather than using dynamic lights in your scene, which can get expensive, and can severely affect performance if you're rendering a lot of 3D content. Take the time to craft those really good shadows. And ensure that they will fit in as many real world situations as possible. Ah, environment maps. If you really want to astound people, you definitely want to use these maps, especially on those AR objects that exhibit reflectivity. It'll make it seem like your AR content exists right here, in the real world. And, to make it super easy to leverage their powers we want to show you how one of the new features in iOS 12 and ARKit 2.0, automatic environmental mapping, can help you achieve this effect. If you take a close look at the kettle, originally we're using a baked environment map that has a kind of blush tinge to it. Once we've activated the automatic environmental mapping, notice how the kettle now reflects a lot of the ground, and the surrounding color of the current environment that we're in. You can even see a little bit of the green from the grass that this kettle's sitting in. This is a fantastic feature that'll really help ground your objects into the real world, and with careful use of roughness, can really enhance the believability of your scene. So, now why is automatic environmental mapping so actually incredible? Typically, these maps are used to simulate the ability of metallic surfaces to mirror the environment around them. You can see example of a cube environment map here. Now, before ARKit added automatic environmental mapping, you had to provide your own image, and hope that it was generic enough to work in a large variety of situations that your app may be used in. But now, with ARKit 2.0, you can kiss those sleepless night, fretting about whether your environment map is actually ruining your AR experience. For more information about environmental mapping, and other new features found in ARKit 2.0, please check out the talk by Arsalan and Reinhard called What's New in ARKit 2. And now, to close out CampfiAR, let's put all this together, and add the final touches to the campfire. With a little bit of animations and [inaudible] effects, along with the applications of all the techniques discussed today, CampfiAR is ready for primetime. Now, if I ever get the crazy urge to go outside, I can suppress those urges, and stay safely at my desk, enjoying the joys of simulated outdoors with CAmpfiAR. Who needs EpiPens? Not this guy. We went through lot of different topics today. So, I quickly want to reiterate some of the important things to keep in mind while you're developing your app. Remember that your app can be used in a wide variety of real world conditions, so always make sure aesthetic choices allow your content to blend almost anywhere. Once you've decided what kind of AR experience you wish to build, adhere to what you think your rendering budget is, and work as many optimizations as you can to get that smooth performance and efficiently use your power. And, finally leverage the use of various material properties, as well as the built-in features of ARKit to get your AR content looking great, in order to delight people who use your app. And, for your reference, here's a table of all the material properties we worked with today to build out CampfiAR. You can get additional information at the link provided. Thank you.  Hello, everybody. I'm very excited to be here today to talk about understanding ARKit Tracking and Detection to empower you to create great augmented reality experiences. My name is Marion and I'm from the ARKit Team. And what about you? Are you an experienced ARKit developer, already, but you are interested in what's going on under the hood? Then, this talk is for you. Or you may be new to ARKit. Then, you'll learn different kind of tracking technologies, as well as some basics and terminology used in augmented reality, which will then help you to create your very own first augmented reality experience. So, let's get started. What's tracking? Tracking provides your camera viewing position and orientation into your physical environment, which will then allow you to augment virtual content into your camera's view. In this video, for example, the front table and the chairs is virtual content augmented on top of the real physical terrace. This, by the way, is Ikea. And the virtual content will appear always virtually correct. Correct placement, correct size, and correct perspective appearance. So, different tracking technologies are just providing a difference reference system for the camera. Meaning the camera with respect to your world, the camera with respect to an image, or maybe, a 3D object. And we'll talk about those different kind of tracking technologies in the next hour, such that you'll be able to make the right choice for your specific use case. We'll talk about the already existing AR technologies' Orientation Tracking, World Tracking, and Plane Detection. Before we then have a close look at our new tracking and detection technologies which came out now with ARKit 2. Which are saving and loading maps, image tracking, and object detection. But before diving deep into those technologies, let's start with a very short recap of ARKit like on a high level. This is, specifically, interesting if you are new to ARKit. So, the first thing you'll do is create an ARSession. An ARSession is the object that handles everything from configuring to running the AR technologies. And also, returning the results of the AR technologies. You then, have to describe what kind of technologies you actually want to run. Like, what kind of tracking technologies and what kind of features should be enabled, like Plane Detection, for example. You'll then, take this specific ARConfiguration and call run method on your instance of the ARSession. Then, the ARSession, internally, will start configuring an AVCaptureSession to start receiving the images, as well as a call motion manager to begin receiving the motion sensor, so, data. So, this is, basically, the built-in input system from your device for ARKit. Now, after processing the results are returned in ARFrames at 60 frames per second. An ARFrame is a snapshot in time which gives you everything you need to render your augmented reality scene. Like, the captured camera image, which would then be, which will be rendered in the background of your augmented reality scenario. As well as a track camera motion, which will then be applied to your virtual camera to render the virtual content from the same perspective as the physical camera. It also contains information about the environment. Like, for example, detected plates. So, let's now start with our first tracking technology and build up from there. Orientation Tracking. Orientation Tracking tracks, guess what? Orientation. Meaning it tracks the rotation, only. You can think about it as you can only use your hat to view virtual content, which also, only allows rotation. Meaning you can experience the virtual content from the same positional point of view, but no change in the position is going to be tracked. The rotation data is tracked around three axles. That's why it's also, sometimes, called the three degrees of freedom tracking. You can use it, for example, in a spherical virtual environment. Like, for example, experience a 360-degree video, in which the virtual content can be viewed from the same positional point. You can also, use it to augment objects that are very far away. Orientation Tracking is not suited for physical world augmentation, in which you want to view the content from different points of views. So, let's now have a look at what happens under the hood when Orientation Tracking is running. It is, actually, quite simple. It only uses the rotation data from core motion, which applies sensor fusion to the motion sensors data. As motion data is provided at a higher frequency than the camera image, Orientation Tracking takes the latest motion data from commotion, once the camera image is available. And then, returns both results in an ARFrame. So, that's it. Very simple. So, please note that the camera feed is not processed in Orientation Tracking. Meaning there's no computer version under the hood here. Now, to run Orientation Tracking you only need to configure your ARSession with an AROrientation TrackingConfiguration. The results will then be returned in an ARCamera object provided by the ARFrames. Now, an ARCamera object always contains the transform, which in this case of Orientation Tracking, only contains the rotation data of your tracked physical camera. Alternatively, the rotation is also represented in eulerAngles. You can use whatever fits best to you. Let's now move over to more advanced tracking technologies. We'll start with World Tracking. World Tracking tracks your camera viewing orientation, and also, the change in position into your physical environment without any prior information about your environment. Here, you can see on the left side the real camera's view into the environment, while on the right side you see the tracked camera motion while exploring the world represented in the coordinate system. Let's now explain better, what happens here, when World Tracking is running. World Tracking uses a motion sensor, the motion data of your device's accelerometer and gyroscope to compute its change in orientation and translation on a high frequency. It also provides its information in correct scale in Metal. In literature, just this part of the tracking system is also called Inertial Odometry. While this motion data provides good motion information for movement across small time intervals and whenever there's like, sudden movement, it does drift over larger time intervals as the data is not ideally precise and subject to cumulative errors. That's why it cannot be used just by its own for tracking. Now, to compensate this drift, World Tracking, additionally, applies a computer version process in which it uses the camera frames. This technology provides a higher accuracy, but at the cost of computation time. Also, this technology is sensitive to fast camera motions and this results in motion blur in the camera frames. Now, this version only part of the system is also called Visual Odometry. Now, by fusing those both systems, computer vision and motion, ARKit takes the best of those both systems. From computer version, it takes a high accuracy over the larger time intervals. And from the motion data it takes the high update rates and good precision for the smaller time intervals, as well as the metric scale. Now, by combining those both systems World Tracking can skip the computer version processing for some of those frames, while still keeping an efficient and responsive tracking. This frees CPU resources, which you can then, additionally, use for your apps. In Literature, this combined technology is also called Visual Inertial Odometry. Let's have a closer look at the visual part of it. So, within the computer version process interesting regions in the camera images I extracted, like here, the blue and the orange dot. And they are extracted such that they can robustly all to be extracted and other images of the same environment. Those interesting regions are also called features. Now, those features are then matched between multiple images over the camera stream based on their similarity and their appearance. And what then happens is pretty much how you are able to see 3D with your eyes. You have two of them and they are within the sidewise small distance. And this parallax between the eyes is important as this results in slightly different views into the environment, which allows you to see stereo and perceive the depth. And this is what ARKit now, also, does with the different views of the same camera stream during the process of triangulation. And it does it once there's enough parallax present. It computes the missing depth information for those matched features. Meaning those 2D features from the image are now reconstructed in 3D. Please, note that this reconstruction to be successful, the camera position must have changed by a translation to provide enough parallax. For example, with the sidewise movement. The pure rotation does not give enough information here. So, this is your first small map of your environment. In ARKit we call this a World map. In this same moment, also, the camera's positions and orientations of your sequences are computed, denoted with a C here. Meaning, your World Tracking just initialized. This is the moment of initialization of the tracking system. Please note that also in this moment of this initial reconstruction of the World map, the world origin was defined. And it is set to the first camera's origin of the triangulated frames. And it is also set to be gravity aligned. It's denoted with a W in the slides. So, you now have a small representation of your real environment reconstructed as a World map in its own world coordinates system. And you have your current camera tracked with respect to the same world coordinate system. You can now start adding virtual content to augment them into the camera's view. Now, to place virtual content correctly to an ARSession, you should use ARAnchors from ARKit, which are denoted with an A here. ARAnchors are reference points within this World map, within this world coordinates system. And you should use them because the World Tracking might update them during the tracking. Meaning that, also, all the virtual content that is assigned to it will be updated and correctly augmented into the camera's view. So, now that you've used the ARAnchors you can add virtual content to the anchor, which will them be augmented correctly into the current camera's view. From now on, this created 3D World map of your environment is your reference system for the World Tracking. It is used to reference new images against. And features are matched from image to image and triangulated. And at the same time, also, new robust features are extracted, matched, and triangulated, which are then extending your World map. Meaning ARKit is learning your environment. This then allows, again, the computation of tracking updates of the current camera's position and orientation. And finally, the correct augmentation into the current camera's view. While you continue to explore the world, World Tracking will continue to track your physical camera and continue to learn your physical environment. But over time, the augmentation might drift slightly, which can be noticed like you can see in the left image, in a small offset of the augmentation. This is because even small offsets, even small errors will become noticeable when accumulated over time. Now, when the device comes back to a similar view, which was already explored before, like for example, the starting point where we started the exploration, ARKit can perform another optimization step. And this addition makes, a Visual Intertial Odometry system, makes the system that ARKit supplies to a Visual Inertial SLAM System. So, let's bring back this first image where the World Tracking started the exploration. So, what happens now is that World Tracking will check how well the tracking information and the World map of the current view aligns with the past views, like the one from the beginning. And will then perform the optimization step and align the current information and the current World map with your real physical environment. Have you noticed that during this step, also the ARAnchor was updated? And that is the reason why you should use ARAnchors when adding virtual content to your scenario. In this video, you can see the same step again with a real camera feed. On the left side you see the camera's view into the environment, and also, features which are tracked in the images. And on the right side, you see a bird eye's view onto the scenario, showing what ARKit knows about it and showing the 3D reconstruction of the environment. The colors of the points are just encoding the height of the reconstructed points with blue being the ground floor and red being the table and the chairs. Once the camera returns back to a similar view it has seen before, like here the starting point, ARKit will now apply this optimization step. So, just pay attention to the point cloud and the camera trajectory. Have you noticed the update? Let me show you, once more. This update aligns the ARKit knowledge with your real physical world, and also, the camera movement and results in the better augmentation for the coming camera frames. By the way, all those computations of World Tracking, and also, all this information about your learned environment, everything is done on your device only. And all this information, also, stays on your device only. So, how can you use this complex technology, now, in your app? It is actually quite simple. To run World Tracking you just configure your ARSession with an ARWorldTrackingConfiguration. Again, its results are retuned in an ARCamera object of the ARFrame. An ARCamera object, again, contains the transform, which in this case of World Tracking contains, additionally, to the rotation, also, the translation of the track camera. Additionally, the ARCamera also contains information about the tracking state and trackingStateReason. This will provide some information about the current tracking quality. So, tracking quality. Have you ever experienced opening an AR app and the tracking worked very poorly or maybe it didn't work at all? How did that feel? Maybe frustrating? You might not open the app, again. So, how can you get a higher tracking quality for your app? For this, we need to understand the main factors that are influencing the tracking quality. And I want to highlight three of them here. First of all, World Tracking relies on a constant stream of camera images and sensor data. If this is interrupted for too long, tracking will become limited. Second, World Tracking also works best in textured and well-lit environments because World Tracking uses those visually robust points to map and finally triangulate its location. It is important that there is enough visual complexity in the environment. If this is not the case because it's, for example, too dark or you're looking against a white wall, then also, the tracking will perform poorly. And third, also, World Tracking works best in static environments. If too much of what your camera sees is moving, then the visual data won't correspond with the motion data, which might result in the potential drift. Also, device itself should not be on a moving platform like a bus or an elevator. Because in those moments the motion sensor would actually sense a motion like going up or down in the elevator while, visually, your environment had not changed. So, how can you get notified about the tracking quality that the user is currently experiencing with your app? ARKit monitors its tracking performance. We applied machine learning, which was trained on thousands of data sets to which we had the information how well tracking performed in those situations. To train a classifier, which tells you how tracking performs, we used annotations like the number of visual-- visible features tracked in the image and also, the current velocity of the device. Now, during runtime, the health of tracking is determined based on those parameters. In this video, we can see how the health estimate, which can be seen-- which, is reported in the lower left, gets worse when the camera is covered while we are still moving and exploring the environment. It also shows how it returns back to normal after the camera view is uncovered. Now, ARKit simplifies its information for you by providing a tracking state. And the tracking state can have three different values. It can be normal, which is the healthy state and is the case in most of the time. It's the case in most of the times. And it can also be limited, which is whenever tracking performs poorly. If that's the case, then the limited state will also come along with the reason, like insufficient features or excessive motion or being currently in the initialization phase. It can also be not available, which means that tracking did not start yet. Now, whenever the tracking state changes, a delegate is called. The camera did change tracking state. And this gives you the opportunity to notify the user when a limited state has been encountered. You should, then, give informative and actionable feedback what the user can do to improve his tracking situation, as most of it is actually in the user's hand. Like for example, as we learned before, like a sidewise movement to allow initialization or making sure there's enough adequate lightning for enough visual complexity. So, let me wrap up the World Tracking for you. World Tracking tracks your camera 6 degree of freedom orientation and position with respect to your surrounding environment and without any prior information about your environment, which then allows the physical world augmentation in which the content can actually be viewed from any kind of view. Also, World Tracking creates a World map, which becomes the tracking's reference system to localize new camera images against. To create a great user experience, the tracking quality should be monitored and feedback and guidance should be provided to your user. And World Tracking runs on your device only. And all results stay on your device. If you have not done it already, try out one of our developer examples. For example, the Build Your First AR Experience, and play a bit around, just 15 minutes with the tracking quality in different situations; light situations or movements. And always remember to guide the user whenever he encounters a limited tracking situation to guarantee that he has a great tracking experience. So, World Tracking is about the camera-- where your camera is with respect to your physical environment. Let's now talk about how the virtual content can interact with the physical environment. And this is possible with Plane Detection. The following video, again, from the Ikea app, shows a great use case for the Plane Detection, placing virtual objects into your physical environment, and then interacting with it. So first, please note how, also, in the Ikea app the user is guided to make some movement. Then, once a horizontal plane is detected, the virtual table set is displayed and is waiting to be placed by you. Once you position it, rotate it as you want it, you can lock the object in its environment. And did you notice the interaction between the detected ground plane and the table set in the moment of locking? It kind of bounces shortly on the ground plane. And this is possible because we know where the ground plane is. So, let's have a look at what happened under the hood here. Plane Detection uses the World map provided by the world I just talked about, just talked about a moment ago, which is represented here in those yellow points. And then, it uses them to detect surfaces that are horizontal or vertical, like the ground, the bench, and the small wall. It does this by accumulating information over multiple ARFrames. So, as the user moves around the scene, more and more information about the real surface is acquired. It also allows the Plane Detection to provide and like extent the surface, like a convex hull. If multiple planes belonging to the same physical surface are detected, like in this part now, the green and the purple one, then they will be merged once they start overlapping. If horizontal and vertical planes intersect they are clipped at the line of intersection, which is actually a new feature in ARKit 2. Plane Detection is designed to have very little overhead as it repurposes the mapped 3D points from the World Tracking. And then it fits planes into those point clouds and over time continuously aggregates more and more points and merge the planes that start to overlap. Therefore, it takes some time until the first planes are detected. What does that mean for you? If your app is started, there might not directly be planes to place objects on or to interact with. If the detection of a plane is mandatory for your experience, you should again guide the user to move the camera with enough translation to ensure a dense reconstruction based on the parallax, and also, enough visual complexity in the scene. Again, for the reconstruction, a rotation only is not enough. Now, how can you enable the Plane Detection? It's, again, very simple. As the Plane Detection reuses the 3D map from the World Tracking, it can be configured by using the ARWorldTrackingConfiguration. Then, the property planeDetection just needs to be set to either horizontal, vertical, or like in this case, both. And then, just call your ARSession with this configuration. And the detection of the planes will be started. Now, how are those, the results of the detected planes returned to you? The detected planes are returned as an ARPlaneAnchor. An ARPlaneAnchor is a subclass of an ARAnchor. Each ARAnchor provides a transform containing the information where the anchor is in your World map. Now, a plane anchor, specifically, also has information about the geometry of the surface of the plane, which is represented in two alternative ways. Either like a bounding box with a center and an extent, or as a 3D mesh describing the shape of the convex hull of the detected plane and its geometry property. To get notified about detected planes, delegates are going to be called whenever planes are added, updated, or removed. This will then allow you to use those planes, as well as react to any updates. Now, what can you do with planes? Like what we've seen before on the Ikea app, these are great examples. Place virtual objects, for example, with hit testing. Or you can interact with some, for example, physically. Like we've seen bouncing is a possibility. Or you can also use it by adding an occlusion plane into the detected plane, which will then hide all the virtual content below or behind the added occlusion plane. So, let me summarize what we've already gone through. We've had a look at the Orientation Tracking, the World Tracking, and the Plane Detection. Next, Michele will explain, in depth, our new tracking technologies, which were introduced in ARKit 2. So, welcome Michele. Thank you, Marion. My name is Michele, and it's a pleasure to continue with the remaining topics of this session. Next up is saving and loading maps. This is a feature that allows to store all the information that are required in a session. So, that it can literally be restored in another session at a later point in time to create augmented reality experiences that persist to a particular place. Or that could, also, be stored by another device to create multiple user augmented reality experiences. Let's take a look at an example. What you see here is a guy; let's name him Andre, that's walking around the table with his device having an augmented reality experience. And you can see his device now is making this seem more interesting by adding a virtual vase on the table. A few minutes later his friends arrive at the same scene. And now, they're both looking at the scene. You're going to see Andre's device on the left and his friend on the right now. So, you can see that they're looking at the same space. They can see each other. But most importantly, they see the same virtual content. They're having a shared augmented reality experience. So, what we have seen in these examples can be discovered in three stages. First, Andre went around the table and acquired the World map. Then, the World map was shared across devices. And then, his friend's device re-localized to the World map. This means that the object was able to understand in the new device that this was the same place as the other device, computed the precise position of the device with respect to the map, and then, started tracking from there just like the new device acquired the World map itself. We're going to go into more detail about these three phases. But first, let's review what's in the World map. The World map includes all the tracking data that are needed for the system to be localized, which includes the feature points as Marion greatly explained before. As well as local appearance for this point. They also contain all the anchors that were added to the session, either by the users, like planes, for example. I mean by the system-- like planes. Or by the users, like the vase, as we have seen in the example. This data is serializable and available to you so that you can create compelling persistent or multiple user augmented reality experiences. So, now let's take a look at the first stage, which is acquiring the World map. We can play back the first video where Andre went around the table that you can see his device on the left, here. And on the right, you see the World map from a top view as acquired by the tracking system. You can [inaudible] is the table and the chair around it. There's a few things to pay attention to during this acquisition process. First, everything that Marion said during tracking also applies here. So, we want enough visual complexity on the scene to get dense feature points on the map. And the scene must be static. Of course, we can deal with minor changes, as you have seen the tablecloth moving by the wind. But the scene must be mostly static. In addition, when we are specifically acquiring a World map for sharing we want to go around the environment from multiple points of view. In particular, we want to cover all the direction from which we want to later be localized from. To make this easy, we also made available a world mapping status which gives you information about the World map. If you guys have been to the What's New in ARKit talk, [inaudible] greatly expand this to quickly recap. When you start the session the World map status will start limited. And then, will switch to a standing as more of the scene is learned by the device. And then, finally, we go to mapped when the system is confident you're staying in the same place. And that's what you want to save the map in the mapped state. So, that's good information. But this is mostly on the user side applied to acquire the session. So, what does this mean to you as a developer? That you need to guide the user. So, we can indicate the mapping status and even disabling the saving or sharing of the World map until the mapping status goes to the mapped state. We can also, monitor the tracking quality during the acquisition session and report to the user if the tracking state has been limited for more than a few seconds. And maybe even give an option to restart the acquisition session. On the receiving end of the device, we can also guide the user to better localization process. So, when we are, again, in the acquisition device, when we are in the map state we can take a picture of the scene and then, ship that together with the World map. And on the receiving end we can ask the user find this view to start your shared experience. That was how to acquire the World map. Now, let's see how you can share the World map. First, you can get the World map by simply calling the getCurrentWorldMap method in the ARSession. And this will give you the World map. The World map is a serializable class. So, then we can simply use NSKeyedArchiver utility to serialize it to a binary stream of data, which then, you can either save to disk in case of a single user persistent application. Or you can share it across devices. And for that, you can use the MultiPeerConnectivity framework, which has great feature like automatic device, nearby device discovery, and allows efficient communication of data between devices. We also, have an example of how to use that in ARKit called Creating a Multiuser AR Experience that you can check out on our developer website. On the receiving end of the device, once you've got the World map let's see how you can set up the World Tracking configuration to use it. Very simple. You just set the initial World map property to that World map. When you run the session, the system will try to find that previous World map. But it may take some time, even because the user may not be pointing at the same scene as before. So, how do we know when localization happen? That information is available in the tracking state. So, as soon as you start the session with the initial World map, the tracking state will be limited with reason Relocalizing. Note that you will still get the tracking data available here, but the world origin will be the first camera, just like a new session. As soon as the user points the device to the same scene, the system will localize. The tracking state will go to normal and the world origin will be the same as the recorded World map. At this point, all your previous anchors are also available in your session, so you can put back the virtual content. Note here that because of what's happening behind the hood, behind the scenes, is that we're matching those feature points, there needs to be enough visual similarity between the scenes where you acquired the World map and the scene where you want to relocalize. So, if you go back to this table at night, chances are it's not going to work very well. And that was how you can create multiple user experiences or persistent experiences using the saving and loading map. Next, image tracking. So, augmented reality is all about adding visual content on top of the physical world. And on the physical world, images are found everywhere. Think about [inaudible] the world, magazine covers, advertisements. Image tracking is a tool that allows you to recognize those physical images and build augmented reality experiences around them. Let's see an example. You can see here; two images being tracked simultaneously. On the left, a beautiful elephant is put on top of the physical image of the elephant. On the right, the physical image turned into a virtual screen. Note also, that the images can freely move around the environment as tracking around at 60 frames per second. Let's talk about looking at what's happening behind the scenes. So, let's say you have an image like this one of the elephant and you want to find it in a scene like this. We're using grayscale for this. And the first type is pretty similar to what we do in tracking. So, we'll track those interesting points from both the reference image and the current scene. And then, we try to go in the current scene and match those features to the one on the reference image. By applying some projected geometry and linear algebra, this is enough to give an initial estimation of the position orientation of the image with respect to the current scene. But we don't stop here. In order to give you a really precise pose and track at 60 frames per second, we then do a dense tracking stage. So, with that initial estimate we take the pixels from the current scene and warp them back to a rectangular shape like you see on the right-- top right there. So, that's a reconstructed image by warping the pixels of the current image into the rectangle. We can then compare the reconstructed image with a reference image that we have available to create an error image like the one you see below. We then optimize the position orientation of the image, such that that error is minimized. So, what this means to you that the post would be really accurate. Thank you. And will still track at 60 frames per second. So, let's see how we can do all of this in ARKit. As usual, the ARKit API is really simple. We have three simple steps. First, we want to collect all the reference images. Then, we set up the AR Session Configuration. There are two options here. One is the World Tracking configuration that gives, also, the device position. And this is the one we have talked, so far. And in iOS12, introduced a new configuration, which is a standalone image tracking configuration. Once you start the session you will start receiving the results in the form of an ARImageAnchor. We're now going into more details of these three steps, starting from the reference images. The easiest way to add reference images to your application is through the, called asset catalog. You simply create an AR Resource Groups and drag and drop your images in there. Next, you have to set the physical dimension of the image, which you can do on the property window on the top right. Setting the physical dimension is a requirement and there's a few reason for that. First, it allows the pose of the image to be in physical scale. Which means, also, your content will be in physical scale. In ARKit, everything is in meters, so also, your visual content will be in meters. In addition, it's especially important to set the correct physical dimension of the image in case we combine the image tracking with the World Tracking. As this will give immediately consistent pose between the image and the world. Let's see some example of this reference images. You can see here, two beautiful images. These images will work really great with image tracking. They have high texture, high level of contrast, well distributed histograms, as well as they do not contain repetitive structures. There are, also, other kinds of images that will work less good with the system. You can see an example of this on the right. And if we take a look at these top two examples, you can see that the good image we have a lot of those interesting points. And you can see that the histogram is well distributed across the whole range. While, [inaudible] image, there's only a few of those interesting points and the histogram is all skewed toward the whites. You can get an estimation of how good an image will be directly in Xcode. As soon as you drag an image in there, the image is analyzed and [inaudible] to you in the form of warnings to give you early feedback, even before you run your application. For example, if you click on this bottom image that could be a magazine page, for example, we can see that the Xcode says that the histogram is not well distributed. In fact, you can see there's a lot of whites in the image. And it would also say that this image contains repetitive structures, mainly caused by the text. Another example, if you have two images which are too similar and are at risk of being confused at detection time, also, Xcode warns you about that. You can see an example of these two images of the same mountain range, the Sierra. There's a few things that we can do to deal with this warning. For example, let's go back to this image that had repetitive structures and not well distributed histograms. You can try to identify a region of this image which is distinctive enough, like in this case, for example, the actual image of the page. And then, you can crop that out and use this as the reference image, instead. Which will give you, of course, all the warnings are going to be removed and will give you better tracking quality. Another thing that we can do is use multiple AR Resource Groups. This allow many more images to be detected. As with the commands to have a maximum of 25 images per group to keep your experience efficient and responsive. But you can have as many groups as you want. And then, you can switch between groups programmatically. For example, if you are want to create an augmented reality experience in a museum that may have hundreds of images. Usually though, those images are actually physically located in different rooms. So, what you can do is put the images that physically will be present in the room into a group. And images of another room into another group. And then use, for example, core location to switch between rooms. Note also, that you can have similar images, now, as long as they are in different groups. So, that was all about reference images. Let's now, see our two configurations. The ARImageTrackingConfiguration is a new standalone image tracking configuration, which means it doesn't run the World Tracking. Which also, means there is no world origin. So, every image will be given to you with respect to the current camera view. You can also combine image tracking with a World Tracking configuration. And in this case, you will have all the scene understanding capability available like Plane Detection, light estimation, everything else. So, what is more appropriate to use which configurations? Let's see. So, in the ARImageTrackingConfigurations is really tailored for use cases which are built around images. We can see an example on the left here. We can have an image that could be a page of a textbook. And to make the experience more engaging, we are overlaying [inaudible] graph. In this case, on how to build an equilateral triangle. So, you can see that this experience is really tailored around an image. If you have, let's see this other example. Image tracking is used to trigger some content that then goes beyond the extent of the image. In this case, you want to use the ARWorldTrackingConfiguration as you will need the device position to keep track of that content outside the image. Also, note that the image tracking doesn't use the motion data, which means it can also be used on a bus or an elevator, where the motion data don't agree with the visual data. So, let's see now, how we can do this in code. You can easily recognize those three steps here. The first one is to gather all the images. And there's a convenience function for that in the ARReferenceImage class that gathers all the images that are in a particular group. In this case, it's named Room1. We can then simply set the trackingImages property to those images in the ARImageTrackingConfigurations. And run the session. You will then start receiving the results, for example, to the session, didUpdate anchors delegate method, where you can check if the anchors is of type ARImageAnchor. In the anchor, you will find, of course, the position and orientation of the image, as well as the reference image itself. Where you can find, for example, the name of the image as you named it in the actual title so that you know which image has been detected. There's also another Boolean property, which tells you if this image is currently being tracked in the frame. Note here that other than these use cases that we have seen so far when you build [inaudible] around images, image detection and tracking allows a few more things. For example, if two devices are looking at the same physical image, you can detect this image from both devices. And this will give you a shared coordinate system that you can then use as an alternative way to have a shared experience. Another example, if you happen to know where an image is physically located in the world, like for example, you know that the map of this park is in the physical world. You can use image tracking to get the position of the device with respect to the image and, therefore, also the position of the device with respect to the world, which, you can then use, for example, to overlay directions really attached to the physical world. So, that concludes the image tracking. Let's now go and look at the Object Detection. So, with image tracking we have seen how we can detect images, which are planar objects in the physical world. Object detection extends this concept to the third dimension allowing the detection of more generic objects. Note, though, that this object will be assumed to be static in the scene, unlike images that can move around. We can see an example here. That's the Nefertiti bust. It's a statue that could be present in a museum. And now, you can detect it with ARKit. And then, for example, display some information on top of the physical object. Note also that in the object detection in ARKit, we are talking about specific instances of an object. So, we're not talking about detecting statues in general, but this particular instance of the Nefertiti statue. So, how do we represent these objects in ARKit? You first need to scan the object. So, really, there's two steps to it. First, you scan the object and then you can detect it. Let's talk about the scanning part, which mostly is going to be on your side as a developer, to basically, create that representation of the object that can be used for detection. Internally, an object is represented in a similar way as the world map. You can see an example of the 3D feature points of the Nefertiti statue there on the left. And to scan the object, you can use the Scanning and Detecting 3D Objects developer sample that's available on the website. And note here, that the detection quality that you will get at runtime, later, is highly affected by the quality of the scan. So, let's spend a few moments to see how we can get the best quality during the scanning. Once you build and run this developer sample you will see something like this on your device. The first step is to find the region of space around your object. The application will try to automatically estimate this bounding box, exploring different feature points. But you can always adjust this box by dragging on a side to shrink it or make it larger. Note here, that what is really important that when you go around the object you make sure that you don't cut any of the interesting points of the object. You can also, rotate the box with a two-finger gesture from top. So, make sure that this box is around the object and not cutting any interesting part of it. The next part is the actual scanning. In this phase what we want to do is really go around the objects from all the points of view that you think your users will want to detect it later. In order to make it easy for you to understand which part of the objects have been, already, acquired like this beautiful tile representation. And you also can see a percentage on top which tells you how many tiles have already been acquired. And it's really important in this phase that you spend time on the regions of the object which have a lot of features that are distinctive enough. And you go close enough to capture all the details. And again, that you really go around from all the sides. Like you see here. Once you're happy with the coverage of your objects, you can go to the next step, which is allows you to adjust the origin by simply dragging on the coloring system. And this will be the coloring system that will be later given to you at detection time in the anchor. So, make sure that you put it in a place which makes sense for your virtual content. So, at this point, you have a full representation of your object, which you can use for detection. And the application will now switch to a detection mode. We encourage you to use this mode to get early feedback about the detection quality. So, you may want to go around the object from different points of view and verify that the object is detected from all these different point of view. You can point your device away, come back from another angle, and make sure that the scan was good to detect the object. You can also, move these objects around so that the light condition will be different. And you want to make sure that those are still detected. This is particularly important for objects like toys that you don't know where they're actually going to be physically located. We, also, suggest that you take the object and put it in a completely different environment and still make sure that it is detected. In case this is not detected you may want to go back to the scanning and make sure that your environment is well lit. We really like, well lit environment during the scanning is very important. If your Verilux meter, it will be about 500 lux will be best. And if that is still not enough, you may want to keep different versions of the scans. So, at this point, once you're happy with the detection quality you can simply drop the model to your Mac and add it to the AR Resource Groups, just like you did for the images. Also note that there are some objects that will work really great with this system. Object like you can see on the left. First of all, they are rigid objects and they are, also, rich of texture, distinctive enough. But there are also certain kinds of object that will not work well with the system. You can see an example of this on the right. And for example, metallic, transparent, or metallic or reflective objects will not work. Or transparent objects like glass material object will also not work because the appearance of these objects will really depend on where they are in the scene. So, that was how to scan the objects. Again, make sure that you have well-lit environment. Let's now see how we can detect this in ARKit. If this looks familiar to you, it's because the API is pretty similar to the one of the images. We'll have convenience metered to gather all the objects in a group. This time is in the ARReferenceObjects class. And to configure your ARWorldTracking configuration, you simply pass this object to the detectionObjects property. Once you run the session, again, you will find your results. And in this case, you want to check for the ARObjectAnchor, which will give you the position and orientation of the object with respect to the world. And also, the name of the object as was given in the asset catalog. So, you guys may have noticed some similarities between the object detection and the world mapping relocalization. But there's also few differences. So, in the case of the object detection we are always giving the object position with respect to the world. While in the world map relocalization is the camera itself that adjusts to the previous world map. In addition, you can detect multiple objects. And object detection works best for objects which are tabletop, furniture sized. While, the world map is really the whole scene that's been acquired. With this side, we conclude the object detection. Let's summarize what you have seen, today. Orientation tracking tracks only the rotation of the device and can be used to explore statical environments. World Tracking is the fully featured position and orientation tracking, which will give you the device position with respect to a world origin. And enables all the scene understanding capabilities like the Plane Detection, which will make you able to interact with the physical, horizontal, and vertical planes where you can then put virtual objects. We have seen how you can create persistent or multiuser experiences with the saving and loading map features in the architecture. And how you can detect physical images and track them at 60 frames per second with the image tracking and how you can detect more generic objects with the object detections. And with this, I really hope you guys have a better understanding, now, of all the different tracking technology that are present in ARKit and how they work behind the scenes. And how you can get the best quality out of it. And we're really looking forward to see what you guys are going to do with that. More information can be found at the session link in the developer website. And we have an ARKit Lab tomorrow, 9 a.m. We will both, me and Marion will be there answering any question on ARKit you may have. And with that, thank you, very much and enjoy the bash.  Good afternoon everyone. My name is John Hess. Today I'm going to be joined by Matthew Lucas, and we are going to be talking to all of you about practical approaches to great app performance. Now, I'm an engineer on the Xcode team, and I've had the luxury of spending the last several years focused on performance work. First, with Project Find, and Open Quickly, two areas of Xcode that treat performance as the primary feature. Most recently, I've had the opportunity to do a survey of Xcode GY responsiveness, and I want to share with you the approaches that I take to performance work, both in code that I'm intimately familiar with, and in code that I'm just experiencing for the first time. Now, if I could get everyone in today's presentation to just take one lesson away, it is that all of your performance work should be based on measurement. Before you start solving a performance problem, you should measure, to establish a baseline so you know where you stand. As you iterate on solving a performance problem, you should measure it each step of the way to ensure that your performance changes are having the impact that you expect. When you're done solving a performance problem, you should measure again, so that you can compare to your original baseline, and make a quantified claim about just how much you've improved the performance of your application. You want to share this with your boss, your colleagues, and your users. Now, when you think about improving performance for your users, you need to think about what I like to call the total performance impact. If you improve the functionality and performance of one area of your application, by 50%, but it's something that just 1% of your users encounter, that does not have nearly the breadth of impact as improving some other feature by just 10% that all of your users use all the time. So make sure you're not optimizing edge cases, and make sure that your changes are impacting all of your users. Now how do we fix performance bugs? Well, how do we fix regular bugs? Normally it starts with some sort of defect report from users, and we take this report of the application not behaving the way that people expect, and we find some way to synthesize steps to reproduce so that we can cause the failure at will. Once we've done this, we attach a debugger to our program, so that we can see just what our program is doing while it is misbehaving. We combine that with our knowledge of how the code is supposed to work, to modify it as necessary and eliminate the undesired behavior. We verify that we haven't introduced any unwanted side effects, and we repeat as necessary until we've completely solved the bug. I've fixed performance bugs in just the same way. Except instead of using a debugger, I use a profiler, and a profiler is just a fancy tool for measuring. I find some set of steps to reproduce the program being slow. And I run those steps with a profiler attached, so that I can get an insight into what my code is doing while it's running slowly. I combine that knowledge with what my program has to do to accomplish the task at hand, and I find steps that are happening and remove them, because the primary way you make your code faster is you remove redundant steps from whatever is that is calculating. Now, I make the modifications to the source code, and I repeat and measure as necessary until I'm happy with the total result. When I'm doing this type of performance work, I often find myself in one of a handful of scenarios. And these different scenarios change the way that I go about testing the code in question to reproduce the bugs. Sometimes I'm up against a big performance regression, right? Everything was moving along smoothly, then someone checked something in on our team, maybe it was me, and performance has fallen through the floor, and now we have to go back and find out what caused this regression. If this regression is very pronounced, or it's in an area that I don't think it's likely to regress again in the immediate future, I may just test it with my hands, manually, with the profiler attached. However, your performance victories are going to be hard-won battles, and they can easily be lost through a slow stream of regressions. I would encourage all of you to write automated performance tests to capture your app's performance, so that you can ensure that it's not regressing over time. Another scenario I often find myself in, is, are applications performing the same as it has been for a long time? Maybe it is running at 45 frames a second in some drawing test, but we expect it to run at 60. It needs to be improved marginally, and we have reason to believe through our previous performance work that we can get there through spot fixes and incremental changes. Now, in this type of scenario, I probably also have automated tests already in play, because I understand my performance over time. And a third scenario, our application is just suffering from a poor design and performance is orders of magnitude worse than it should be. We know that we can't improve it with simple spot fixes, because we've tried them in the past, and we are still stuck here with a very sub-par performance. In a situation like this, you'd want to do a total performance overhaul, where you are redesigning some core part of the feature, or the algorithms in question, so that performance is a primary constraint. And definitely in these cases, you would have performance tests to measure that you're actually hitting your performance targets. Now, it is important that you know just what to test. I want to caution you that I don't ever immediately jump to these sort of performance overhauls as a way of fixing a performance problem. I love to do that. It's sort of Greenfield engineering, where you get to design things from the ground up, but it's very risky. You're going to end up with a better product at the end, but it's going to be a turbulent path getting there as you rework an entire feature. When you're doing this style of work, it is imperative you understand not only the functional constraints of the code in question, but also the performance constraints, and the typical use patterns that your users are most frequently applying to this feature, and you only get that by having done performance work in the area in the past. I'd like to share an anecdote about our work on a situation like this, within Xcode. In Xcode 9, we reworked Project Find, with performance as a primary goal. It was our goal to deliver search results in just tens of milliseconds. When we were going to discuss this feature with our colleagues, we were often challenged to perform searches across large projects for things like string, or even the letter E. Things that produce millions of results, right? And certainly if our application could produce millions of results quickly, it would be fast on anything. But if you consider what typical patterns are, we search for APIs we use, the names of our own classes, the names of, you know, images that we're referencing. Things like that. They produce dozens, maybe hundreds of results. Certainly, it is essential that the application works decently when you get a million results, but the normal use case is hundreds of results. Now, some of your work in doing a task like search is going to be proportional on things like generating the raw results, and other work is going to be based on how efficiently you can index the text in the project, and avoid work in the first place. In these two scenarios, you're likely to have completely different targets for what you would optimize to make one of these searches faster than the other, right? So it's essential that you understand how your users are going to use the product, so that you can optimize for the right cases. Now, in all of these cases, I need to do some form of testing, whether it's manual, or automated. I want to share with you two types of performance tests that I will typically write to measure the performance of Xcode. We will either do unit tests, or integration tests. Let's compare and contrast them. In a performance unit test, it's your goal to isolate some feature of your application and measure it all by itself. You might mock out its dependencies, and you might launch it in a context where it has been isolated. If I were to write performance unit tests for Xcode's code completion, I might write a series of three small tests. One of these tests would measure talking to the compiler and getting the raw results, the raw set of code completion candidates back. Another performance test would measure correlating, ranking and scoring those results, so we knew which ones to display to the user. A third test might take those already prepared results, and measure putting them into UI elements for final display. And in covering all three of these areas, I would have pretty good coverage over the major components of code completion in the IDE. Now, there are some great aspects to these performance unit tests. They're going to be highly focused, which means if they regress in the future, I'm going to have a very good idea on where the regression is, because the code that is running has been scoped so well. They are also going to produce much more repeatable results from run to run. They're not going to have a big variance in the times that they produce. Again, because the code is so focused. Now, let's contrast that to an integration test. In an integration test, your job is to measure the performance of your application as your users experience it. Holistically. So, if I was writing code completion unit tests for Xcode, I'm sorry, integration tests, I would launch the full Xcode app. I would open a source file. I would navigate to the source file, and I would type, and I would bring up code completion over and over again. When I profile this, to see what Xcode is doing, and how much time it is taking, I am going to find that this test is anything but focused and quiet. Xcode is going to be doing drawing and layout as I type. It is going to be doing syntax coloring as I type. In the background, it might be indexing, fetching get status, deciding to show new files in the Assistant Editor, and all of these things are going to be competing for CPU resources, along with code completion. Maybe when I look in the Profiler, I'll see that we spend 80% of our time syntax coloring, and 20% of our time in code completion. And with this data, I would know that the best way to improve code completion performance would be to defer syntax coloring. I will never gain that type of knowledge with a highly focused unit test. So if I can get everyone here to take two things away from this presentation, the second one should be that your performance investigations should absolutely start with these wide integration tests that measure how the users experience your application. So I'm talking about testing, measuring and profiling. And right now, I'd like to introduce you to profiling in Xcode with instruments. Let's head over to the demo machine. Today we are going to be looking at a performance problem that we fixed between Xcode 9 and Xcode 10. I want to show it to you. I'm going to launch Xcode 9, and open our solar system application. Now the problem that we are going to be looking at is creating tabs. I'm going to just press Command-T quickly a couple of times, and as you can see, the whole screen flashes black, and it takes several seconds to create those tabs. That definitely doesn't meet my expectations as far as performance goes, and we need to fix this. So let's take a look at how you would do that. First, I'm going to launch Instruments. That is our profiling tool. You can do that from the Xcode menu, under Open Developer Tool, Instruments. Now, I'm currently in Xcode 9, so if I choose this, it's going to launch the Instruments from Xcode 9, and of course, I want the Instruments from Xcode 10, which I've put here in my doc. So I'm going to hide Xcode, and bring up Instruments. Now, when Instruments launches, we're presented with a list of profiling tools that we could use to measure our application. There's all kinds of tools here. They can measure graphics utilization, memory consumption, IO, and time in general. It can be intimidating to know which one of these profilers to start with. I would encourage all of you, if you just learn one of these tools, it should be the Time Profiler. I use it for 95% or more of my performance work. When your users complain about your app being slow, they're complaining about it taking too long, and long is time. If it turns out that you're slow because you're doing too much IO, that is going to correlate with time, and you will be able to see this with the Time Profiler. So if you learn just one instrument, it should be the Time Profiler. Let's take a look at how that works. I'm going to launch the Time Profiler by just double clicking on it here, and make Instruments take the full best op. Now, we'd like to record Xcode. In the upper left-hand corner of the Instruments window, you can control which process you're going to attach to and record. By default, hitting this record button would record all processes on my Mac. I just want to focus on Xcode. I'll switch this popover to Xcode and hit record. Now, I like to keep an eye on this area of the window to track view while I'm recording. So I'm going to resize the Xcode window to be a little shorter, so I can still see that, and then I'm going to do the thing that was slow. I'm going to create a couple more tabs. And you can see the graph changed here. Now, I'm going to go ahead and quit, and return to Instruments. So what just happened? While the Profiler was running, it was attached to our process like a debugger. And it stopped it, thousands of times per second, and as it was stopping it, it gathered back traces. Now, just a reminder, a back trace is a description of how your program got to where it currently is. So if you're on line 6 of function C and you got there because main called A, called B, called C, then your back trace is Main, A, B, C. When Instruments captures one of these back traces, it notes, hey, we just spent one millisecond in function C. It says one millisecond, because that is our sampling interval for recording once every millisecond. Now, on the main thread, all these back traces are going to start with the Main function, and they're probably going to call Application Main, and they're going to branch out, all through your source code after that. We can collapse these back traces together, and overlay them into a prefix tree, so they start at Main and work their way out. And we can bubble up those millisecond counters that we captured at the top, so that we can hierarchically see how much time was spent in all the different areas of our source code. And we are going to look at this data to try and find redundant and unnecessary operations that we can make faster, and that is our primary method that we are going to use to improve the performance of our application. Now, as you can imagine, we're capturing thousands of back traces per second. There is an overwhelming amount of data for you to wade through in instruments. My primary advice to you is that you want to filter this data as much as possible so that you can see the course grain performance leads, and not focus on minutia. All right? So I want to show you how to apply a bunch of powerful filters and instruments. So as I did the recording, you remember, I had the track view visible. I did that because I wanted to see how the CPU utilization changed and where it was changing, while I was creating new tabs, and I noted to myself that it was right here. I simply dragged and selected over that area of the trace, and I've caused instruments to only focus its back trace data on just that time interval. Everything over here, this is before I was creating tabs. Everything over here, this is after I was creating tabs, when I was quitting the application. That's not what I'm trying to optimize right now, so I don't need to see that data. Now, in the bottom area of the Instruments window, Instruments is showing me all the traces it collected. By default, there is one row per thread that was running. And in this example it looks like there was only four threads running. Sometimes you'll have much more. Depends on how concurrent your application is. I often like to collapse these in the name of focusing, and I also like to collapse them so they're based on the top level functions executing in each of the threads, rather than the thread IDs, because that corresponds better with how I use Grand Central Dispatch. Down in the bottom of the Instruments window, I'm going to click on this button that says Call Tree, and I'm going to zoom in on it, so you can see what I'm about to do. There are several filters available here. One of them is separate by thread. It is on by default. I am going to go ahead and disable that, and instead, all of the threads are going to be grouped by their top level entry point, rather than their thread ID. Now, looking at this trace, I can see that of all these threads running, which by the way, below the main trace, which is the aggregate CPU usage, the CPU usage is broken down per thread, I can see that almost all the other threads were largely inactive during this trace. I can focus on just the main thread by selecting it here, and now I'm only looking at traces from the main thread during this time period. I'm ready to start digging into this call hierarchy, so I can see what my application was doing. Often, I'll walk this with the keyboard, by just pressing right arrow and down, over and over again. But I'd like to show you the heaviest back trace inspector that Instruments offers. If your Inspector is not visible, you can toggle it with this button, and the heaviest back trace will be available here, in this tab, Extended Detail. Now, the heaviest back trace is just the trace that occurred most frequently. It's the back trace that happened most frequently while we were recording under the current selection. And you can use this to quickly navigate many frames deep at a time. I typically look through here, looking for my own APIs, and things that would surprise me for taking up this amount of time, or for areas where we make a significant branching point in the number of samples. Now, looking through here, I see this call, which is to IDE Navigator, replacement view, did install view controller. Now, I'm familiar with this API, because it's an internal API of Xcode. And in the trace, I can see over here on the left-hand side of the window that it is responsible for 1.19 seconds of the total time we're recording, or 45% of the time. That is far and away above my expectations for how much this method should cost. However, it's hard to focus on what is happening here. Right? I'm, there is all this other stuff at the bottom of the trace, and it looks like I'm, you know, 30 or 40 stack ranges deep. That can be intimidating. I want to show you how to focus. The first technique is back here in that call tree popover again. I'm going to use this popover to choose the flattened recursion. Let's go ahead and do that. And now you can see that, that repeated set of method calls that was right here, oops, has been collapsed. I'm sorry, let me scroll down. That has been collapsed. In fact, I'm confident that I want to continue my performance investigation inside of this IDE Navigator area, API call, and I can refocus the entire call tree by context, clicking here, and choosing Focus on Subtree. And Instruments is going to take that symbol up to the top of the call graph, it's going to remove everything else, and it is going to reset the percentages at 100% so I can focus on just this. Now, I can continue to walk this sample with the arrow keys to see what we're doing. And I'm familiar with these APIs. And it looks like we're doing state restoration. And as I continue to expand this, I can see that we are sort of deep inside the table view, and in addition to there being this sort of hot call path, you know, that is taking large number of the total percentage, there's all these other incidental samples as well. It's easy to get distracted by these. One of them here is OPC Message Send. This can occur all over your tracers if you're writing objective C. Even if you're writing Swift code, as you work your way into the system libraries, you'll see this. You'll often see its counterpart functions, OPC, Load Strong, Load Weak, etc., Retain, you can remove all that content from the call tree by context clicking on it, and choosing Charge OPC to Callers. That's going to tell Instruments to take all the samples that came from lib OPC and remove them from the call data, but keep the time as attributed to the parent frames that called them. I tend to treat those objective C runtime functions as just the cost of doing business when writing objective C code. It's rarely the case that I'm going to attempt to optimize them out, so I just prefer to remove them from the data, so I can focus on the things that I'm likely to take action on. Another very powerful filter that you can apply, and one that I'm going to use to remove all these small samples that occurred during this set of frames, is here in the call tree constraint section. Let me show you. I'm going to tell Instruments that I would only like to see areas of the trace that accounted for let's say 20 or more samples. I'm picking 20 because I know that I've selected about a two second interval and 20 milliseconds is going to represent about 1% of the total work, and that is about the granularity that I like to work at by default. So with call tree constraints set to a minimum of 20, I now focus this down much more significantly. Now, I mentioned here that we were expanding out my view items. I see that in the fact that we're calling NS outline view, expand item, expand children. Now, a lot of people would stop with the call graph at this point. They'd see I'm calling into a system framework, and I'm spending a lot of time there. This isn't my fault, right? What can I do about this? I can't optimize NS Outline View, Expand Items. You absolutely have the power to influence these situations. For example, the system framework could be spending all of this time because it's operating on data that you provided it. It could be taking a lot of time because you are calling this method thousands or millions of times. It could be taking a lot of time because it's calling back into your code through delegation. And most importantly, you can get an insight into what the system framework is doing by expanding down through the Instruments tree, and looking at the names of functions that are being called. In fact, that's exactly how I learned to fix this bug. As I expand the trace into the outline view, I can see that it is calling these two methods here. Batch Expand Items with item entries, expand children, and do work after end updates. Now, those are big clues to me that there is probably some opportunity for efficiency through batching. As you could imagine, the outline view starts with a small set of items, and then we are trying to restore expansion state in this area of our code, and so we are telling it to open, for example, the top item. And when we tell it to open the top item, internally you might imagine that it moves all the other items down. Then you ask me to expand the second item. It moves all the items down again. And the third item, and so on. And by the time you're done, you've moved those bottom items down thousands of times. That is all redundant work, and that is exactly the sort of thing I'm looking to eliminate when I'm trying to improve performance. Now the fact of these method calls talk about batching leads me to believe that there is probably some API where I can ask the outline view to do the work in bulk so it computes all the positions just once, instead of over and over again as I make the calls. I also see a call that says to do the work after end updates. Now, sometimes an API will offer sort of bulk method that operates on an array, and other times, it will offer a sort of transactional API that says I'm going to begin making changes, then you make a bunch of changes, and then you say you're done, and it computes something that happened for the whole range of your changes, more efficiently than if it had done them all individually. So at this point, I would head over to the NS Outline View, or NS Table View API, and I would look for some such method. And there is exactly one there. In NS Table View, there is methods for beginning and end updating, that allow the table view to coalesce, and make all this work significantly more efficient. Of course, we adopted that in Xcode 10. Let me show you. I'm going to launch Xcode 10. I'm going to open the source as an application, and I'm going to create a couple of tabs. And you can see, there is no awful flashing, and the tabs open much more quickly. Now, I'd like the tabs to open even quicker than that, right? So what am I going to do next? I got lucky here. It's not every day that you're going to go into the trace, and find something so obvious and easy to fix, that is responsible for 50% of the sample. Right? In fact, there is not going to be any other huge lead sitting there waiting for me. Instead, what I'm going to need to do is go through that whole sample, with those course filters applied, so I'm only looking at operations that take about 1% of the time or more, and I'm going to look for every single thing that I see that I think I can come up with some mechanism for making a little bit faster. I'm going to note them all down on a piece of paper or in a text document or something, and then I'm going to start solving them. Now, I need to pick an order to solve them in, right? Because sometimes the fifth thing on the list, fixing it with an obsolete, whatever fix you would do for the second thing on the list, and it feels bad to do them in the wrong order, such that you did redundant work, because that's the whole thing we're trying to remove in the first place, is redundant work. But it's very hard to predict how these things are all going to play out. And you often can't know until you've already done the work. So do not let this stop you from getting started, because you're going to get your second 30% improvement by stacking 10 3% improvements. Okay? Now, I want to go back to the slides, and show you some of the techniques we typically use to make those continued improvements. Far and away, the thing that comes up the most frequently is using those same techniques the outline view was using. Batching and deferring, right? You have an API, and when the API is called, it has some side effect. And then you have some code calling your API in the loop. That's what you're doing-- the primary piece of work that is being requested, and having a side effect. Well, if no one was reading the result of the side effect, then you're doing that work redundantly, over and over again. You can often get a much more efficient interface by using a batch interface, where a client gives you an array or some sort of collection of all the work to be done, so that you can compute that side effect just once. Now, sometimes you have many clients, right? And they can't batch across each other, and you can get even-- you can still get that same style of performance through deferring the work and doing it lazily. A third easy way to improve performance is you look through that instrument's trace, is to find areas where you see the same thing being computed over and over again. For example, you have a method in its computing, the size of some text, then you see the same thing happening several frames later, for the same text, and again, and again. Now, in this situation, of course, you want to try to just compute that value one time. Compute it at the top, and pass it down or maybe cache it. Another technique you have available in your UI applications is considering how many views you are using to render your UI. It can be very great for your source code organization to use very small views, with small sets of functionality, and to compose them together into larger pieces. But the more views you use, the harder you tax the rendering and layout systems. Now, this is a two-way street, because smaller views often led you to have more fine-grain caching, which can be good for performance as well. But generally, you can tweak the number of views that you have in order to have a significant impact on performance. It is not always best to have fewer views, otherwise all of our applications would just have one giant view for the whole thing. Another technique that comes up pretty frequently is using direct observation. We often have two areas of our source code that are loosely coupled. Maybe one area knows about the other, and they're communicating with each other through some indirect mechanism. Maybe they're using NS Notification Center, some block-based call backs, delegation, or key value observing. Now something that I see very frequently is we'll have some model code, and it's going in a loop, being changed, and every time it is going to that loop, it is firing lots of KVO notifications. You can't actually see that in the model code, of course, but over in some other controller, it's madly responding and trying to keep up with whatever is changing in the model, and you're burning lots of CPU time doing this, that ends up being redundant when you consider the whole scope of changes. Now, if this was direct callouts from the model code, either through notifications, delegation or manual block-based call backs, it would be much more obvious that this was happening as you edited that model code. And you might decide that it is totally appropriate to pull some of those notifications out from inside the loop to outside the loop, to have a big impact on performance. Now, alternatively, on the controller side, you could use one of these deferring and batching techniques to avoid the redundant work and just not respond synchronously. Last, this is an easy one. Once your code is already on the [inaudible] happy path, you know, it's already linear, and it's not going to get any better than linear. That's sort of the minimum performance that you're going to get. You're after all the constant time improvements that you can. Now, an easy one is that if you're using dictionaries like they were objects, then you probably know you're doing this, if you have a bunch of string constants for all the keys, then you can get a big improvement to code clarity, to code completion, to re-factoring, to making the validating your source code, by using specific types. It couldn't be easier with strucks and swift with their implicit initializers and conformance to equitable hash. And this can just be hands-down an improvement to your source code, and you'd be surprised at how much time you're spending in string hashing and string equation if you were doing this millions of times on lots of small objects. So with that, I'd like to turn it over to Matthew to talk to you about how we've applied these techniques inside of photos. Thanks Jim. Hi everyone. I'm Matthew Lucas, an engineer in the photos team, and today I want to give you some practical examples on performance from directly from photos. So first, let's talk about photos for a second. We are all familiar with this app. It lets you store, browse, and experience your favorite moments. So you can browse your favorite moments from the moments view, that you can see here. It's is the default view. But you can also get another view from the collection, or the years. And I'll talk more about this view later. Now, libraries today can go from 1,000 to 100,000 assets previous depending on your love for photography. And we all love capturing those fun and precious moments we live every day. So we are patient enough to capture them, but we are less patient when something like this appears. How would you feel if something moments like this would be displayed in Photos the first time you launch the app? Now, you may also experience something like this, where we are showing a lot of placeholders, and that's really not great. Maybe you're soft scrolling, you'll be lost in this gray area, the [inaudible] would start to load, but then you'll keep scrolling and then you'll experience some frame drops because the views are being updated. Well, our goal is to not show views like this. We think this is not providing a great user experience, but we understand that sometimes it's unavoidable. But when it's too frequent, this isn't really great. Now, when you work on an app, you want to make sure that it's responsive, and usable at once. You also want to make sure that the animations are smooth. And these two attributes are really crucial to providing a great user experience. If the users don't find your app relatable or pertinent, they might stop using it. Now, to illustrate these two points, I would like to give you two examples. And the first one is going to be how we optimize launching to this moment view. The second one is how we build the collections and years view for good scrolling preference. First, let's do launching [inaudible]. So what is launch? There are three kinds of launches. The first and more expensive one is the find referred as called, and it depends the first time you are going to relaunch your app after it reboots. So basically, nothing has been cached yet, and it might require some bug run processes or some libraries to load. Now, it also happens when the system goes under memory pressure and starts reclaiming some memory. Now, if you kill an app, it might not trigger a code launch, because the system decides when the resources should be paged out. And when you kill an app, and you relaunch it a few second later, it's almost guaranteed that you'll hit a warm launch. And we call it warm, because the resources or the dependents are still in the cache, so it's faster to launch. Now, the last type is-- we call it hot, and it's basically a resume, because it's when your app is already running and is being brought back to the foreground. So when you start measuring launch, you should start by measuring the warm launch. And the time it takes to launch during this warm is less variable than the cold launch, and the test iteration is much faster as you don't need to reboot your device. Now, the way we measure launch is by evaluating the time it takes from the moment you hit the application icon, and until you can start interacting with the app. And what I mean by interacting is that it's really using and not interacting with a spinner. A common pattern is to dispatch some work and display a spinner in the meantime, well that doesn't make the app usable sooner, so we are trying to avoid that here. Now there are three goals that we are shooting for at Photos, and the first one is that we want to instant, we don't want to display any spinner, and we don't want to display any placeholder or [inaudible]. And I Have to be honest with you, we-- you might see some placeholders the first time you synchronize with iClub, but when the data is local, we really try our best to not display any. Now, what do we mean by instant? Well, the time it takes to launch should be the same time as the zoom animation from the home screen. That is usually between 500 and 600 milliseconds, and that way, the transition from the home screen to the application is seamless for the user, and the user can start interacting with it, as soon as the animation is done. And by the way, this is the lowest recommendation, not something just for photos, so it's valid for any apps. Now, let's look at how photos launches today. If we look more closely at what is happening exactly, you can see that photos is all set up and ready before the animation is done. And if we dive into the launch anatomy, you will see there is mainly two parts. The first part is being spent in DYD, this is the loader that is going to load and link all of your dependent libraries, but it's also going to run your static initializers. And your control over that part is limited, but it's not impossible. I would encourage you to watch the DYD session from last year in order to get more details on that part. Now DYD is also calling Main in your object table, which leads us to the second part here, where you have lots of control over, and this part, you need to make sure that it stays under 500 milliseconds. Now, the first [inaudible] pass that is being scheduled right after the Did Finish launching will mark the end of your launch, and this is basically when your app should be usable. There are a few principles that we will be referring to during this session, and these are really the common pillars of the performance work that we achieved. The first one is that we want to be lazy and defer the work that we don't need. The second one is that we want to be proactive, and it's valid for two things. It's valid for being proactive in order to anticipate the work that we are making it later, we also want to be proactive and catch regressions quickly, so you should make sure that you have continuous integration testing in place. And the last point is we want to be constant, regardless of the total amount of data that we need to load. Now, if we were taking a nave approach, and we were loading everything we needed during launch, this is how long it would take roughly for a 30,000 item library. First you need to initialize the database, then you need to prepare some view controllers. You need to configure the data sources, load some library images, and fetch the cloud status. And keep in mind that this might vary as the data grow, and in fact, the data will grow forever as people takes pictures every day. So at Photos, really keep in mind that we are dealing with a non-bonded data sets. Now, let's see how we optimize each of these steps for Photos, and let's start with initializing the database. So first, usually, the database is initialized and loaded when the first query is being fired. One optimization that we have found was to do it as early as possible in the background thread, so that it doesn't have to do the initialization when the first query has been fired. And this is an issue, especially if the first query is being done from the main thread. Now, we spend a lot of time and we are still spending a lot of time reviewing all the queries that we're doing during launch, and we want to make sure that the work that we are doing is only the necessary one, and we are not doing more. Now, lastly, we want to ensure that all the queries that we are doing are efficient as possible, and we want to avoid the complex query as much as possible as well. And we sometimes we understand that we need this, and for these cases, we are setting up some indexes, so that we can speed them up. Now we are aiming for, at most, 30 milliseconds spent in that initialization. So next, let's look at how we are preparing our view controllers. So we have four tabs representing the main features of the app. And so the first thing that we need to be careful of is we want to minimize the work that is being done in the initialization of these three non-visible ones, and the rule that we are trying to follow here is to do as little work as possible in the initializers. We really want to do the bare minimum, and note all the data in the view that loads. This also allows us to initialize our controllers in constant time. Now, lastly, we also want to ensure that only the visible views are loaded. It's easy, and we often regress on that part, so you should really be careful about that. So preparing the view controllers, we are now aiming for 120 milliseconds. But preparing view controllers implies configuring the data sources, and let's look at that chunk next. So the Moments view is a representation of these things, events in your life, and the UI represents that by having this group of photos, and these headers. In this library, for example, we might have 500 moments, and in order to build a view, we need to load all the moments up front. But the only thing we need really for these moments is only the meta data so we can build the view. We don't need your content. So the first thing we do is we fire that query, which is super fast. And then we are only loading the content that we need here. In that case here, we are only going to load the visible content, which in our case is going to be between 7 to 10 Moments. Since our deficit is limited, and finite, we can allow ourselves to do it synchronously on the main thread. Now, we also want to anticipate and schedule the work so that we can start loading the remaining data as synchronously. And we do that on the bug run thread, with the right quality of service to make sure that it doesn't preempt the main thread from running. Now we are aiming at 100 milliseconds here. So lastly, our data sources are also providing some images and let's see how we optimize that part. So this was by far the biggest chunk here that we are all attacking, and when we realized that we were spending multiple seconds loading this image during launch, we realized that we were doing too much work. So the first thing that we did is that we evaluated the number of images that we needed during launch, and we are only loading that during that first transaction. In that case, that can be up to 60 including some piling above and below. And next, in order to load those images firstly, we need to make sure that we are all loading only low-resolutions one. That way we are loading fewer pixels in memory, and it is much more efficient. That chunk is now representing 200 milliseconds. And this is, by far, the biggest gain that we had. Which I need to be a constant time, and that's really great. Now, sometimes you have to ask yourself the question, is this really needed during launch? And one of our examples here is this footer view. That pulls information via the network or the database, and literally first our design was to not show it during launch. To prioritize all the images that we are seeing here. We wanted to show as much images as possible. So that may be simpler. We are now only scheduling that work post-launch, and we cache to process information for raising later. Now, if we would have had the requirement of displaying this information, one approach could have been to leverage the register background at refresh API from UA kit, that will proactively clear your app so that you can start preparing some content when the user is going to launch your app. So now, that part has gone from launch, and that saves us 400 milliseconds of CPU time. If we look at the updated breakdown here, we can see that we now have only 450 milliseconds worth of work. We are now fitting into that 500 millisecond time window, and regardless of how things can be represented concurrently here, the most important part of that is to really make sure that you think about the cost of preparing your content. And what I mean by think is really measure it. Now, you should strive for doing work in constant time, regardless of the total amount of data you are loading. In our case, really have unbonded data assets, and we need to stay constant. Now that we have launched the app, we need to start using it. And let's see how we did collections and [inaudible] for good [inaudible] performance. So as I mentioned earlier, our users can seamlessly transition with animation from the Moments, through the collections, to the years view. And this is a complex hierarchy. We have thousands of pictures to display. We need to support live updates, we need to also support animation between these layers, and we also have some gestures. Now, we also have some goals here. For the experience we want to provide to our users. The first one is the same as before, we don't want to have any spinner. We don't want to have placeholders, but we also want to have smooth animations. And by smooth animations, I mean 60 or 120 frames per second, depending on the screen you're running on. Now, remember the principles that we've seen before. Well, they are all applicable here. We want to be lazy and defer the work we donate up front. We want to be proactive, and catch regressions quickly, but we also want to be constant in our layout passes, and regardless of a lot of data that we are loading. Now, this time, we also want to be timely, and we want to remember the rendering loop cycle. And what I mean by that is that I want you to remember that we only have 8 or 16 milliseconds to render that frame, so we need to make sure that we are not going over that time, otherwise we would start dropping frames. Now, let's take a step back, and look at what we are trying to achieve here. We wanted to have this portable view, with sections and mini cells in it. And that is basically what your Collection view is providing, right? Except that in this extreme case, we are restricting the limit of what we could achieve with a basic approach. And that resulted in too many views, too many layers. But also in an increased layered complexity, and that also had an increased memory cost. So we needed to innovate here, and we did that by restricting the number of views drastically while still using a collection view. We used a technique more commonly used in video games, that is called atlasing. And it basically consists of combining a set of images into a single one. We do that efficiently by using only very small thumbnails first, then we stamp all the raw image data on the canvas we are using as a strip. Now, we use image raw data so that we can avoid decoding each thumbnail as we send. So basically we are displaying a random strip of images. Now, we generate and cache them on the fly so that we can be more flexible. And as we render multiple images into a single one, we are registering the number of cells, layers, objects drastically, which simplifies the layout and the time spent building it. Now, the separate works well, but it has trade offs to consider as well, and this is one of them. So if someone tries to long press or force search an item here, we will need to figure its position so that we can achieve the preview correctly. And as we display a single image, we need to maintain the mapping of each individual image, and its render strip. Now, you might be thinking, why are we generating them on the fly? Well, we need to support live updates, that's the reason. We need also to support different view sizes. For example, we have landscape here. But we also have portraits. And also we can do that because we can [inaudible] because our user's labor typically grows organically over a long period of time, and the cases where we might need to generate thousands of them are pretty rare. Now, you may be wondering also why are we not generating the whole section then? Well the answer is that our design record is to do this cool animation, where you can see that the collections are expanding into their own sections or collapsing into group ones, and the other way around. So if there is one thing that you should also remember from that second part is you should really think about the layout course of your hierarchy and measure it. Lastly, you should always think about performance. At Photos, we care deeply about it, and this is really part of our daily job. For more information, you can come and see us in these three labs that are mentioned here, and I hope that you have a great conference. Thank you.  Hello, everyone, and welcome to Building Voice with Siri Shortcuts. My name is Amit, and I'm here today with my colleague Ayaka Nonaka. And we are so excited to be here and talk to you about Shortcuts. And you all know, shortcuts allow you to accelerate your users to the things they value the most in your apps. What we are really excited about is that your users can add these shortcuts to Siri and perform them with their voice wherever they are. So, it's paramount that we build a great voice experience for all of our users accessing them through voice. And that's what we'll focus on today. So, let's get started. First, we'll talk about how your users can add shortcuts to Siri and how simple it is to write -- use it right away. Then, we'll talk about how you can provide downloads that can be spoken by Siri, through custom responses. Then Ayaka will go over some of the best practices that you should adopt to create great voice and UI experiences for your shortcuts. And finally, we'll go over how you can bring the experience of setting shortcuts to your apps. So, before we dive into all the exciting details of adopting shortcuts and custom responses, let's take a look at the experience of setting a shortcut with Siri. My teammates and I have been working on this app called Soup Chef. It lets you order soup. Now Soup Chef adopts all our new shortcut APIs, and every time a user places an order, Soup Chef lets the system know by donating a custom intent to Siri. So, yesterday, I ordered some tomato soup. And it was really great. So much so, that I would like to order it again. And wouldn't it be great if I can just do that from Siri? Well, now we can. So, when we go to Siri's Settings UI, you see that Siri's already suggesting that donated intent to be created into a shortcut. So, let's tap on it. And we are presented with this screen, where we can set up a phrase that you can say to Siri and perform your shortcut. There are also a lot of details on this screen from Soup Chef that convey to me as a user what this shortcut would exactly do. How to get these right, we'll talk about that in a little bit. But for now, I would like to associate a phrase with this shortcut. Something that is short. Something that's memorable and I can say it again and again. It's not always easy to come up with that right in the moment, so I figure that Soup Chef is suggesting I call it, Soup Time. I like that. I'll take it. Now, when I record the phrase, I have a shortcut to order soup with Siri. Let's try it out. Now, when I say to Siri, "Soup Time," I'm asked to confirm whether I would like to run the intent, and when I say, "Yes," the order is placed. And it's that easy. Your users will set up shortcuts for things they do most often. And they can use these shortcuts wherever they are, from their iOS devices, from their wrist using the Apple Watch, in their homes using the HomePod, as well as on the go with CarPlay. So, we should pay a lot of attention to the voice experience that they encounter. Towards that goal, let's talk about custom responses. Shortcuts allow you to provide dialogues that can be spoken by Siri, and this gives you the ability to provide relevant information, important information, right with voice. Fundamentally, shortcuts are -- custom responses are template strings whose structure you define in the Intent Definition File. And based on the type of information you provide in your responses, shortcuts can be one of four types. The first is a confirmation custom response. This gives you the opportunity to provide critical information before your users commit to running the shortcut. Next, is a success response. And it gives you a chance to provide an auxiliary relevant information, after the shortcut has run, and the structure of a success response consists of Siri letting the users know that their order was successful, or their shortcut ran successfully, followed by any information that you provide attributed to your app. The third type of custom response is an informational response. And we are really excited about this. Informational response allows you to provide information like transit schedules or ski forecast right within Siri, with voice. This opens the door for a whole new category of apps which was not possible before with Siri. And finally, when things do not work as expected, custom error responses give you the opportunity to explain what went wrong instead of just saying, "Something didn't work." The structure consists of Siri communicating to the users that their intent did not successfully execute, followed by information from your app, and the interaction punctuated by Siri, advising users to continue in your apps. Now, that we understand what custom responses are, let's take a look at how you would adopt them in your apps. Custom responses work hand in hand with custom intents. So, the first step is to define an intent that models the use case you are trying to create a shortcut for. For Soup Chef, we created an intent called Order Soup, and it lets you, you guessed it, order soup. Alongside the intent, you have to pick a category for your intent. And the category you pick influences how Siri speaks about it or what you see across the OS like the order buttons. So, pick something that feels natural, that works well with your use case. Once we have defined our custom intent, let's go over to the Responses Tab. And in the Responses Tab, you see that we already have a success and failure response codes. But these are just a generic response codes which don't convey much more information besides the status of the operation. What I would like to do, is let our users know how long it will be until their order is ready and they can to the store and pick it up. So, what I want to do is define a custom success response for that. Towards that goal, the next step is to declare the response properties that you will use in your templates to provide information at run time. So, if you go over to the Properties Section, I'm going to add a Wait Time and a Soup Property. With that in place, the only thing left to do in our definition of custom responses is to define the template. The template is what is spoken out by Siri. So, when we look at the template section of our Intent Definition File, now I'm going to add my success template. Not just that. Our store is getting a little popular and we tend to run out of soup these days. So, it will be great if instead of just throwing a random error, I can let the users know that the soup is out of stock. So, I will add another response code called, "Failure Out of Stock," with a template that communicates that the soup is out of stock. With that, we have defined our intent responses. And when you do that, XCode automatically cogenerates a custom response subclass. This subclass has all the properties that you defined as well as custom constructors for your templates, which take all the important properties that you need to construct that template. Now the last step that is left to do, is to use these responses in our Intent Handler. Handling a custom intent comprises of two steps: confirm and handle. In confirmation, we are asked whether we are ready to handle this intent at this time. So, we'll first find out whether the soup the user has requested is in our menu. If it is, we'll let the system know that we are ready to place the order. But this is our chance to also check that the soup is available right now, and that we can place the order of that. So, let's do that. First, I'll grab a reference to my menu item. Then check if it's available. And if it isn't, I'm going to call the Completion Handler with our new custom Failure Out of Stock Response Code. If everything looks great, we'll continue as before. This is also our chance to provide custom response -- custom confirmation response. Support for those will be coming in upcoming [inaudible]. Alright, that was confirmation. Now, it's time to handle the intent. For us, that means to place the order for the soup. So, I'm going to build an order from all the information that is present on my intent. And then place the order with our Order Manager. And if it's successful, we'll let the system know, by calling the Completion Handler with the Success Response Code. However, since we defined the Custom Success Response Template, this is our chance to provide the wait time. So, let's do just that. And with that, we have added support for two really useful custom responses to our custom intent. Let's see how it looks. So before, all we could let the users know was, "Your order was placed." But now, Siri can speak out that it will be about 10 minutes before they should come down to the store. And that's really helpful. The improvement to the usability is even more stark if you look at the error case. Before, "Sorry, there was a problem with the app." But now, we can let the users know that the soup is out of stock and then they can decide what they would like to do. Either, they can go into our app and order something else or try again later. So, you saw that Siri and shortcuts allowed you to provide custom dialogues, but that's not all. As soup engineers, we work really hard to build superb experiences for our users. And wouldn't it be great if we can bring those experiences from our apps to our shortcuts wherever they are run? With Intents UI extensions, you can do just that. Intents UI extensions allow you to provide custom UI, familiar UI from your apps, to shortcuts. And it is displayed all across the OS, on Lock Screen, in Search , and of course, within Siri. To learn more about how you can build great, Intents UI extensions, please take a look at the WWDC 2017 session, "What's New in SiriKit?" So, we built a custom Intents UI extension for Soup Chef app. And let's see how that looks. Again, this is where we started out with. But with a custom Intents UI Extension, now I'm able to show the users the entire invoice and convince that we got all the details right, that it will be 2.95, before they place the order, which is incredibly helpful. Once they place the order, instead of just saying, "It's done," we can now show them the receipt and reaffirm that we got everything right and it will be ten minutes before they should come down there. Alright, so you saw how easy it is to add support for Custom UI and Custom Responses. Now, let's take a look at how we did that in Soup Chef. Alright, what you see here is our app Soup Chef. If we go to the Intents Definition File, you can see that we have a custom order soup intent defined. This intent is off-category order. Now, let's look at the response section that is associated with my intent. Here, I have properties and template sections. What I would like to do is add the custom success response that we just talked about. So, let's start by adding the wait time property. I'll click the Plus button, and name my property Wait Time. I'll leave it of the Type String, so I can format the date however I see fit before providing it into the dialogue. To reaffirm which soup the user ordered, I'm also going to add the soup property. Now, soup is a custom object in our app. So, I'm going to select the type "Custom." Now, let's take a look at the response codes. We have a failure and a success code. Here, I'm going to define the template for our Wait Time Response Template. So, let's do that. "Your Soup Will Be Ready in Wait Time." I would also like to add our Out of Stock Error Code. For that, I'll click the Plus Button and create another response code. I'll call it "Failure, Out of Stock." Since this is an error code, I'll uncheck the Success checkbox. And now, finally, let's define the template for it. "Sorry, Soup is Out of Stock." And with that, we have defined our custom responses. Now, let's head over to the Intent Handler file. Alright, here we are. As you see here, we have a Confirm and a Handle Method. Let's look at Confirm first. We find out which soup the user has requested, and then check with our menu manager whether that soup exists on our menu. If it does, we return the response code "Ready." What I would like to do here, is to check whether that soup item is available currently to order. So first, let's grab reference to the menu item from our menu manager. Then, I'm going to check if it's available. If it isn't, I'm going to call the Completion Handler with our new Failure, Out of Stock Error. And provided the soup object that the user requested. Alright, so we have provided the custom error code here in completion. Let's take a look at Handle. Handling this intent means ordering our soup. So first, we build the Order Object from Intent. Then we place the order, using our order manager. And finally, let the system know that the order was successful. But this is our chance to provide users with more helpful information like wait time. So, let's call the Completion Handler with our New Custom Response Code. This response template takes the soup and wait time strengths. So, I'll provide that from my intent. And we can grab the wait time from our order. And that's it. So, you saw how easy it is to add support for custom responses to our custom intent. You start by defining a custom intent that works for your use case. You follow it up by defining custom responses that you want Siri to speak out. And finally, you provide them in your Intent Handler. Now, I placed that order for that soup earlier, so I better head back and get it. So, please welcome Ayaka Nonaka, who will talk to you about some of the best practices that you should adopt in order to create great voice and UI experiences in your apps. Thank you and welcome Ayaka. Thanks so much. Hi, everyone. So, as Amit just showed you, it's super easy to get started with Siri Shortcuts. Once a shortcut is donated, it is available for the system to suggest as a shortcut to use with Siri straight out of the box. I'm going to be showing you some more new shortcuts APIs, and the best practices to help bring the greatest experience to our users. And I know you care about this because your users are going to notice these small details and the large amount of care that you put into your apps. If you saw the first session, "Introduction to Siri Shortcuts," you learned that there are two ways to donate shortcuts. The first way is through NS User Activity. User Activities are great if you want to do a basic integration, where you just want to open the app and show your user a particular piece of content. And the second way is through Intents. Intents are great if you want to do a deeper integration where you can run things in the background and inline within the Siri experience, so you don't have to context with your users out of that to get things done. And for both NS User Activities and Intents, there are several parameters that we can configure. The first one is the title. So, in this case, Order Clam Chowder, with the subtitle underneath it and image next to it, and finally, a suggested invocation phrase which you'll get to later. For each of these, we're going to go over how best to configure them. So, let's get started with the title and subtitle. So, when you're thinking about titles and subtitles, there are two high-level things that you should keep in mind. First, the title should represent what happens when the user runs the shortcut. Your user is going to want to know exactly what happens before they run the shortcut or add that shortcut to use with Siri. And secondly, the subtitles will provide more information, but only if you need it. So, let's dive into a little bit more of the details. The first easy thing to do is see a sentence case. This is to make sure that we provide a consistent experience across all different apps and their shortcuts. Next, it's important to keep the title concise. This is because we're dealing with potentially a limited amount of [inaudible]. And you can do this by dropping articles like "a" and "an" if your language allows for it. Next, it's super important to include critical information. For example, if you have a payments app that lets you send money to your friends and you have a shortcut for that, it's important to put the dollar amount or your currency amount in that. Probably in the title in this case. Next, if we're working with Intents, it's important to include a verb. And if you're working with English, we should put that at the beginning and prepend the verb to your phrase. And last but not least, to make sure that we provide the shortcuts experience to everyone around the world, it's very important to localize. So, not let's take a look at some things that we should avoid. So, the first thing that we don't need to include, is the name of your app. This is because our RUI already attributes the name of your app, and by not including this, you can save even more space. And second, we don't need to put duplicate information in the title and subtitle for similar reasons to save space and leave room for critical information. And lastly, we don't want to use quotation marks unless you're actually quoting something that's going to be used verbatim in the shortcut. For example, if you have a shortcut to send a message, you can put the content of the message in the quotation marks. So, let's take a look at an example. This example says, "A clam chowder from Soup Chef. The best way to get soup, Soup Chef." And just based on what we learned about, I think we can all agree that this is not a good example. So, let's try to make it better. So, the first easy thing that we can do is to drop the "From Soup Chef," because as you see, Soup Chef is already attributed in this UI. So, now we have "A Clam Chowder." But when I'm looking at this, I'm not sure what's going to happen when I run the shortcut. Am I ordering the soup or just looking at it? Who knows? So, let's fix that by adding a verb. And while I'm doing that, I'm also going to drop the article "a" to save some space. So now I have "Order Clam Chowder." So, when I'm looking at this, I'm thinking, "I want to order a Clam Chowder, not a Clam Chowder, which doesn't sound very delicious and we're trying to sell soup here, so that's really not ideal. But the good thing is, we can fix this really easily by simply dropping the quotation marks. And now we have, "Order Clam Chowder." And I think at this point, our title's looking pretty good. But let's take a look at the subtitle. The subtitle says, "The best way to get soup." But that's basically Soup Chef's tag line. I already know that Soup Chef is the best way to get my soup. And it's not adding any extra information that is specific to the shortcut. And, so that's already not good, but a more critically bad thing is that Soup Chef recently started offering delivery service, and I don't know where I'm getting my soup delivered to. Am I getting it to my home, my office, someone else's office, no idea. So, let's fix that by adding a delivery location. So, now I have "Order Clam Chowder to 1 Apple Park Way." And now I know exactly what happens when I run the shortcut. So, that is great. And in the Settings UI, it might look something like this. And this is looking pretty good. But I think we can do a little bit better. So, as you can see, the Soup Chef app icon is used for both the Clam Chowder and Tomato Soup cases, but wouldn't it be great if we can show an image of a clam chowder for the clam chowder case and an image for the tomato soup for the tomato soup case? Sort of like this. And for this, we provide two APIs. One for User Activities and another for Intents. So, let's take a look. So, to set some context, the user has just viewed one of their past soup orders and we want to donate it, so we can associate it -- we can associate that with a shortcut. So, the first thing that we want to do is create a CS Searchable Item Attribute Set which by the way is the same thing that you want to use if you want to provide a subtitle to your user activity, but in this case, I want to set an image [inaudible]. First, make an image with chowder. Get the PNG data out of it and set it as the thumbnail data property on my Attributes. And next, I'm going to take those attributes, set it as the Content Attribute Set on the User Activity, and finally I'm going to donate the User Activity by setting it as the User Activity property on my view controller. So, let's take a look at how it works during Intents. For Intents, I'm actually able to set multiple images. One for each parameter that is defined as an intent. So, I'm going to set an image of a chowder for the parameter named "Soup," and an image of an office for the parameter named "Delivery Location." And once I have that, I'm going to create an IN Interaction out of it and donate it. So, at this point, you might be wondering, "If I have a shortcut that is based on a shortcut type that contains multiple parameters that have images, how do I know which image gets used?" For that, we're going to go to our Intent Definition File. And in particular, the Parameters Section. So, in the Parameters Section, we want to make sure that these parameters, at least the ones with images are ordered from least specific to most specific. So, in my case, I care more about the soup image than the delivery location. So, I'm going to make sure the soup comes below delivery location. And once I do that, my list of shortcuts might look like this. So, in the first two cases, where the shortcut type contains both a soup and a delivery location, I see an image of a soup, but in the last case, where the shortcut type just contains a delivery location, I just see the image of a delivery location. Now, these shortcuts are looking so great, that I just want to tap on one and add it to Siri, so I can use it. And our users are super excited about this new feature too, and they're all so eager to start setting up new shortcuts to use with Siri. But as more users starting using this feature, we noticed something. We noticed a problem. When they got to the recording screen, they had no idea what to say. They could use some creative inspiration in choosing a great suggested shortcut phrase. And as Soup Chef app developers and general soup enthusiasts, we think we have some pretty great ideas on what would make a great soup ordering phrase. You know, something like "Tomato, Tomato" or "Chowder Time." So, would it be great if we can show a suggestion like this? "Hey user, you can say something like, 'Chowder Time.'" And of course, we have an API for this, so let's take a look. So, building on a previous example, all we have to do is add one line. So, User Activity has a property called Suggested Indication Phrase. And I'm going to set that to Chowder Time. And for Intents, it's the exact same thing. Intents also have a property called Suggested Indication Phrase which I'll also set to "Chowder Time." So, the API for this as you can see, is super simple. But what actually makes a good suggested invocation phrase? So, the most important thing to keep in mind is that the suggested invocation phrase is both short and memorable. You have to remember that your users are going to have to say this phrase exactly every, single time they run their shortcut. So, ideally it would be somewhere between two to three words. Next, it's important to not include the phrase, "Hey Siri," in your shortcut phrase, because if your user accidentally sets their shortcut phrase as "Hey, Siri, Chowder Time," when they actually run the shortcut, they might have to say, "Hey Siri, Hey Siri, Chowder Time," and no one wants to do that. So, let's take a look at some examples. This example says, "Hey, Siri, please place an order, thank you." Although this example is polite, it's not a good example. First, it contains, "Hey, Siri," which is pretty obvious, but it's a little bit too long for me to want to repeat it every, single time I want to run the shortcut. So, that's out. What about this example? This example says, "Order a clam chowder to my office." Seems pretty reasonable, but this is still not good because it's still too long. Like, it's not short and memorable. And I don't think I can or want to remember to say this phrase every, single time I want clam chowder delivered to my office. What about this example? This simply says, "Chowder Time." This is both short and memorable and doesn't include the phrase, "Hey, Siri." So, this passes all of our tests and that's awesome. But we can't [inaudible] in order to provide a truly great shortcuts experience to your users, around the world, you have to make sure it's localized. For example, I speak both English and Japanese, and it would be great if I can suggest a Use Shortcuts in Japanese sort of like this. So, instead of "Chowder Time," I can be like [foreign language spoken] which means, "I'll eat chowder." To do this, we can localize. And when we're thinking about localization, there are a couple of things to keep in mind. So, in addition to localizing your code, we want to make sure to localize the content in the Intent Definition Files, because as you can remember, there are a lot of strings in there. And second, we have to think about pluralization. So, even if you're not localizing to any other language besides English, if you have to handle things like, "Order one clam chowder," versus, "Order three clam chowders," you have to think about localization. And to learn about all of this stuff, check out the Localization Session and their labs. They'll teach you everything that you need to know, and more. So, cool. For now, let's go back to English. Now, that we've followed these best practices, when we go to the Settings UI, you are presented with a beautiful list of suggested shortcuts. And right now, these are based on a combination of suggestions based on the user's past routines and the things that the user did recently. But in addition, we provide an API to allow developers to suggest their own shortcuts. And this is useful because this allows you to suggest things for things that the user hasn't necessarily done yet or hasn't done recently. For example, if you have a music app, you might want to offer a shortcut for playing a particular playlist, even if the user hasn't played it before or hasn't played it recently, because presumably, if a user has created the playlist, they probably ought to play it at some point in the future. So, in the Soup Chef app, I think it'd be great if we can suggest a shortcut for ordering the soup of the day, even if they haven't ordered it recently, or have never ordered it, because based on past experience, our users seem to love it because it's both delicious and offered at a special price. So, let's take a look at the API on how to do this. So, for Intents, you can create an IN Shortcut out of Intent, well, and for User Activities, you can also create an IN Shortcut out of it. I'm going to wrap it in a suggestions list and pass it into IN Voice Shortcut Centers Set Shortcut Suggestions Method. And once you do that, you'll see my suggestions alongside all the other system's suggestions. And by the way, the same set of suggestions is shown in the shortcuts app, so you can create custom shortcuts, using your shortcuts. So, now that we put in all this work to provide a great voice experience for your users, we want to make sure that your users know that they can add shortcuts to use with Siri. And the best way to do that is bringing shortcuts into your app, so that users can create shortcuts right from inside your app. So, for example, I just ordered a soup, and I'm going to need a suggestion to add this to Siri. And when I tap, "Add to Siri," I'm presented with the same system UI that allows you to configure a new shortcut to use with Siri. So, let's take a look at the API to do this. So again, I'm going to take my User Activity and create an IN Shortcut out of it, and then using that, I'm going to create an IN UI Voice Shortcut View Controller, set the Delegate, and present it. And it's the same thing for Intents. So, instead of creating the IN Shortcut with a User Activity, I will create it with Intent, create my View Controller, set the Delegate, and present it. And once I do that, your user will see this setup screen, right from inside of your app. In addition, rewrite an API to allow users to delete and edit any existing shortcuts. We also have an API that lets you get a list of shortcuts that are already added to Siri by our users. So, for example I can use this information to indicate which soups on my menu have the user has already added the shortcut to use with Siri. So, as you can see, for clam chowder, I have "Chowder Time" associated and for tomato soup, I have "Tomato, Tomato." Awesome. So, to summarize, let's go over what we learned about today. First, we learned that Custom Responses are critical in providing a great voice experience to our users. And Amit showed us just how simple that can be. Next, we saw how Custom Intents UI can help bring the apps experience into Siri and to the other places in the system like Search and Lock Screen. We also saw that details matter. Even something small like adding an image to your shortcuts, or just dropping the quotation marks from around "clam chowder" to make it a big difference. And bigger details like localization can make a huge difference. Lastly, we learned about the various different APIs that you can use to bring the shortcuts experience into your app, so that your users can create shortcuts to user with Siri at the most relevant moments in your app. So, the documentation on how to do this is all on developer.apple.com, and in addition, we'll be holding shortcuts labs throughout the week, including one later today. And we also have a Watch Session right after this, that will show you how to bring the shortcuts experience to the Watch and to the Siri Watch face so that -- and they're also going to show you how it works, even if you don't have a Watch app. So, it's a total must watch. Alright. Alright, well, now you know everything that you need to know to create a great shortcuts experience for your users. We can't wait to see what you build with our new APIs. Thank you for listening, and I hope you have a great rest of WWDC.  Hi, welcome to Getting to Know Swift Package Manager. I'm Rick Ballard. With me today is Boris Buegling. And we're very excited to tell you all about Swift's Package Manager, or as we call it sometimes SwiftPM. We're going to focus today on the open source project. And not on Apple's other developer tools. But we've got plenty to tell you about here today. The Swift Package Manager makes it easier to develop and distribute source code in the Swift ecosystem. Today we're going to talk about its goals, some of its design, and a little bit about where we can go from here. We'll start by telling you a little bit about why we decided to create a new Package Manager as part of the Swift project. We'll show you little bit about how to use it. And then dive into its design and features. We'll tell you a little bit about where we can go in the future and close out by describing SwiftPM's open source process and how you can get involved if you're interested. I'm sure most of you are familiar with package managers they're a great way to share and reuse code. But why did we decide to create a new one for Swift? First of all, Swift is a cross-platform language. So we wanted a great cross-platform tool for building your Swift code. This makes it easy to configure your code in a consistent way and run it on all of Swift's supported platforms. SwiftPM includes its own complete build system, allowing you to configure your software, build it, test it, and even run it from one tool. We also wanted to make it as easy as possible for you to share your Swift libraries with anyone wherever they are by providing a canonical package manager in the Swift project, we hope to define a common standard for the way that you can distribute your libraries. This makes it easy to grow the Swift ecosystem and make Swift better for everyone. Many of you may have great ideas for features that you'd like to add. But we'd like to be careful about what is added to the core libraries so we can maintain a careful and curated API. A great package manager makes it easy to distribute these ideas as packages instead of having to put them into the core library. The best ideas can organically gain traction with the community over time and become increasingly standardized. Finally, by building a package manager alongside Swift, we're able to take advantage of Swift's power and philosophy. SwiftPM is itself written in Swift and it's even a Swift package. Beyond that, we have the opportunity to work closely with the Swift language and core library projects to build great package manager features that will help your Swift code sing. SwiftPM is part of the Swift open source project, and has its own presence on Swift.org, and on GitHub. The Swift Package Manager section of Swift.org is a great place to go to get started. When you're ready to try it out you can find it included in every Swift toolchain, also downloadable from Swift.org. And of course, it's included in every release of Xcode alongside the Swift tools. So, to start telling you a little bit about how to use it, I'd like to invite Boris Buegling up to show you the basics. Thanks, Rick. Let's take a look at how to use SwiftPM. SwiftPM consists of four command line tools, and at the top level Swift Command. Swift Build, to build your package. Swift Run to run its executable products. Swift Test to run tests. And Swift Package to run various non-build operations on the package. Packages are stored in git repositories. And diversions are represented by git tags. Next, I'm going to show you a demo of how easy it is to create your own first Swift Package. We start in terminal, and we create a new directory, called helloworld. This will also be the name of our package. Switch to that directory and we will run Swift Package init with the type executable. With this, SwiftPM creates a basic package and structure for us. Let's open finder to look at it a little bit more closely. We have the Package.swift manifest file, which describes the structure of the package. We get a basic README. You have the Sources directory with a subfolder for our helloworld target. And the main.swift file for our executable. You also get a test directory, where we could later put some unit tests. Let's switch back to terminal. And we will type swift run to build and run this package. This compiles the package, links executable, and we see helloworld is the output. Next, I'm going to switch to another terminal window, where I've prepared a more complex package. We will use this in the following to discuss the basic concepts of SwiftPM. But, first, let's also just run it to see what it does. So, you can see, it outputs randomly generated playing cards to the terminal. Now, we can switch back to the slides to talk about SwiftPM's basic concepts. A package consists of three major parts; dependencies, targets and products. And we'll look into each of these in more detail in the following. Dependencies are those Swift packages that you can use when developing your features. Each dependency provides one or more products such as libraries that your package can use. Let's take a look at how dependencies look in the package of Swift manifest file. Each dependency has a source location and it is versioned. Targets are the basic building blocks of packages. A target describes how to build a set of source files into either a module or a test suite. Targets can depend on other targets of the same package and on products exported from other packages, declared as dependencies. Products are executable to libraries and products are assembled from the build artifacts of one or more target. Packages provide libraries for other packages by defining products. By default, you do not have to declare the type of library explicitly, but SwiftPM will choose it for you based on its use. If needed, you can explicitly declare a library a static or dynamic. Let's take a look how our targets are configured in the manifest. In our example, we have three targets. The first is called libdealer, and it contains the implementation of our main functionality. And it has one dependency, the deck of playing cards product which comes from the dependency we declared earlier. Second target, dealer depends on that to provide the command line tool that we just run earlier. And finally, we also have a test target that depends on the two other targets and this is where we can unit test our functionality. In our example package, we have also configured two products. The first is a library product corresponding to the libdealer target. And this provides our implementation as a library for external consumption. Second, we have an executable target depending on the dealer target which provides an executable for command line use. To complete this section, I will show you how we can use a package to add a new feature to the example. This, we switch to a new terminal window and we open up the package.swift manifest file. We want to add a new dependency. In this case, this is actually SwiftPM itself. As Rick told you, it is itself, its own Swift Package. It doesn't provide a stable API, though, that is why we're depending on an exact version number. We also want to depend on one of its products in the libdealer target. It is called utility. And among other things, it has a class called terminal controller, which provides us with the possibility to color the terminal output. Note however, that this is no official Apple API, we're just using it for the demo. Let's switch back to the terminal. I already changed the code to make use of the new dependency before this demo. So, we can just run it to see the result. And as you can see, we have the same output, but now it's a little bit more fun with some colors. I want to show you one last demo which is how SwiftPM can run tests. For this, we're using the Swift Neo package. A networking library that Apple open source earlier in the spring. We will run Swift Test with a parallel option. This allows us to run tests in parallel. So, you get your test results faster. And we also pass the filter option. This allows you to run a subset of tests to you can iterate on a single future. This will now again compile our package and run the tests in just a few seconds. And as you can see, we get a nice progress bar and the tests finish really fast, because we were running them in parallel. Let's switch back to the slides again. Next, I'm going to talk to you about the design of the Swift Package Manager. SwiftPM follows Swift's philosophy. It is fast, safe, and expressive. It is safe due to its isolated build environment and the fact the builds cannot run arbitrary commands. It is fast due to using a build engine that is scalable to large dependency graphs. And it's expressive due to using the Swift language for the package manifest. And this also allows you to use a programming language you're already familiar with. For the rest of the section, I will take you on a journey through the different steps you will encounter when creating your own Swift packages. We will start with configuration. As we saw earlier, SwiftPM's manifest is based on Swift. Using Swift makes it easy to understand because there is no new language to learn. We follow Swift's API design guidelines to make it even more familiar. And, it allows us to take advantage of existing tooling written for Swift, but when writing your own manifest, you should still prefer declarative syntax and avoid side effects. Because SwiftPM makes no guarantees about when or how often your source code is being evaluated. On the left-hand side here, you see an example that is not really declarative. We cannot see the name that is being generated and it's used in a couple of times across the package. In contrast, on the right-hand side, we have a fully declarative manifest by using string constants. It is easy to understand and see what the targets are. So, as you can see, not using declarative syntax also makes your manifest harder to understand for you and your users. Source files organized on disks in folders named after each target in the package. This makes it easy to get started and allows packages to adopt a common structure, so that you can navigate them quickly. Package Managers and other build tools often have attention between what is explicitly configured by the user and the conventions that are imposed by the Package Manager. As I told you earlier, the source file is automatically picked up from convention base locations on disk, so that you can very easily add or remove source files without having to edit the package manifest. Products and targets, however, are worth explicitly configuring to make it easier to understand the package and what it defines without needing to cross reference with the layout on disk. It also makes it easy for clients to see what a package provides just by looking at the manifest. SwiftPM also supports building source code for other program manager languages, such as C, C++, and Objective-C. This allows integration with existing code. Note however that we do not support mixing those languages with Swift in the same target. Next, we're going to look at dependencies and versioning. To make sure your packages can benefit from bug fixes without constant churn, Swift packages should adhere to semantic versioning. This is a commonly used standard which assign specific semantic meaning to each of a version number's components. The major version signifies breaking changes which required clients to update their code. Examples for the changes could be deleting an existing type, deleting a message, or changing its signature. But they also include backwards incompatible bug fixes, or major changes to the behavior of existing API. The minor version should be updated if functionality is added in a backwards compatible manner. Examples for this is adding a new method or type. And finally, the patch version should be increased when you're making backwards compatible bug fixes. This allows clients to benefit from bug fixes without risking breaking the source code. SwiftPM needs to determine the exact versions of all packages in the package graph before it is ready to build. We do this with a process called dependency resolution. As part of this, SwiftPM looks at all the requirements specified for packages and finds the latest version that is compatible with all of them. Let's take a closer look at what SwiftPM is doing in the process using the demo I showed you before. The dealer package from the demo has two direct dependencies. One, is SwiftPM itself, and the other one is deck of playing cards. SwiftPM will resolve the versions of these direct dependencies. For the first one, this is straightforward because we specified an exact version, beginning exactly that tag. For the second one, we're using the from syntax. That means we're getting updates to the minor or patch components. In this case, we're ending up with a tag 3.1.4. The whole process is recursive. So next SwiftPM will look at the transitive dependencies of all diary points. So, PM has no further dependencies, so there's nothing to do there. But a deck of playing cards depends on the Fisher-Yates and playing card packages. Next, SwiftPM has to resolve the versions of these packages again. For the Fisher-Yates package, this works the same way as before because we are also using the from syntax. In this case, we're ending up with a tag, 2.2.5. For the playing card package, we're using up to the next minor syntax. This means, we're getting only updates to the patch component. You might want to use that syntax if you want to be more conservative as a dependency and only take bug fixes. In this case, we're ending up with a tag 3.0.2. Finally, when looking at a target, SwiftPM has to match up its required products with the packages that we resolved. For this, we're looking at the dealer target from the demo, and as you can see, the utility product is provided by the SwiftPM package. And the rest of the packages provide the other products. After dependency resolution, the resolves are recorded in a file called package.resolved. the purpose of this file is so that you can share the resolve versions with other members of your team, or your continuous integration infrastructure, so that you get dependable build results, and you can deliberately choose when you want to update a dependency. You do so by running Swift Package Update, when you're ready to update. Note also, that this is your top-level package that contains package.resolved. If one of the transitive dependencies contains a package.resolve file, it will be ignored for dependency resolution. Next, let's take a look at building your package. SwiftPM uses llbuild as its underlying build engine. llbuild is a set of libraries for building build systems. And it's built around a general purpose and reusable build engine. This provides us with ability to do fast as well as correct incremental builds. And is also used by Xcode's new build system. It is also the part of the Swift open source project. Developing software in isolation with all dependencies explicitly declared ensures that even packages with complex requirements can be reliably built and used in different environments. Instead of installing packages globally into the system, SwiftPM only allows you to use packages that you explicitly depend on. We also leverage build sandboxing so that nothing can write to arbitrary locations on the file system during the build. SwiftPM does not allow executing arbitrary commands, or shell scripts, as part of the build. This allows us to fully understand your build graph and all of its inputs and outputs to do fast, as well as correct, incremental builds. Because we have a view of all your dependencies. As I showed you in the demo before, SwiftPM also supports testing. This is based on the XCTest framework that you're already familiar with. We support parallel testing, so that you can get your test results faster. And we support test filtering so that you can run a subset of tests and iterate on a single feature. As we're evolving SwiftPM, we're thinking about workflow features, especially so that you can do all of your development on the command line. One such feature is edit mode, which allows overwriting all transitive occurrences of a specific package, with a locally checked out copy so that temporary edits can be made, and changes to transitive dependencies can be tested without having to forward all packages in the graph upfront. Branch dependencies allow depending on packages without strict versioning requirements. This is useful when you're developing multiple packages together. This is a development only feature, so you have to change to certain version dependencies before you publish a tag. Local packages allow you to use packages directly from the file system, instead of from a git repository. This is useful to make it possible to bring up multiple packages during the initial creation. So, last topic, I want to talk to you about adopting new versions of SwiftPM and the Swift language. Each new version of Swift can bring a new version of the package.swift manifest API. The previous API is still available, so that you can take advantage of new source tools without having to update your package or losing access to existing packages. New API can be adopted independently of changing to a new Swift language version for your packages' source code. To specify which version of the API is being used, we're using the Swift Tools Version command at the top of the package.swift manifest file. This specifies the minimum required version of the Swift tools that is needed to process the given manifest. Each package can also declare which versions of the Swift language it uses for compiling its source code. This is a list, so you can choose to support multiple versions of Swift with the same version of your package, by using compiler directives. A package graph can be a mix-and-match of packages with different language versions. I told you a lot about how SwiftPM works today, next I'd like to invite Rick back up to the stage to tell you where we can go from here. Thanks, Boris. So, Boris has shown you what you can do today, but there's a lot more potential out there. SwiftPM is still a young project with lots of room to grow. Swift uses an open evolution process which means that anyone, including you, can contribute your ideas. If you're looking for some inspiration, we'd like to share some of our ideas, but none of this is a plan of record. We're sharing these ideas so that you can see the potential of the Swift Package Manager. And we welcome you to provide your feedback, comments, and your own ideas as we evolve is this product. The ideas I'm going to cover today break down into four different themes. Letting the Swift Package Manager integrate with other tools that may want to sit on top of it. Helping you publish new versions of your package and deploy their products. Supporting more complex packages than SwiftPM is able to build today. And finally, some forward looking thoughts on package discovery and trust. While the SwiftPM command line experience is important, we want to make sure that SwiftPM can integrate with other tools, such as development environments, automation, and more. We've already laid the groundwork for this with SwiftPM's library-based architecture. SwiftPM doesn't have a stable API today, but for tools that are willing to keep up with the SwiftPM changes, it's available for adoption and additions today. If you're looking to build support for SwiftPM into your developer tools, we welcome your contributions and discussion. We want to make SwiftPM part of a thriving ecosystem of developer tools. One thing we've seen requested recently on the Swift forums is a way for people to edit their package.swift manifest from automated tools, instead of making their users always edit the source code directly. We think that it's possible for SwiftPM to support this, probably by using libSyntax. libSyntax is a library being developed in the Swift open source project that makes it easier for you to understand and manipulate Swift syntax from other tools. Boris told you earlier that you should prefer declarative syntax for your package.swift manifest, and this is another reason why. That will make it much easier for SwiftPM to understand your manifest, so that it can make automatic changes, such as adding new dependencies or targets as shown here. So, there's a lot of room for SwiftPM also to add new functionality to help you publish new versions of your packages and deploy their products. Today, when you want to publish a new version of your package, you tag it manually with git. And if you want to inspect your published tags, you use git directly for that as well. We could add new functionality to automate this process and perform additional housekeeping, validation, and other auxiliary tasks you might want as part of a streamlined publishing workflow. One especially useful feature we could add here would be assistance with maintaining correct semantic versioning. We could have SwiftPM analyze the API differences in the new version of your package and detect when you've made a change that is not compatible at compile time, so that it can suggest updating the major version of your package. Another thing we could do is make it easier to deploy the products of your packages from SwiftPM. You may want to customize the linkage with libraries, or the product layout for your specific deployment environment, whether local or on a server. Or, maybe you want to include version information about what packages were built into the product. Or, you otherwise want to use the context that SwiftPM has about your packages somewhere in your product. SwiftPM could add new commands to support all of these needs. There's a lot that you can build with SwiftPM today, but we also want to be able to support more complex packages with more sophisticated needs. The biggest gap we have right now, is probably support for resources. If you have any images, data files, or other assets, SwiftPM currently provides no way to bundle these up you're your products. The foundation core library, actually just added API this spring for doing resources in a cross-platform manner so SwiftPM could adopt this API if we want to build this feature. We know that some users also want support for specifying compiler flags, linker flags, and other properties that SwiftPM doesn't support today. It would be really great for us to add a robust build settings model, potentially including things like conditional settings, or fine-grain control over what parts of the package get which setting values. Boris also talked to you earlier about SwiftPM build isolation, and why it's important. We don't let you run arbitrary shell scripts. But many users may want some level of customization for their build, either because they want to support custom languages, or processors, they want to run their own documentation generator implementor, or they have other steps that they need to bring to the build process. We think that SwiftPM could support this safely, possibly even through real tools packages that bring new tools into your build process. The important thing we need to make sure here, if we do such a feature, is that any new tool that's brought into the build process have to correctly declare their input and output dependencies, so SwiftPM can continue to maintain correct incremental, and parallelizable builds. Finally, I want to talk a little bit about some forward-looking thoughts on package discovery, trust, and management. Git itself supports, the protocols that get supports, provides security mechanisms like TLS to make sure that you're actually talking to the remote repository that you think you are. But a malicious actor could still compromise remote repository and put malicious content in. This is actually something anytime you're using third-party code, you should be aware of these sorts of risks. But the Swift Package Manager provides a great opportunity for us to build security features to make sure that you're actually getting the package content that you expected. SwiftPM also prevents your package.swift manifest evaluation in your build from escaping and writing things out into your file system or accessing the network. We're using macOS' sandboxing technology for this today. And it's great. But we'd like to bring this kind of security to other platforms as well. Many users may want to be able to fork their packages easily, either because they want to make a private customization to one of the packages in their graph. Or, even because they just want to override the origin URL of where they're getting each of those packages from, so that they can point at a private mirror that they control and not depend on the original package always being there. Ultimately, I'd like some day for us to have a real index for Swift packages. In addition to providing a standardized namespace and making it easier to discover new packages, we could even support things like quality metrics for a package. Like what is its automated test coverage? Or ways for you to evaluate the trustworthiness of a new package that you're considering adopting. So, I've gone over a lot here. But these are just some of the possibilities. Ultimately for those of you, who are interested, we're interested in your feedback, ideas, and contributions to help make Swift Package Manager the best tool that it can be for the developer community. So, to talk about how you can do that if you'd like to, I'd like to talk about Swift's open source process. As I said earlier, the Package Manager is part of the Swift open source project. And Swift.org is also a great place to go if you want to learn about the community and the process. SwiftPM uses the Swift language evolution process which means anyone can submit a proposal for major new features or changes to the Swift Package Manager. Before you go off and draft a whole formal proposal though, I recommend that you swing by the Package Manager section of the Swift forums and socialize your idea with the community. You may get a lot of feedback that helps make your idea even better. If you're interested in dipping your toe in the water with a smaller contribution the Swift bug tracker at bugs.swift.org has plenty of ideas. In particular, you may want to look for bugs tagged with this starter bug tag. And since, as I said, SwiftPM is written in Swift, you may find it's actually pretty easy to dive in and take a look. Of course, if you find bugs when you're using SwiftPM, we encourage you to go file them on bugs.swift.org as well, where you can track how we handle them. SwiftPM gets to take advantage of the same great continuous integration infrastructure that the Swift project has. Which means that poll requests can be automatically built and had their tests run before their merged. Because the SwiftPM code base itself has great test coverage, we found that this infrastructure is really useful for us. When you're ready to try out the latest changes, you can download the Trunk Snapshot Toolchains that are updated on a regular basis available on Swift.org. We've been really happy with the growth of the SwiftPM community so far. We've had over 180 people contribute, either with bug features or new features. And the Swift Package ecosystem is growing at a healthy rate as well, often with cross-platform packages, and many public packages available on GitHub. What this means is that you can focus on what makes your product special and let package dependencies handle the rest. There are a couple things that I recommend you try SwiftPM out for today, even though it has a lot of room to grow in the future. Those two things are command line utilities and libraries and for developing with Swift on the server. The server-side Swift community has been making extensive use of the Swift package manager. And server-side Swift has been growing well itself with many frameworks now available for doing web and backend development. If you would like to take a look at this approach, I think you'll find that Swift is a great language to do this kind of cross-platform development on. But you could also go ahead and use SwiftPM for creating command-line utilities and libraries today, whatever makes sense for you. Ultimately getting started is as easy as opening a terminal window and running Swift package init. So, the next time you're thinking about trying something new, I encourage you to give it a try. And if you're interested in contributing, swing by the Swift forums and start a conversation. If you'd like to come chat with us, you can find us in the labs tomorrow at 3 p.m. Ultimately, I'm really excited about where we can go from here and what this developer community can do together. Your contributions will help us design a package manager that will be great for the whole Swift community. Thank you. Enjoy the rest of WWDC.  Good morning, everyone. Thank you all for joining me, even before the coffee's kicked in. My name is Itai and I work on the foundation team. In this session, I'd like to talk to you about how the data that flows through your app can affect it, and how you can better protect your customers by building trust in that data. Let's get started. Apps don't live in a vacuum. In order for your apps to do something useful, they have to draw in data from external sources, like the disc, or the network, or your customers themselves. And then do something meaningful with that data, and then present it to your customers. In order for that data to be consumable, it has to come in some known format or structure. What happens when it doesn't? Usually, this means the data is corrupted, or invalid in some way and can be ignored. But sometimes, this data can invalidate assumptions that your app makes, and it can cause your app to misbehave, or maybe even crash. This can be a bad experience for your customers who might have to wait for your app to get updated in the app store. And it's an even worse experience if it's a crash on launch, because they can't even use the app. And, in the meantime, while you're waiting, you'll get a wave of one-star reviews. It's a bad experience for everyone. This is something to be even more cognizant of, if you're a framework author. Because it's not just one app that could possibly be affected, but maybe many apps. Today, we're going to be talking about trust. And specifically, we're going to be talking about how to build trust in data, by making sure of two things. One, that the data that we're going to be using, hasn't been modified from underneath us, and two that it contains what we expect it to in the format and structure that we want. So, we'll do just that by taking a look at the lifecycle of our data, and what we can validate about that data at every stage in the lifecycle. Then, we'll see what sort of type-level validation we can apply with the NS Secure coding protocol. And then apply those same concepts to codable types. Let's get started. In order to talk about data, we're going to want to build a mental model of the forms that data can take within our app. At the most basic level, data makes its way into our app as a stream of bites. There's not much we can tell about this data at this stage without looking at it, but this we'll call raw data. Now, to get working with that data, we need to make sure it conforms to that known format or structure. And in this case, each of those code points correspond into a UTF code point, and sorry, one moment, let me make that a little bit more readable. It looks like this is JSON. And so, once we've made sure that that data conforms to some format that we want to work with, we'll call this formatted data. Now, formatted data, on its own doesn't mean much, until we create primitive values out of it, strings, and arrays, and dictionaries that we can then use as building blocks for further algorithms. So, this we'll call our primitive data. Now, there's building blocks we most often want to work with, not as just primitive values, but as our own model types. So, once we do that, we'll make use of this as we're going to call, structure data. Now, these forms of data in our apps, form an abstraction timeline. Raw data is the least abstract data that we're going to work with, and our own structured model types are the most abstract. So, our goal for today is to take that data as far along the spectrum as we can. Now, our apps can stop at any point, make use of the data, however, we see fit. But we really want to work with our own model types wherever possible. Now, the goal for today is to not just go as far along the abstraction spectrum as we can, but to build trust as we do so. At every stage, the data is going to get more complicated, and there's going to be more that we need to validate about it. But once we do that, there's also going to be more that we can trust about it. Now, for our use case today, we're not really going to be talking about formatted data. Very often it's just a stepping stone between raw and primitive data, and you don't work with it directly. For instance, given raw data, foundations JSON serialization will give you primitive data back. You don't see just the formatted date directly, and you won't make use of it. So, today we'll just be talking about raw, primitive, and structured data. Now, let's start by talking about raw data. Again, as we mentioned, raw data is just a stream of bytes that's made its way into your app. Until you inspect that data and you give it meaning, there's not much you can do with it. Now, we might care to know what we can take a look at about that data before we start interpreting it. Is it even safe to do that? One thing we can validate about this data before making use of it is its length. Say your app expects to load a 1-kilobyte file from disc, but finds a 1-gigabyte file in disc. Does it make sense to even load that data in the first place, and start reading it? Almost certainly not. Now, sometimes we might not be able to have length expectations about the data. Maybe it's external data we don't own. We don't' know how much data there could be. But in some cases, we might also be able to verify a checksum, or a cryptographic signature that represents what the data might look like, even if we don't know what's inside. Checksum is built by hashing all of the data. And if any bit in the data changes, either due to a potential malicious third party, or just regular data corruption, bad blocks on disc, a bad network connection. If any one of the bits flips in that data, it will invalidate the checksum, or the signature. And we'll know before even reading any of those bytes, that the data is incorrect, and you shouldn't trust it. Now, we also don't always have a checksum. Maybe it's data we don't own, where you can't get that ahead of time. So, at this stage, there isn't much we can do with this data, besides read it and inspect it. And so, once we do that, we can get primitive data out. Now, as we've mentioned, we can take that raw data and pass it through, usually deserialize it, like foundations JSON serialization. When we do that, we'll get inert strings and dictionaries and arrays of numbers back out that we can use. And if this process exceeds, we know two things about that data. One, that the data was indeed in the correct format that we expected. For instance, XML data won't pass through JSON serialization. And two, if we trust the deserializer, we know that the run-time objects we get back out are going to be valid. Again, foundations JSON serialization will always give you strings and numbers and arrays that you can actually work with. It's individual values that we can trust. But at this stage, we might wonder okay how can we make use of this data, or what can we trust about it, and what validation do we still need to do? Well, we don't actually know much about the contents of this data yet until we start looking at it. And in fact, we might not know anything about the structure of the data until we start inspecting it. If you've ever worked with dynamic deserialization in this way, you'll know that there's a lot of downcasting from anys. There's no upfront expectation what the data can be because it's very generalized. And so, we'll want to check to see what the data contains and how we can work with it. So, let's motivate this with an example. I've been working with an app lately called Sell My Old Junk, which allows me to sell my old junk to some friends and family. And when one of them opens up my app, my app makes a request to my server. Which requests a list of products that are currently available for sale to my friends and family. When the server receives this request, it responses with JSON, that indicates here are the products available to sell. Now, this is what an API response from my server might look like. It's an array of product listings, which have some interesting fields that you might care to look at. For instance, each listing has a product ID. A positive integer that uniquely identifies the product. And in my case, these are sequential integer IDs. Every listing also has a name and a description which are strings. And there are a few other type fields here that we might care to look at. For instance, there's a field that's a Boolean that indicates whether or not this listing has already been sold. And there's some internal structure here. This list of tags, which are strings, which we might care to use. There are also a few fields here, which come to us as strings, but really represent other forms of data that we might care to look at. For instance, URLs and dates. So, let's make use of this data. In my app, I can fetch that data from the network say with URL session, and wherever possible I'll validate the length. Maybe my server can produce a checksum that I can validate. Or a cryptographic signature. Once I've done that, I can take the data and pass it off JSON serialization. If deserializing the data fails, JSON serialization will throw an error. Will check and then catch and handle. May display dialogue to my customers. Again, in our purlins of the day, we've just taken raw data and carried along it the abstraction spectrum to primitive data. And if anything went wrong, we can handle that failure. So, now we need to make use of this data. How can we consume it? Well, JSON is an any variable that contains the actual values. So, I can downcast it to the array of dictionaries that I expect it to be. Now, this part of my app cares only about product listings that are related to music. So, will filter out any products that don't contain the music tag. And here, I have that substructure. That list of tags, which will downcast an array of strings and make use of it, right. Whoops. Each of those forced downcasts, actually contains a hidden fatal error. If either of those casts fails because the API changed or the data changed along the way before it made into my app, again due to data corruption or malicious changing, those downcasts will fail. And when they do fail, they'll abort, and they'll crash my app. And again, that's a bad experience for my customers. Let's take a look at how this could happen. So, here again is that sample API response. And we'll take a look at the list of tags here. And say that second tag in there is modified. Instead of a string, we have a number. It's maliciously changed by a third-party, or maybe again due to regular data corruption. We can't always tell. Downcasting this list of tags will fail because they're not strings and we never checked to make sure they work before we cast. So, to avoid this, our main tenant for the day will always be validate first, execute later. Instead of asserting that you know what the structure of the data is, check first. Don't blindly assume. So, let's see how we can do that. So, here again is that first forced downcast. And instead of forcibly downcasting these values, I can conditionally downcast. This allows me to validate that the data actually contains what I want and if that cast fails, well I can handle that error instead of fatally erroring. Now, similarly, later downcasting that list of strings, instead of forcibly downcasting, again I can conditionally downcast. And in this case, instead of throwing an error, I can give a default value that allows execution to continue. In this case, I'll simply ignore any product listings that don't have a valid list of tags. I could throw an error, but in this case, I chose not to. Now, type validation isn't the only form of validation that you want to perform at this stage. For instance, if that had been replaced by null, which is totally valid in JSON, I would've seen a similar crash. In Swift strong static type system nullability is part of the type. And indeed, you can't downcast null to a string. And so, again, this cast would have failed. Now, even if all of these values are of the correct type and nullability, there's other forms of validations that we should care about here. For instance, I said that each product listing has a positive integer ID. In my case, they're all sequential integers. Does it make sense for one of these IDs to be negative? No, it doesn't. But even if it is always positive, does it make sense for it to be such a large positive integer value? I'm not selling that many things. So, no it doesn't. and in this case, this might be due to somebody trying to cause overflow in my app. This is something you need to watch out for. Now similar to range validation is length validation. Again, every product listing has a description. Does it make sense for that description to be empty? Well, in my case, I know that anytime I upload a product listing, I'm always going to put a description in there. So, in my case, no it doesn't make sense for it to be empty. But even if it's not empty, does it make sense for it to be the full length and contents of "Romeo and Juliet?" Also no, it doesn't make sense. Something here's gone wrong and I need to look for that. Now, there's additional forms of validation that we really do care about here. Even if all of these fields are right type, nullability, and fit within the range and length that we expect, their values and contents are also equally important. Every product listing has a URL that I can send a customer to see more information about that product listing. This comes to me as a string, but it actually has to contain a URL. Does it make sense for it to be any arbitrary string? No, and in my case, I want to make sure that it actually represents a URL. But more importantly, just because it looks like a URL, doesn't mean it will point to my domain. And this is something I care very deeply about. I really, really want to keep my customers safe. I don't want to possibly send them to a phishing domain, which could look like mine, but not really be my site. So, this is something that I care about watching out for. Now, lastly, even if each of these fields is valid on its own, sometimes the relationships between fields that matters. For instance, each product listing has a date when it was created, and a date when it was last updated. Each of these can be valid on their own, but does it make sense for the date that it was last updated to come before when it was created? No. And in my case, this might not open a security vulnerability in my app. But this is something that maybe you watch out for because it's a good indicator that something's gone wrong, and maybe I shouldn't trust this data. So, let's take a look at how we can start doing that. Here, I've started writing a function which will take one such listing and start validating all of the contents. So, I'll take a listing, and I'll start pulling out the product ID. And we've learned here not to forcibly downcast this ID to an Int, but conditionally downcast. And if the cast fails the guard will fail and will throw an error. Now I don't want to stop there, I want to perform the range validation that ensures that product ID is also valid. That it's positive and not too large. And again, if something goes wrong, I'll throw an error. Now, later on I might care to check out that URL. And again, I'll downcast it to a string, instead of forcibly downcast. And here, I can check the link. In this case, I know my server will never produce URLs that are too long. So, if I find a really long URL, I'll know that it's invalid. Once I've validated that, I can send it off to the URL type to perform that domain-specific validation to make sure it actually is a URL, and not just a garbage string. And again, if anything goes wrong, I'll throw an error. But, again, I don't want to stop there, once I have an actual URL, I want to make sure it points to my domain and not a phishing domain, so I'll keep working with that. Now, at this point, I can apply the same types of validation to the other fields here. And again, if anything goes wrong, I'll throw an error. But once I have this function, I can apply it to all the product listings that I've loaded from the payload. And again, I'll stop executing if anything goes wrong. Now, this is how we can work to validate primitive data. But as we just saw, primitive data can be very general. A string can be a string, but it can also be a date. And it can also be a URL. Sometimes we care to work with data with the semantics that matter to us. We wanted to make sure that the host of that URL was our domain, and we can't do that with just a regular string. Similarly, a dictionary can represent a model like a listing here, or it can represent arbitrary customer data that we know nothing about. Instead of performing the same validations everywhere to make sure all the fields that we care about are there, isn't it nicer to work with our own model types, where that guarantee is always present? It is. And in our case, we want to work with structured data wherever possible. We can use primitive date as a building block to get there. But we want to work with that form of data. So, let's take a look at how we can do that. Elsewhere in my app, I have a purchase type, which does just this. When a customer makes a purchase, I store that data to disc, so that later, when they open the app, even if they're not connected to the network, they can view their purchase history. Each purchase keeps track of the product listing it was associated with, and when the purchase was made, and a receipt. I can save it to disc in this way using NS coding and NS key to archiver. And I'll archive it. But as we saw, when we unarchive data, and we handle raw and primitive data, we want to validate it. So, let's do that by taking a look at how doing it with coder here could work. If you've ever written in a note with coder, this might look familiar to you. We'll start by decoding the product listing. And again, we've learned to conditionally downcast, instead of forcibly downcasting. And if something goes wrong, well this is a fail-able initializer, we'll simply return nil, right? if decoding succeeds, I'll assign this to my property, and I'll keep going. I'll do the same thing with the purchase data, and again, conditionally downcast to a date. If something goes wrong, I'll fail, and so on. And repeat this for each of the fields in my type. When I want to save one of these purchases to the history, well I have a function, which does this. It archives a purchase to binary data. And then, I can save it out to disc, or shrill that data off into database or similar. When I want to load that data back, well I can do the same thing. I can get that raw data and then pass it along to KeyedUnarchiver, to get an object back out. Now, as we've said, at every point here, the data is getting more complicated. There might be more to validate about it before we can trust it, just as well. So, you might wonder, okay, what's the catch here? What else is there left to validate? And this downcast, here is a good hint. Note that this downcast happens after we've unarchived an object. How could this ever fail? It's a good indicator that something else might be going on here. So, let's take a look at that. This is an abstract representation of what these model objects might look like in my archive. Here we have all the fields that we cared about in coding. And each of them contains their own structure, and substructure, and contents, and so on. But, interestingly here this representation also contains the name of the class of this object. Let's take a look at how KeyedUnarchiver can make use of this information. So, we have a decode call we're currently making. And this under the hood creates a KeyedUnarchiver, and decodes an object for the object key. When you perform this in KeyedUnarchiver, KeyedUnarchiver finds that class name in the object in the archive and pulls it out. And dynamically finds a class with that same name in your app. It then allocates an instance of that class and then initializes it to allow it to decode its own contents. Afterwards, it awakens the object to give it a chance to finalize its state. Now, this works great for our objects. But now, we might wonder what happens if the data is maliciously changed to contain an object of a class that we didn't expect? Well, this whole process that we just performed happened on a different type. We just allocated, initialized and awoke an object of a class that we didn't expect. What kind of effect can this have in our app? Well, as we saw before, the conditional downcast here, prevents us from accidentally using an object of this unexpected class. We're only going to work with objects of the types that we did expect. The downcast fails, well we'll fail. But decoding one such object can have a lasting impact in our app. Say that class has an alloc method, which changes global state. Maybe it allocates a singleton or changes some global data. Even though we throw the object away, if this fails, this can have a lasting impact in our app. And can cause differing behavior somewhere else and an archive can be maliciously constructed in this way to cause this to happen in our apps. So, how can we validate the data to prevent this from happening? This is exactly where the NSSecureCoding protocol comes in. NSSecureCoding is a protocol inheriting from NSCoding, whose goal is to prevent exactly this sort of attack. By allowing you to pass in type information upfront, it prevents arbitrary code execution by validating the contents of the archive to make sure it only contains the types that you expect. The hallmark introduction of NSSecureCoding were two alternative methods to decode object for key, which allow you to pass that type information upfront. And using that type information, NSKeyedUnarchiver can keep you safe. So, let's take a look at the current decode object for key call that we have. This top level decode. Now, here, if instead we use the variant that allows us to pass in the class upfront, in this case, we want to decode a purchase, instead of performing this whole process and whatever is in the archive, you can first gate it on a class check. Let's examine this class check for a moment. At every stage in decoding, if secure coding is on, NSKeyedUnarchiver maintains a list of classes, which are valid to decode. When we make such a call, NSKeyedUnarchiver, takes the object that we used, this class, and constructs an allowed class list from it. When we go ahead and decode an object from an archive, it's class is first checked against the allowed class list. And if it isn't present, the decode call will be rejected. Now, if the class of the object that we find in the archive is in the allowed class list, there's a few checks that we need to perform. Specifically, we'll need to make sure that this class itself also conforms to NSSecureCoding. If it doesn't, we can't be sure that it itself will be able to further securely decode its own contents. And so, we can't safely decode one of these objects. In our case, the purchase class will. And so, it's safe to decode and keep track of it. Now, there's two other checks that are related to superclass-subclass relationships. If you have two classes, one of which is a subclass of another, both conforming to NSCoding, and the superclass adopts NSSecureCoding conformance, the subclass will inherit that conformance. Now, the subclass may never have had a chance to rewrite its init with coder to do the secure thing. And so, we have an escape hatch here. The support secure coding method, allows you to say, actually I don't really support secure coding, and you can turn this off to say, I'm not ready yet. Alternatively, if you still say yes, we have to make sure that you either inherited the full conformance to NSSecureCoding from the superclass, or that you overrode both of the methods to indicate yes I really do support secure coding. There isn't a mismatch here. In our case, purchase meets both of these requirements and so we can safely decode it from the archive. Now, we go ahead and decode a purchase, it itself decodes a listing. And it can make use of the same type of call to indicate that it wants a listing. When it does that NSKeyedUnarchiver uses that class to build a new allowed class list. And everything is checked against this new version of the list. We go ahead and decode an object, the same checks are performed and in this case a listing is still valid to decode. But again, if we try to decode an object of an unexpected class that's not in the list, it will be rejected. Let's take a look at what that rejection might look like. And this is called decoding failure and there are a few other types of failures that we might care to look at. So, when secure coding is on, we might be able to see secure coding violations in cases like this. But there's other forms of failure here, too. For example, a type mismatch can happen if you try to decode an object and instead there's a primitive value, like an integer in the archive at that location. Or, you try to decode a primitive, like an integer, and instead we find an object or a primitive of an incompatible type like double. These can cause decoding failures. There's another form of failure that can happen here. And that's archive corruption. If the archive itself is too corrupted and doesn't follow the expected format for NSKeyedUnarchiver, well we won't be able to decode anything, and so you'll get that same sort of failure. Now, what happens on failure is decided by the decoding failure policy on the Unarchiver. There's two options here. On failure, we can either raise an exception or store information about what happened and continue execution. Raising exceptions is currently the default. So, if we have a call, again this is our call from earlier, trying to decode a listing. And we find an object of an unexpected class in the archive, under the hood this calls the failWithError method, on the Unarchiver and passes in an error that indicates what went wrong and where. Now, under the hood, failWithError has a decision to make. If the decoding failure policy is to raise exceptions, it will raise exceptions. If you're writing a Swift app, this is something you have to be mindful of. Swift can't catch Objective-C or C++ exceptions, and so this can lead to a fatal error in your app. Now again, this can lead to a crash and a bad experience our customers. If the decoding failure policy to set error and return, the error will be assigned to the Unarchiver's error property. And execution has to continue. And in this case, because execution does continue, the decode call has to return something, and so it will return nil to indicate that nothing could be decoded. And as mentioned, if you're decoding a primitive type, here and we find an object or a primitive type that's incompatible, the same series of steps has to happened. And in this case, instead of returning nil, we'll simply return 0. Now failWithError is API on NSKeyedUnarchiver, and we urge you to use in your own code to indicate when failures happen and what went wrong. Instead of simply returning nil, failWithError first to record that information. If you do, there are a few things to keep in mind. If the decoding failure policy is to set an error, and return, you have to keep in mind, that once an error is set on Unarchiver, it won't later be reset. And that's because one decoding failure often leads to a cascade of decoding failures. And we don't want to lose sight of what originally went wrong. Second, keep in mind that any given failWithError call, can either throw an exception or continue execution, so you have to keep both of those options in mind. Especially if you're working Objective-C. Maybe you can catch the exception. So, there's things to handle there. And lastly, keeping an eye out for nil, or 0 return values. This could either happen because of a decoding failure, if the decoding failure policy is to set an error and return. Or, the data could have just been missing. Or, even encoded as nil or 0. So, to disambiguate between these cases, check the error property on the Unarchiver. All right, so that was a lot of information. Let's distill this down into a checklist to see how we can adopt NSSecureCoding on our own types. We'll start by converting all decode object for key calls to the variant which allows us to pass in that type information up front. And then, if something goes wrong instead of just returning nil, let's failWithError to indicate what happened. And lastly, this is a great opportunity to audit for further points of failure, where we weren't performing validation before. So, let's do just that. So, here again is a decode call to decode a listing. And you'll notice that if we pass in that type information up front, the conditional downcast later can go away. There's a generic overload of this method, which when given the type information statically causes you to not have to conditionally downcast anymore. You'll always get an object of that class. Now, again, as we said, instead of returning nil on failure, we want to fail meaningfully. So, here we can fail with an error to indicate what went wrong and where. And in this case, we can use one of the conveniences on CocoaError to return a meaningful error that has a good localized description for our customers and that indicates what went wrong. We can always add a debug description for ourselves, to later log if we care to. But this is the core. We want to failWithError before returning nil. And then, later on, again, we were decoding the purchase date. And here, there is a great opportunity to add further validation where we weren't before. Here, if we can decode a date, well, I was simply storing the property. But instead, I want to make sure that that date is valid. For instance, a purchase couldn't possibly had been made before my app went live on the app store. So, this is something you could check. And again, if something goes wrong here, we want to fail with a meaningful error. In this case, it wasn't that the data was missing, it was that the data was corrupted or invalid in some way. And so, we'll indicate just that. Now, that we've gone ahead and done exactly this on our type, we can go ahead and claim that it supports secure coding. And lastly, conform to the NSSecureCoding protocol instead to indicate to the runtime that this is what we intended. It really does support secure coding. And after that, well, congratulations. We've earned our NSSecureCoding badge. Physical badge is sold separately. Now, we think it's so important for you to earn your own NSSecureCoding badges and use them that this year, we've added new API and NSKeyedUnarchiver to make sure that NSSecureCoding is done wherever possible. This new initializer and convenience methods, turns on secure coding by default and sets the default decoding failure policy to set error in return. So, unless you change the decoding failure policy on your own, you don't have to worry about exceptions in Swift. And indeed, the old initializer and convenience methods are deprecated in favor of these versions. So, we really want you to use them. And similarly, we've introduced the same APIs on NSKeyedUnarchiver, to make it easier for you to turn SecureCoding on when archiving. And this is equally important because it makes sure you can't accidentally archive an object which doesn't conform to NSSecureCoding. And you wouldn't later able to decode it. And again, the old initializer and convenience methods are deprecated. And so, this means that if you have old code that looks like this, well turn on SecureCoding when archiving. And switch to the convenience methods that allow you to pass in that type information upfront. This way, you can actually make use of your SecureCoding badge. Make sure you've earned it. Now, if you can't yet support SecureCoding, because your types can't conform, or they, themselves depend on types which don't yet conform, that's okay. You can still use these methods. On nCode, simply turn off the secure coding requirement. And on decode, these conveniences always have SecureCoding enabled. So, instead use the new initializer to create a KeyedUnarchiver, and then turn SecureCoding off, manually. This also gives you the opportunity to reset the decoding failure policy back to the old default in case you need it. Once you have such an Unarchiver, well you can decode as usual. Now, if you're working on a Swift app, NSSecureCoding isn't the only way for you to convert your own model types to external representations and back. Last year with Swift 4, we introduced the Codable protocols which make it easier to do just that. And importantly, all the design decisions that made their way into NSSecureCoding were also present in Codable from day one. Specifically, Codable never writes type information into archives. So, there's nothing to trust later on. By requiring you to pass in static type information upfront about what you expect to decode, you can prevent exactly this sort of attack. Now, Codable also comes with conveniences. If you have a type whose fields are all themselves codable, well you'll get a synthesized implementation of the init from and encode to requirements. And the synthesized implementation gives you type and nullability checking for free. But as we saw, most types require additional validation if they come from external sources, just like ours do. So, we need to further validate those. So, we can do that for our own types by overwriting in it with init from decoder. And in this case, again, here's that JSON response from earlier. And I can trivially turn it into a Codable type by simply creating a type with all the same name fields. Because all these fields are codable, I get that free implementation of init from and encode to. But, I want to override init from to make sure I'm performing the same validation that I was before with the primitive values. So, I can do that here, the same way. Where my old code decoded the ID from the payload by downcasting to an int, I can do similarly here by decoding an int from the decoder. If the type found in the payload is of a different type or is missing, this will throw an error that indicates what happened. Now, more important than that is my own validation that I added in that validate method. Here, I want to make sure I keep performing that same validation to make sure the ID is valid. And here, I can use a convenience method to throw a meaningful error that indicates what happened. Now, later in that validate function I might've pulled that creation date out as a string and then had to pass it along to a date formatter to get a valid date back out. Here, because we can use JSON decoder, we can simply decode a date directly, we don't have to worry about that type conversion. We can use JSON decoders date decoding policy to indicate what sort of conversion can happen here. And this is nice because this conversion became a one-liner. And, the other decode call is also a one-liner, which makes it easy for me to focus on the types of validation that I care about. Now, later on in the validate function, I might have also pulled out that substructure of tags out as an array of strings, and later had to map those strings to my own type, maybe for further validation later on. Here, again with Codable, I have the benefit of tag conforming to Codable itself. And so, I can just decode an array of tags directly. This happens automatically. And instead of focusing on the type conversion, allows me to focus on the validation that matters to me. This lets me make sure that this data is data I can trust. We talked about a lot today. We started with raw data and carried it all away across the abstraction spectrum to our own model types. And along the way we saw how to build trust into that data by validating a checksum, or the length on that raw data, we made sure that it was valid to work with an inspect, and even check whether it conforms to some format. Once we've made sure that it conforms to a known format, we knew that that formatted data was valid to work with and pull primitive values out of. By validating the contents and structure of those primitive values, we made sure that it was valid to work with them to turn them into our own model types. By validating the semantics and relationships between those model types, we made sure that they were valid to work with and that they were data we could trust. So, where to go from here? Well, now that you know all of this, I encourage you to go and look at your code and do just this. Validate data at every stage in the lifecycle of that data. Check for types and nullability. But more importantly, range, and length, and domain-specific applications. If you have NSCoding types, audit those types and earn your NSSecureCoding badge. And don't just earn it, use it. Turn on secure coding wherever you can. And lastly, for new data types, adopt Codable where possible and make sure to perform that validation. Make sure you're only ever working with data you can trust. For more information about Codable, specifically, see last year's talk about What's New in Foundation. But if you have more questions about this topic or want to come to us for help on how to apply this in your own apps, come to our lab later today, we have a foundation lab, where we can help you do just that. With that, I'd like to say thank you very much. Have a great day.  Good morning, everyone. Thank you so much for coming to our session. My name is Sara Radi. I'm a software engineer in the Localization Team at Apple and today we'll be joined by my colleagues, Vivian and Chris, to talk about new location workflows with Xcode 10. So first I would like to start with a quick overview of how the localization process used to work before Xcode 10. Then I will introduce the Xcode Localization Catalog, which is the new format for exports and imports in your localizations in Xcode 10. Finally, Chris will talk about how to localize Siri Shortcuts by using an Intent definition file which is a new file format we introduced this year as well in Xcode 10. So let's get started with a quick overview. So before Xcode 10, if you had a project that contains localizations that your application supports, we look for strings base localizable resources in your project. So these strings can be defined in your source code, your storyboard files, your strings files, or your strings dictionaries. So once we identify these resources, we extract strings from them and we export them in an XLIFF format. So XLIFF is an industry standard and most localizers are already familiar with. So that's what you will be sending over for translations. So once you get your XLIFF files back translated on imports, we add them to localizable resources in your Xcode projects with the new translations that are provided in the XLIFF files. So that is pretty straightforward process. Now, the XLIFF format provides a lot of benefits for you as a developer. First, it helps you to extract the localization work from your code so you don't have to make any language assumptions in your source code. XLIFF files also holds the content to be localized from your development language along with the translations so your translators can do their work right in the XLIFF files that you have provided. In addition to that, instead of handing over multiple file types and keep track of them, XLIFF is a really nice tool that consolidates strings from different resources and different file types into one single document. But what XLIFF does not provide your localizers, it does not give them that visual and functional context. It does not provide the resource data like assets in your project. For example, a storyboard file will be very helpful for your localizers to picture how your UI is laid out in your development language. It also does not provide with the custom metadata about the generated XLIFF. And finally, the XLIFF we generate today does not have size and length restrictions attributes and that might be very helpful for your localizers to know if a given string will be showing on a Watch screen or an iPhone screen or an iPad screen to make decisions about the length of the translation. So why is context important for your localizers? So if we take a look at this example from a travel app that we will be showing you later in our demo, so if you look at the string that says "Book" on the right corner of the Navigation bar, it's kind of clear to all of us here in the room that "book" refers to something related to travels because we have that visual context right in front of us that says start day and dates, number of travelers and the price. But if you send your translators an XLIFF files, the only thing they see there is just the string "Book" and that don't have that visual context that we were looking at earlier. So Book can refer to two different things. It can be a noun in the context of reading a book or it can be a verb in the context of booking a hotel room somewhere or a flight ticket or travel in general. I actually asked our Russian translator if he can translate the word "book" for me without giving him any additional context and he came up with all these different options. So without giving him that extra context, it would be very difficult for him to decide which string to show in the UI for Russian users, for example. Context can also be important for your localizers to decide about the length of the translations they are using. So if we take a look at this example, so I have this phrase that says, "Booking confirmed" in English and it fits just fine on a Watch screen, but in French, for example, it truncates. So first of all, when it truncates, it doesn't look that nice in the UI and sometimes the truncation can change the meaning of the word. It can even be offensive for your users. It can be like funny, like in this example, like the phase does not say that your booking was confirmed anymore. It says our booking was stupid which is not what I really wanted. So we see that context is very important to provide to your customers or to your localizers to ship high-quality localization. So now I would like to introduce the Xcode Localization Catalog, which is the new format for exporting and importing your localizations in Xcode 10. So what is an Xcode Localization Catalog? It is a new type of localization artifact that has that .xcloc extension. It is produced by the export localization command in Xcode, so it's the same command that used to export XLIFF files before Xcode 10. And it is consumed by the import localization command. And the main thing about the Localization Catalog is that it supports all of your localizable assets, which means anything that you mark as localizable in Xcode projects, it will be supported by Localization Catalog and that is beyond strings files. Another important thing about the Localization Catalog is that it provides that additional contextual information for your localizers. So we built the Xcode Localization Catalog on top of the XLIFF format. So if we take a look at our example earlier, so all of our strings base localizable resources will be extracted in an XLIFF format and anything else that you have marked as localizable in your project will be supported by the Localization Catalog and exported in an xcloc along with the XLIFF files that contains the strings. So let's take a look closely at what is inside a Localization Catalog. So inside your xcloc, you will find a contents.json file, which contains metadata about the exported Localization Catalog. So it has information like the development language of your application, the target language of the exported Localization Catalog. It has information about the tool that generated that Localization Catalog. In this case, it's Xcode, so you will find information like the version number and the build number of Xcode. And finally, it has the version number of the exported Localization Catalog. Next inside the generated xcloc, you will find Localized Contents directory. And Localized Contents contains all of your localizable resources in the project and this is the main directory where your localizers will be doing their work in. So inside Localized Contents, as I mentioned, you will find an XLIFF document that has all the project localized strings as well as non-strings localizable assets like images, for example, if you want to localize those. So those assets will be organized into the same file system hierarchy as your Xcode projects inside the Localization Catalog. And your localizers can also use Localized Content to override any resources specifically for the language like interface builder files, for example. Next, inside the Localization Catalog, we generate a Source Contents directory and this is mainly provided for context. So Source Contents contains assets that were used to produce the Localized Contents like storyboards for example that will be helpful for your localizers to see how your UI is laid out in your development language so they can make decisions about the length of the translations. So even if your localized strings are coming from your source code, we do not generate your source code inside the Source Contents. So those assets that are mainly provided for context will be also organized into the same file system hierarchy are your Xcode project inside the Localization Catalog and, as I mentioned, this is mainly provided for context even if your localizers go and change your source context on import, Xcode will just ignore them and will not import these resources back into your projects. Finally, inside your Localization Catalog, you will find a Notes directory and this is mainly provided for you to give any additional contextual information for your localizers. That can be screenshots data from your UI Attachments for example. It can be a Readme file or a movie that explains information about your app or anything really you think that will be important for your localizers. So that was the Xcode Localization Catalog structure. And we have been also working on updating the Xcode build command line tools. So now on exports, we generate an Xcode Localization Catalog. And on imports, the imports command is backwards compatible. So if you still have those old XLIFF files, you can still important them back into your project or you can import back the new Localization Catalog. And with that, I would like to hand it over to Vivian to give you a demonstration about the Localization Catalog. Thank you so much. Thanks, Sara. Alright, so we've been working on a travel app and what makes this special is it's the first app to let you plan vacations to other planets. So we call it Vacation Planet. Let's take a look. Alright, so on the splash screen we have this lovely logo with the name of our app. And I'm just going to click Browse here. Alright, so we can book a flight, an expedition, a cruise, or an entire vacation package. Let's go to Mars. Okay, now I need to choose where on Mars to go and I think we should check in on the NASA Curiosity Rover. So let's go to Gale Creator. We should definitely go after the conference. And let's go for a month. It's a really fast spaceship. I'm going to take Sara and Chris with me, so three travelers. That's a really cheap trip to Mars. Alright, here's that Book button you saw earlier. And great, our booking is confirmed. Now, of course, there's people all over Earth who would love to visit Mars and they don't all use their iOS devices in English. So we're going to need to localize the app. Let's go into Xcode and do that. Alright, so I just need to select my project here. And under Info, under Localizations, click Plus, and we're going to add French for starters. Okay. So my storyboards are localizable by default, so they're already checked. And a have a localizable stringsdict that I also already marked localizable. So I just need to click Finish. Alright, at this point, Xcode has created the fr.lproj in my project and for by storyboards has created strings files with copies of my English strings. So also copy my image because I have not marked it localizable. So let's do that. Over the right, just lick Localize. There's no content shared between the languages in this file. So I need to put it in English because that's my development language. Click Localize and check French to make that copy. Alright, there it is. Okay, now I'm ready to export for localization. I'll just select my project, go to Editor, Export for Localization. Alright, I'm going to call this vacationplanetloc, so I don't have to type out localization, and double check that I'm exporting for French and click Save. So now Xcode generates that Xcode Localization Catalog for me and we can go into Finder and look at it. Alright, here we go. Here is my French Xcode Localization Catalog with that .xcloc extension. And inside, as Sara said, there's the contents.json with metadata about my Xcode Localization Catalog. Next, we have Localized Contents. So in here, I have an XLIFF, the same as if I had used Xcode 9. You can see here it has all of my strings and here we can see highlighted in red the untranslated copies of the English. And also in Localized Contents mimicking the file hierarchy of my project, I have that logo image. So next let's look at Source Contents. This likewise mimics the file hierarchy of my Xcode project. So I have my storyboards that are in base and I have the English for my info plist strings, my localizable.strings for the strings defined in code, my localizable stringsdict and the English version of the image. So the last part of Xcode Localization Catalog is the Notes folder. And Xcode doesn't put anything in here, but yesterday, I ran my XCUI tests, which are a really easy way to create screenshots. And all my screenshots are saved as attachments to my test. So we can go back and look at my test results, see them. So for instance, I have my splash screen added as an attachment here. There we go. Alright, so I'm just going to pull these up in Finder and copy them into the Notes folder. So select Show in Finder. Alright, Show Package Contents, look at Attachments. There they are. Copy and Paste. And I think this Travel Details one is going to be particularly helpful because it has that troublesome Book button in it. Alright, now I'm ready to send my entire Xcode Localization Catalog off to my localizers. Now the great thing is they're really fast and they're already done. So we can go ahead and import those translations. Just going to go back to my project. Make sure I have the project selected. And as before, go to Editor and this time choose Import Localizations. Okay? So I'm going to pick my Return to Xcode Localization Catalog. Click Open. And now I get a comparison view the same as in Xcode 9 that shows me any warnings, errors, and changes regarding my strings. So here, it's just warning me I didn't get any translations for my infoplist.strings. But that's okay. I didn't ask for those translations, so I'm just going to ignore this warning. I can also check on what's changing. So here if we look at the storyboard, we can see I'm getting real French translations to replace those copies of the English in the strings. So I'm just going to click Import. And just like that, all of the translations for my strings and non-strings resources are in the right place in my project. Alright, so let's double check on that. I'll just do spot check of a couple files. Okay, main.strings. That looks like French to me. I don't actually speak it. Let's check the image. Yes, that's also in French. Okay, so let's run through the app one more time in French and make sure everything looks good. So to do that, I'm going to select my Active Scheme, choose Edit Scheme. Set the application language to French and the application region to France. Now I'm ready to run. We're going to stop the English from running. Okay? So in just a moment here, we should get our app entirely in French. Okay, looks good so far. I'm just going to go through really fast so I can go check on that Book button. Alright, Browse, let's go to Mars again. I really like Gale Creator. Okay, great. That Book button was translated correctly. It was probably really helpful that I included screenshot of this part. If we check on the dates here, alright, I did not have to send a list of months to my translators. Instead, I used this nice date picker that in turn uses a formatter and I got these localizations for free. And likewise, by using a formatter for the price, I got the correct number formatting and the price given in Euros. Alright, so that was super easy to get my strings and non-string resources all in French as easily as I would've done with just the string resources in Xcode 9. So with that, let's go back to the slides and wrap up our discussion of Xcode Localization Catalog. Alright, so we have introduced the Xcode Localization Catalog as Xcode's new standard for localization export and import. It provides more flexibility for your translators and allows you to conveniently provide more visual and functional context to better help them generate high-quality translations the first time. It has increased file type support, so you can export and import your non-string resources as easily as you've been doing with your string based resources. We've also updated Xcode build to use the Xcode Localization Catalog for continuous integration support. And if you have content out for localization right now, don't worry, you can still import those XLIFFs when they come back to you both Xcode and Xcode Build are backwards compatible. Alright, so with that, I'm going to hand it over to Chris to talk to you about localizing intent definition files for the creation of Siri Shortcuts. Thank you, Vivian. Wow, that's really awesome. I'm really looking forward to that trip to Mars. So I'm going to be showing you how to localize Siri Shortcuts by localizing your intent definitions. And your intent definitions are created in intent definition files in Xcode. And these are what you use to define your custom intents for Siri Shortcuts. They're just files with an intent definition extension that have a custom editor in Xcode. And we've designed them to take advantage of base localization so when you go to translate your intents, you don't have a bunch of different intent definition files, one per language, you just use strings files to localize the strings that are in your intent definitions. And, of course, we also support stringsdict files for a variety of cases like pluralization rules and variable-width strings. And if you haven't yet, I strongly encourage you to check out the Introduction to Siri Shortcuts Session that happened yesterday and that's available for streaming. Now let's take a quick walk through the Intent Definition Editor. Here we have an intent definition for our Vacation Planet application and we have an intent for booking a vacation. Of course, one of the things that you'll want to localize in your intent is the title and the description of the intent itself. But most of what you're going to be localizing for your intent definition are the shortcut types that are composed together from the intents parameters. And of course, when an intent is invoked, it produces a response as well and these can also be customized with Response Templates and these Response Templates are localized in exactly the same way as your shortcut types. Now let's take a deeper look at the actual structure of an intent and what's localizable within it. An intent is really a collection of parameters that have a variety of types. So they can have types like integer or string and you saw yesterday that they also support custom objects and they support enumerations that are defined right in your intent definition. And these parameters are combined together into different shortcut types that have a nice human readable title. And of course, responses are also collections of properties that have well understood types again like string or integer and can also have types from your enumerations. And when you go to localize your intent definition, the things that you're going to be localizing the most are your shortcut types and your response templates. And for the most part, what we're going to talk about today is how to localize your shortcut types because you localize your response templates in exactly the same way. So here's an example from our Vacation Planet app. This is the shortcut type for booking a trip and that type of trip there is actually a placeholder for one of our enumerations. This enumeration lets you book a flight or it lets you book a cruise, or it lets you book a vacation package. And when Xcode goes to localize your intent definition file, when you press that localize button that Vivian showed you, what's going to happen is Xcode will produce one each of its strings for the members of that enumeration. And that happens to work because of the particular words we chose. Because our template says "Book a flight" and uses that indefinite article, we can or "Book a type" and uses that indefinite article, we can book a flight, book a cruise, or book a vacation package because they all start with consonants because that's the rule in English. But what happens if I add a new value to my enumeration to represent booking an expedition, say to Mars? Well, Xcode is just going to generate in its strings file "Book a expedition" which isn't really very grammatical. Fortunately, though, these strings files are keyed by well-defined IDs and stable IDs so you can just go into the strings file and change it so that it's properly grammatical. Now English isn't the only language that has this kind of grammatical agreement. Different languages, of course, have different grammatical agreement. So you'll need to do the same kind of thing say in French, where you'll need to make the gender of the article agree with the gender of the noun that it's referring to. Now let's take a quick look at a more complex example. Here we have a shortcut type for booking a vacation for some number of people and once again, when generating that strings file, Xcode, well, it's going to combine together the template with the values from the enumeration. However, it's not going to generate one string for every possible integer as well. Instead, it's going to leave the strings that it generates with a placeholder in them that the shortcut system will replace at runtime. And once again, this looks like it should be reasonable. Right? it says, you know, book a vacation or book a flight for some number of people. And in our strings file, we get that generated ID and that string that says, oh, book a cruise for some number of people with that placeholder still in there; however, when I go to book a cruise for just myself to celebrate a great WWDC, what's going to be shown as an actual shortcut is the string "Book a cruise for 1 people" which isn't grammatical English. However, these support stringsdict files as well so you can do all of the pluralization that you need and have the system take care of it for you. Here in the stringsdict file, we have the dictionary itself keyed by that string that I showed you from the strings file. You can either use that or you can use the ID as the key. And then we've produced another string that is what is actually going to be shown in the user interface. And here, we're referring to another variable which can then be substituted for either the one or other cases based on English's pluralization rules. And to learn more about stringsdict files, you should see our session from last year Localizing with Xcode 9 that's available in the WWDC application. Here, if I go to book my cruise now, this operating system will say, oh, I'm booking for one person, so let's use the case for one and I get a properly grammatical phrase. And similarly, if I want to book a cruise for all of us, you know, after all I'm not the only one up here, the system will say "Book a cruise for 3 people." Now let's talk about intents that are created in code. If you're creating an intent in code to donate it to the system, that isn't necessarily the point at which a shortcut will be displayed. That can be displayed far off at some point in the future based on the user's actions. So we have a new API that creates a deferred localizable string that will be localized at the point where the string is presented to the user. And this is necessary because donating an intent to the system is really telling the system, hey, the user performed some action in my application that you might want to offer them again at some other point. And, fortunately, this API is extremely simple to use. It's just one call added to NS String that you can use to wrap the string that you're going to assign to that intent you created in code. So let's talk quickly about what we've seen. We've seen that Xcode combines your shortcut types with the enumerations that you've defined in your intent definition when it's generating the strings files. And you can use both strings and stringsdict files to localize your shortcuts. And if you're creating an intent in code to donate it to the system, to inform the system of a user action that might be repeated, you can use deferred Localized Intents String to wrap that string in such a way that it'll be localized when it's presented to the user, not at the point where it's created in your application. So if the user changes their phone's language in between when you've donated that intent, and when a shortcut is displayed, it still follows the user's phone language. Now when you define strings in a stringsdict file, they are actually going to be preferred at runtime. That means you can do the majority of your localization in your strings files for all of the cases where you don't need things like plural agreement and then you can just provide a strings file or a stringsdict file with the same name as your intent definition as an override. And when you create that stringsdict file, you can use either the strings themselves or the strings IDs that Xcode assigns as keys in your strings file as the Localized String Key and string somewhat measure of stability in case you actually decide to change things in your intent definition. So now, I'd like to bring Vivian back up to show you exactly how to do this, how this whole process works in Xcode. Thanks, Chris. Alright, so once you go to Mars once, you're probably going to want to go again and again and again because those trips are really cheap, as we saw. And it would be nice if our app could suggest additional trips to these frequent fliers and give them a shortcut to book the next vacation. So with that in mind, I've added an intent definition to our project. Let's go look at it. Okay, here's our intent definition. So we have a custom intent called "Book Item," has a title, a description, and down here is the actual suggestion. So it is book a type for travelers, people. Like Chris said, it's a combination of parameters. So we can see travelers is an integer and type is defined in this enum here, so we can see it can be a flight, a cruise, a vacation package, or an expedition. Now if our user follows through with a suggestion and books another trip, we need to give them some kind of feedback. So for that we have a response and there's just a success or a failure message as to whether we could book their trip. And this is likewise a combination of parameters. Now, of course, since we have user facing strings, we need to localize them. So let's do that. Just like I did for the image, I just go to the right here, click Localize. Now intent definition files are going to be like interface builder files where we have one intent definition and all of our translations in strings files. So I want to put it in base. Click Localize. Okay. And I want to check French. Okay, so at this point, Xcode has created a strings file for the French translations to be put in. And here we go. Alright, and you can see as I scroll through, it has every possible string given the possible values for the parameters defined in enums. This is great because, again, I don't speak French but I know from the slides that flight and cruise have different grammatical gender, they're going to need different forms of the word for "a." Alright, but there's still some grammar problems I can fix and those are the ones in English, the case of one person and an expedition. So to do that, I need, or at least to fix the plural case, I'm going to add a stringsdict file with the same name as my intent definition. So just go over here to my project. Create new file. And I'm going to use the stringsdict template that was added in Xcode 9. Click Next. Alright, here's the important part. I need to make sure I get the name exactly right. Intents. Okay. Create. Now I don't need to go into my French strings file to find the ID for my intent. I can actually go into the intent definition file because these are required to be unique. I can actually copy directly from here, alright, copy, and go my stringsdict, and paste. Alright, you can see here Xcode actually formatted the parameters for me when I pasted so that this is already ready for Siri Shortcuts. Okay, so at this point, I'm ready to fill in the rest of my plural dictionary. I'm only going to need one and other because I'm doing English. So I have singular and plural. But like any good cooking show, I have the finished file ready. So I'm going to swap that in. Alright, say goodbye to this guy. And let's go find the finished one. There we go. Yes, Xcode, I want to make a copy. There we go. Alright, let's look at it. So we're going to look at a case where I fixed two problems at once. So if we check Book Expedition, the string here is "Book a expedition" but then in the plural dictionary, I have the case of one person or any other number of people. What I also did here was fix that a/an situation so the string is actually displayed we'll use an. Okay. So I have the finished version of the app running on my phone. So let's run through it and check the shortcut that's created. Alright, I'm just going to go to the same place as before. I really like Gale Creator. Okay, Browse, Mars, Gale Creator, and let's not go for a day. We'll go -- Sure, that'll work. Okay, this time I'm going to go by myself because I want to check the one case. There. When I tap Book, that actually created the shortcut and donated it. So now we can go look at it. you can find these under Settings, Siri & Search. There. Book an expedition for one person. Alright, so this is great for English. I fixed it. but I'm still going to need to localize this into French. And the great thing is, it's just as easy as the first demo. So we can go back to Xcode. Alright. I'm going to make sure I localize my stringsdict because I didn't do that yet. English, yes. Check French. Otherwise, it won't be exported. Okay, so now I just select my project again. Go to Editor. Export for Localization. I'm going to call this vacation plan loc2 since it's the second round of localization. Double check, yes, I'm doing French again. Click Save. And now Xcode creates another Xcode Localization Catalog. So if we go look at it, okay, let's check localized contents. If we look at the XLIFF, you can see here highlighted in red are all the new strings from the intent definition but we also have all the translations I imported the first time. So you can have an incremental localization process where you develop your apps sum, you get that version localized, you make some tweaks for version two, and you don't lose anything when you export for another round of localization. And this works for the non-string resources as well. So this will have the French version of my image. Great. So I can develop my app, keep localizing, keep working and it's all just as easy as the first time and it covers both my strings and my non-string resources and my intent definitions so that I can suggest additional trips to Mars. Alright, so with that, I'm going to turn it back over to Chris for the last time to summarize. Thank you again, Vivian. Wow, that's really great. I'm really looking forward to those trips now. So as you've seen, Xcode Localization Catalogs are our new standard in Xcode for localization. And these support all of the localizable resources in your project, not just the strings based resources. And they also provide flexibility to provide context to localizers, whether that's putting in screenshots from your tests or putting in readme files or movies of the use of your application, you can give your localizers the context that they need to ensure you get exactly the right translations the first time. And you've also seen that Siri Shortcuts are easy to localize by localizing your intent definition files, that these use base localization with strings files and that you can use stringsdict files where appropriate to handle things like pluralization rules in a variety of languages, and that when you're creating an intent to donate in code, you can use deferred localized intent strings to ensure that a shortcut that's created from that donated intent is localized at the time that it's displayed to the user, rather than at the point where you donated it. For more information, you can see our session page on the WWDC website. Thank you again so much for coming and I hope you continue to have a great WWDC.  Good afternoon. My name is Ali Ozer. I, with my colleagues, Chris Dreessen and Jesse Donaldson, will be talking to you about what's new in Cocoa in macOS Mojave. As you saw yesterday, we have some amazing new features in AppKit this year. We'll be touching up on many topics, among which are these. We'll be talking about some of the API refinements we've been doing. The amazing new Dark Mode, and related features, some changes in layer backing, and also custom Quick Actions. So, let's dive right in. Now, as you know, we pride ourselves on our API's, and our goal always, is to have API's that are capable, and consistent, and powerful. So, with that in mind, we continue to do refinements that improve both the Objective-C and Swift exposure of our API's. And, this is not just in AppKit or Foundation, but also other frameworks including UIKit, as you might have heard in this morning's "What's New in Cocoa Touch." Now, the API changes we're doing are fully source compatible in Objective-C, and we also expect them to be 100% migratable in Swift by the time we GM the SDK. So, with that, let's look at some of the API refinements within this release. First I'm going to talk about some updates to string types. Last release, we introduced string enumeration types as a way to collect groups of related string constants together. These helped make API's that deal in these types much clearer. And, here are some examples. The first one is declared as NS STRING ENUM. This is used for string enumerations, where we provide some values out of the frameworks with no ability to extend them, so it's a fixed set of values. The next two here, NS EXTENSIBLE STRING ENUM, this is used to declare string enumerations where we might provide some values out of the box, but other frameworks and applications can also add to that set. So, we've done two changes here. First one is a simple one. We've simply replaced NS STRING ENUM and NS STRING EXTENSIBLE ENUM with their typed variants. This is effectively a no-op change. These are just more general forms of the same declarations. So, no changes in your code, or call sites, or anything. Now, the next one, NSImageName, underwent a bigger change, a more significant change, instead of a string enumeration, it's now declared as NS SWIFT BRIDGED TYPEDEF, which is effectively a typedef. Now, here's what the Swift exposure of this looks like. In Swift 4, NSImage.Name came across as a struct, which is the way you would declare the string enumerations. In Swift 4.2, it's as a typealias, a simple, good old, garden variety typealias. Much simpler. So, the question is, why did we do this? Let's look at a call site example. Here in Swift 4 is how you would take a string, and create an NSImage with it, by using the named method. As you'll see here, you'll be taking the string, and we're converting it to an NSImage.Name into the [inaudible] name before we call NSImage named. This does not feel super-streamlined to have to repeat NSImage.Name here. Now, with changes in Swift 4.2, this is all we have to write. You do not have to convert to the NSImage name, which is more streamlined, a little cleaner, less redundancy there. So, we believe this typedef approach is appropriate for passed-through values. Yes, we had heard from some of you. So, we believe this appropriate for passed-through values, such as resource names or identifiers. Basically, values that are not interpreted by the framework but are just used in a passed-through fashion. So, image name, color name, window frame autosave names, and so on. So, these are the types that are appropriate for this. Now, note that you still have the benefits of a specific API declaration however with this new approach. Here's the declaration for NSImage named method. You'll note that the argument is still NSImage.Name, as opposed to a string. So, we still have that come through. Now, here's the full list of types. We did this to NSAppKit. Turns out a lot of types could benefit from this. It's not just this, it's also this set as well. So, a number of types have changed in this fashion. So, next, I'm going to talk about common prefixes. As you've seen in previous years, over time, we've been switching from common suffixes, which is what we used to to many years ago, to common prefixes in Objective-C. Using common prefixes enables names to group together, and become more easily discoverable, and come across better in Swift. So, let's look at an example. Here is NSLineJoinStyle as it appears in the 10.13 SDK. And, here is how it appears in 10.14. You'll note that enumeration values such as MiterLineJoinStyle now have become LineJoinStyleMiter. So, a common prefix. The Swift exposure changes from miterLineJoinStyle to just miter. So, it's-- you know, you don't have to repeat the type any more; it's pretty obvious in the call site, so much cleaner. And, so good it deserves a happy emoji. [ Scattered Applause ] Thank you. And, we did this to a number of other types that we had not done, applied this change to, and here is that list of types. Next, I want to talk about formalized protocols. In the olden days, we used to use informal protocols, which are basically categories on NSObject to group related methods together. And, since then, we added features such as optional methods on protocols and so on, and we've been switching to formal protocols where possible. And, I'll show you an example of one of the ones we did this release. Here is the method validateMenuItem, and it used to be an informal protocol, a categorization object in 10.13. Now, it's a formal protocol, called NSMenuItemValidation, with .method in it. The Swift exposure changes from an extension NSObject to a formal protocol, of course. NSMenuItemValidation in Swift 4.2. Of course, the benefits here are that objects that do menu item validation now have a way to formally declare that they do that by conforming to this protocol. Again, we like this so much, we did it across a bunch of other API's. So, here's the full list of formal protocols we added in AppKit. You'll notice things like color changing, font changing, NSEditor, NSEditorRegistration combines the bindings-related methods, and so on. So, it's a good list of new, formal protocols. Next, I want to talk about direct instance variable access. Now, most-- in our API's almost all of the instance variables are private. And, we've said so, but in a way that they have been declared, especially in some older AppKit classes, subclasses were allowed to touch the instance variables, directly access those instance variables. Now, some of you may not even be aware of this, so please don't go ahead and start using them, because this is probably an old code, code I'm sure you didn't write, but maybe inherited, that may be using instance variables directly. So, for now, we are going to be frowning upon this practice a bit more vigorously by deprecating it. Now, you'll be-- code that accesses instance variables directly will get a warning, and our intent is to break this in future updates. So, as you get the chance, please go ahead and clean these usages. And, the fix is pretty straightforward. Instead of accessing the instance variable directly, please go ahead and call the getter, or the property accessed, or whatever there might be. And, if you have some reason to access the instance variable, and you don't see a way around it, you might want to let us know. Now, speaking of deprecation, we're doing one more thing called formal soft deprecation. So, over the years we have deprecated a lot of API's, and have replaced them with better ones. In cases where the deprecation isn't urgent, we usually go through an informal deprecation phase, where we tell you the API's deprecated, we release note it, we comment it, and so on, before we actually mark the API's deprecated. Usually to reduce disruption. But now, we have a way to mark API's as to be formally deprecated. Let me give you an example. Here we have a symbol, NSBoxOldStyle, which of course happens to be a name that begs to be deprecated. And, you'll note that we've marked it as deprecated. And, the version number for the deprecation is API TO BE DEPRECATED. So, what this does is, it tells the compiler not to generate a deprecation warning or an error, however our intent is that if you try to use the symbol in Xcode, or new code, or in access documentation, you will get a warning that the symbol is deprecated, and it will be pointed at the replacement symbol. So, of course, comes across in Swift as well. As you can see here, one thing to note, the version number 100,000. This is not a pre-announcement, or a leak of some far, future SDK. It's just that's a placeholder number we're using for now to indicate this feature. Now, we use formal soft deprecation in a few other cases. Earlier I showed you this. And, I told you that we renamed MiterLineJoinStyle to by LineJoinStyleMiter. And, I also said that Objective-C source code was 100% compatible. So, you might be wondering, well, what happened to that old symbol that you renamed? Well, we actually declared that old symbol, as you see here, by using this new API TO BE DEPRECATED. So, we declare it as deprecated, as API TO BE DEPRECATED, meaning any new attempts to use it will get warnings, but existing uses will be left alone, because we really don't want to disrupt uses of the symbol in Objective-C source code. Now, turns out there was a lot of symbols that were waiting for this facility, so a lot of API's are marked with API TO BE DEPRECATED. A bunch of these are because we did the common suffix to common prefix naming, and some of the others are symbols that we are de-emphasizing deprecating, because we're bringing new ones in. Especially in support of features such as Dark Mode, which you'll hear about later today. So, the last topic I want to talk about is secure coding. As you may be aware, we introduced the concept of secure coding back in 10.8 and iOS 6. It basically allows class-- when you are archiving, basically allows you to specify what classes are expected, that way it can be an explicit error if those classes are not encountered in the archive. Now, the way we did secure coding, the secure coding was an optional feature. But, we now have new API's that enable both secure coding as a default behavior, and as a bonus, they enable error returns. Our archiver and unarchiver API's worked with exceptions, but of course we prefer error returns. And, the new API's enabled error return behaviors by default. So, I'll show you the API's NSKeyedUnarchiver, since that's the most interesting. Here is an NSKeyedUnarchiver. One new method is in it. It simply creates a keyedUnarchiver, securely, and in a way will return errors. Two other new methods are these convenience API's, unarchivedObject(ofClasses from, and unarchivedObject(ofClass from. These basically unarchive a single object and return it. They do it securely, and they will return an error if some problem's encountered. Now, note the second method here. It's sort of decorated like a crazy fancy peacock. All that decoration enables Swift to infer the return type much better, which is of course a trick Swift is really good at. Now, note that all the way out at this SDK this year, they do work back to 10.13, and iOS 11. So, you can start using them, even with those deployment targets. These methods replace the methods on this slide here. Now, you'll note that these are being deprecated in 10.14 and also iOS 12. Since these are not doing secure coding, we are deprecating them immediately, rather than going through that formal soft deprecation, because we really encourage you, we really want you to switch to the secure coding if you haven't done so yet. Now, one more thing about secure coding is a new value transformer. As you may know, NSValueTransformer is a class used for automatically transforming values from one to another. These two valueTransformers here-- unarchiveFromData and keyedUnarchivedFromData-- the first one does unkeyed archiving, the second one does keyed archiving, but not securely. And so, these are now not working in the way we like, so we're deprecating these two. And, replacing them with this new secure Unarchive FromDataTransformerName, which will do the unarchiving securely. So, we urge you to switch to this one as well. Now, on the secure coding front, we've also gone ahead and adopted secure coding in a number of AppKit classes that didn't do it yet. Note here, NSAppearance, which is a recent, relatively recent class that is increasingly becoming your friend, as you'll see in later talks about Dark Mode and other features we've added in AppKit. We've also added secure coding to a few foundation API's that did not have it. And, here is that list. Now, one more note on secure coding. We have a talk Thursday morning, "Data You Can Trust," where we'll talk about doing coding and archiving and unarchiving in a robust and secure fashion. So, I invite you to attend that. Thursday morning at 9. So, at this point, the rest of the talk is going to be about the new features in AppKit and related areas. And, to kick that off I invite Chris on stage. Thanks, Ali. So, Dark Mode is one of the most exciting new features in macOS 10.14. And, I'm sure you all saw yesterday, I bet some of you are running it right now. But, let's go ahead and take a look. So, we have these great new sets of system artwork. It makes our system UI look great. It makes your application look great. It's going to make your user content look great. And, what we all want to know is what we need to do to adopt this. So, the first step is really simple. We need to relink against the macOS 10.14 SDK. That's easy enough. That might be enough for some of us, too, but most of us are going to need to do a little bit more work to make an app that looks great. So, the next thing we're going to want to do, is we're going to search our application for places we've hardcoded color values. And, we're going to want to replace them with an appearance-sensitive color instead. For, most system UI elements, AppKit actually offers a lot of dynamic system colors that will react with the current appearance, and look great for whatever UI element you're trying to come across with. And, to flesh out the list, we've added even more in macOS 10.14. But, in some cases you're not trying to make a system UI element, you're trying to make some piece of your document model that also looks great in Dark Mode. Now, you can do this by sorting your colors in asset catalogs. So, if you go to the color editor in Xcode, you can configure which appearances you'd like to set up specific colors for, using the sidebar on the right. In this case, we've picked colors explicitly for the light appearance, for the dark appearance, and a general fallback color for any other appearances. Similar to with colors, we're going to want to go through our UI's and find places we can use template images. Template images are great because the image artwork will be tinted with the right color for whatever appearance we're using. And, you might have been skating by with places in your app where you included a dark gray piece of artwork, or solid black artwork, which looked fine in light mode, and is going to look absolutely wrong in Dark Mode. So, you can make template images programatically. You can also set them up in your asset catalog. But, you don't need to limit yourself to template images to make your Dark Mode UI. You can also specify colors-- or, sorry, images that look different in Dark Mode. In this case, for our planet app, we decided that we wanted nighttime view of North America more in Dark Mode, but for other appearances, we're going to use a daytime view. So, something that's really great about Dark Mode is how we handle desktop pictures. And, let me show you what I mean by this. If you take a look at the system preferences UI, it kind of looks like it's just a dark gray at first glance, but it's more complicated than that. If we look behind the window, we can see that we have these gorgeous sand dunes, and there's a lot of blues and light and dark grays in there. And, if we do the masked pick, an average color for this rectangle, we wind up with this nice dark blue color instead. So, when we construct our UI and add back in this gray color, it's not solid gray. We're keeping that dark blue color with us. And, this permeates even when we add back the individual controls in the window. They all have this nice character from the desktop picture. So, let me show you what this looks like with a different desktop picture, in this case, a flower. You can see we have these much brighter purples and greens in this desktop picture. And, that affects the system preferences window here. Likewise, if we used a red flower instead, you can really see in this case, how the System Preferences window is taking that wonderful warm character from the desktop picture and propagating it to all of the UI elements. So, a really common thing you might be wondering with this is, that sounds like a lot of work to dynamically figure out where a window is, what the average color is, and you know, update it live. So, my first advice to you is don't be daunted by this task. AppKit is going to help you. So, there's some great classes you're already familiar with that are just going to do the right thing out of the box. And, it's Window, and it's ScrollView, and it's TableView, and it's CollectionView. All of these will look great in Dark Mode without any changes. But, if you want to get your hands on them, you can also tweak these a little bit. Each of these classes has a background color property. And, there's four very special NS colors I want to mention, the control background color, the window background color, and the underpage and text background colors. And these all, when used with these classes, get that nice bit of desktop picture infusion, and all look slightly different depending on the role of the UI element you're trying to construct. One other class I really want to call out for this purpose is the NSBox class. If you configure a box as a custom style, you can use its fill color property with one of these special NS colors, or really any other NS color, too. But, that's significant, because NSBox can be used to just add wonderful color fills to pieces of your UI, whereas these other classes are a little bit more special case. So, if you really want to get into detail on this, there's another AppKit class I want to mention, which is NSVisualEffectView. And, NSVisualEffectView has this material property that allows you to determine how the visual effect you use is going to process the background behind it, and what sort of blending operations it's going to do. And, we have a few of these to describe where the visualEffectView's being used in your UI. In macOS 10.14, we've added a lot more. So, pretty much whatever sort of UI you're trying to construct, we should have a material that's appropriate for that use case. In previous OS's, you'll note we had some materials labeled explicitly as light or dark. And, you're going to want to stay away from those, as they're not going to look right across our many new appearances. So, that brings me to another topic, which is accent colors. If we go ahead and look at these UI elements, we can see there's this delightful splash of view, of color, in a lot of these elements. And, in macOS 10.14, we've added a number of new accent colors for users to select. And, all of these look absolutely great. But, if you're making your own UI elements-- I'll pause for applause. Thank you, accent colors. Anyway, if you're making your own UI elements, you might be trying to make this motif yourself, and incorporate that splash of color. So, if you've done that in the past, you've probably been using the NSColor.currentControlTint method, which returns this enumeration saying whether the system's running in aqua or graphite mode. So, we have way more colors than that now. That enumeration's not going to do the job. So, in macOS 10.14, we'd urge you to instead, switch to the controlAccentColor method on NSColor. So, NSColor doesn't stop helping you with accent colors. There's a number of other things it does. If you're making a UI element, one of the common features you're going to want to do is adjusting the color of that UI element to reflect user interaction with it. So, NSColor introduces a new method called .withSystemEffect. And, we've defined a number of system effects for interaction, like the pressed state or the disabled state, and we'll go ahead and apply a recipe to a base color to produce a new color that's appropriate for the current appearance as well as a sort of interaction being done with that control. So, this will save you the trouble of having to develop a formula yourself for modifying a color for these states. And, it'll also save you from cases where you might have a really long list of hard-coded colors for different interactions. So, it's a great API to make use of. We're going to talk about color for a bit more. In this case, a new feature of macOS 10.14, is what we call the content tint color. If you look at my mock application here, you can see it's mostly user content, it's mostly text. But, there's a few elements that I want to call attention to. These are things where the user can click on them to perform more actions. And, I didn't want to use the normal button borders because I felt that, kind of, overwhelmed the content. But, in macOS 10.14, we're going to let you tint borderless buttons and image views to call them out, so the user can still recognize these as clickable and interactable. So, that's really easy to do. NSButton and NSImageView, both have a new property called contentTintColor. You can set it to any color you want, to those dynamic colors I mentioned earlier are great candidates. You can also set these up in Interface Builder. So, this is what the UI looks like for configuring buttons. And, this is what it looks like for configuring image views. The tint option is here on the right, in the sidebar. So, we've covered a lot of great stuff about what you can do with the new appearance in macOS 10.14. We have more sessions on it, but they're in the WWDC app, if you look at our latest sessions. They're both absolutely great. Which brings me to my next topic. No discussion of Cocoa is complete without some talk of layer backing. So, I wanted to let you all know that in macOS 10.14, when you link against the new SDK, AppKit's not going to use a legacy window backing store any more. It's going to provide all of this content to the window server using core animation layers. And, a lot of you who do development on iOS are going to think this sounds really familiar to me. But, let's take a look at what actually goes on here. So, if we have a tree of views like this, in UIKit, the relationship between views and layers is really simple. Every view gets exactly one layer. And, the parent/child relationship between views is mirrored in the layer tree also. But in AppKit, we create the layer tree as a process of-- or as a side effect of processing the view hierarchy. So, we can wind up in cases where we might decide to take many views, and use a single layer for that. And, that's great because it can reduce system memory consumption, and GPU memory consumption, and also gives the window server a little less load to process when it's rendering the screen. Something I really want to point out here, though, is that this is dynamic based on the configuration of the view hierarchy. So, it can change moment to moment. So, you really can't rely on having this fixed parent/child relationship between views and layers like you might on iOS. So, programmatically one of the changes you no longer have to care about here is that you don't have to explicitly set .wantsLayer on your views to use layers anymore. AppKit will take care of this for you when you're deploying against macOS 10.14. If you're deploying against-- In fact, we generally encourage you not to even use this property, because if you set it explicitly to true, we're going to make sure your view gets its own layer, and we're not going to do the optimizations we can do, where we render multiple views into a single layer. You might also need to still use this if you're deploying to earlier OS's, but usually you can still get away with ignoring it. I wanted to talk about some other patterns you might have in NSView UI's you are making that use CALayers. So, one of the easiest ways to draw a CALayer, is to just override the draw method in the CALayer class. Or, implement a delegate method. And, this is mostly fine, but NSView actually gives you a lot of functionality you probably don't want to have to replicate yourself. If you use NSView's draw method, it'll go ahead and take care of things like making sure that the appearance works correctly for dynamic colors. It'll manage the backing store resolution for you. And, it's really just as simple as implementing the layer methods. So, I really encourage you to override drawing at the view level instead of the layer level. Sometimes you'll have cases where you were implementing the display method of CALayer instead, and you're updating layer properties directly, because maybe it's more efficient, or really expresses what you're trying to accomplish better. You can still do that using the NSView API by overriding the update layer method, and you get all the same benefits you do by using the NSView draw rect method. A quirk I want to point out, is you can implement both update layer, and the draw methods on NSView. If you do this, when your view has a single layer backing it, we'll go ahead and use the optimal layer version. And, if you're being merged with other views to save memory, we'll go ahead and use the draw rect version. And, we also use that for things like printing. So, it's fine to implement both of these. If you have a view that you really can't express using the CG drawing API's, or the AppKit drawing API's, you can, in addition to the update layer method, override wantsUpdateLayer, and if you just return "true" from that, we know that you need an explicit layer to do what you want to accomplish. There's another way of taking best advantage of AppKit and core animations features here, and that's just to build your UI's out of a very simple vocabulary of basic NSViews. NSImageView, NSBox, and NSTextField, these are all really great building blocks to make complicated UI's, and they'll do the right thing no matter what technologies we pick to actually render to the screen. With our changes to layer backing, there's a few patterns I want to call out that aren't going to work in macOS 10.14 anymore. If you're using NSView lockFocus and unlockFocus, or trying to access the window's graphics contents directly, there's a better way of doing that. You should just subclass NSView and implement draw rect. Both of those methods have been kind of finicky for a while. So, you'll be saving yourself some trouble. The other thing I want to point out is I've actually written these in Objective-C, which is a little weird for a talk that's mostly in Swift. And, the really great news about this is I've never actually seen any Swift code using these. The takeaway from that is I really don't want any of you to be the first to go ahead and surprise me. So, we have one more thing about our changes with layer backing. If you're using NSOpenGL classes to render with OpenGL, and you link against macOS 10.14, some of our implementation details for how we bind the OpenGL system to our layers are a bit different. And, you may notice a few small changes there. But, more importantly, I want to call out that as of macOS 10.14, OpenGL on our platform is deprecated. If you've been using NSOpenGL view, we really encourage you to adopt MTKView instead. And, there's a great session coming up later today about adopting Metal for OpenGL developers. There's one last change I want to talk about, which is a change we've made to font antialiasing. If you go ahead and look at this screen comparison, I have macOS 10.13 on the left, and macOS 10.14 on the right. And, if you look at the text in this window, it's basically identical. But, if we zoom in, all the way to a 48X scale factor, we can see that macOS 10.13 is using this color-fringing effect for its font rendering. In macOS 10.14, we no longer use that effect. And, this means our text looks great on a much wider variety of panel technologies, as well as scaling modes. So, we have a bunch of other great things to cover. And, at this point I'd like to invite Jesse onstage to go over those. Thanks, Chris. Hi everyone. It's great to see you here today. I have a bunch of topics to cover. And, I'd like to start with the user notifications framework. This has been available in iOS for some time now, and with macOS Mojave, we're bringing it to the Mac. This allows for better control of user notifications. And, it also means that your apps can interact with them the same way that they do on iOS. They should do that using the NSApplication method, registerForRemoteNotifications, as well as the requestAuthorization method on userNotificationCenter. As a part of this work, we're also deprecating some existing user notification-related API's. Specifically, in NSApplication, we're deprecating the remoteNotificationType OptionSet, as well as the registerForRemoteNotifications method and the enabledRemoteNotificationTypes property. We're also deprecating all of NSUserNotification. So, as you rebuild with the new SDK, you should try to update to the user notifications framework. Next, I'd like to talk a little bit about NSToolbar. When you wanted to center an item in the toolbar, you've maybe been tempted to put a flexible space on both sides of your item. And, this works, but it has some drawbacks. Notably, when you add extra items to the toolbar, it'll push your item off-center. So, NSToolbar now exposes a new property, the centeredItemIdentifier. You can set this to the identifier of an item you'd like to remain centered, and NSToolbar will put it there. It should stay there unless other Toolbar items actually force it to be out of place. There's another change here worth noting as well, which is that auto layout is now used to measure toolbar items when the minimum and maximum sizes are not specified. This applies only to apps on the 10.14 SDK, but it means that you can do things like change the size of the button, and the measurement will happen for you. The centeredItemIdentifier behavior is also available through Interface Builder. So, here's the inspector pane for a Toolbar item. You can see there's a new checkbox at the bottom, "Is Centered Item." You can click this instead of setting the property from your code, and so there's no need to fall back to the programmatic API. You can continue to do all your UI work inside Interface Builder. And, speaking of Interface Builder, I can't tell you how excited I am about Interface Builder's new support for editing, NSGridViews. If you're not familiar with gridView, we introduced it a couple of years ago, and it's a layout primitive for rendering your views in a grid-like pattern. This is an example from a keychain access app. And, you can imagine how many little constraints would be necessary to create this layout by hand. You could also build it with stackViews, but NSGridView makes the whole thing much easier, and the new editing support in Interface Builder is just fantastic. Let me show it to you. So, here's some UI from a storyboard file. You can select these controls, and embed them in a grid view. And, once you've done that, you can go through and adjust the padding and the alignment of the cells in order to achieve the layout that you want. The editing UI works a lot like the numbered spreadsheet app. So, you can drag and drop views into cells. You can select cells in rows and columns, and adjust their properties. You can even merge cells, as you see in the bottom two rows here. Here's an example where we select a column. And, this is what the inspector pane looks like, so you can see you can adjust the placement of the cells in that column. You can adjust the leading and trailing padding. If we switch over to the Size Inspector, you can specify an explicit width for the column. Or, if you don't do that, then the column will be sized automatically based on the content. And, one of the other really nice things about this feature is that it's backwards-deployable. GridViews authored in Interface Builder can be used back to macOS 10.13.4, or if you're not using merged cells, you can actually go all the way back to 10.12. So, if you need to deploy your app to an older version of macOS, there's still no reason to wait to use this great new functionality. The next topic I'd like to cover is some changes to NSTextView. First off,there's a few new factory methods. The first one here, fieldEditor configures a textView to act as the fieldEditor for an NSTextField. These all provide a much easier way to configure textViews for common use cases. The latter three provide textViews wrapped in scrollViews. This is by far the most common use case for a textView, but if you have to do additional configuration on the textView, it's important to remember to look at the scrollView's documentView. These are also available through Interface Builder. So, again, there's no need to fall back to the programmatic API here. So, let's see what they look like. Here's a sample window that shows all four. TextViews are sometimes misconfigured when clients need to override the fieldEditor in a textField. And so, using a fieldEditor factory method can help avoid problems there. The next one, scrollableTextView should be used for textViews that are for auxiliary text in popovers and inspector panes. Things like that. And then, the bottom two are for text that's main document content. The one on the left is for rich text; the one on the right is for plain text. You might be wondering at this point what the distinction is, because they all look fairly similar. The main benefit is that you don't need to worry about the system configuration. For example, if the system's in Dark Mode, they begin to look more distinct. The rich text's textView retains its white background, and the plain text turns dark to match the rest of the system for example. So, in general if you use these factory methods, it'll help keep your application consistent with the specifications for the rest of the system. The other change to textView that I'd like to talk about is a new method for modifying the text, PerformValidatedReplacement. The idea behind this method is that it gives you a very easy way to manipulate the text in the textView, and it gives you behavior as if the user had performed the change themselves. So, it performs all the appropriate delegate methods, as you'd expect. But, the really interesting part is that any attributes that are not specified on the input string are automatically filled in using the textView's typingAttributes. So, let me give you an example. Here's a window with some rich text in it, and a little snippet of code that calls performValidatedReplacement to insert the word "Developers" in the middle. If we run this, this is what we get. The word appears and it matches the style of the surrounding text, and we didn't have to specify any attributes. There's a subtlety here to be aware of, though. And, that's because the fallback attributes come from the typingAttributes. So, if you start with some rich text like this, and the insertion point is in the lighter portion at the end, and we run the same code; this is the result. The style attributes come from the lighter portion at the end. So, for this reason, you may find that you need to set the selective range for the range you're about to replace before you call performValidatedReplacement. If you do that, this is the result that you get. So, the next topic I'd like to cover very briefly, is continuity camera. This is another fantastic feature in macOS Mojave. And, if you're just using the standard framework classes like NSTextView, there's nothing special you need to do in order to take advantage of it. So, framework will handle everything for you. But, if you have a more application-specific need for this, it is possible to use it more directly. And then, it's important to understand that it's implemented using the existing services API's. So, all you need to do is tell a framework that your responder class is able to handle image data. And, you can do this by implementing validRequestor. If you want to try this out, I'd encourage you to check out the documentation for validRequestor and some of the related methods. Next, I'd like to talk about custom Quick Actions. You heard a little bit about Quick Actions from the State of the Union session yesterday. And, they make it very easy to perform simple actions like opening a favorite app, for complex ones like filtering files, or invoking scripts. You can build custom Quick Actions using app extensions or action bundles from Automator. They're useful in so many different places that there's a variety of ways to invoke them. But, my favorite by far is the Touch Bar. If you put your Quick Actions in the Touch Bar, it just makes it very easy to get to them, wherever you are, whenever you need them. And, you can get this behavior by looking in the keyboard preferences panel, and reconfiguring your Touch Bar to either always show them, or to flip to them when you hold down the function key. Or, you can customize your Touch Bar and drag the workflows button into your control script. It's also worth noting that you can go over to the Shortcuts pane, and look under Services. And, here you can turn them on and off to control which ones show up. They don't only show up in the Touch Bar, though, as I mentioned. So, here's a Finder window for example. And, the contextual menu has a Quick Actions submenu where you'll see them. Finder's preview pane also has a couple of Quick Actions at the bottom, and then the full list underneath the "More" button. And, action bundles from Automator will show up inside the Services menu. So, TrimLogs is one that I wrote to filter my debug logs, for example. And, that brings me to the next topic I'd like to talk about, building action bundles, also known as contextual workflows. This is a new feature in Automator. When you go to Automator, and create a new document, there's a new option available now for contextual workflows. They look a lot like regular workflows except there's a new block at the top that allows you to configure the input and output as well as pick an icon and the color. So, let's go though a quick example. I often have a problem where there's some file I want to open in TextEdit, but I can't because it doesn't have a file extension. This is super easy to fix with Automator. All you need to do is look inside the library, and drag out the Open Finder Items action. You can configure it to open the items with TextEdit instead of with a default application. Any file selected in the Finder automatically become the input to this action, and then if you save it with some name, it will automatically show up in the Touch Bar, or in other contexts where it's useful. So, to summarize, we've talked about a variety of new features, and other changes that will make your development experience richer, and your applications even more awesome. Check out the new SDK and begin implementing some of these things in your applications. They'll make your applications shine, and your customers will appreciate it. For more information, you can follow this URL. And, also look inside the WWDC app under this session. All of the related sessions are linked there. Thank you very much. 